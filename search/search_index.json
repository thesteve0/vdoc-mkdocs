{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FiftyOne \u00b6","text":"<p>The open-source tool for building high-quality datasets and computer vision models</p> <p>Nothing hinders the success of machine learning systems more than poor quality data. And without the right tools, improving a model can be time-consuming and inefficient.</p> <p>FiftyOne supercharges your machine learning workflows by enabling you to visualize datasets and interpret models faster and more effectively.</p> <p>Improving data quality and understanding your model\u2019s failure modes are the most impactful ways to boost the performance of your model.</p> <p>FiftyOne provides the building blocks for optimizing your dataset analysis pipeline. Use it to get hands-on with your data, including visualizing complex labels, evaluating your models, exploring scenarios of interest, identifying failure modes, finding annotation mistakes, and much more!</p> <p>Install FiftyOne!</p> <p>FiftyOne integrates naturally with your favorite tools. Click on a logo to learn how:</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Note</p> <p>FiftyOne is growing! Sign up for the mailing list to learn about new features as they come out.</p>"},{"location":"#core-capabilities","title":"Core Capabilities \u00b6","text":""},{"location":"#curating-datasets","title":"Curating datasets","text":"<p>Surveys show that machine learning engineers spend over half of their time wrangling data, but it doesn't have to be that way. Use FiftyOne's powerful dataset import and manipulation capabilities to manage your data with ease.</p> <p>Learn how to load data into FiftyOne</p> <p></p>"},{"location":"#evaluating-models","title":"Evaluating models","text":"<p>Aggregate metrics alone don\u2019t give you the full picture of your ML models. In practice, the limiting factor on your model\u2019s performance is often data quality issues that you need to see to address. FiftyOne makes it easy to do just that.</p> <p>See how to evaluate models with FiftyOne</p> <p></p>"},{"location":"#visualizing-embeddings","title":"Visualizing embeddings","text":"<p>Are you using embeddings to analyze your data and models? Use FiftyOne's embeddings visualization capabilities to reveal hidden structure in you data, mine hard samples, pre-annotate data, recommend new samples for annotation, and more.</p> <p>Experience the power of embeddings</p> <p></p>"},{"location":"#working-with-geolocation","title":"Working with geolocation","text":"<p>Many datasets have location metadata, but visualizing location-based datasets has traditionally required closed source or cloud-based tools. FiftyOne provides native support for storing, visualizing, and querying datasets by location.</p> <p>Visualize your location data</p> <p></p>"},{"location":"#finding-annotation-mistakes","title":"Finding annotation mistakes","text":"<p>Annotations mistakes create an artificial ceiling on the performance of your model. However, finding these mistakes by hand is not feasible! Use FiftyOne to automatically identify possible label mistakes in your datasets.</p> <p>Check out the label mistakes tutorial</p> <p></p>"},{"location":"#removing-redundant-images","title":"Removing redundant images","text":"<p>During model training, the best results will be seen when training on unique data. Use FiftyOne to automatically remove duplicate or near-duplicate images from your datasets and curate diverse training datasets from your raw data.</p> <p>Try the image uniqueness tutorial</p> <p></p>"},{"location":"#core-concepts","title":"Core Concepts \u00b6","text":""},{"location":"#fiftyone-library","title":"FiftyOne Library \u00b6","text":"<p>FiftyOne\u2019s core library provides a structured yet dynamic representation to explore your datasets. You can efficiently query and manipulate your dataset by adding custom tags, model predictions and more.</p> <p>Explore the library</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset(\"my_dataset\")\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\nsample.tags.append(\"train\")\nsample[\"custom_field\"] = 51\n\ndataset.add_sample(sample)\n\nview = dataset.match_tags(\"train\").sort_by(\"custom_field\").limit(10)\n\nfor sample in view:\n    print(sample)\n</code></pre> <p>Note</p> <p>FiftyOne is designed to be lightweight and flexible, making it easy to load your datasets. FiftyOne supports loading datasets in a variety of common formats out-of-the-box, and it also provides the extensibility to load datasets in custom formats.</p> <p>Check out loading datasets to see how to load your data into FiftyOne.</p>"},{"location":"#fiftyone-app","title":"FiftyOne App \u00b6","text":"<p>The FiftyOne App is a graphical user interface that makes it easy to explore and rapidly gain intuition into your datasets. You can visualize labels like bounding boxes and segmentations overlaid on the samples; sort, query and slice your dataset into any subset of interest; and more.</p> <p>See more of the App</p> <p></p>"},{"location":"#fiftyone-brain","title":"FiftyOne Brain \u00b6","text":"<p>The FiftyOne Brain is a library of powerful machine learning-powered capabilities that provide insights into your datasets and recommend ways to modify your datasets that will lead to measurably better performance of your models.</p> <p>Learn more about the Brain</p> <pre><code>import fiftyone.brain as fob\n\nfob.compute_uniqueness(dataset)\nrank_view = dataset.sort_by(\"uniqueness\")\n</code></pre>"},{"location":"#fiftyone-plugins","title":"FiftyOne Plugins \u00b6","text":"<p>FiftyOne provides a powerful plugin framework that allows for extending and customizing the functionality of the tool to suit your specific needs.</p> <p>With plugins, you can add new functionality to the FiftyOne App, create integrations with other tools and APIs, render custom panels, and add custom buttons to menus.</p> <p>With FiftyOne Teams, you can even write plugins that allow users to execute long-running tasks from within the App that run on a connected compute cluster.</p> <p>Install some plugins!</p> <p></p>"},{"location":"#dataset-zoo","title":"Dataset Zoo \u00b6","text":"<p>The FiftyOne Dataset Zoo provides a powerful interface for downloading datasets and loading them into FiftyOne.</p> <p>It provides native access to dozens of popular benchmark datasets, and it als supports downloading arbitrary public or private datasets whose download/preparation methods are provided via GitHub repositories or URLs.</p> <p>Check out the Dataset Zoo</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p></p>"},{"location":"#model-zoo","title":"Model Zoo \u00b6","text":"<p>The FiftyOne Model Zoo provides a powerful interface for downloading models and applying them to your FiftyOne datasets.</p> <p>It provides native access to hundreds of pre-trained models, and it also supports downloading arbitrary public or private models whose definitions are provided via GitHub repositories or URLs.</p> <p>Check out the Model Zoo</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\n    \"clip-vit-base32-torch\",\n    text_prompt=\"A photo of a\",\n    classes=[\"person\", \"dog\", \"cat\", \"bird\", \"car\", \"tree\", \"chair\"],\n)\n\ndataset.apply_model(model, label_field=\"zero_shot_predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"#whats-next","title":"What\u2019s Next? \u00b6","text":"<p>Where should you go from here? You could\u2026</p> <ul> <li> <p>Install FiftyOne</p> </li> <li> <p>Try one of the tutorials that demonstrate the unique capabilities of FiftyOne</p> </li> <li> <p>Explore recipes for integrating FiftyOne into your current ML workflows</p> </li> <li> <p>Check out the cheat sheets for topics you may want to master quickly</p> </li> <li> <p>Consult the user guide for detailed instructions on how to accomplish various tasks with FiftyOne</p> </li> </ul>"},{"location":"#need-support","title":"Need Support? \u00b6","text":"<p>If you run into any issues with FiftyOne or have any burning questions, feel free to connect with us on Discord or reach out to us at support@voxel51.com.</p>"},{"location":"community/","title":"Come join the best community in THE WORLD","text":""},{"location":"release-notes/","title":"FiftyOne Release Notes \u00b6","text":""},{"location":"release-notes/#fiftyone-teams-230","title":"FiftyOne Teams 2.3.0 \u00b6","text":"<p>Released December 20, 2024</p> <p>Includes all updates from FiftyOne 1.2.0, plus:</p> <ul> <li> <p>Added an example Databricks connector showing how to connect FiftyOne Teams to your lakehouse via Data Lens</p> </li> <li> <p>Added a Data Lens connector that demonstrates how to allow users to dynamically configure the field(s) that are imported</p> </li> <li> <p>Data Lens now supports previewing 3D data imports</p> </li> <li> <p>Guest users can now open Data Lens</p> </li> <li> <p>When scanning for issues with the Data Quality Panel, any fields created are now added to a <code>DATA QUALITY</code> sidebar group</p> </li> <li> <p>Prevented unnecessary scrollbars from appearing when using the Data Quality Panel</p> </li> <li> <p>AWS session tokens are now supported when configuring cloud credentials</p> </li> <li> <p>Fixed a bug that could cause <code>StopIteration</code> errors when performing long-running operations like computing embeddings when using API connections</p> </li> </ul>"},{"location":"release-notes/#fiftyone-120","title":"FiftyOne 1.2.0 \u00b6","text":"<p>Released December 20, 2024</p> <p>App</p> <ul> <li> <p>Added support for instance segmentations whose masks are stored on-disk #5120, #5256</p> </li> <li> <p>Optimized overlay rendering for dense label fields like segmentations and heatmaps #5156, #5169, #5247</p> </li> <li> <p>Improved stability of frame rendering for videos #5199, #5293</p> </li> <li> <p>Sidebar groups that contain only list fields are no longer collapsed by default #5280</p> </li> <li> <p>The Model Evaluation panel now filters both ground truth and prediction fields when you perform interactive filters via the TP/FP/FN icons, per-class histograms, and confusion matrices #5268</p> </li> <li> <p>When comparing two models in the Model Evaluation panel, interactive filters now apply to both evaluation runs #5268</p> </li> <li> <p>The Model Evaluation panel now supports evaluations that were performed on subsets (views) of the full dataset #5267</p> </li> <li> <p>The Model Evaluation panel now shows mask targets for segmentation evaluations when they are available #5281</p> </li> <li> <p>The Model Evaluation panel now hides metrics that aren\u2019t applicable to a given evaluation type #5281</p> </li> <li> <p>Fixed an issue where backtick can\u2019t be typed when editing markdown notes in the Model Evaluation panel #5233</p> </li> <li> <p>Fixed a race condition that could cause errors when performing text similarity searches #5273</p> </li> <li> <p>Fixed a caching bug that prevented label overlay font sizes from dynamically resizing as expected in some cases #5287</p> </li> <li> <p>Fixed a bug that excluded selected samples from the counter above the Samples panel #5286</p> </li> </ul> <p>SDK</p> <ul> <li>Optimized <code>dataset.first()</code> calls #5305</li> </ul> <p>Brain</p> <ul> <li>Upgraded the MongoDB vector search integration to use the <code>vectorSearch</code> type #218</li> </ul> <p>Zoo</p> <ul> <li>Fixed a bug with loading the rtdetr-l-coco-torch and rtdetr-x-coco-torch zoo models #5220</li> </ul>"},{"location":"release-notes/#fiftyone-teams-220","title":"FiftyOne Teams 2.2.0 \u00b6","text":"<p>Released December 6, 2024</p> <p>Includes all updates from FiftyOne 1.1.0, plus:</p> <ul> <li> <p>All Teams deployments now have builtin compute capacity for executing delegated operations in the background while you work in the App</p> </li> <li> <p>Introduced Data Lens, which allows you to explore and import samples from external data sources into FiftyOne</p> </li> <li> <p>Added a Data Quality Panel that automatically scans your data for quality issues and helps you take action to resolve them</p> </li> <li> <p>Added a Query Performance Panel that helps you create the necessary indexes to optimize queries on large datasets</p> </li> <li> <p>Added support for creating embeddings visualizations natively from the Embeddings panel</p> </li> <li> <p>Added support for evaluating models natively from the Model Evaluation panel</p> </li> <li> <p>Added support for configuring an SMTP server for sending user invitiations via email when running in Internal Mode</p> </li> </ul>"},{"location":"release-notes/#fiftyone-110","title":"FiftyOne 1.1.0 \u00b6","text":"<p>Released December 6, 2024</p> <p>What\u2019s New</p> <ul> <li> <p>Added a Model Evaluation panel for visually and interactively evaluating models in the FiftyOne App</p> </li> <li> <p>Introduced Query Performance in the App, which automatically nudges you to create the necessary indexes to greatly optimize queries on large datasets</p> </li> <li> <p>Added a leaky splits method for automatically detecting near-duplicate samples in different splits of your datasets</p> </li> <li> <p>Added a near duplicates method that scans your datasets and detects potential duplicate samples</p> </li> </ul> <p>App</p> <ul> <li> <p>Added zoom-to-crop and set-look-at for selected labels in the 3D visualizer #4931</p> </li> <li> <p>Gracefully handle deleted + recreated datasets of the same name #5183</p> </li> <li> <p>Added a <code>referrerPolicy</code> so the App can run behind reverse proxies #4944</p> </li> <li> <p>Fixed a bug that prevented video playback from working for videos with unknown frame rate #5155</p> </li> </ul> <p>SDK</p> <ul> <li> <p>Added <code>min()</code> and <code>max()</code> and aggregations #5029</p> </li> <li> <p>Optimized object detection evaluation with r-trees #4758</p> </li> <li> <p>Improved support for creating summary fields and indexes #5091</p> </li> <li> <p>Added support for creating compound indexes when using the builtin <code>create_index</code> operator that optimize sidebar queries for group datasets #5174</p> </li> <li> <p>The builtin <code>clear_sample_field</code> and <code>clear_frame_field</code> operators now support clearing fields of views, in addition to full datasets #5122</p> </li> <li> <p>Fixed a bug that prevented users with <code>pydantic</code> installed from loading the quickstart-3d dataset from the zoo #4994</p> </li> <li> <p>Added optional <code>email</code> parameter to the CVAT integration #5218</p> </li> </ul> <p>Brain</p> <ul> <li> <p>Added support for passing existing similarity indexes to <code>compute_visualization()</code>, <code>compute_uniqueness()</code>, and <code>compute_representativeness()</code> #201, #204</p> </li> <li> <p>Upgraded the Pinecone integration to support <code>pinecone-client&gt;=3.2</code> #202</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>Added an Execution Store that provides a key-value interface for persisting data beyond the lifetime of a panel #4827, #5144</p> </li> <li> <p>Added <code>ctx.spaces</code> and <code>set_spaces()</code> to the operator execution context #4902</p> </li> <li> <p>Added <code>open_sample()</code> and <code>close_sample()</code> methods for programmatically controlling what sample(s) are displayed in the App\u2019s sample modal #5168</p> </li> <li> <p>Added a <code>skip_prompt</code> option to <code>ctx.prompt</code>, allowing users to bypass prompts during operation execution #4992</p> </li> <li> <p>Introduced a new <code>StatusButtonView</code> type for rendering buttons with status indicators #5105</p> </li> <li> <p>Added support for giving <code>ImageView</code> components click targets #4996</p> </li> <li> <p>Added an allow_legacy_orchestrators config flag to enable running delegated operations locally #5176</p> </li> <li> <p>Fixed a bug when running delegated operations programmatically #5180</p> </li> <li> <p>Fixed a bug when running delegated operations with output schemas on MongoDB &lt;v5 #5181</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-213","title":"FiftyOne Teams 2.1.3 \u00b6","text":"<p>Released November 8, 2024</p> <p>Includes all updates from FiftyOne 1.0.2.</p>"},{"location":"release-notes/#fiftyone-102","title":"FiftyOne 1.0.2 \u00b6","text":"<p>Released November 8, 2024</p> <p>Zoo</p> <ul> <li> <p>Added SAM 2.1 to the Model Zoo #4979</p> </li> <li> <p>Added YOLO11 to the Model Zoo #4899</p> </li> <li> <p>Added generic model architecture and backbone tags to all relevant models in the zoo for easier navigation #4899</p> </li> </ul> <p>Core</p> <ul> <li> <p>Fixed input shape in the depth estimation transformer #5035</p> </li> <li> <p>Added graceful handling of empty datasets when computing embeddings #5043</p> </li> </ul> <p>App</p> <ul> <li> <p>Added a new <code>TimelineView</code> for building custom animations #4965</p> </li> <li> <p>Fixed overlay z-index and overflow for panels #4956</p> </li> <li> <p>Fixed bug where timeline name wasn\u2019t being forwarded in seek utils #4975</p> </li> <li> <p>Performance improvements in the grid and modal #5009, #5015, #5018, #5019, #5022</p> </li> <li> <p>Fixed batch selection with ctrl + click in the grid #5046</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-212","title":"FiftyOne Teams 2.1.2 \u00b6","text":"<p>Released October 31, 2024</p> <ul> <li>Fixed an issue that prevented <code>delegation_target</code> from being properly set when running delegated operations with orchestrator registration enabled</li> </ul>"},{"location":"release-notes/#fiftyone-teams-211","title":"FiftyOne Teams 2.1.1 \u00b6","text":"<p>Released October 14, 2024</p> <p>Includes all updates from FiftyOne 1.0.1, plus:</p> <ul> <li> <p>Fixed an issue with Auth0 connections for deployments behind proxies</p> </li> <li> <p>Bumped dependency requirement <code>voxel51-eta&gt;=0.13</code></p> </li> </ul>"},{"location":"release-notes/#fiftyone-101","title":"FiftyOne 1.0.1 \u00b6","text":"<p>Released October 14, 2024</p> <p>App</p> <ul> <li> <p>Video playback now supports the timeline API #4878</p> </li> <li> <p>Added utils to support a rerun panel #4876</p> </li> <li> <p>Fixed a bug that prevented <code>Classifications</code> labels from rendering #4891</p> </li> <li> <p>Fixed a bug that prevented the <code>fiftyone quickstart</code> and <code>fiftyone app launch</code> commands from launching the App #4888</p> </li> </ul> <p>Core</p> <ul> <li> <p>COCO exports now use 1-based categories #4884</p> </li> <li> <p>Fixed a bug when passing the <code>classes</code> argument to load specific classes in COCO format #4884</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-210","title":"FiftyOne Teams 2.1.0 \u00b6","text":"<p>Released October 1, 2024</p> <p>Includes all updates from FiftyOne 1.0.0, plus:</p> <ul> <li> <p>Super admins can now migrate their deployments to Internal Mode via the Super Admin UI</p> </li> <li> <p>Added support for sending user invitations in Internal Mode</p> </li> <li> <p>Optimized performance of the dataset page</p> </li> <li> <p>Fixed a BSON serialization bug that could cause errors when cloning or exporting certain dataset views from the Teams UI</p> </li> </ul>"},{"location":"release-notes/#fiftyone-100","title":"FiftyOne 1.0.0 \u00b6","text":"<p>Released October 1, 2024</p> <p>What\u2019s New</p> <ul> <li> <p>The FiftyOne Brain is now fully open source. Contributions are welcome!</p> </li> <li> <p>Added Modal Panels, bringing the ability to develop and use panels in the App\u2019s sample modal #4625</p> </li> <li> <p>All datasets now have automatically populated <code>created_at</code> and <code>last_modified_at</code> fields on their samples and frames #4597</p> </li> <li> <p>Added support for loading remotely-sourced zoo datasets whose download/preparation instructions are stored in GitHub or public URLs #4752</p> </li> <li> <p>Added support for loading remotely-sourced zoo models whose definitions are stored in GitHub or public URLs #4786</p> </li> <li> <p>Added Med-SAM2 to the model zoo! #4733, #4828</p> </li> </ul> <p>App</p> <ul> <li> <p>Added dozens of builtin operators for performing common operations directly from the App #4830</p> </li> <li> <p>Label overlays in the grid are now scaled proportionally to grid zoom #4747</p> </li> <li> <p>Improved support for visualizing and filtering <code>DynamicEmbeddedDocument</code> list fields #4833</p> </li> <li> <p>Added a new timeline API for synchronizing playback of multiple modal panels #4772</p> </li> <li> <p>Improved UI, documentation, and robustness when working with custom color schemes #4763</p> </li> <li> <p>Fixed a bug where the active group slice was not being persisted when navigating between groups in the modal #4836</p> </li> <li> <p>Fixed a bug when selecting samples in grouped datasets in the modal #4789</p> </li> <li> <p>Fixed heatmaps rendering for values outside of the <code>range</code> attribute #4865</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for creating summary fields to optimize queries on large datasets with many objects #4765</p> </li> <li> <p>Dataset fields now have automatically populated <code>created_at</code> attributes #4730</p> </li> <li> <p>Upgraded the <code>delete_samples()</code> and <code>clear_frames()</code> methods to support bulk deletions of 100k+ samples/frames #4787</p> </li> <li> <p>The <code>default_sidebar_groups()</code> method now correctly handles datetime fields #4815</p> </li> <li> <p>Fixed an off-by-one error when converting semantic segmentations to/from instance segmentations #4826</p> </li> <li> <p>Protect against infinitely growing content size batchers #4806</p> </li> <li> <p>Removed the deprecated <code>remove_sample()</code> and <code>remove_samples()</code> methods from the <code>Dataset</code> class #4832</p> </li> <li> <p>Deprecated Python 3.8 support</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>Added <code>ctx.group_slice</code> to the operator execution context #4850</p> </li> <li> <p>Added <code>set_group_slice()</code> to the operator execution context #4844</p> </li> <li> <p>Improved styling for <code>GridView</code> components #4764</p> </li> <li> <p>A loading error is now displayed in the actions row when operators with placements fail to load #4714</p> </li> <li> <p>Ensure the App loads when plugins fail to load #4769</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0252","title":"FiftyOne 0.25.2 \u00b6","text":"<p>Released September 19, 2024</p> <ul> <li> <p>Require <code>pymongo&lt;4.9</code> to fix database connections</p> </li> <li> <p>Require <code>pydicom&lt;3</code> for DICOM datasets</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-201","title":"FiftyOne Teams 2.0.1 \u00b6","text":"<p>Released September 6, 2024</p> <p>Includes all updates from FiftyOne 0.25.1, plus:</p> <ul> <li> <p>Optimized the <code>Manage &gt; Access</code> page for datasets</p> </li> <li> <p>Added support for configuring a deployment to allow Guests to run custom plugins</p> </li> <li> <p>Fixed a bug where dataset permissions assigned to groups were not correctly applied to users that do not otherwise have access to the dataset</p> </li> <li> <p>Fixed a bug where a deployment\u2019s default user role as configured on the <code>Security &gt; Config</code> page would not be respected</p> </li> <li> <p>Fixed a bug that could cause 3D scenes stored in Azure to fail to load</p> </li> <li> <p>Fixed a bug that erroneously caused the currently selected samples to be cleared when navigating between samples or closing the sample modal</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0251","title":"FiftyOne 0.25.1 \u00b6","text":"<p>Released September 6, 2024</p> <p>App</p> <ul> <li> <p>Fixed an issue with sidebar state persistence when opening and closing the sample modal #4745</p> </li> <li> <p>Fixed a bug with sample selection in the Map panel when the grid is reset #4739</p> </li> <li> <p>Fixed a bug when filtering <code>Keypoint</code> fields using the App sidebar #4735</p> </li> <li> <p>Fixed a bug when tagging in the sample modal with active sidebar filters #4723</p> </li> <li> <p>Disabled <code>fiftyone-desktop</code> builds until package size can be optimized #4746</p> </li> </ul> <p>SDK</p> <ul> <li> <p>Added support for loading lists of TXT files in YOLOv5 format #4742</p> </li> <li> <p>Fixed a bug with the <code>match_expr</code> argument of <code>group_by()</code> #4754</p> </li> <li> <p>Fixed a regression when running inference with Ultralytics models that don\u2019t support track IDs #4720</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>Fixed a bug that caused <code>TabsView</code> components to erroneously reset to their default state #4732</p> </li> <li> <p>Fixed a bug where calling <code>set_state()</code> and <code>set_data()</code> to patch state/data would inadvertently clobber other existing values #4753</p> </li> <li> <p>Fixed a spurious warning that would appear for delegated operations that don\u2019t return outputs #4715</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-200","title":"FiftyOne Teams 2.0.0 \u00b6","text":"<p>Released August 20, 2024</p> <p>Includes all updates from FiftyOne 0.25.0, plus:</p> <p>What\u2019s New</p> <ul> <li> <p>Added a Can tag permission to allow users to tag samples/labels but not otherwise perform edits</p> </li> <li> <p>Added support for authorized user credentials and external account credentials when configuring GCP credentials</p> </li> <li> <p>All plugin execution is now user-aware and will respect the executing user\u2019s role and dataset permissions</p> </li> <li> <p>All deployments now include a LICENSE file that enforces user quotas</p> </li> <li> <p>Guests can no longer access operators/panels in custom plugins</p> </li> </ul> <p>App</p> <ul> <li> <p>Added a caching layer to optimize media serving in the App</p> </li> <li> <p>Cloning an entire dataset via the <code>Clone</code> button now includes saved views, saved workspaces, and runs</p> </li> <li> <p>Optimized the performance and UX of the <code>Settings &gt; Users</code> page</p> </li> <li> <p>The users table on the <code>Settings &gt; Users</code> page is now sortable</p> </li> <li> <p>Fixed a bug when updating the user role of a pending invitation</p> </li> <li> <p>Fixed a bug that prevented the Recent views widget from showing all recently loaded views as intended</p> </li> </ul> <p>CAS</p> <ul> <li> <p>Added an <code>Audit</code> page to the Super Admin UI that shows current license utilization and RBAC settings</p> </li> <li> <p>Super admins can now disable manual group management in the App. This is useful, for example, if groups are defined via hooks</p> </li> <li> <p>Legacy mode deployments now have access to the relevant pages of the Super Admin UI</p> </li> </ul> <p>SDK</p> <ul> <li> <p>Added a <code>user_groups</code> module to the Management SDK for programmatically managing user groups</p> </li> <li> <p>The <code>fiftyone delegated</code> CLI command is now available to Teams users</p> </li> <li> <p>Upgraded the upload_media() function to gracefully support fields with missing media paths</p> </li> <li> <p>Added an <code>overwrite</code> parameter to <code>add_cloud_credentials()</code> to control whether existing cloud credentials with the same prefix for a provider are overwritten</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0250","title":"FiftyOne 0.25.0 \u00b6","text":"<p>Released August 20, 2024</p> <p>What\u2019s New</p> <ul> <li> <p>Introducing Python panels, a powerful framework for building custom App panels via a simple Python interface that includes a wealth of builtin components to convey information, create tutorials, show interactive graphs, trigger operations, and more</p> </li> <li> <p>Released a Dashboard panel that allows users to build custom no-code dashboards that display statistics of interest about the current dataset (and beyond)</p> </li> <li> <p>Added Segment Anything 2 to the model zoo! #4671</p> </li> <li> <p>Added an Elasticsearch integration for native text and image searches on FiftyOne datasets!</p> </li> <li> <p>Added an image representativeness method to the Brain that can be used to find the most common/uncommon types of images in your datasets</p> </li> </ul> <p>App</p> <ul> <li> <p>You can now link directly to a sample or group in the App by copy + pasting URLs into your browser bar or programmatically via your App <code>session</code> #4281</p> </li> <li> <p>Added a config option to disable frame filtering in the App globally or on specific datasets #4604</p> </li> <li> <p>Added support for dynamically adjusting 3D label linewidths #4590</p> </li> <li> <p>Added a status bar when loading large 3D assets in the modal #4546</p> </li> <li> <p>Added support for visualizing heatmaps in <code>.jpg</code> format #4531</p> </li> <li> <p>Exposed camera position as a recoil atom #4535</p> </li> <li> <p>Added anonymous analytics collection on an opt-in basis #4559</p> </li> <li> <p>Fixed a bug when viewing dynamic groups of 3D scenes in the modal #4527</p> </li> <li> <p>Fixed a bug when rendering scenes with relative 3D asset paths on Windows #4579</p> </li> <li> <p>Fixed keyboard shortcuts when viewing dynamic groups in the modal #4510</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Added support for annotating frame views #4477</p> </li> <li> <p>Added support for annotating clip views #4511</p> </li> <li> <p>Added support for preserving existing COCO IDs when exporting in COCO format #4530</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for save contexts to generated views (patches, frames, and clips) #4636</p> </li> <li> <p>Added support for downloading plugins from branches that contain slashes <code>/</code> #4614</p> </li> <li> <p>Added support for including index statistics in <code>Dataset.stats()</code> #4653</p> </li> <li> <p>Added a source install script for Windows #4582</p> </li> <li> <p>Ubuntu 24.04 users no longer have to manually install MongoDB #4533</p> </li> <li> <p>Removed Python 3.7 support and marked Python 3.8 as deprecated #4538</p> </li> <li> <p>Fixed a bug that could cause side effects when creating clip views defined by expressions #4492</p> </li> <li> <p>Fixed a concatenation bug when downloading videos from CVAT #4674</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>The actions row now automatically overflows into a <code>More items</code> menu as necessary when there is insufficient horizontal space #4595</p> </li> <li> <p>Added a <code>set_active_fields()</code> operator for programmatically controlling the selected fields in the sidebar #4482</p> </li> <li> <p>Added a <code>notify()</code> operator for programmatically showing notifications in the App #4344</p> </li> <li> <p>Added <code>ctx.extended_selection</code> to retrieve the current extended selection #4413</p> </li> <li> <p>Added a <code>set_extended_selection()</code> operator for programmatically setting the extended selection #4409</p> </li> <li> <p>Added a <code>track_event()</code> operator for logging plugin events in the App #4489</p> </li> </ul> <p>Zoo</p> <ul> <li> <p>Added YOLOv10 and RT-DETR models to the zoo #4544</p> </li> <li> <p>Added YOLOv8 classification models to the zoo #4549</p> </li> <li> <p>Added support for storing object track IDs if present when running Ultralytics models from the zoo #4569</p> </li> <li> <p>Added support for GPU inference when running Hugging Face Transformers models from the zoo #4587</p> </li> <li> <p>Extended support for group datasets, masks, heatmaps, and thumbnails when uploading FiftyOne datasets to Hugging Face Hub #4566</p> </li> <li> <p>Allow <code>ragged_batches</code> to be configured when using Torch models with custom transforms #4509, #4512</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-171","title":"FiftyOne Teams 1.7.1 \u00b6","text":"<p>Released June 11, 2024</p> <p>Includes all updates from FiftyOne 0.24.1, plus:</p> <ul> <li> <p>Improved stability of loading/navigating to saved views in the App</p> </li> <li> <p>Fixed a notification error when deleting users from the Team Settings page</p> </li> <li> <p>Improved stability of the Team Groups page after deleting users</p> </li> <li> <p>Optimized export of cloud-backed 3D scenes</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0241","title":"FiftyOne 0.24.1 \u00b6","text":"<p>Released June 11, 2024</p> <p>What\u2019s New</p> <ul> <li>Added Ultralytics YOLOv8 models trained on Open Images v7 to the model zoo! #4398</li> </ul> <p>App</p> <ul> <li>Fixed a regression from FiftyOne 0.24.0 that would prevent operator outputs and error states from displaying in the App #4445</li> </ul> <p>Core</p> <ul> <li> <p>Optimized metadata computation for 3D scenes #4442</p> </li> <li> <p>Fixed a bug that could cause 3D assets to be omitted when exporting 3D scenes #4442</p> </li> </ul> <p>Utils</p> <ul> <li>The <code>make_patches_dataset()</code>, <code>make_frames_dataset()</code>, and <code>make_clips_dataset()</code> utilities can now be directly called #4416</li> </ul> <p>Annotation</p> <ul> <li>Added support loading annotations for large CVAT tasks with many jobs #4392</li> </ul>"},{"location":"release-notes/#fiftyone-teams-170","title":"FiftyOne Teams 1.7.0 \u00b6","text":"<p>Released May 29, 2024</p> <p>Includes all updates from FiftyOne 0.24.0, plus:</p> <ul> <li> <p>Added a Roles page that summarizes the actions and permissions available to each user role</p> </li> <li> <p>Added support for customizing the role that a user will have when sending an invitation for a new user to access a specific dataset</p> </li> <li> <p>Added the ability to configure the expiration time for signed URLs used by your FiftyOne Teams deployment</p> </li> <li> <p>Fixed a regression from FiftyOne Teams 1.6 that could cause login errors when accepting invites</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0240","title":"FiftyOne 0.24.0 \u00b6","text":"<p>Released May 29, 2024</p> <p>What\u2019s New</p> <ul> <li> <p>Added support for 3D meshes and 3D geometries! #3985</p> </li> <li> <p>Added a quickstart-3d dataset to the zoo! #4406</p> </li> <li> <p>Added support for saving custom workspaces! #4205, #4211</p> </li> <li> <p>You can now scroll/customize the content displayed in the App tooltip! #4254</p> </li> <li> <p>FiftyOne now lazily connects to the database only when needed #4236</p> </li> <li> <p>Added Grounding DINO as an option for zero shot object detection #4292</p> </li> <li> <p>Added a new anomaly detection tutorial #4312</p> </li> </ul> <p>App</p> <ul> <li> <p>Added a <code>media_fallback</code> option to the dataset App config #4280</p> </li> <li> <p><code>launch_app()</code> now respects the current <code>group_slice</code> when loading grouped datasets #4423</p> </li> <li> <p>Allow sidebar changes during lightning loading states #4319</p> </li> <li> <p>Fixed overlay processing for empty label lists #4345</p> </li> <li> <p>Fixed <code>support</code> filtering in the sample modal for <code>TemporalDetections</code> fields #4346</p> </li> <li> <p>Fixed a regression from FiftyOne 0.23.8 when viewing dynamically grouped views into group datasets #4299</p> </li> </ul> <p>Core</p> <ul> <li> <p>Gracefully handle None-valued <code>tags</code> fields #4351</p> </li> <li> <p>More robust path normalization when importing FiftyOneDataset exports from other operating systems #4353</p> </li> <li> <p>Fixed possible concurrency bugs when updating/deleting runs #4323</p> </li> <li> <p>Fixed possible concurrency bugs when updating views, workspaces, and group slices #4350</p> </li> <li> <p>Fixed a timezone bug with <code>DateField</code> for GMT+ users #4371</p> </li> </ul> <p>Utils</p> <ul> <li> <p>Added support for non-sequential category IDs when importing/exporting data in COCO format #4354, #4309</p> </li> <li> <p>Added a <code>DeepSort</code> tracking utility #4372, #4296</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>Added a <code>DrawerView</code> option to render your operators as a side drawer in the grid/sample visualizer rather than as a modal #4240</p> </li> <li> <p>Added a <code>set_spaces()</code> method for setting the current spaces layout from operators #4381</p> </li> <li> <p>Added support for numpy dtypes when serializing operator results #4324</p> </li> <li> <p>Fixed a bug where recently used operators may not appear first in the Operator browser #4287</p> </li> <li> <p>Fixed logging syntax in the builtin <code>set_progress()</code> operation #4417</p> </li> </ul> <p>Zoo</p> <ul> <li>Fixed a bug with YOLO-NAS inference #4429</li> </ul>"},{"location":"release-notes/#fiftyone-teams-161","title":"FiftyOne Teams 1.6.1 \u00b6","text":"<p>Released May 10, 2024</p> <p>Bugs</p> <ul> <li>Fixed an issue with logging into FiftyOne Teams in Enterprise Proxy enviornments</li> </ul>"},{"location":"release-notes/#fiftyone-teams-160","title":"FiftyOne Teams 1.6.0 \u00b6","text":"<p>Released April 30, 2024</p> <p>What\u2019s New</p> <ul> <li> <p>Added Groups for managing and dataset access for groups of users</p> </li> <li> <p>Introduced a new Pluggable Authentication system for customizing FiftyOne Teams authentication</p> </li> <li> <p>Removed Auth0 as a hard dependency for Teams deployments with the introduction of Internal Mode</p> </li> <li> <p>Added support for directly authenticating with Identity Providers</p> </li> <li> <p>Added a Super Admin UI for administering FiftyOne Teams deployments</p> </li> <li> <p>Added the ability to search for users on the Users page</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-1510","title":"FiftyOne Teams 1.5.10 \u00b6","text":"<p>Released April 18, 2024</p> <ul> <li>Fixed an issue where video datasets were not loading due to ffmpeg dependency</li> </ul>"},{"location":"release-notes/#fiftyone-teams-159","title":"FiftyOne Teams 1.5.9 \u00b6","text":"<p>Released April 15, 2024</p> <p>Includes all updates from FiftyOne 0.23.8, plus:</p> <ul> <li> <p>Download contexts now support batching based on content size</p> </li> <li> <p>All builtin methods that require access to cloud media now use download contexts to download media in batches during execution rather than downloading media in a single batch up-front</p> </li> <li> <p>The <code>export()</code> method no longer caches all cloud media involved in the export</p> </li> <li> <p>Optimized the localhost App experience when using API connections</p> </li> <li> <p>Optimized performance of data-intensive API calls when using API connections</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0238","title":"FiftyOne 0.23.8 \u00b6","text":"<p>Released April 15, 2024</p> <p>News</p> <ul> <li>Released a Hugging Face Hub integration for programmatically publishing and downloading datasets to/from Hugging Face Hub! #4193</li> </ul> <p>App</p> <ul> <li> <p>Space sizes are now persisted when the App is refreshed #4171</p> </li> <li> <p>Added support for rendering detections with empty instance masks in the App #4227</p> </li> <li> <p>Enhanced label overlay processing to support empty label lists #4215</p> </li> <li> <p>Optimized by the App server by removing unnecessary server lock-ups due to synchronous IO calls #4180</p> </li> <li> <p>Optimized sidebar performance for grouped datasets #4182</p> </li> <li> <p>Optimized dataset counting for index page queries #4114</p> </li> <li> <p>Fixed a bug where sidebar group name changes in the App were not persisted #4241</p> </li> <li> <p>Fixed a bug when applying filters to <code>Keypoint</code> fields #4201</p> </li> <li> <p>Fixed a bug where in-App tagging actions may not be restricted to the currently selected samples #4195</p> </li> <li> <p>Fixed a bug when bookmarking sidebar filters for group datasets #4097</p> </li> <li> <p>Fixed a bug where the saved view dropdown would cover the view stage popover #4242</p> </li> </ul> <p>Core</p> <ul> <li> <p>All autosave contexts now respect the default batching strategy and can be configured to use content size-based batching #4243</p> </li> <li> <p>All SDK methods now use autosave contexts rather than calling <code>sample.save()</code> in a loop #4243</p> </li> <li> <p>Added a <code>read_files()</code> utility to efficiently read from multiple files in a threadpool #4243</p> </li> <li> <p>Optimized segmentation mask conversion #4185, #4188</p> </li> <li> <p>Resolved singularity issues in <code>compute_orthographic_projection_images()</code> #4206</p> </li> <li> <p>Fixed matplotlib style deprecation error #4213</p> </li> </ul> <p>Docs</p> <ul> <li> <p>Added a clustering tutorial #4245</p> </li> <li> <p>Added a small object detection tutorial #4263</p> </li> <li> <p>Refreshed many popular tutorials #4207</p> </li> </ul> <p>Annotation</p> <ul> <li>Upgraded the Labelbox integration to support the Export V2 API #4260</li> </ul> <p>Plugins</p> <ul> <li> <p>Secrets are now available to operators in their <code>resolve_input()</code>, <code>resolve_output()</code>, and <code>resolve_execution_options()</code> methods #4169</p> </li> <li> <p><code>ctx.view</code> now reflects when the current view is saved #4200</p> </li> <li> <p>Fixed a regression in debounce behavior in operator input forms that could potentially result in degraded performance #4199</p> </li> <li> <p>Fixed a bug when using the <code>set_view()</code> method in operators #4198</p> </li> </ul> <p>Zoo</p> <ul> <li> <p>Added support for loading YOLOv8 and YOLOv9 segmentation models from the Model Zoo #4220</p> </li> <li> <p>Added support for applying YOLO oriented bounding box models to FiftyOne datasets #4230, #4238</p> </li> <li> <p>Added support for applying Segment Anything models to the frames of video datasets #4229</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-158","title":"FiftyOne Teams 1.5.8 \u00b6","text":"<p>Released March 21, 2024</p> <p>Includes all updates from FiftyOne 0.23.7.</p>"},{"location":"release-notes/#fiftyone-0237","title":"FiftyOne 0.23.7 \u00b6","text":"<p>Released March 21, 2024</p> <p>App</p> <ul> <li> <p>Updated <code>Have a Team?</code> link in the App to point to the Book a demo page #4127</p> </li> <li> <p>Fixed indexed boolean fields in lightning mode #4139</p> </li> <li> <p>Fixed app crash when many None-valued fields exist in the sample modal #4154</p> </li> </ul> <p>Docs</p> <ul> <li> <p>Added an Albumentations integration for performing data augmentation on FiftyOne datasets #4155</p> </li> <li> <p>Added Places2 dataset to the zoo #4130</p> </li> <li> <p>Added a zero-shot image classification tutorial #4133</p> </li> <li> <p>Improved documentation for configuring AWS and GCP cloud credentials #4151</p> </li> <li> <p>Added YOLOv8, YOLOv9, and YOLO-World to the FiftyOne Model Zoo #4153</p> </li> <li> <p>Added <code>og:image</code> meta tag to all documentation pages for better page sharing on socials #4173</p> </li> <li> <p>Updated the lightning mode docs to clarify that wildcard indexes should not generally be used by default #4138</p> </li> </ul> <p>Plugins and Operators</p> <ul> <li> <p>Added support for executing operators programmatically in notebook contexts #4134</p> </li> <li> <p>Improved execution of operators during loading of the App #4136</p> </li> <li> <p>Added a new on_dataset_open hook to auto-execute operators when datasets are opened in the App #4137</p> </li> <li> <p>Improved performance of operator type resolution by only calling <code>resolve_input()</code> on demand #4152</p> </li> <li> <p>Added support for loading saved views by name or slug when using the <code>set_view()</code> operator #4159 and #4178</p> </li> <li> <p>Added ability to trigger builtin operators during operator execution via <code>ctx.ops</code> #4164</p> </li> <li> <p>Fixed issue where JS operator input was not validated when calling <code>ctx.trigger()</code> or <code>executeOperator()</code> directly #4170</p> </li> <li> <p>Show execution error of an operator in a notification when calling <code>ctx.trigger()</code> or <code>executeOperator()</code> directly #4170 and #4178</p> </li> </ul> <p>Core</p> <ul> <li> <p>Improved SuperGradients inference performance #4149</p> </li> <li> <p>Passing a grouped collection to a method that was not specifically designed to handle them now raises better validation errors #4150</p> </li> <li> <p><code>MediaExporter</code> no longer re-exports media unnecessarily #4143</p> </li> <li> <p>Added explicit support for Python 3.11 and 3.12 #4157</p> </li> <li> <p>Added a <code>perform_nms()</code> utility for non-maximum suppression on object detections #4160</p> </li> <li> <p>Improved error message when the given dataset name is unavailable #4161</p> </li> <li> <p>Removed use of deprecated non-integer arguments in <code>take()</code> and <code>shuffle()</code> #4052</p> </li> <li> <p>Added ability to change <code>map_type</code> from the default <code>roadmap</code> ( carto-positron) to <code>satellite</code> ( public USGS map imagery) in <code>location_scatterplot()</code> #4075</p> </li> <li> <p>Cloning a dataset or view now includes any custom MongoDB indexes #4115</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-157","title":"FiftyOne Teams 1.5.7 \u00b6","text":"<p>Released March 6, 2024</p> <p>Includes all updates from FiftyOne 0.23.6, plus:</p> <ul> <li> <p>Improved performance of <code>values()</code> when using API connections</p> </li> <li> <p>Improved stability of long-running operations when using API connections</p> </li> <li> <p>Added support for including prefixes when providing bucket-specific credentials</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0236","title":"FiftyOne 0.23.6 \u00b6","text":"<p>Released March 6, 2024</p> <p>What\u2019s New</p> <ul> <li> <p>Added a dimensionality reduction tutorial #4033</p> </li> <li> <p>Added a data augmentation tutorial #4109</p> </li> <li> <p>Added a formal Open CLIP integration page #4049</p> </li> <li> <p>Documented support for open-world object detection with YOLO World #4112</p> </li> <li> <p>Added support for importing/exporting contours in YOLO format #4094</p> </li> <li> <p>Added cosine metric as an option for Milvus similarity indexes #4081</p> </li> <li> <p>Added support for local files when using the Label Studio integration #3969</p> </li> <li> <p>Removed App dependency on <code>_cls</code> for embedded documents #4090</p> </li> </ul> <p>Bugs</p> <ul> <li> <p>Fixed issue with filter counts on video datasets in the App #4095</p> </li> <li> <p>Fixed issue with color scheme initialization in the App #4092</p> </li> <li> <p>Fixed issue when changing group slice with filters in the App #4098</p> </li> <li> <p>Fixed issue with zero-shot detection batching #4108</p> </li> <li> <p>Fixed issue with the operator target view utility when no view or sample selection is present #4113</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-156","title":"FiftyOne Teams 1.5.6 \u00b6","text":"<p>Released February 14, 2024</p> <p>Includes all updates from FiftyOne 0.23.5, plus:</p> <ul> <li> <p>Improved dataset search user experience</p> </li> <li> <p>Post login redirects will now send the user to the correct page</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0235","title":"FiftyOne 0.23.5 \u00b6","text":"<p>Released February 14, 2024</p> <p>What\u2019s New</p> <ul> <li> <p>Added subcounts to search results in the sidebar #3973</p> </li> <li> <p>Added <code>fiftyone.operators.types.ViewTargetProperty</code> to make it simpler to add view selection to a <code>fiftyone.operators.Operator</code> #4076</p> </li> <li> <p>Added support for apply monocular depth estimation transformers from the Hugging Face <code>transformers</code> library directly to FiftyOne datasets #4082</p> </li> </ul> <p>Bugs</p> <ul> <li> <p>Fixed an issue where increments were padded improperly #4035</p> </li> <li> <p>Fixed an issue when setting <code>session.color_scheme</code> #4060</p> </li> <li> <p>Fixed sidebar groups resolution when the dataset app config setting is configured #4064</p> </li> <li> <p>Fixed issue when <code>SelectGroupSlices</code> view stage is applied with only one slice within video grouped datasets #4066</p> </li> <li> <p>Fixed non-default pcd slice rendering in the App #4044</p> </li> <li> <p>Dynamic groups configuration options are now only shown when relevant #4068</p> </li> <li> <p>Fixed issue with dynamic groups mode pagination #4068</p> </li> <li> <p>Enabled tagging in sidebar lightning mode #4048</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-155","title":"FiftyOne Teams 1.5.5 \u00b6","text":"<p>Released January 25, 2024</p> <p>Includes all updates from FiftyOne 0.23.4, plus:</p> <p>Bugs</p> <ul> <li>Fixed a proxy URL bug that prevented custom JS panels from launching</li> </ul>"},{"location":"release-notes/#fiftyone-0234","title":"FiftyOne 0.23.4 \u00b6","text":"<p>Released January 25, 2024</p> <p>Core</p> <ul> <li>Added support for passing kwargs directly when creating custom runs #4039</li> </ul> <p>Brain</p> <ul> <li>Added support for registering custom visualization methods #4038</li> </ul>"},{"location":"release-notes/#fiftyone-teams-154","title":"FiftyOne Teams 1.5.4 \u00b6","text":"<p>Released January 19, 2024</p> <p>Includes all updates from FiftyOne 0.23.3, plus:</p> <p>General</p> <ul> <li> <p>Optimized <code>export()</code> calls involving cloud-backed media</p> </li> <li> <p>Deployments with their <code>FIFTYONE_API_URI</code> environment variable set will now display the API URI to users in the Teams App</p> </li> <li> <p>Improved debug logs by adding the head and tail of large results</p> </li> <li> <p>Updated <code>motor</code> dependency to 3.3.0</p> </li> </ul> <p>Bugs</p> <ul> <li> <p>Fixed a regression when exporting cloud-backed media to CVAT for annotation</p> </li> <li> <p>Fixed an issue where API requests were not being prefixed with the correct proxy URL</p> </li> <li> <p>Fixed running <code>compute_similarity()</code> over API connections with the MongoDB backend</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0233","title":"FiftyOne 0.23.3 \u00b6","text":"<p>Released January 19, 2024</p> <p>News</p> <ul> <li> <p>Released a Hugging Face integration for running inference with <code>transformers</code> models on your FiftyOne datasets!</p> </li> <li> <p>Released a SuperGradients integration for running inference with YOLO-NAS architectures!</p> </li> </ul> <p>App</p> <ul> <li> <p>Primitive values in <code>DynamicEmbeddedDocument</code> list fields are now displayed as comma-separated values (previously displayed as None) in the sample modal #3963</p> </li> <li> <p>Improved field visibility\u2019s show metadata toggle #3926</p> </li> <li> <p>Fixed issues for unknown operator types and defaults #3851</p> </li> <li> <p>Miscellaneous saved view improvements #3974</p> </li> <li> <p>Fixed a bug where images in the sample modal errored when frame fields were added to video slices in mixed datasets #3966</p> </li> <li> <p>Fixed in-App sort by similarity for datasets with a color scheme #3966</p> </li> <li> <p>Fixed issues where media and other URLs were constructed incorrectly #3976</p> </li> <li> <p>Fixed keyboard navigation for dropdowns throughout the App #3965</p> </li> </ul> <p>Brain</p> <ul> <li> <p>Added support for passing Hugging Face, Ultralytics, and SuperGradients models directly brain methods #4004</p> </li> <li> <p>Added support to <code>register_run()</code> for configuring whether run cleanup happens #3978</p> </li> <li> <p>Added support for passing model kwargs to <code>compute_similarity()</code> and <code>compute_visualization()</code></p> </li> <li> <p>Fixed issues with similarity searches on views and with pre-computed embeddings using the MongoDB backend</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added dynamic batching to bulk writes like <code>set_values()</code> #4015</p> </li> <li> <p>Added support for customizing progress bar rendering at method level #3979</p> </li> <li> <p>Include sample/frame singletons when clearing dataset cache via <code>clear_cache()</code> #4016</p> </li> <li> <p>Fixed issues with embedded document field schemas #4002</p> </li> </ul> <p>Models</p> <ul> <li> <p>Added support for directly passing Ultralytics models models to <code>apply_model()</code></p> </li> <li> <p>Added GPU support for OpenCLIP models #3986</p> </li> <li> <p>Added prompt embedding capabilities to OpenCLIP models #3960</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>Added a builtin <code>delete_selected_labels</code> operator #4001</p> </li> <li> <p>Updated <code>ctx.selected_labels</code> format to be consistent with other SDK methods #3998</p> </li> </ul> <p>Tutorials</p> <ul> <li>Added a monocular depth estimation tutorial #3991</li> </ul>"},{"location":"release-notes/#fiftyone-teams-153","title":"FiftyOne Teams 1.5.3 \u00b6","text":"<p>Released December 21, 2023</p> <p>Includes all updates from FiftyOne 0.23.2, plus:</p> <p>General</p> <ul> <li> <p>Improved performance of <code>add_samples()</code>, <code>set_values()</code>, <code>compute_metadata()</code>, and other large batched computations when using API connections</p> </li> <li> <p>Added <code>label</code> as a searchable field for delegated operations</p> </li> <li> <p>Fixed issue where invalid tokens were not causing redirects</p> </li> <li> <p>Re-running a delegated operation now uses dataset ID instead of name</p> </li> <li> <p>Trimmed API logging of large batch SDK operations</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0232","title":"FiftyOne 0.23.2 \u00b6","text":"<p>Released December 21, 2023</p> <p>News</p> <ul> <li>Added OpenCLIP to the FiftyOne Model Zoo! #3925</li> </ul> <p>App</p> <ul> <li> <p>Added support for frames-as-videos in nested groups #3935</p> </li> <li> <p>Fixed an issue where embeddings legend did not display full names #3927</p> </li> <li> <p>Added a toggle to show/hide fields in the sample modal that have undefined values #3937</p> </li> <li> <p>Fixed an issue with the Lightning threshold reset button #3933</p> </li> <li> <p>Fixed an issue where similarity search only worked on the default group slice #3912</p> </li> <li> <p>Fixed issue where users could not select scalar fields in the sidebar #3938</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added configurable batching choices to optimize throughput for operations like <code>add_samples()</code> #3923</p> </li> <li> <p>IoU computations for non-filled polylines now uses keypoint similarity #3930</p> </li> <li> <p>Optimized bulk write database operations like <code>set_values()</code> #3942</p> </li> <li> <p>Added configurable batch sizes to bulk write operations #3944</p> </li> <li> <p>Added builtin support for Ubuntu 23 #3936</p> </li> <li> <p>Fixed an issue where exporting patches would have incorrect path names #3921</p> </li> <li> <p>Removed loading from mongoengine cache #3922</p> </li> <li> <p>Fixed overwriting dataset metadata with empty values during import #3913</p> </li> </ul> <p>Annotation</p> <ul> <li>Added support for annotating multiple label fields using the Label Studio backend #3895</li> </ul> <p>Plugins</p> <ul> <li> <p>Added support for delegating function calls via the new @voxel51/utils/delegate operator #3939</p> </li> <li> <p>Added the ability to search multiple fields in a delegated operation list query #3892</p> </li> <li> <p>Delegated operators now reference datasets by ID rather than name for robustness to dataset name changes #3920</p> </li> <li> <p>Improved validation for the builtin <code>delete_selected_samples</code> and <code>clone_selected_samples</code> operators #3914</p> </li> <li> <p>Fixed backwards compatibility issues with <code>ctx.secrets</code> #3908</p> </li> <li> <p>Fixed issue with JS plugin App configs #3924</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-152","title":"FiftyOne Teams 1.5.2 \u00b6","text":"<p>Released December 11, 2023</p> <p>Bugs</p> <ul> <li> <p>Avoid creating non-existent database indexes on API startup</p> </li> <li> <p>Avoid errors when archiving snapshots with corrupted run results</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-151","title":"FiftyOne Teams 1.5.1 \u00b6","text":"<p>Released December 8, 2023</p> <p>Includes all updates from FiftyOne 0.23.1</p>"},{"location":"release-notes/#fiftyone-0231","title":"FiftyOne 0.23.1 \u00b6","text":"<p>Released December 8, 2023</p> <p>App</p> <ul> <li> <p>Fixed Python 3.8 installations #3905</p> </li> <li> <p>Fixed App error pages #3903</p> </li> <li> <p>Fixed <code>session.dataset = None</code> #3890</p> </li> </ul> <p>Core</p> <ul> <li> <p>Fixed inferring doubly-nested dynamic list field types #3900</p> </li> <li> <p>Fixed <code>compute_metadata()</code> when <code>Pillow&lt;7</code> is installed #3897</p> </li> <li> <p>Fixed default group indexes creation when importing a FiftyOneDataset #3894</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-150","title":"FiftyOne Teams 1.5.0 \u00b6","text":"<p>Released December 6, 2023</p> <p>Includes all updates from FiftyOne 0.23.0, plus:</p> <p>Features</p> <ul> <li> <p>Added support for archiving older dataset snapshots to cold storage</p> </li> <li> <p>Added support for executing operators on dataset snapshots</p> </li> <li> <p>Added support for uploading multiple sets of cloud credentials, some of which may only apply to data in certain bucket(s)</p> </li> <li> <p>Added support for uploading media to Labelbox directly from S3 buckets</p> </li> <li> <p>Added support for executing the builtin <code>open_dataset</code> operator in the Teams UI</p> </li> <li> <p>Added support for executing operators when viewing datasets with no samples, for example to add media/labels to the dataset from within the App</p> </li> <li> <p>Added support for editing the label of a delegated operation</p> </li> <li> <p>Added support for manually marking delegated operations as failed</p> </li> <li> <p>Added support for monitoring the progress of delegated operations</p> </li> <li> <p>Improved handling of plugin secrets</p> </li> <li> <p>Added the ability to attach authorization tokens to media/asset requests</p> </li> <li> <p>Added new filter options to the dataset listing page</p> </li> <li> <p>Filters/searches on the dataset listing page are now persisted through URL query parameters</p> </li> <li> <p>Validate regexes before searching datasets to stop hard crashes</p> </li> <li> <p>Enforce exact version of <code>auth0</code> python package</p> </li> <li> <p>Added debug logging on API startup</p> </li> </ul> <p>Bugs</p> <ul> <li> <p>Fixed an issue with the Runs page when viewing delegated operations that were scheduled via the SDK</p> </li> <li> <p>Users with special access to a dataset are now displayed properly</p> </li> <li> <p>Fixed an issue when loading certain datasets with saved color schemes in the Teams UI</p> </li> <li> <p>Fixed an issue on the dataset listing page where the page size menu would sometimes stay open after making a selection</p> </li> <li> <p>Fixed an issue when downloading plugins via the API that contain bytes data or <code>.pyc</code> files</p> </li> <li> <p>Fixed an issue where certain disabled operators were not correctly appearing as disabled in the operator browser</p> </li> <li> <p>Improved reliability of similarity sort actions</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0230","title":"FiftyOne 0.23.0 \u00b6","text":"<p>Released December 6, 2023</p> <p>News</p> <ul> <li> <p>Released a Redis integration for native text and image searches on FiftyOne datasets!</p> </li> <li> <p>Released a MongoDB integration for native text and image searches on FiftyOne datasets!</p> </li> <li> <p>Released a V7 integration for annotating FiftyOne datasets!</p> </li> </ul> <p>App</p> <ul> <li> <p>Added a new Lightning mode to the App sidebar that provides an optimized filtering experience for large datasets #3807</p> </li> <li> <p>Added support for viewing image groups as a video #3812</p> </li> <li> <p>Added support for configuring custom color schemes for semantic segmentation labels via the color scheme editor #3727</p> </li> <li> <p>Added support for configuring custom Heatmap colorscales via the color scheme editor #3804</p> </li> <li> <p>Improved rendering and customizability of label tags in the color scheme #3622</p> </li> <li> <p>Added an empty dataset landing page that allows for importing media and/or labels to the dataset from the App by running operators #3766</p> </li> <li> <p>Added a landing page that appears when no dataset is currently selected that allows for creating/opening datasets in the App by running operators #3766</p> </li> <li> <p>Added support for executing operators when the sample modal is open #3747</p> </li> <li> <p>Added a keyboard shortcut for batch selecting samples in grid and modal #3718</p> </li> <li> <p>Made field visibility\u2019s selections persistent across page refreshes #3646</p> </li> <li> <p>Introduced error alert for view bar errors in view stages #3613</p> </li> <li> <p>Ensure that the last used brain key is loaded by default in the similarity search menu #3714</p> </li> <li> <p>Added support for launching the App with a non-default browser #3789</p> </li> <li> <p>Upgraded <code>werkzeug</code> from 2.0.3 to 3.0.1 in requirements for improved compatibility #3723</p> </li> </ul> <p>Core</p> <ul> <li> <p>Adding support for registering custom evaluation methods #3695</p> </li> <li> <p>Optimized the <code>compute_metadata()</code> implementation #3801</p> </li> <li> <p>Added full support for working with images that use <code>EXIF</code> tags #3824</p> </li> <li> <p>Added support for parsing and exporting visibility attribute for keypoints in COCO format #3808</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>Added <code>ctx.current_sample</code> to operator\u2019s <code>ExecutionContext</code> to support applying operators to the current sample open in the App modal #3792</p> </li> <li> <p>Added support for configuring an operator\u2019s available execution options in cases where immediate and/or delegated execution should be available #3839</p> </li> <li> <p>Added support for programmatically executing generator operators via the SDK #3803</p> </li> <li> <p>Added a builtin <code>clear_sample_field</code> operator for clearing sample fields #3800</p> </li> <li> <p>Loosened the <code>OperatorConfig</code> constructor signature for enhanced forward/backward compatibility #3786</p> </li> <li> <p>Fixed an issue where operator form defaults were not always applied #3777</p> </li> <li> <p>Improved handling of fields in operator forms #3728</p> </li> <li> <p>Improved default value control in operator forms #3371</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Updated the Labelbox integration to support the latest version of the Labelbox API #3781</p> </li> <li> <p>Removed the need for prepending sequence numbers to filenames when uploading images to the CVAT integration with sufficiently new versions of the CVAT SDK #3823</p> </li> </ul> <p>Bugs</p> <ul> <li> <p>Improved the implementation of saved view loading in the App #3788</p> </li> <li> <p>Fixed an issue where typing the backtick key would close the operator palette #3790</p> </li> <li> <p>Fixed orthographic projection bug for more accurate 3D rendering #3864</p> </li> <li> <p>Addressed missing notifications when scheduling certain delegated operations from the App #3861</p> </li> <li> <p>Resolved issues with generator operators #3861</p> </li> <li> <p>Fixed operator form exception when <code>onChange</code> is missing #3840</p> </li> <li> <p>Corrected operator form crash and changed field re-render #3833</p> </li> <li> <p>Fixed select/show samples builtin operator for better sample management #3818</p> </li> <li> <p>Addressed hidden validation error bug for more accurate error handling #3776</p> </li> <li> <p>Fixed issue with custom colors when switching between name and list #3847</p> </li> <li> <p>Various improvements and fixes around color management #3649</p> </li> <li> <p>Resolved issue where tag labels in multiple samples could only tag labels in the last sample #3858</p> </li> <li> <p>Prevent operator list from rendering behind the sample modal #3757</p> </li> <li> <p>Fixed boolean not displayed in modal view sidebar entry for consistent data representation #3713</p> </li> <li> <p>Fixed random seed issue when creating <code>Take</code> view stages in the App #3855</p> </li> <li> <p>Fixed dynamically grouped views for non-group parent media types of grouped datasets #3798</p> </li> <li> <p>Addressed media fields issues for more reliable media handling #3722</p> </li> <li> <p>Fixed an issue with selecting group slices in views that contain a <code>Select</code> view stage #3852</p> </li> <li> <p>Fixed an issue with view reloading for datasets that have saved views #3838</p> </li> <li> <p>Fixed rendering of semantic segmentation masks within <code>DynamicEmbeddedDocument</code> fields #3825</p> </li> <li> <p>Resolved an issue with the slice/group statistics selector where no default option is selected #3698</p> </li> <li> <p>Fixed various issues with builtin operators #3817</p> </li> <li> <p>Addressed a potential data duplication issue when merging in-memory samples into grouped datasets #3816</p> </li> <li> <p>Resolved possible malformed FiftyOneDataset format exports due to concurrent edits #3726</p> </li> <li> <p>Fixed the plugin cache check #3676</p> </li> <li> <p>Fixed an error when pressing the esc key in the App #3662</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-145","title":"FiftyOne Teams 1.4.5 \u00b6","text":"<p>Released November 21, 2023</p> <p>General</p> <ul> <li>Added debug log events to API server startup</li> </ul>"},{"location":"release-notes/#fiftyone-teams-144","title":"FiftyOne Teams 1.4.4 \u00b6","text":"<p>Released November 3, 2023</p> <p>Includes all updates from FiftyOne 0.22.3, plus:</p> <p>General</p> <ul> <li> <p>Optimized iterator operations such as export</p> </li> <li> <p>Improved plugin upload reliability</p> </li> <li> <p>Further improved dataset listing queries</p> </li> </ul> <p>Bugs</p> <ul> <li> <p>Fixed clips, frames, and patches views for grouped datasets in the App</p> </li> <li> <p>Fixed cloud credential initialization during deployment restarts</p> </li> <li> <p>Fixed snapshot diff computation in large datasets with MongoDB &lt; v6.0</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0223","title":"FiftyOne 0.22.3 \u00b6","text":"<p>Released November 3, 2023</p> <p>Core</p> <ul> <li>Optimized <code>sort_by_similarity()</code> #3733</li> </ul> <p>App</p> <ul> <li> <p>Fixed rendering of <code>BooleanFields</code> in the sample modal #3720</p> </li> <li> <p>Optimized the Embeddings panel #3733</p> </li> <li> <p>Fixed media field changes in the sample modal #3735</p> </li> <li> <p>Fixed sidebar reordering edge case #3753</p> </li> <li> <p>Fixed the Operator browser in the sample modal #3764</p> </li> <li> <p>Fixed 3D detections in the grid #3761</p> </li> </ul> <p>Brain</p> <ul> <li> <p>Optimized similarity backends when performing KNN queries against their entire indexes</p> </li> <li> <p>Fixed performing similarity queries on filtered views in the LanceDB integration</p> </li> <li> <p>Fixed calling <code>remove_from_index()</code> on an index that uses the <code>embeddings_field</code> parameter</p> </li> <li> <p>Fixed <code>compute_embeddings()</code> when <code>skip_existing=True</code> is provided</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>Fixed <code>on_startup</code> Operator execution #3731</p> </li> <li> <p>Fixed <code>selected_labels</code> in Operator contexts #3740</p> </li> <li> <p>Improved Operator placements #3742</p> </li> <li> <p>Fixed <code>async</code> generator results in delegated operations #3754</p> </li> <li> <p>Fixed <code>ctx.secrets</code> in <code>resolve_input()</code> #3759</p> </li> </ul> <p>CLI</p> <ul> <li>Added fiftyone delegated fail and fiftyone delegated delete commands #3721</li> </ul>"},{"location":"release-notes/#fiftyone-teams-143","title":"FiftyOne Teams 1.4.3 \u00b6","text":"<p>Released October 20, 2023</p> <p>Includes all updates from FiftyOne 0.22.2, plus:</p> <p>General</p> <ul> <li> <p>Improved dataset listing queries</p> </li> <li> <p>Improved error handling when listing datasets</p> </li> <li> <p>Fixed issues with offline access and auth errors requiring cookies to be cleared manually</p> </li> <li> <p>Reduced max export size of datasets to 100MB</p> </li> <li> <p>Users will now only see an operator if their role meets the required role</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0222","title":"FiftyOne 0.22.2 \u00b6","text":"<p>Released October 20, 2023</p> <p>Core</p> <ul> <li> <p>Added a <code>fiftyone_max_thread_pool_workers</code> option to the FiftyOne config #3654</p> </li> <li> <p>Added a <code>fiftyone_max_process_pool_workers</code> option to the FiftyOne config #3654</p> </li> <li> <p>Added support for directly calling <code>export()</code> on patches views to export image patches #3651</p> </li> <li> <p>Fixed an issue where CVAT import fails when <code>insert_new</code> is <code>False</code> #3691</p> </li> </ul> <p>App</p> <ul> <li> <p>Fixed dataset recreation across processes #3655</p> </li> <li> <p>Fixed the <code>Session.url</code> property in Colab #3645</p> </li> <li> <p>Fixed converting to patches in grouped datasets when sidebar filters are present #3666</p> </li> <li> <p>Fixed browser cache issues when upgrading #3683</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>Use a fallback icon when an operator cannot be executed #3661</p> </li> <li> <p><code>FileView</code> now captures content as well as filename and type of the <code>UploadedFile</code> #3679</p> </li> <li> <p>Fixed issue where the <code>fiftyone delegated launch</code> CLI command would print confusing errors #3694</p> </li> <li> <p>Added a <code>list_operators()</code> utility for listing operators #3694</p> </li> <li> <p>Added a <code>operator_exists()</code> utility for checking if an operator exists #3694</p> </li> <li> <p><code>Number</code> properties now support <code>min</code> and <code>max</code> options in various views and validation #3684</p> </li> <li> <p>Improved validation of primitive types in operators #3685</p> </li> <li> <p>Fixed issue where non-required property validated as required #3701</p> </li> <li> <p>Fixed an issue where plugin cache was not cleared when a plugin was deleted #3700</p> </li> <li> <p><code>File</code> now uses <code>FileExplorerView</code> by default #3656</p> </li> </ul> <p>Zoo</p> <ul> <li>Fixed issue preventing DINOv2 models from being loaded #3660</li> </ul>"},{"location":"release-notes/#fiftyone-teams-142","title":"FiftyOne Teams 1.4.2 \u00b6","text":"<p>Released October 6, 2023</p> <p>Includes all updates from FiftyOne 0.22.1, plus:</p> <p>General</p> <ul> <li> <p>Error messages now clearly indicate when attempting to use a duplicate key on datasets a user does not have access to</p> </li> <li> <p>Fixed issue with setting default access permissions for new datasets</p> </li> <li> <p>Deleting a dataset now deletes all dataset-related references</p> </li> <li> <p>Default fields now populate properly when creating a new dataset regardless of client</p> </li> <li> <p>Improved complex/multi collection aggregations in the api client</p> </li> <li> <p>Fixed issue where users could not list other users within their own org</p> </li> <li> <p>Snapshots now properly include all run results</p> </li> <li> <p>Fixed issue where reverting a snapshot behaved incorrectly in some cases</p> </li> <li> <p>Fixed Python 3.7 support in the fiftyone-teams SDK</p> </li> </ul> <p>App</p> <ul> <li> <p>Searching users has been improved</p> </li> <li> <p>Resolved issue with recent views not displaying properly</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0221","title":"FiftyOne 0.22.1 \u00b6","text":"<p>Released October 6, 2023</p> <p>App</p> <ul> <li> <p>Fixed empty detection instance masks #3559</p> </li> <li> <p>Fixed a visual issue with scrollbars #3605</p> </li> <li> <p>Fixed a bug with color by index for videos #3606</p> </li> <li> <p>Fixed an issue where <code>Detections</code> (and other label types) subfields were properly handling primitive types #3577</p> </li> <li> <p>Fixed an issue launching the App in Databrick notebooks #3609</p> </li> </ul> <p>Core</p> <ul> <li> <p>Resolved groups aggregation issue resulting in unstable ordering of documents #3641</p> </li> <li> <p>Fixed an issue where group indexes were not created against the correct <code>id</code> property #3627</p> </li> <li> <p>Fixed issue with empty segmentation mask conversion in COCO-formatted datasets #3595</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>Added a new <code>fiftyone.plugins.utils</code> module that provides common utilities for plugin development #3612</p> </li> <li> <p>Re-enabled text-only placement support when icon is not available #3593</p> </li> <li> <p>Added read-only support for <code>FileExplorerView</code> #3639</p> </li> <li> <p>The <code>fiftyone delegated launch</code> CLI command will now only run one operation at a time #3615</p> </li> <li> <p>Fixed an issue where custom component props were not supported #3595</p> </li> <li> <p>Fixed issue where <code>selected_labels</code> were missing from the <code>ExecutionContext</code> during <code>resolve_input()</code> and <code>resolve_output()</code> #3575</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-141","title":"FiftyOne Teams 1.4.1 \u00b6","text":"<p>Released September 21, 2023</p> <p>Bugs</p> <ul> <li>Patched a regression that prevented the Teams App from working behind proxies</li> </ul>"},{"location":"release-notes/#fiftyone-teams-140","title":"FiftyOne Teams 1.4.0 \u00b6","text":"<p>Released September 20, 2023</p> <p>Includes all updates from FiftyOne 0.22.0, plus:</p> <p>News</p> <ul> <li> <p>Added support for dataset versioning!</p> </li> <li> <p>Added support for scheduling delegated operations via the App</p> </li> </ul> <p>App</p> <ul> <li> <p>Admins can now upload secrets via the UI which are made available to all plugins and delegated operations at runtime</p> </li> <li> <p>Optimized page load times when accessing the Team Settings page</p> </li> <li> <p>Optimized page load times when opening a dataset for the first time in a new web session</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0220","title":"FiftyOne 0.22.0 \u00b6","text":"<p>Released September 20, 2023</p> <p>News</p> <ul> <li> <p>Added a native Ultralytics integration! #3451</p> </li> <li> <p>Added support for scheduling delegated operations from within the App! #3312</p> </li> </ul> <p>App</p> <ul> <li> <p>Updated the Histograms panel to only render one field at a time to improve performance #3419</p> </li> <li> <p>Gracefully fallback to <code>filepath</code> if a dataset\u2019s <code>app_config</code> has a custom grid media field that has been excluded from the current view #3498</p> </li> <li> <p>Improved rendering of 2D polylines #3476</p> </li> <li> <p>Prevented unnecessary page reloads when clearing selections in the Embeddings panel #3507</p> </li> <li> <p>Removed unnecessary page reloads when resetting field visibility filters #3441</p> </li> <li> <p>Fixed an off-by-one bug when paging in the sample grid #3416</p> </li> <li> <p>Fixed a bug when applying field visibility filters to fields of type <code>DateField</code> and <code>DateTimeField</code> #3418</p> </li> <li> <p>Fixed a bug when changing slices for grouped datasets in the sample modal when sidebar filters have been applied #3545</p> </li> <li> <p>Fixed a bug when visualizing dynamic groupings of grouped datasets with sparse (missing) slices #3470</p> </li> <li> <p>Fixed a bug that prevented the group media visibility dropdown from opening #3480</p> </li> <li> <p>Fixed a bug where attributes of grouped samples were missing in the modal #3436</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for grouping by compound keys using <code>group_by()</code> #3515</p> </li> <li> <p>Added <code>create_index=False</code> options to <code>sort_by()</code> and <code>group_by()</code> #3515</p> </li> <li> <p>Added a new <code>tags</code> filter option to <code>list_datasets()</code> #3492</p> </li> <li> <p>Added a <code>fiftyone.core.storage</code> module that provides a common interface for filesystem I/O #3406</p> </li> <li> <p>Added dataset tag and label filters when exporting datasets via the CLI #3412</p> </li> <li> <p>Added support for running FiftyOne in podman containers #3483</p> </li> <li> <p>Optimized the <code>list_datasets(info=True)</code> implementation #3528</p> </li> <li> <p>Added support for providing frame sizes when constructing rotated boxes and cuboids #3409</p> </li> <li> <p>Fixed a bug with automatic non-persistent dataset cleanup when running MongoDB v4.4 and later #3486</p> </li> <li> <p>Fixed a bug where default indexes for grouped datasets were not created via <code>clone()</code> and <code>merge_samples()</code> #3515</p> </li> <li> <p>Fixed a bug where NaNs were causing orthographic projection computations to crash #3427</p> </li> <li> <p>Fixed a bug with the OpenLABEL importer when given incomplete keypoint skeletons #3429</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>Added a new <code>FileExplorerView</code> type that allows for browsing file systems and selecting files or directories #3459</p> </li> <li> <p>Added <code>ctx.secrets</code> to plugins #3453</p> </li> <li> <p>Added a builtin <code>set_progress</code> operator #3516</p> </li> <li> <p>Fixed broken wiring of the <code>MarkdownView</code>, <code>SwitchView</code>, and <code>Placement</code> components #3537</p> </li> </ul> <p>Zoo</p> <ul> <li> <p>Graceful handling of empty prompts when using Segment Anything models #3505</p> </li> <li> <p>Fixed bugs where Segment Anything model weights were not loaded and auto-inference would only return one set of masks #3465</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-136","title":"FiftyOne Teams 1.3.6 \u00b6","text":"<p>Released August 8, 2023</p> <p>Includes all updates from FiftyOne 0.21.6.</p>"},{"location":"release-notes/#fiftyone-0216","title":"FiftyOne 0.21.6 \u00b6","text":"<p>Released August 8, 2023</p> <p>App</p> <ul> <li> <p>Fixed the Embeddings panel #3401</p> </li> <li> <p>Fixed a bug when using the sidebar to filter views that have selected fields #3405</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-135","title":"FiftyOne Teams 1.3.5 \u00b6","text":"<p>Released August 7, 2023</p> <p>Includes all updates from FiftyOne 0.21.5, plus:</p> <p>App</p> <ul> <li> <p>Fixed a bug with dataset search where suggestions may not appear when matches across multiple types collide</p> </li> <li> <p>Upgraded the Plugin configuration UI to better explain the available Operator permission configuration options</p> </li> </ul> <p>SDK</p> <ul> <li>Significant performance optimizations by introducing cursor batching for relevant API endpoints</li> </ul>"},{"location":"release-notes/#fiftyone-0215","title":"FiftyOne 0.21.5 \u00b6","text":"<p>Released August 7, 2023</p> <p>News</p> <ul> <li> <p>Added Segment Anything to the Model Zoo! #3330</p> </li> <li> <p>Added DINOv2 to the Model Zoo! #2951</p> </li> <li> <p>Added support for loading models from PyTorch Hub! #2949</p> </li> </ul> <p>App</p> <ul> <li> <p>Added support for controlling field visibility in the grid independent of filtering #3248</p> </li> <li> <p>Added support for filtering by label tags in individual label fields #3287</p> </li> <li> <p>Added support for specifying custom colors for list fields #3319</p> </li> <li> <p>Added support for opening the color panel when the sample modal is open #3355</p> </li> <li> <p>Added helper text explaining custom color options #3383</p> </li> <li> <p>Added support for viewing slices of grouped datasets in the Embeddings panel #3351</p> </li> <li> <p>Added support for coloring embeddings plots by list fields #3326</p> </li> <li> <p>Improved overflow when the actions row contains many icons #3296</p> </li> <li> <p>Added support for tagging all visible PCD slices #3384</p> </li> <li> <p>Improved handling of group datasets whose groups may contain missing samples for certain slices #3333</p> </li> <li> <p>Fixed various issues when visualizing grouped datasets #3353, #3322, #3318, #3379, #3318</p> </li> <li> <p>Added bazel support #3338</p> </li> <li> <p>Removed the maximum <code>starlette</code> version requirement #3297</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>Added support for accessing the currently selected labels in the App within plugin execution contexts #3295</p> </li> <li> <p>Added support for configuring custom Operator icons #3299</p> </li> <li> <p>Improved Operator form validation debounce behavior #3291</p> </li> <li> <p>Fixed some bugs that prevented customer visualizer plugins from being recognized #3357</p> </li> </ul> <p>Core</p> <ul> <li> <p>Improved robustness of concurrent schema updates #3308</p> </li> <li> <p>Schema changes are now maintained by the <code>select_group_slices()</code> stage #3336</p> </li> <li> <p>Added support for exporting keypoints with nan-valued coordinates in COCO format #3316</p> </li> <li> <p>Updated YOLOv5 exports to use dict-style class names #3393</p> </li> <li> <p>Fixed a bug when passing an RGB hex string to <code>to_segmentation()</code> #3293</p> </li> <li> <p>Fixed a bug where <code>has_field()</code> would not recognize dynamic fields #3349</p> </li> <li> <p>Fixed a bug when applying <code>merge_sample()</code> to grouped datasets #3327</p> </li> </ul> <p>Zoo</p> <ul> <li> <p>Use <code>weights</code> parameter instead of deprecated <code>pretrained</code> parameter for torchvision models #3348</p> </li> <li> <p>Added support for running zoo models with the MPS backend #2843</p> </li> <li> <p>Fixed YouTube video downloading for zoo datasets like ActivityNet and Kinetics #3382</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Upgraded the Labelbox integration to support the latest Labelbox API version #3323</p> </li> <li> <p>Fixed text and checkbox attribute usage when using CVAT 2.5 #3373</p> </li> </ul> <p>Brain</p> <ul> <li> <p>Added support for gRPC connections when using the Qdrant similarity backend #3296</p> </li> <li> <p>Improved support for creating similarity indexes with embeddings stored in dataset fields</p> </li> <li> <p>Resolved bugs with similarity queries using the sklearn backend #3304, #3305</p> </li> </ul> <p>Docs</p> <ul> <li>Fixed some documentation typos #3283, #3289, #3290</li> </ul>"},{"location":"release-notes/#fiftyone-0214","title":"FiftyOne 0.21.4 \u00b6","text":"<p>Released July 14, 2023</p> <ul> <li>Fixed <code>Session</code> event emission #3301</li> </ul>"},{"location":"release-notes/#fiftyone-teams-133","title":"FiftyOne Teams 1.3.3 \u00b6","text":"<p>Released July 12, 2023</p> <p>Includes all updates from FiftyOne 0.21.3, plus:</p> <p>SDK</p> <ul> <li> <p>Added a <code>cache=True</code> option to the upload_media() utility that allows for automatically adding any uploaded files to your local cache</p> </li> <li> <p>Fixed a bug when launching the App locally via API connections</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0213","title":"FiftyOne 0.21.3 \u00b6","text":"<p>Released July 12, 2023</p> <p>News</p> <ul> <li> <p>Released a Milvus integration for native text and image searches on FiftyOne datasets!</p> </li> <li> <p>Released a LanceDB integration for native text and image searches on FiftyOne datasets!</p> </li> </ul> <p>App</p> <ul> <li> <p>Added support for embedded keypoint fields in <code>filter_keypoints()</code> #3279</p> </li> <li> <p>Fixed keypoint filtering #3270</p> </li> <li> <p>Fixed a bug that caused non-matching samples to remain in the grid when applying multiple sidebar filters #3270</p> </li> <li> <p>Fixed a bug when filtering by IDs in the sidebar #3270</p> </li> <li> <p>Fixed label tags grid bubbles for filterless views #3257</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added a <code>merge_sample()</code> method for merging individual samples into existing datasets #3274</p> </li> <li> <p>Fixed a bug when passing dict-valued <code>points</code> to <code>compute_visualization()</code> #3268</p> </li> <li> <p>Fixed a bug when filtering keypoints stored in embedded documents #3279</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-132","title":"FiftyOne Teams 1.3.2 \u00b6","text":"<p>Released July 5, 2023</p> <p>Includes all updates from FiftyOne 0.21.2.</p>"},{"location":"release-notes/#fiftyone-0212","title":"FiftyOne 0.21.2 \u00b6","text":"<p>Released July 3, 2023</p> <p>App</p> <ul> <li> <p>Fixes grid pagination results after applying sidebar filters #3249</p> </li> <li> <p>Fixes redundant sidebar groups for custom schemas #3250</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-131","title":"FiftyOne Teams 1.3.1 \u00b6","text":"<p>Released June 30, 2023</p> <p>Includes all features from FiftyOne 0.21.1, plus:</p> <p>General</p> <ul> <li> <p>App containers no longer need to be restarted in order for Azure/MinIO credentials uploaded via the Teams UI to be properly recognized</p> </li> <li> <p>Fixed an intermittent bug when computing metadata for remote filepaths</p> </li> <li> <p>Reverted a change from Teams 1.3.0 so that the SDK again supports the declared minimum version requirement of <code>pymongo==3.12</code></p> </li> </ul> <p>SDK</p> <ul> <li> <p>Updated the order of precedence for SDK connections so that API connections take precedence over direct database connections</p> </li> <li> <p>Fixed a bug when connecting to Teams deployments with non-standard database names via API connections</p> </li> <li> <p>Fixed a bug when saving run results using API connections</p> </li> <li> <p>Fixed a bug when deleting datasets using API connections</p> </li> </ul> <p>Management SDK</p> <ul> <li> <p>Added support for deleting user invitations by email in addition to invitation ID</p> </li> <li> <p>Added support for configuring permissions for invited users that have not yet logged in</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0211","title":"FiftyOne 0.21.1 \u00b6","text":"<p>Released June 30, 2023</p> <p>App</p> <ul> <li> <p>Sidebar filters can now leverage indexes for improved performance! #3137</p> </li> <li> <p>Optimized the App grid\u2019s loading performance, especially for datasets with large samples #3137</p> </li> <li> <p>Improved the usability of the field visibility modal #3154</p> </li> <li> <p>Added support for visualizing Label fields stored within dynamic embedded documents #3141</p> </li> <li> <p>Added support for coloring embeddings plots by list fields #3230</p> </li> <li> <p>Added a <code>proxy_url</code> setting to the App config that allows for overriding the server URL #3222</p> </li> <li> <p>Added support for configuring custom colors for sample tags #3171</p> </li> <li> <p>Fixed a bug that caused the point cloud selector from disappearing #3200</p> </li> <li> <p>Fixed various minor bugs when viewing dynamic groups in the App #3172</p> </li> </ul> <p>Core</p> <ul> <li> <p>Methods like <code>tag_labels()</code>, <code>select_labels()</code>, <code>export()</code>, and <code>draw_labels()</code> now automatically detect and properly handle label fields stored within embedded documents #3152</p> </li> <li> <p>All <code>Document</code> objects now support <code>doc[\"nested.field\"]</code> key access #3152</p> </li> <li> <p>Dynamic field detection now automatically detects dynamic attributes of list fields with inhomogeneous values #3152</p> </li> <li> <p>Fixed a bug that would cause dynamic field schema methods to erroneously declare subfields of <code>Polyline</code> points #3152</p> </li> <li> <p>Fixed a bug when applying <code>merge_samples()</code> to video dataset views #3159</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>Added support for rendering markdown-style tables using the Operator table view type #3162</p> </li> <li> <p>Added support for multiselect to the Operator string type #3192</p> </li> <li> <p>Added <code>--all</code> flags to plugin CLI methods #3177</p> </li> <li> <p>Placements and on-startup hooks are now omitted for disabled Operators #3175</p> </li> <li> <p>Fixed a bug with <code>read_only=True</code> mode for certain Operator view types #3225</p> </li> </ul> <p>Annotation</p> <ul> <li>Added support for CVAT\u2019s <code>frame_start</code>, <code>frame_stop</code>, and <code>frame_step</code> options when creating annotation tasks #3181</li> </ul>"},{"location":"release-notes/#fiftyone-teams-130","title":"FiftyOne Teams 1.3.0 \u00b6","text":"<p>Released May 31, 2023</p> <p>Includes all features from FiftyOne 0.21.0, plus:</p> <p>General</p> <ul> <li> <p>Added a Management SDK subpackage for programmatically configuring user roles, dataset permissions, plugins, and more</p> </li> <li> <p>Added support for authenticated API connections when using the Python SDK that respect user roles, dataset permissions, etc</p> </li> <li> <p>Logins now automatically redirect back to the page you were trying to access</p> </li> <li> <p>Improved non-persistent dataset cleanup behavior</p> </li> <li> <p>Fixed a bug that could cause the media cache to erroneously garbage collect large files while they are downloading</p> </li> <li> <p>Fixed a bug when cloning views into new datasets via the Teams UI</p> </li> </ul> <p>Admin</p> <ul> <li> <p>Added support for uploading and managing plugins via the Teams UI</p> </li> <li> <p>Added support for cross account IAM roles when configuring cloud storage credentials</p> </li> <li> <p>Fixed a bug that prevented Azure/MinIO credentials uploaded via the Teams UI from being properly recognized by the App</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0210","title":"FiftyOne 0.21.0 \u00b6","text":"<p>Released May 31, 2023</p> <p>App</p> <ul> <li> <p>Added support for viewing and executing operators in the App! #2679</p> </li> <li> <p>Added support for creating dynamic groups in the App #2934</p> </li> <li> <p>Added support for overlaying multiple point cloud slices in Looker3D #2912</p> </li> <li> <p>Added support for customizing the App color scheme via a new color scheme modal #2824</p> </li> <li> <p>Added support for configuring field visibility in the App\u2019s sidebar #2924, #3024</p> </li> <li> <p>Added support for visualizing <code>Label</code> fields stored within top-level embedded document fields #2885</p> </li> <li> <p>Optimized App loading for datasets with large sample documents #3139</p> </li> <li> <p>Optimized App routes that involve synchronous computations #3066</p> </li> <li> <p>Fixed a URL filepath bug that could cause orthographic projections to fail to render #3122</p> </li> <li> <p>Fixed a layout bug when working with long brain keys in the Embeddings panel #3026</p> </li> <li> <p>Added a welcome message that displays when the App is launched for the first time with a new FiftyOne version #3092</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for creating dynamic grouped views #2475</p> </li> <li> <p>Added support for storing default color schemes for datasets #2824</p> </li> <li> <p>Added support for selecting/excluding fields via dynamically defined filters via <code>select_fields()</code> and <code>exclude_fields()</code> #2898</p> </li> <li> <p>Added support for evaluating keypoints #2776, #2928</p> </li> <li> <p>Added support for computing DICE score when evaluating segmentations #2777, #2901</p> </li> <li> <p>Added a new <code>list_schema()</code> aggregation for inferring the contents of nested list fields #2882</p> </li> <li> <p>Added support for declaring dynamic nested list fields #2882</p> </li> <li> <p>Handling missing label fields when deleting labels #2918</p> </li> <li> <p>Only match .txt files when reading YOLO labels #3127</p> </li> <li> <p>Improved behavior of <code>transform_images()</code> and <code>transform_videos()</code> utilities when processing media in-place #2931</p> </li> <li> <p>Added utils and helpful warnings that advise how to patch broken saved views and runs #2970, #2971</p> </li> <li> <p>Replaced <code>pkg_resources</code> with <code>importlib.metadata</code> #2930</p> </li> </ul> <p>Plugins</p> <ul> <li> <p>Added Operators to the plugin framework #2679</p> </li> <li> <p>Added CLI methods for plugins and operators #3025, #3038</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Added support for CVAT 2.4 #2959</p> </li> <li> <p>Added support for importing/exporting instances when using the Label Studio integration #2706, #2917</p> </li> <li> <p>Added support for importing multiclass classifications from Scale #3117</p> </li> <li> <p>Updated Scale integration to assume that imported line annotations are not closed shapes #3123</p> </li> <li> <p>Fixed broken Scale docs links and unlabeled annotation task support #2916</p> </li> </ul> <p>Zoo</p> <ul> <li>Added the Sama-COCO dataset to the zoo! #2904</li> </ul> <p>Tutorials</p> <ul> <li>Updated detection mistakes tutorial to avoid unnecessarily resetting the App #3034</li> </ul>"},{"location":"release-notes/#fiftyone-teams-121","title":"FiftyOne Teams 1.2.1 \u00b6","text":"<p>Released April 5, 2023</p> <p>Includes all features from FiftyOne 0.20.1, plus:</p> <p>General</p> <ul> <li> <p>When your session expires, you are now automatically logged out rather than being presented with a cryptic server error</p> </li> <li> <p>Improved the accuracy of size estimates when exporting filepaths and/or tags from the Teams UI</p> </li> </ul> <p>Admin</p> <ul> <li>Added support for uploading Azure storage credentials for your deployment via the <code>Settings &gt; Cloud storage</code> page</li> </ul> <p>SDK</p> <ul> <li>Added support for working with media in Azure cloud storage. Refer to this section to see how to provide your storage credentials</li> </ul> <p>Deployment</p> <ul> <li> <p>Added support for deploying into Microsoft Azure environments</p> </li> <li> <p>Fixed a bug that prevented the dataset page from loading for deployments running MongoDB 4.4</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0201","title":"FiftyOne 0.20.1 \u00b6","text":"<p>Released April 5, 2023</p> <p>App</p> <ul> <li> <p>Added support for storing datetimes as field metadata and viewing them in the App\u2019s field tooltip #2861</p> </li> <li> <p>Fixed a bug when pulling color-by data for sample embeddings plots when viewing patches in the sample grid #2846</p> </li> <li> <p>Fixed a bug that prevented the sample grid from refreshing when composing multiple sidebar filters #2849</p> </li> <li> <p>Fixed a bug that prevented field-specific mask targets from being recognized when rendering segmentations in the App #2879</p> </li> <li> <p>Fixed a bug when rendering heatmaps stored as images on disk #2872, #2880</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for dynamically inferring fields on embedded lists and documents #2863, #2882</p> </li> <li> <p>Added support for listing datasets matching a glob pattern #2868</p> </li> <li> <p>Improved the robustness of <code>merge_samples()</code> when cleaning up after a failed merge #2844</p> </li> <li> <p>Using new libraries for ndjson and archive extraction #2864</p> </li> <li> <p>Fixed a bug that prevented text similarity searches from succeeding when GPU is available #2853</p> </li> <li> <p>Fixed a bug where <code>stats()</code> would report the wrong size for dataset views that select/exclude fields on MongoDB 5.2 or later #2840</p> </li> <li> <p>Fixed a bug with dynamic schema expansion of list fields #2855</p> </li> <li> <p>Fixed a bug when merging video samples into a grouped dataset that did not previously contain videos #2851</p> </li> <li> <p>Fixed a validation bug when importing COCO datasets whose description is not a string #2848</p> </li> </ul> <p>Documentation</p> <ul> <li> <p>Updated the source URLs for the Caltech-101 and Caltech-256 datasets #2841</p> </li> <li> <p>Fixed a typo in the Caltech-256 dataset documentation #2842</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-12","title":"FiftyOne Teams 1.2 \u00b6","text":"<p>Released March 22, 2023</p> <p>Includes all features from FiftyOne 0.20.0, plus:</p> <p>Admin settings</p> <ul> <li> <p>Admins who use SSO to authorize new users to auto-join their FiftyOne Teams deployment can now configure the default role for those users</p> </li> <li> <p>Admins can now configure the default access level that Members receive on newly created datasets Dataset page</p> </li> </ul> <p>Dataset page</p> <ul> <li> <p>Added support for viewing Segmentation and Heatmap data stored as images in the cloud in the App</p> </li> <li> <p>Added support for exporting one or more fields of a dataset in CSV format through the Teams UI</p> </li> <li> <p>Stack traces for unhandled errors are now presented directly in the App so that users can self-diagnose issues</p> </li> </ul> <p>Deployment</p> <ul> <li>Added support for sharded databases</li> </ul>"},{"location":"release-notes/#fiftyone-0200","title":"FiftyOne 0.20.0 \u00b6","text":"<p>Released March 22, 2023</p> <p>News</p> <ul> <li> <p>Added support for querying by arbitrary text prompts in the App! #2633</p> </li> <li> <p>Released a Qdrant integration for native text and image searches on FiftyOne datasets!</p> </li> <li> <p>Released a Pinecone integration for native text and image searches on FiftyOne datasets!</p> </li> </ul> <p>App</p> <ul> <li> <p>Switched the default sidebar mode to <code>fast</code> #2714</p> </li> <li> <p>Refactored sample/label tags in the App so that they are treated the same as any other list field #2557</p> </li> <li> <p>Added support for visualizing orthographic projection images for point cloud datasets/slices #2660</p> </li> <li> <p>Added a filter/selection indicator to the title of all Panels that can be clicked to clear the Panel\u2019s current state #2652</p> </li> <li> <p>Any selection state associated with a Panel is now automatically cleared when the Panel is closed #2652</p> </li> <li> <p>Added a button to the saved view selector for clearing the current view #2661</p> </li> <li> <p>Added support for maximizing/hiding individual panels of the grouped modal #2688</p> </li> <li> <p>Added support for switching between multiple point cloud slices #2675</p> </li> <li> <p>Added keyboard shortcuts for opening Panels directly in split mode #2663</p> </li> <li> <p>Upgraded Looker3D controls #2753</p> </li> <li> <p>Upgraded the modal\u2019s JSON viewer #2677</p> </li> <li> <p>Selected labels are not reset after applying a similarity search #2820</p> </li> <li> <p>Stack traces for unhandled errors are now presented directly in the App so that users can self-diagnose issues #2795, #2797</p> </li> <li> <p>Improved error handling when loading invalid/missing brain results in the Embeddings panel #2651, #2790</p> </li> <li> <p>More intuitive behavior when combining Embedding panel selections and sidebar filters #2741</p> </li> <li> <p>Ensure that URL is updated when loading saved views via a Python session #2740</p> </li> <li> <p>Switched to wildcard-based string matching in the sidebar #2736</p> </li> <li> <p>Plugins can now load components and utilities from runtime instead of compiling their own #2680</p> </li> <li> <p>Stability improvements when loading and handling errors in plugins #2758</p> </li> <li> <p>Informative error messages are now displayed when visualization results fail to load in the Embeddings panel #2751</p> </li> <li> <p>Resolved some edge cases when loading views with different schemas via Python sessions #2730</p> </li> <li> <p>Fixed a bug that would cause saving views to intermittently fail #2667</p> </li> <li> <p>Fixed a bug when using saved views with Python &lt;3.9 in the App #2676, #2728</p> </li> <li> <p>Fixed a bug that could cause App crashes when loading <code>SelectGroupSlices</code> stages in the view bar #2669, #2743</p> </li> <li> <p>Fixed a bug that could cause App crashes when filtering keypoints #2774, #2779</p> </li> <li> <p>Fixed a bug when lassoing patch embeddings with the Map panel open #2754</p> </li> <li> <p>Fixed inconsistencies with selection, tagging, active slices, and sidebar stats in the modal for grouped datasets #2785, #2782, #2769, #2759, #2749, #2731</p> </li> <li> <p>Fixed a bug when pressing enter twice in a label tag popover #2757</p> </li> <li> <p>Fixed a bug where keyboard listeners in the modal would interfere with other input interactions #2786</p> </li> <li> <p>Fixed a bug where some users would see erroneous scrollbars #2794</p> </li> <li> <p>Fixed bugs when tagging labels in the grouped modal #2820</p> </li> <li> <p>Fixed a bug when retrieving values for filter dropdowns in the grouped modal #2817</p> </li> <li> <p>Fixed a bug that would raise an App error after deleting certain saved views #2801</p> </li> <li> <p>Fixed the formatting of the <code>support</code> field in the modal sidebar for clip views #2800</p> </li> <li> <p>Fixed a bug with URL rendering in the sidebar #2735</p> </li> <li> <p>Fixed a bug when streaming filtered frame labels #2682, #2733</p> </li> <li> <p>Fixed a bug when adding new tags to a selected sample or label #2703</p> </li> <li> <p>Fixed a bug when matching by tags that contain spaces #2658</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for querying by vectors and text prompts #2569</p> </li> <li> <p>Upgraded the similarity index interface, including Qdrant and Pinecone support, and the ability to add/remove embeddings to an existing index #2792</p> </li> <li> <p>Added support for storing and visualizing cuboids and rotated bounding boxes in the App #2296</p> </li> <li> <p>Added support for evaluating 3D object detections #2486</p> </li> <li> <p>Added a <code>to_trajectories()</code> view stage #1300</p> </li> <li> <p>Added support for generating orthographic projection images for point cloud datasets/slices #2656</p> </li> <li> <p>Added validation to <code>set_values()</code> #2770</p> </li> <li> <p>Frame collections are now lazily created only when necessary #2727</p> </li> <li> <p>Upgraded the document save implementation to only use upsert operations when explicitly required #2727</p> </li> <li> <p>Added <code>_dataset_id</code> to all sample/frame documents in datasets #2711</p> </li> <li> <p>Added a <code>save()</code> and <code>save_config()</code> methods to <code>RunResults</code> #2696, #2772</p> </li> <li> <p>Added support for renaming existing runs via new <code>rename_annotation_run()</code>, <code>rename_brain_run()</code>, and <code>rename_evaluation()</code> methods #2696</p> </li> <li> <p>Added support for filtering by run type and config parameters when using <code>list_annotation_runs()</code>, <code>list_brain_runs()</code>, and <code>list_evaluations()</code> #2696, #2772</p> </li> <li> <p>Added an <code>add_group_slice()</code> method to declare new slices on grouped datasets #2727</p> </li> <li> <p>Added support for controlling whether saved views and runs are imported/exported in FiftyOneDataset format #2806</p> </li> <li> <p>Added support for negative integer mask targets #2686</p> </li> <li> <p>Downward migrations for future-but-compatible versions of FiftyOne are now skipped rather than raising an error #2683</p> </li> <li> <p>Fixed a bug when cloning datasets with run results #2772</p> </li> <li> <p>Fixed a bug with the <code>dynamic=True</code> syntax for declaring dynamic fields on list documents #2767</p> </li> <li> <p>Fixed a bug in deferred saves where filtered list updates were not being applied #2727</p> </li> </ul> <p>Annotation</p> <ul> <li>Added support for passing CVAT organization to annotation jobs #2716</li> </ul> <p>Docs</p> <ul> <li> <p>Added documentation for working with point cloud-only datasets #2724</p> </li> <li> <p>Added documentation for on-the-fly custom embedded document creation #2687</p> </li> <li> <p>Fixed broken torchvision dataset links in the docs #2771</p> </li> </ul> <p>Zoo</p> <ul> <li>Added a <code>tensorflow-macos</code> option when loading TF models from the Model Zoo #2685</li> </ul> <p>Tutorials</p> <ul> <li> <p>Added a Point-E tutorial showcasing the 3D Visualizer\u2019s capabilities in the context of building a 3D self-driving dataset #2818</p> </li> <li> <p>Added a YOLOv8 tutorial #2755</p> </li> <li> <p>Updated the media in the Open Images tutorial #2665</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-111","title":"FiftyOne Teams 1.1.1 \u00b6","text":"<p>Released February 14, 2023</p> <p>Includes all features from FiftyOne 0.19.1, plus:</p> <p>Plugins</p> <ul> <li>Resolved a bug that prevented Teams deployments from recognizing installed plugins</li> </ul>"},{"location":"release-notes/#fiftyone-0191","title":"FiftyOne 0.19.1 \u00b6","text":"<p>Released February 14, 2023</p> <p>App</p> <ul> <li> <p>Fixed a bug when launching the App in Python 3.8 or earlier #2647</p> </li> <li> <p>Fixed a bug that prevented launching the App in Databricks notebooks #2647</p> </li> </ul> <p>Core</p> <ul> <li>Fixed a bug in certain environments that prevented progress bars from rendering correctly #2647</li> </ul>"},{"location":"release-notes/#fiftyone-teams-11","title":"FiftyOne Teams 1.1 \u00b6","text":"<p>Released February 9, 2023</p> <p>Includes all features from FiftyOne 0.19.0, plus:</p> <p>User roles</p> <ul> <li> <p>Renamed the existing Guest role to Collaborator</p> </li> <li> <p>Added a new Guest role. Note that Guest is a view-only role and does not contribute to your license count. You can add unlimited Guest users to your deployment!</p> </li> </ul> <p>Homepage</p> <ul> <li>Added a Recent views widget to the homepage that shows the most recent saved views that you have viewed in the Teams UI</li> </ul> <p>Dataset page</p> <ul> <li> <p>Added support for cloning the current view (including any filters, selections, etc) into a new dataset from the UI</p> </li> <li> <p>Added support for exporting the current view to local disk or a cloud bucket in various formats (filepaths only, filepaths and tags, media only, labels only, media and labels)</p> </li> </ul> <p>Deployment</p> <ul> <li>Added support for deploying Teams into environments with proxy networks</li> </ul>"},{"location":"release-notes/#fiftyone-0190","title":"FiftyOne 0.19.0 \u00b6","text":"<p>Released February 9, 2023</p> <p>News</p> <ul> <li>FiftyOne Teams documentation is now publicly available! #2388</li> </ul> <p>App</p> <ul> <li> <p>Added the Spaces framework #2524</p> </li> <li> <p>Added native support for visualizing embeddings #2524</p> </li> <li> <p>Refactored the map tab into a dedicated map panel #2524</p> </li> <li> <p>Refactored the histograms tab into a dedicated histograms panel #2524</p> </li> <li> <p>Added support for loading and saving views #2461</p> </li> <li> <p>Added support for visualizing <code>Segmentation</code> and <code>Heatmap</code> masks stored on disk #2358</p> </li> <li> <p>Added support for visualizing RGB segmentations #2483</p> </li> <li> <p>Added retries for all network requests to improve stability #2406</p> </li> <li> <p>Optimized the tagging menu #2368</p> </li> <li> <p>Optimized sample tagging on video datasets #2440</p> </li> <li> <p>Don\u2019t refresh the background grid when applying tags in the modal #2594</p> </li> <li> <p>Only show supported keys in the evaluations dropdown #2427</p> </li> <li> <p>Fixed handling of None values when filtering numeric/list fields #2422, #2412, #2403</p> </li> <li> <p>Never show expanded filter list for ID fields #2408</p> </li> <li> <p>Ensure that the bookmark icon displays when extended selections exist #2366</p> </li> <li> <p>Automatically clear sample selection after sorting by similarity #2595</p> </li> <li> <p>Use consistent loading dots throughout the App #2321</p> </li> <li> <p>Fixed a bug when filtering by custom embedded list fields #2407</p> </li> <li> <p>Fixed bugs when screenshotting the App in notebook contexts #2398</p> </li> <li> <p>Fixed bugs when launching the App in Databricks notebooks #2397</p> </li> <li> <p>Show metadata for frame-level fields in the fields tooltip #2386</p> </li> <li> <p>Fixed bugs when configuring plugin settings and modal media fields #2383</p> </li> <li> <p>Fixed bugs with multiple media fields when loading views that exclude fields #2378, #2303</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for programmatically configuring space layouts #2524</p> </li> <li> <p>Added support for loading and saving views #2461</p> </li> <li> <p>Added support for storing <code>Segmentation</code> and <code>Heatmap</code> masks on disk #2301</p> </li> <li> <p>Added support for RGB segmentations in <code>evaluate_segmentations()</code> #2483</p> </li> <li> <p>Added a new <code>transform_segmentations()</code> utility #2483</p> </li> <li> <p>Added support for declaring dynamic fields on generated views via <code>set_values()</code> #2513</p> </li> <li> <p>Added support for importing and exporting datasets in CSV format #2616, #2450</p> </li> <li> <p>Added support for importing and exporting directories of arbitrary media files #2605</p> </li> <li> <p>Added a dedicated <code>clear_cache()</code> method for clearing a dataset\u2019s run cache #2471</p> </li> <li> <p>Updated all plotting methods, eg <code>scatterplot()</code> to always rely on sample/label IDs when pulling data for plots #2614</p> </li> <li> <p>Updated <code>compute_patch_embeddings()</code> to store patch embeddings directly on <code>Label</code> objects when the <code>embeddings_field</code> argument is provided #2626</p> </li> <li> <p>Added support for passing frame-level fields directly to <code>export()</code> #2418</p> </li> <li> <p>Added an optional <code>dynamic=True</code> flag to <code>set_values()</code> #2372</p> </li> <li> <p>Added support for declaring custom <code>Label</code> attributes via <code>set_values()</code> #2372</p> </li> <li> <p>Adds a new <code>set_label_values()</code> utility for setting attributes on <code>Label</code> instances by their IDs #2372</p> </li> <li> <p>Always update dataset\u2019s <code>last_loaded_at</code> property when they are loaded #2375</p> </li> <li> <p>Migrated runs to a separate database collection, for efficiency #2189</p> </li> <li> <p>Added an <code>exact_frame_count()</code> utility for computing exact video frame counts #2373</p> </li> <li> <p>Updated the 3D visualizer to use true centroid (not bottom-center) coordinates for 3D detections #2474</p> </li> <li> <p>Added support for loading specific group slice(s) when using <code>iter_groups()</code> and <code>get_group()</code> #2528</p> </li> <li> <p>Added an <code>exclude_groups()</code> view stage #2451</p> </li> <li> <p>Added support for importing annotations directly on grouped datasets #2349</p> </li> <li> <p>Added a <code>group_collections()</code> utility for merging multiple collections into a grouped dataset #2332</p> </li> <li> <p>Added support for converting an existing dataset into a grouped dataset via <code>set_values()</code> #2332</p> </li> <li> <p>Added support for deleting grouped fields when the dataset contains only one media type #2332</p> </li> <li> <p>Updated <code>Dataset.stats()</code> to include media from all slices of grouped datasets #2635</p> </li> <li> <p>Fixed a bug when calling <code>to_frames()</code> on a view that filters the frames of the input dataset #2361</p> </li> <li> <p>Fixed some bugs when passing multiple aggregations with the same field name and type to <code>aggregate()</code> #2617</p> </li> <li> <p>Fixed a bug when manually unwinding list fields in aggregations #2608</p> </li> <li> <p>Fixed a bug when loading datasets with CVAT attributes stored in VOC format #2359</p> </li> <li> <p>Fixed a bug in default sidebar group expansion #2441</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Added support for CVAT 2.4 #2597</p> </li> <li> <p>Added support for providing custom task names for CVAT tasks #2353</p> </li> <li> <p>Fixed a bug when checking if CVAT projects exist #2491</p> </li> <li> <p>Fixed a bug when checking if CVAT tasks exist #2070</p> </li> </ul> <p>Zoo</p> <ul> <li> <p>Added Open Images V7 to the zoo #2446</p> </li> <li> <p>Updated the KITTI multiview and quickstart-groups datasets to not use legacy 3D visualizer settings #2474</p> </li> <li> <p>Added support for filtering datasets when using <code>list_zoo_datasets()</code> #2448</p> </li> </ul> <p>Docs</p> <ul> <li> <p>Added detailed plugin documentation #2442</p> </li> <li> <p>Added documentation for converting between common label formats #2498</p> </li> <li> <p>Added a pandas vs FiftyOne tutorial #2310</p> </li> <li> <p>Added a pandas vs FiftyOne cheat sheet #2329</p> </li> <li> <p>Added a FiftyOne terminology cheat sheet #2484</p> </li> <li> <p>Added a view stage cheat sheet #2452</p> </li> <li> <p>Added a filtering cheat sheet #2447</p> </li> </ul>"},{"location":"release-notes/#fiftyone-teams-10","title":"FiftyOne Teams 1.0 \u00b6","text":"<p>Released November 8, 2022</p> <p>Includes all features from FiftyOne 0.18.0, plus:</p> <p>News</p> <ul> <li>FiftyOne Teams is now generally available, read more here!</li> </ul>"},{"location":"release-notes/#fiftyone-0180","title":"FiftyOne 0.18.0 \u00b6","text":"<p>Released November 8, 2022</p> <p>App</p> <ul> <li> <p>Significantly optimized the performance of the sidebar by lazily computing statistics only for currently visible fields #2191</p> </li> <li> <p>Added new sidebar modes with updated default behavior that further optimizes the performance of the App for large datasets #2191</p> </li> <li> <p>Added support for configuring the sidebar mode dynamically in the App and programmatically on a per-dataset basis #2191</p> </li> <li> <p>Added support for programmatically configuring sidebar groups and default expansion states on a per-dataset basis #2190</p> </li> <li> <p>Added support for viewing field-level descriptions via a new field tooltip #2216</p> </li> <li> <p>Added support for filtering by and viewing stats for custom embedded document attributes #1825</p> </li> <li> <p>Added a new light mode option! #2156</p> </li> <li> <p>Improved responsiveness of the sidebar when toggling fields on and off #2247</p> </li> <li> <p>Improved responsiveness and state management of the view bar #2230</p> </li> <li> <p>Restored the ability to shift-select multiple samples in the grid view #2110</p> </li> <li> <p>Fixed an issue that could cause unselected label fields to be inadvertently tagged when using the label tagging UI #2121</p> </li> <li> <p>Fixed an issue that would prevent label tags applied on patch views in the tagging UI from persisting to the underlying dataset #2113</p> </li> <li> <p>Fixed an issue that could arise when loading a group dataset with sparse alternate media fields #2164</p> </li> <li> <p>Fixed some issues with datetime rendering and timezone handling #2111, #2112</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for declaring custom dynamic attributes on datasets! #1825</p> </li> <li> <p>Added support for storing field-level metadata on datasets #2216</p> </li> <li> <p>Added native support for installing on Apple Silicon with MongoDB 6 #2165</p> </li> <li> <p>Dataset creation using default naming is now multiprocess-safe #2097</p> </li> <li> <p>Optimized the implementation of tagging samples and labels #2203, #2208</p> </li> <li> <p>Optimized the implementation of <code>select()</code>, <code>select_by()</code>, and <code>select_groups()</code> when performing ordered selections #2227</p> </li> <li> <p>Updated the logic of <code>exists()</code> to be more intuitive for frame fields #2209</p> </li> <li> <p>Upgraded server and MongoDB requirements to <code>pymongo&gt;=3.11</code>, <code>motor&gt;=2.3</code> and newer pinned versions of <code>mongoengine</code>, <code>starlette</code>, and <code>strawberry-graphql</code> #2215</p> </li> <li> <p>Added support for modifying the filepaths of a frame view #2193</p> </li> <li> <p>Improved the implementation of <code>merge_samples()</code> and related methods to safely cleanup in case of failed merges #2135</p> </li> <li> <p>Fixed some bugs that could occur when creating frame views into grouped collections #2144</p> </li> <li> <p>Fixed a bug when using <code>select_by()</code> with <code>ObjectId</code> fields #2140</p> </li> <li> <p>Added an option to import annotation IDs when loading data stored in COCO format #2122</p> </li> <li> <p>Added support for including the export directory in the <code>dataset.yaml</code> file generated by YOLOv5 exports #2114</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Updated the default CVAT endpoint to https://app.cvat.ai #2228</p> </li> <li> <p>Fixed a bug that would cause annotation runs involving unlabeled samples to crash when using the Label Studio backend #2145</p> </li> </ul> <p>Zoo</p> <ul> <li>Added support for using CUDA devices when running the CLIP model from the zoo #2201</li> </ul>"},{"location":"release-notes/#fiftyone-0172","title":"FiftyOne 0.17.2 \u00b6","text":"<p>Released September 20, 2022</p> <p>App</p> <ul> <li>Fixed a backward compatibility bug when connecting to older database versions #2103</li> </ul>"},{"location":"release-notes/#fiftyone-0171","title":"FiftyOne 0.17.1 \u00b6","text":"<p>Released September 20, 2022</p> <p>Core</p> <ul> <li>Removed <code>TypedDict</code> usage introduced in v0.17.0 that is not supported in Python 3.7 #2100</li> </ul>"},{"location":"release-notes/#fiftyone-0170","title":"FiftyOne 0.17.0 \u00b6","text":"<p>Released September 19, 2022</p> <p>App</p> <ul> <li> <p>Added support for visualizing grouped datasets in the App #1765</p> </li> <li> <p>Added support for visualizing point cloud samples in the modal #1765</p> </li> <li> <p>Added support for visualizing and interacting with <code>GeoLocation</code> data in a new Map panel #1976</p> </li> <li> <p>Added initial support for custom App plugins #1765</p> </li> <li> <p>Added support for configuring multiple media fields #1765</p> </li> <li> <p>Fixed Google Colab screenshotting and cell updates #2069</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for grouped datasets, e.g., multiple camera view scenes #1765</p> </li> <li> <p>Added support for point cloud samples in grouped datasets #1765</p> </li> <li> <p>Added an <code>app_config</code> property to datasets for configuring App behavior on a per-dataset basis #1765</p> </li> <li> <p>Added an optional <code>rel_dir</code> parameter to <code>export()</code> and <code>draw_labels()</code> #2060</p> </li> <li> <p>Added an optional <code>abs_paths=True</code> option to <code>export()</code> #2060</p> </li> <li> <p>Added an optional <code>use_dirs=True</code> parameter to <code>export()</code> that causes metadata to be exported in per-sample/frame JSON files #2028</p> </li> <li> <p>Updated the COCO importer to load all available label types by default #1869</p> </li> <li> <p>Fixed a bug when passing <code>ordered=True</code> to <code>select_by()</code> #2059</p> </li> <li> <p>Fixed an error that would occur when storing custom embedded documents on dynamic label attributes #2051</p> </li> <li> <p>Fixed a <code>match_frames()</code> bug that caused all frames to be included, even if the view filters the frames #2029</p> </li> </ul> <p>Docs</p> <ul> <li>Added a tutorial showing how to integrate FiftyOne into a Detectron2 model training pipeline #2054</li> </ul> <p>Annotation</p> <ul> <li> <p>Fixed a bug that occurred when checking if tasks exist on CVAT v2 servers #2070</p> </li> <li> <p>Fixed an error that occurred when deserializing Label Studio annotation results #2074</p> </li> </ul> <p>Zoo</p> <ul> <li> <p>Added clip-vit-base32-torch to the model zoo! #2072</p> </li> <li> <p>Added the Quickstart Groups dataset to the dataset zoo! #1765</p> </li> <li> <p>Added the KITTI Multiview dataset to the dataset zoo! #1765</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0166","title":"FiftyOne 0.16.6 \u00b6","text":"<p>Released August 25, 2022</p> <p>App</p> <ul> <li> <p>Fixed a bug that caused the App to break when sample tags contained <code>.</code> #1924</p> </li> <li> <p>Fixed search results alignment #1930</p> </li> <li> <p>Fixed App refreshes after view changes had occurred from the view bar #1931</p> </li> <li> <p>Fixed mask targets rendering in the tooltip #1943 #1949</p> </li> <li> <p>Fixed classification confusion matrix connections #1967</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added a save context that enables efficient batch edits of datasets and views #1727</p> </li> <li> <p>Added Plotly v5 support #1981</p> </li> <li> <p>Added a quantiles aggregation #1937</p> </li> <li> <p>Added support for writing transformed images/videos to new locations in the <code>transform_images()</code> and <code>transform_videos()</code> functions #2007</p> </li> <li> <p>Added support for configuring the package-wide logging level #2009</p> </li> <li> <p>Added more full-featured support for serializing and deserializing datasets, views, and samples via <code>to_dict()</code> and <code>from_dict()</code> #1922</p> </li> <li> <p>Added support for dynamic attributes when performing coerced exports #1993</p> </li> <li> <p>Introduced the notion of client compatibility versions #2017</p> </li> <li> <p>Extended <code>stats()</code> to all sample collections #1940</p> </li> <li> <p>Added support for serializing aggregations #1911</p> </li> <li> <p>Added <code>weighted_sample()</code> and <code>balanced_sample()</code> utility methods #1925</p> </li> <li> <p>Added an optional <code>new_ids=True</code> option to <code>Dataset.add_collection()</code> that generates new sample/frame IDs when adding the samples #1927</p> </li> <li> <p>Added support for the <code>path</code> variable in <code>dataset.yaml</code> of YOLOv5 datasets #1903</p> </li> <li> <p>Fixed a bug that prevented using <code>set_values()</code> to set frame-level label fields #1922</p> </li> <li> <p>Fixed automatic declaration of frame fields when computing embeddings on a frame view #1922</p> </li> <li> <p>Fixed a regression that caused label ID fields to be returned as <code>ObjectID</code> #1922</p> </li> <li> <p>Fixed a bug that allowed default frame fields to be excluded #1922</p> </li> <li> <p><code>ClipsView</code> instances will now report their <code>metadata</code> type as <code>VideoMetadata</code> #1922</p> </li> <li> <p>Fixed <code>load_evaluation_view()</code> when <code>select_fields</code> is <code>True</code> #1922</p> </li> <li> <p>Fixed boolean field parsing when declaring fields #1922</p> </li> <li> <p>Fixed a bug that caused nested embedded documents to corrupt datasets #1922</p> </li> <li> <p>Fixed a bug that prevented assignment of array-valued dynamic attributes to labels #1922</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Added a new Label Studio integration! #1848</p> </li> <li> <p>Optimized loading CVAT annotations and performing operations on <code>CVATAnnotationResults</code> #1944</p> </li> <li> <p>Upgraded the <code>AnnotationAPI</code> interface #1997</p> </li> <li> <p>Fixed loading group IDs in CVAT video tasks #1917</p> </li> <li> <p>Fixed uploading to a CVAT project when no label schema is provided #1926</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0165","title":"FiftyOne 0.16.5 \u00b6","text":"<p>Released June 24, 2022</p> <p>App</p> <ul> <li> <p>Fixed dataset selection searches #1907</p> </li> <li> <p>Fixed dataset results for long dataset names #1907</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0164","title":"FiftyOne 0.16.4 \u00b6","text":"<p>Released June 21, 2022</p> <p>App</p> <ul> <li>Fixed frame fields omission in the sidebar #1899</li> </ul>"},{"location":"release-notes/#fiftyone-0163","title":"FiftyOne 0.16.3 \u00b6","text":"<p>Released June 20, 2022</p> <p>App</p> <ul> <li> <p>Added hotkey to hide overlays while pressed #1779</p> </li> <li> <p>Changed expanded view ESC sequence to reset zoom before frame scrubbing #1810</p> </li> <li> <p>Fixed the expanded view tooltip when a keypoint has <code>nan</code> point(s) #1828</p> </li> <li> <p>Fixed initial loading of keypoint skeletons #1828</p> </li> <li> <p>Fixed <code>Classifications</code> rendering in the grid #1828</p> </li> <li> <p>Fixed App loads for environments with old ( <code>&lt;=v0.14.0</code>) datasets that have yet to be migrated #1829</p> </li> <li> <p>Fixed spurious loading states from tagging in the expanded view #1834</p> </li> <li> <p>Fixed a bug that caused frame classifications to be incorrectly rendered in the grid #1877</p> </li> <li> <p>Fixed active (checked) field persistence in the grid when changing views #1878</p> </li> <li> <p>Fixed views and actions that contain <code>BSON</code> #1879</p> </li> <li> <p>Fixed <code>JSON</code> rendering in the expanded view for nested data #1880</p> </li> <li> <p>Fixed selection and expansion for bad media files #1882</p> </li> <li> <p>Fixed <code>Other</code> plot tab <code>date</code> and <code>datetime</code> fields with <code>None</code> values #1817</p> </li> <li> <p>Increased results from 10 to 200 for search selectors #1875</p> </li> <li> <p>Fixed App issues related to dataset deletion and dataset schema changes #1875</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added <code>skeleton</code> and <code>skeleton_key</code> to the OpenLABEL image and video importers #1812</p> </li> <li> <p>Fixed a database field issue in <code>clone_frame_field()</code> and <code>clone_sample_field()</code>, #1824</p> </li> <li> <p>Fixed using zoo models with the newest version of Torchvision #1838</p> </li> <li> <p>Added <code>classifications_to_detections()</code> for converting classifications to detections #1842</p> </li> <li> <p>Set forking as the default for macOS multiprocessing #1844</p> </li> <li> <p>Added <code>dataset.tags</code> for organizing datasets #1845</p> </li> <li> <p>Added functionality to explicitly define classes for evaluation methods #1858</p> </li> <li> <p>Fixed <code>tfrecord</code> shard enumeration, i.e. zero indexing #1859</p> </li> <li> <p>Added support for label field dicts when importing labeled datasets #1864</p> </li> <li> <p>Removed non-XML or non-TXT files from CVAT, KITTI, CVATVideo #1884</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Updated CVAT task and project processing #1839</p> </li> <li> <p>Added the ability to upload and download group ids from CVAT #1876</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0162","title":"FiftyOne 0.16.2 \u00b6","text":"<p>Released June 2, 2022</p> <p>App</p> <ul> <li> <p>Added explicit error handling when <code>FFmpeg</code> is installed so it is made clear to the user that it must be installed to use video datasets in the App #1801</p> </li> <li> <p>Fixed range requests for media files, e.g. mp4s, on the server #1786</p> </li> <li> <p>Fixed tag rendering in the grid #1808</p> </li> <li> <p>Fixed tagging selected labels in the expanded view #1808</p> </li> <li> <p>Fixed <code>session.view = None</code> #1808</p> </li> <li> <p>Fixed issues with patches views #1808</p> </li> </ul> <p>Core</p> <ul> <li>Fixed errors related to session-attached plots #1808</li> </ul>"},{"location":"release-notes/#fiftyone-0161","title":"FiftyOne 0.16.1 \u00b6","text":"<p>Released May 26, 2022</p> <p>App</p> <ul> <li> <p>Fixed a bug that caused label rendering to be delayed until statistics were loaded #1776</p> </li> <li> <p>Fixed the <code>v0.16.0</code> migration that prevents label lists, e.g. <code>Detections</code> from showing their label filters when expanded in the sidebar #1785</p> </li> <li> <p>Fixed expanded samples in clips views which appeared to be empty #1790</p> </li> <li> <p>Fixed \u201cSort by similarity\u201d with a <code>dist_field</code> #1790</p> </li> <li> <p>Fixed \u201cColor by\u201d for simple values (classifications, tags, etc.) #1790</p> </li> <li> <p>Fixed changing datasets when sort by similarity is set #1790</p> </li> <li> <p>Fixed mask and map coloring #1790</p> </li> <li> <p>Fixed fortran array handling for masks and maps #1790</p> </li> </ul> <p>Core</p> <ul> <li>Fixed a formatting issue when raising an exception because unsupported plotting backend was requested #1794</li> </ul>"},{"location":"release-notes/#fiftyone-0160","title":"FiftyOne 0.16.0 \u00b6","text":"<p>Released May 24, 2022</p> <p>App</p> <ul> <li> <p>Added routing, e.g. <code>/datasets/:dataset-name</code> #1713</p> </li> <li> <p>Redesigned the sidebar to support custom grouping and sorting of fields and tags #1713</p> </li> <li> <p>Added graceful handling of deleted datasets in the App #1713</p> </li> <li> <p>Fixed epoch rendering #1713</p> </li> <li> <p>Fixed empty heatmap rendering #1713</p> </li> <li> <p>Added stack traces to the new error page #1713</p> </li> <li> <p>Fixed <code>ESC</code> when viewing single frame clips #1713</p> </li> <li> <p>Fixed handling of unsupported videos #1713</p> </li> <li> <p>Added support for opening the expanded view while sample(s) are selected #1713</p> </li> <li> <p>Fixed keypoint skeleton rendering for named skeletons of frame fields #1713</p> </li> </ul> <p>Core</p> <ul> <li> <p>Fixed edge cases in <code>clone_frame_field()</code>, <code>merge_samples()</code>, and <code>rename_frame_field()</code> #1749</p> </li> <li> <p>Fixed a bug that would cause non-persistent datasets to be prematurely deleted #1747</p> </li> <li> <p>Fixed loading relative paths in YOLOv5 format #1721</p> </li> <li> <p>Fixed image lists for the <code>image_path</code> parameter when importing GeoTIFF datasets #1728</p> </li> <li> <p>Added a <code>find_duplicates()</code> utility to automatically find duplicate objects based on IoU #1714</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0151","title":"FiftyOne 0.15.1 \u00b6","text":"<p>Released April 26, 2022</p> <p>App</p> <ul> <li> <p>Added support for rendering keypoint skeletons #1601</p> </li> <li> <p>Added support for rendering per-point confidences and other custom per-point attributes on <code>Keypoint</code> objects #1601</p> </li> <li> <p>Added support for rendering Fortan-ordered arrays #1660</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for storing keypoint skeletons on datasets #1601</p> </li> <li> <p>Added a <code>filter_keypoints()</code> stage that applies per- <code>point</code> filters to <code>Keypoint</code> objects #1601</p> </li> <li> <p>Added support for rendering keypoints skeletons and missing keypoints to <code>draw_labels()</code> #1601</p> </li> <li> <p>Added support for per-point confidences and other custom per-point attributes on <code>Keypoint</code> objects. See this section for details #1601</p> </li> <li> <p>Added a <code>concat()</code> view stage that allows for concatenating one collection onto another #1662</p> </li> <li> <p>Non-persistent datasets are now automatically deleted when using a custom <code>database_uri</code> #1697</p> </li> <li> <p>Added a <code>database_admin</code> config setting that can control whether database migrations are allowed. See this page for details #1692</p> </li> <li> <p>Added a <code>database_name</code> config setting that allows for customizing the MongoDB database name #1692</p> </li> <li> <p><code>Classification</code> attributes are now exported as tag attributes when exporting in CVATImageDataset format #1686</p> </li> <li> <p>The <code>iscrowd</code> attribute is now always populated when exporting in COCO format #1664</p> </li> <li> <p>Fixed a <code>KeyError</code> bug when loading dataset with relative paths on Windows #1675</p> </li> </ul> <p>Brain</p> <ul> <li> <p>Added <code>fiftyone-brain</code> wheels for Python 3.10</p> </li> <li> <p>Added support for installing <code>fiftyone-brain</code> on Apple Silicon</p> </li> </ul> <p>Annotation</p> <ul> <li>Fixed a <code>CSRF Failed</code> error when connecting to some CVAT servers #1668</li> </ul> <p>Integrations</p> <ul> <li>Updated the Lightning Flash integration to support Flash versions 0.7.0 or later #1671</li> </ul> <p>Zoo</p> <ul> <li>Added the Families in the Wild dataset to the FiftyOne Dataset Zoo! #1663</li> </ul>"},{"location":"release-notes/#fiftyone-0150","title":"FiftyOne 0.15.0 \u00b6","text":"<p>Released March 23, 2022</p> <p>App</p> <ul> <li>Fixed <code>Regression</code> rendering in the visualizer #1604</li> </ul> <p>Core</p> <ul> <li> <p>Added a <code>Dataset.delete_frames()</code> method that allows for deleting frames by ID #1650</p> </li> <li> <p>Added a <code>keep_fields()</code> method to <code>DatasetView</code> and its subclasses #1616</p> </li> <li> <p>Added a <code>lines()</code> method that allows for plotting lines whose scatter points can be interactively selected via the typical interactive plotting workflows #1614</p> </li> <li> <p>Added an optional <code>force_rgb=True</code> syntax when importing/exporting/creating TF records using all relevant methods in <code>fiftyone.utils.tf</code> #1612</p> </li> <li> <p>Added support for passing additional kwargs to the <code>fiftyone convert</code> CLI command #1612</p> </li> <li> <p>Added support for annotating video-level labels when using <code>draw_labeled_videos()</code> #1619</p> </li> <li> <p>Added the ability to slice using a <code>ViewField</code> #1630</p> </li> <li> <p>Fixed bug in <code>from_images_dir()</code> where attempting to load 4-channel images errored even if <code>force_rgb=True</code> #1632</p> </li> <li> <p>Fixed a bug that prevented frames from being attached to video collections when aggregating expressions that involve both <code>Sample</code>-level and <code>Frame</code>-level fields #1644</p> </li> <li> <p>Added support for importing image and video datasets in OpenLABEL format #1609</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Added support for CVATv2 servers when using the CVAT backend #1638</p> </li> <li> <p>Added an <code>issue_tracker</code> argument to <code>annotate()</code> when using the CVAT backend #1625</p> </li> <li> <p>Added a <code>dest_field</code> argument to <code>load_annotations()</code> which allows you to specify the name of the field to which to load annotations #1642</p> </li> <li> <p>Added a property to annotation backends that decides whether to allow annotation of video-level labels #1655</p> </li> <li> <p>Fixed a bug where views that dynamically modify label strings would result in labels not being uploaded to the annotation backend #1647</p> </li> </ul> <p>Docs</p> <ul> <li> <p>Added documentation for defining custom <code>EmbeddedDocument</code> and <code>DynamicEmbeddedDocument</code> classes #1617</p> </li> <li> <p>Added documentation about boolean view indexing to the user guide #1617</p> </li> <li> <p>Added a recipe for creating views and view expressions #1641</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0144","title":"FiftyOne 0.14.4 \u00b6","text":"<p>Released February 7, 2022</p> <p>News</p> <ul> <li>With support from the ActivityNet team, FiftyOne is now a recommended tool for downloading, visualizing, and evaluating on the Activitynet dataset! Check out this guide for more details</li> </ul> <p>App</p> <ul> <li> <p>Fixed encoding of sample media URLs so image and video filepaths with special characters are supported</p> </li> <li> <p>Fixed an error that would occur when rendering empty <code>Keypoint</code> instances</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added an official Dockerfile</p> </li> <li> <p>Changed the default implementation of <code>to_frames()</code> to assume that the user has already sampled the frames offline and stored their locations in a <code>filepath</code> field of each <code>Frame</code> in their video dataset. See this section for more details</p> </li> <li> <p>Updated <code>DatasetView.save()</code> to save changes to (only) the samples in the view to the underlying dataset</p> </li> <li> <p>Added a new <code>DatasetView.keep()</code> method that deletes any samples that are not in the view from the underlying dataset</p> </li> <li> <p>Added <code>InteractivePlot.save()</code> and <code>ViewPlot.save()</code> methods that can be used to save plots as static images</p> </li> <li> <p>Added support for populating query distances on a dataset when using <code>sort_by_similarity()</code> to query by similarity</p> </li> <li> <p>Added a <code>instances_to_polylines()</code> utility that converts instance segmentations to <code>Polylines</code> format</p> </li> <li> <p>Added support for frame labels to all conversion methods in the <code>fiftyone.utils.labels</code> module</p> </li> <li> <p>Updated the implementation of <code>Detection.to_polyline()</code> so that all attributes are included rather than just ETA-supported ones</p> </li> <li> <p>Added support for including empty labels labels via an <code>include_missing</code> keyword argument in <code>add_yolo_labels()</code></p> </li> <li> <p>Added a <code>download_youtube_videos()</code> utility for efficiently and robustly downloading videos or specific segments from YouTube</p> </li> <li> <p>Added a <code>skip_failures</code> flag to <code>transform_images()</code> and <code>transform_videos()</code></p> </li> <li> <p>Added <code>shuffle</code> and <code>seed</code> parameters to <code>FiftyOneImageLabelsDatasetImporter</code> and <code>FiftyOneVideoLabelsDatasetImporter</code></p> </li> <li> <p>Added an <code>include_all_data</code> parameter to <code>YOLOv5DatasetImporter</code></p> </li> <li> <p>Resolved a bug that would previously cause an error when writing aggregations on video datasets that involve applying expressions directly to <code>\"frames\"</code></p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Added support for importing and exporting sample-level tags in CVAT format</p> </li> <li> <p>Fixed a bug that prevented existing label fields such as <code>Detections</code> that can contain multiple annotation types (boxes or instances) from being specified in calls to <code>annotate()</code></p> </li> <li> <p>CVAT login credentials are no longer included in exception messages</p> </li> </ul> <p>Zoo</p> <ul> <li> <p>Added ActivityNet 100 to the dataset zoo!</p> </li> <li> <p>Added ActivityNet 200 to the dataset zoo!</p> </li> <li> <p>Added Kinetics 400 to the dataset zoo!</p> </li> <li> <p>Added Kinetics 600 to the dataset zoo!</p> </li> <li> <p>Added Kinetics 700 to the dataset zoo!</p> </li> <li> <p>Added Kinetics 700-2020 to the dataset zoo!</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0143","title":"FiftyOne 0.14.3 \u00b6","text":"<p>Released January 13, 2022</p> <p>Core</p> <ul> <li> <p>Added hollow support for 32-bit systems (a database_uri must be used in such cases)</p> </li> <li> <p>Added support for indexing into datasets using boolean arrays or view expressions via new <code>dataset[bool_array]</code> and <code>dataset[bool_expr]</code> syntaxes</p> </li> <li> <p>Added support for registering custom <code>EmbeddedDocument</code> classes that can be used to populate fields and embedded fields of datasets</p> </li> <li> <p>Added support for importing and exporting <code>confidence</code> in YOLO formats</p> </li> <li> <p>Added support for directly passing a <code>filename -&gt; filepath</code> mapping dict to the <code>data_path</code> parameter to dataset importers</p> </li> <li> <p>Added graceful casting of <code>int</code>-like and <code>float</code>-like values like <code>np.float(1.0)</code> to their respective Python primitives for storage in the database</p> </li> <li> <p>Changed the default to <code>num_workers=0</code> when using methods like <code>apply_model()</code> to apply Torch models on Windows, which avoids multiprocessing issues</p> </li> <li> <p>Fixed a bug when calling <code>evaluate_detections()</code> with both the <code>classes</code> and <code>compute_mAP=True</code> arguments provided</p> </li> <li> <p>Fixed a bug that could arise when importing segmentation data from a COCO JSON that contains objects with <code>[]</code> segmentation data</p> </li> <li> <p>Fixed a bug in expressions containing near-epoch dates</p> </li> <li> <p>Added support for setting frame-level fields by passing frame number dicts to <code>set_values()</code></p> </li> <li> <p>Fixes a bug that prevented <code>set_values()</code> from working as expected when <code>key_field=\"id\"</code> argument is used</p> </li> <li> <p>Fixed a bug that occurred when computing patch embeddings defined by polylines</p> </li> <li> <p>Added decision thresholds to the tooltips of PR/ROC curves plotted via the following methods:</p> </li> <li> <p><code>BinaryClassificationResults.plot_pr_curve()</code></p> </li> <li> <p><code>BinaryClassificationResults.plot_roc_curve()</code></p> </li> <li> <p><code>COCODetectionResults.plot_pr_curves()</code></p> </li> <li> <p><code>OpenImagesDetectionResults.plot_pr_curves()</code></p> </li> </ul> <p>Brain</p> <ul> <li> <p>Graceful handling of missing/uncomputable embeddings in <code>compute_uniqueness()</code></p> </li> <li> <p>Graceful handling of edge cases like <code>fraction &lt;= 0</code> in <code>find_duplicates()</code>,</p> </li> <li> <p>Removed a spurious warning message that was previously logged when computing patch embeddings for a collection containing samples with no patches</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Added a new Labelbox integration!</p> </li> <li> <p>Added an <code>import_annotations()</code> method for importing existing CVAT projects or task(s) into FiftyOne</p> </li> <li> <p>Added support for configuring the size of CVAT tasks via a new <code>task_size</code> parameter</p> </li> <li> <p>Added graceful handling of deleted tasks when importing annotations from CVAT via <code>load_annotations()</code></p> </li> <li> <p>Added an <code>unexpected</code> parameter that provides a variety of options for handling unexpected annotations returned by the CVAT API</p> </li> <li> <p>Added support for passing request headers to the CVAT API</p> </li> <li> <p>Fixed a bug that occurred when importing single frame track segments from CVAT</p> </li> </ul> <p>Zoo</p> <ul> <li> <p>Fixed a regression in <code>fiftyone==0.14.1</code> that prevented zoo datasets that use the Torch backend from being downloaded</p> </li> <li> <p>Added the following TF2 models to the Model Zoo!</p> </li> <li> <p>centernet-hg104-1024-coco-tf2</p> </li> <li> <p>centernet-resnet101-v1-fpn-512-coco-tf2</p> </li> <li> <p>centernet-resnet50-v2-512-coco-tf2</p> </li> <li> <p>centernet-mobilenet-v2-fpn-512-coco-tf2</p> </li> <li> <p>efficientdet-d0-512-coco-tf2</p> </li> <li> <p>efficientdet-d1-640-coco-tf2</p> </li> <li> <p>efficientdet-d2-768-coco-tf2</p> </li> <li> <p>efficientdet-d3-896-coco-tf2</p> </li> <li> <p>efficientdet-d4-1024-coco-tf2</p> </li> <li> <p>efficientdet-d5-1280-coco-tf2</p> </li> <li> <p>efficientdet-d6-1280-coco-tf2</p> </li> <li> <p>efficientdet-d7-1536-coco-tf2</p> </li> <li> <p>ssd-mobilenet-v2-320-coco17</p> </li> <li> <p>ssd-mobilenet-v1-fpn-640-coco17</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0142","title":"FiftyOne 0.14.2 \u00b6","text":"<p>Released November 24, 2021</p> <p>App</p> <ul> <li>Improved mask loading times for <code>Segmentation</code>, <code>Heatmap</code>, and <code>Detection</code> labels with instance masks</li> </ul> <p>Core</p> <ul> <li> <p>Optimized image metadata calculation to read only the bare minimum byte content of each image</p> </li> <li> <p>Improved handling of relative paths and user paths in config settings and environment variables</p> </li> <li> <p>Optimized database I/O and improved the helpfulness of warnings/errors that are generated when applying models via <code>apply_model()</code>, <code>compute_embeddings()</code>, and <code>compute_patch_embeddings()</code></p> </li> <li> <p>Resolved a memory leak that could occur when computing predictions/embeddings for very large datasets with Torch models</p> </li> </ul> <p>Brain</p> <ul> <li> <p>Added the <code>points</code> keyword argument to <code>compute_visualization()</code> for providing your own manually computed low-dimensional representation for use with interactive embeddings plots</p> </li> <li> <p>Graceful handling of missing/uncomputable embeddings in <code>compute_visualization()</code> and <code>compute_similarity()</code></p> </li> <li> <p>Added checks that occur at the start of all methods to ensure that any required dependencies are installed prior to performing any expensive computations</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Changed CVAT uploads to retain original filenames</p> </li> <li> <p>A helpful error is now raised when the <code>\"frames.\"</code> prefix is omitted from label fields when requesting spatial annotations on video datasets</p> </li> </ul> <p>Zoo</p> <ul> <li>Patched an issue that prevented downloading the VOC-2007 and VOC-2012 datasets from the zoo</li> </ul>"},{"location":"release-notes/#fiftyone-0141","title":"FiftyOne 0.14.1 \u00b6","text":"<p>Released November 15, 2021</p> <p>App</p> <ul> <li> <p>Optimized grid loading for collections that do not have metadata computed</p> </li> <li> <p>Fixed filtering by label for Colab notebooks</p> </li> <li> <p>Fixed a bug where the App would crash if an image or video MIME type could not be inferred from the filepath, e.g. without an extension</p> </li> <li> <p>Fixed first pixel coloring for segmentations</p> </li> <li> <p>Added graceful handling of nonfinites ( <code>-inf</code>, <code>inf</code>, and <code>nan</code>)</p> </li> </ul> <p>Core</p> <ul> <li> <p>Fixed <code>clone()</code> for views with a parent dataset that has brain runs</p> </li> <li> <p>Fixed sampling frames when using <code>to_frames()</code></p> </li> <li> <p>Fixed importing of <code>FiftyOneDataset</code> with run results</p> </li> <li> <p>Added a <code>Regression</code> label type</p> </li> <li> <p>Added a <code>random_split()</code> method</p> </li> <li> <p>Added support for negating <code>match_labels()</code> queries</p> </li> <li> <p>Added a <code>MaxResize</code> transform</p> </li> <li> <p>Added <code>image_max_size</code> and <code>image_max_dim</code> parameters to <code>TorchImageModelConfig</code></p> </li> <li> <p>Added support for non-sequential updates in <code>set_values()</code></p> </li> <li> <p>Added a <code>compute_max_ious()</code> utility</p> </li> <li> <p>Added support for labels-only exports when working with <code>YOLOv4Dataset</code> and <code>YOLOv5Dataset</code> formats</p> </li> <li> <p>Added <code>fiftyone.utils.beam</code> for parallel import, merge, and export operations with Apache Beam</p> </li> <li> <p>Added an <code>add_yolo_labels()</code> utility that provides support for adding YOLO-formatted model predictions to an existing dataset</p> </li> <li> <p>Added support for importing/exporting multilabel classifications when using FiftyOneImageClassificationDataset format</p> </li> <li> <p>Fixed the <code>force_reencode</code> flag for <code>reencode_videos()</code></p> </li> <li> <p>Converted COCO and Open Images dataset downloads to use multithreading rather than multiprocessing</p> </li> <li> <p>Updated evaluation confusion matrices to always include rows and columns for missing/other</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Added support for annotating multiple label fields in one CVAT task</p> </li> <li> <p>Added an <code>allow_index_edits</code> parameter to <code>annotate()</code> for disallowing video track index changes</p> </li> <li> <p>Improved label ID tracking in CVAT by leveraging CVAT\u2019s server IDs in addition to <code>label_id</code> attributes</p> </li> <li> <p>Fixed a bug when annotating videos in CVAT with <code>None</code> label fields</p> </li> <li> <p>Fixed a bug when annotating new fields in CVAT</p> </li> <li> <p>Fixed a bug when annotating non-continuous tracks in CVAT</p> </li> <li> <p>Fixed a bug when annotating a track in CVAT that is present on the last frame of a video</p> </li> <li> <p>Fixed a bug when annotating with <code>allow_additions=False</code></p> </li> </ul> <p>Docs</p> <ul> <li> <p>Added a section on adding model predictions to existing datasets to the user guide</p> </li> <li> <p>Added explicit examples of labels-only imports and exports for all relevant datasets to the docs</p> </li> <li> <p>Documented how class lists are computed when exporting in formats like YOLO and COCO that require explicit class lists</p> </li> <li> <p>Documented the supported label types for all exporters</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0140","title":"FiftyOne 0.14.0 \u00b6","text":"<p>Released October 15, 2021</p> <p>App</p> <ul> <li> <p>Added support for visualizing heatmaps using either transparency or a customizable colorscale</p> </li> <li> <p>Added a label opacity slider in both the sample grid and the expanded sample view</p> </li> <li> <p>Added support for visualizing clips views</p> </li> <li> <p>Added support for rendering and filtering <code>DateField</code> and <code>DateTimeField</code> data</p> </li> <li> <p>Improved error handling in the grid and when streaming frames</p> </li> <li> <p>Fixed a bug that caused incorrect label rendering for sparse frame labels in the video visualizer</p> </li> <li> <p>Added a <code>default_app_address</code> setting to the FiftyOne config for restricting sessions to a hostname. See this page for more details</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added a Heatmap label type</p> </li> <li> <p>Added support for adding date and datetime fields to FiftyOne datasets</p> </li> <li> <p>Added the <code>to_clips()</code> method for creating clips views into video datasets</p> </li> <li> <p>Added clip views sections to the App user guide page and dataset views user guide page</p> </li> <li> <p>Added support for exporting video clips in labeled video formats</p> </li> <li> <p>Added a <code>trajectories=True</code> flag to <code>filter_labels()</code> that allows for matching entire object trajectories for which a given filter matches the object in at least one frame of the video</p> </li> <li> <p>Added set operations <code>is_subset()</code>, <code>set_equals()</code>, <code>unique()</code>, <code>union()</code>, <code>intersection()</code>, <code>difference()</code>, and <code>contains(all=True)</code> to the view expression API</p> </li> <li> <p>Added date operations <code>to_date()</code>, <code>millisecond()</code>, <code>second()</code>, <code>minute()</code>, <code>hour()</code>, <code>day_of_week()</code>, <code>day_of_month()</code>, <code>day_of_year()</code>, <code>month()</code>, and <code>year()</code> to the view expression API</p> </li> <li> <p>Missing ground truth/predictions are now included by default when viewing confusion matrices for detection tasks</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Added support for specifying per-class attributes when defining a label schema for an annotation task</p> </li> <li> <p>Added support for specifying whether labels can be added, deleted or moved and whether certain label attributes are read-only when configuring an annotation task</p> </li> <li> <p>Added support for respecting keyframe information when adding or editing video annotations</p> </li> <li> <p>Fixed a 0-based versus 1-based frame numbering bug when importing and exporting labels in CVAT video format</p> </li> <li> <p>Added support for adding/editing bounding box shapes (not tracks) if desired when annotating video frames using the CVAT backend</p> </li> <li> <p>Fixed a bug that prevented importing of video annotations from the CVAT backend that involved the splitting or merging of object tracks</p> </li> <li> <p>Added a <code>project_name</code> parameter that allows for creating annotation tasks within a new project when using the CVAT backend</p> </li> <li> <p>Added support for specifying a list of task assignees when creating video annotation tasks (which generate one task per video) using the CVAT backend</p> </li> <li> <p>Fixed a bug when adding/editing boolean attributes in an annotation task using the CVAT backend</p> </li> <li> <p>Added a new <code>occluded</code> attribute type option that links an attribute to the builtin occlusion icon when annotating label attributes using the CVAT backend</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0133","title":"FiftyOne 0.13.3 \u00b6","text":"<p>Released September 22, 2021</p> <p>App</p> <ul> <li> <p>Improved the efficiency of loading label graphs for fields with many distinct values</p> </li> <li> <p>Fixed some audio-related bugs when viewing video samples with audio channels</p> </li> <li> <p>Fixed a bug that prevented boolean App filters from working properly</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for importing/exporting segmentation masks with greater than 256 classes when working with the ImageSegmentationDirectory format</p> </li> <li> <p>Added support for importing GeoTIFF images via a new GeoTIFFDataset format</p> </li> <li> <p>Added new <code>split_labels()</code> and <code>merge_labels()</code> methods that provide convenient syntaxes for moving labels between new and existing label fields of a dataset</p> </li> <li> <p>Added <code>ensure_frames()</code> and <code>clear_frames()</code> methods that can be used to conveniently initialize and clear the frames of video datasets, respectively</p> </li> <li> <p>Added support for using a MongoDB dataset whose version is not explicitly supported</p> </li> <li> <p>Removed the <code>opencv-python-headless</code> maximum version requirement</p> </li> <li> <p>Fixed a race condition that could prevent callbacks on interactive plots from working properly on sufficiently large datasets</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Added support for annotating semantic segmentations and instance segmentations using the CVAT backend</p> </li> <li> <p>Added support for annotating polylines using the CVAT backend</p> </li> <li> <p>Added support for immutable attributes when annotating object tracks for video datasets using the CVAT backend</p> </li> <li> <p>Exposed the <code>use_cache</code>, <code>use_zip_chunks</code>, and <code>chunk_size</code> parameters when uploading annotations via the CVAT backend</p> </li> <li> <p>Fixed a bug that prevented multiple imports of the same annotation run from working as expected when a label is deleted but then later re-added</p> </li> <li> <p>Fixed a bug that prevented annotations for new label fields of video datasets from being imported properly</p> </li> <li> <p>Fixed a bug that would cause unsuppoted shapes such as polygons with less than 3 vertices to be deleted when editing existing labels with the CVAT backend</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0132","title":"FiftyOne 0.13.2 \u00b6","text":"<p>Released September 3, 2021</p> <p>App</p> <ul> <li> <p>Improved aggregation queries resulting in ~10x faster statistics load times and time-to-interaction in the Fields Sidebar!</p> </li> <li> <p>Optimized in-App tagging actions</p> </li> <li> <p>Fixed count inconsistencies for large sets of <code>StringField</code> results in the Fields Sidebar</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for providing compound sort criteria when using the <code>sort_by()</code> stage</p> </li> <li> <p>Added support for configuring the wait time when using <code>Session.wait()</code> to block execution until the App is closed, including support for serving forever</p> </li> <li> <p>Fixed errors experienced by Windows users when running FiftyOne APIs that involved multiprocessing</p> </li> <li> <p>Resolved errors that could occur when importing CVAT XML files with empty-valued attributes in their schema and/or individual labels</p> </li> <li> <p>Added support for importing CVAT-style attributes when loading labels in COCO and VOC formats</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0131","title":"FiftyOne 0.13.1 \u00b6","text":"<p>Released August 25, 2021</p> <p>App</p> <ul> <li>Fixed <code>id</code> rendering in the grid when the <code>id</code> checkbox is active</li> </ul> <p>Annotation</p> <ul> <li>Fixed a bug that could cause mismatches between media and their pre-existing labels when uploading data to CVAT for annotation whose source media lives in multiple directories</li> </ul>"},{"location":"release-notes/#fiftyone-0130","title":"FiftyOne 0.13.0 \u00b6","text":"<p>Released August 24, 2021</p> <p>App</p> <ul> <li> <p>Added support for visualizing and filtering list fields</p> </li> <li> <p>Added support for visualizing segmentation masks of any integer type (uint8, uint16, etc.)</p> </li> <li> <p>Improved handling of falsey field values in the fields sidebar and image vizualizer</p> </li> <li> <p>Improved the JSON display format available from the expanded sample modal</p> </li> <li> <p>Resolved an issue that caused some users to see duplicate App screenshots when calling <code>Session.freeze()</code> in Jupyter notebooks</p> </li> <li> <p>Fixed a bug that prevented users from being able to click left/right arrows to navigate between samples in the expanded sample modal when working in Jupyter notebooks</p> </li> <li> <p>Fixed a bug where pressing the <code>ESC</code> key had no effect in the expanded sample modal when working with datasets with no label fields</p> </li> <li> <p>Fixed a bug that prevented the desktop App from launching when using source builds</p> </li> </ul> <p>Brain</p> <ul> <li> <p>Added new <code>find_unique()</code>, <code>unique_view()</code>, and <code>visualize_unique()</code> methods to the <code>SimilarityIndex</code> object returned by <code>compute_similarity()</code> that enable you to identify a maximally unique set of images or objects in a dataset</p> </li> <li> <p>Added new <code>find_duplicates()</code>, <code>duplicates_view()</code>, and <code>visualize_duplicates()</code> methods to the <code>SimilarityIndex</code> object returned by <code>compute_similarity()</code> that enable you to identify near-duplicate images or objects in a dataset</p> </li> <li> <p>Added a new <code>compute_exact_duplicates()</code> method that can identify exactly duplicate media in a dataset</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for pip-installing FiftyOne on Apple Silicon Macs. Note that MongoDB must be self-installed in this case</p> </li> <li> <p>Added support for using a self-installed MongoDB database</p> </li> <li> <p>Added a <code>group_by()</code> view stage that allows for reorganizing the samples in a collection so that they are grouped by a specified field or expression</p> </li> <li> <p>Added a <code>selection_mode</code> property that enables users to change the behavior of App updates when selections are made in an interactive plot linked to labels. See this page for details</p> </li> <li> <p>Added support for YOLOv5 YAML files with multiple directories per dataset split</p> </li> <li> <p>Added support for importing/exporting confidences via the <code>score</code> field when working with BDD format</p> </li> <li> <p>Fixed some Windows-style path bugs</p> </li> </ul> <p>Annotation</p> <ul> <li> <p>Added a powerful annotation API that makes it easy to add or edit labels on your FiftyOne datasets or specific views into them</p> </li> <li> <p>Added a native CVAT integration that enables you to use the annotation API with CVAT</p> </li> </ul> <p>Docs</p> <ul> <li> <p>Added a CVAT annotation tutorial</p> </li> <li> <p>Added a new example to the brain user guide that demonstrates unique and near-duplicate image workflows</p> </li> <li> <p>Added an object embeddings example to the embeddings visualization section of the brain user guide</p> </li> <li> <p>Added a new section to the plots user guide page explaining how to control the selection mode of interactive plots linked to labels</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0120","title":"FiftyOne 0.12.0 \u00b6","text":"<p>Released August 10, 2021</p> <p>App</p> <ul> <li> <p>Resolved performance issues with scrolling via grid virtualization. Toggling fields or selecting samples is no longer impacted by the amount of samples that have been loaded</p> </li> <li> <p>Added the <code>Show label</code> option in the expanded sample view to toggle the label text above detections boxes</p> </li> <li> <p>Added support for zooming and panning in the expanded sample view</p> </li> <li> <p>Added support for cropping and zooming to content in the expanded sample view</p> </li> <li> <p>Added support for visualizing multiple segmentation frame fields simultaneously</p> </li> <li> <p>Added label streaming to the video visualizer</p> </li> <li> <p>Added volume and playback rate settings to the video visualizer</p> </li> <li> <p>Added the <code>Crop to content</code> option in patches or evaluation patches views which crops samples to only show the labels that make up the patch. Defaults to <code>True</code></p> </li> <li> <p>Added count and filtered count values to categorical filters ( <code>BooleanField</code> and <code>StringField</code> fields)</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added support for importing DICOM datasets</p> </li> <li> <p>Added better default behavior for the <code>label_field</code> parameter when importing datasets using methods like <code>from_dir()</code> and exporting datasets using <code>export()</code></p> </li> <li> <p>When adding samples to datasets, <code>None</code>-valued sample fields are now gracefully ignored when expanding dataset schemas</p> </li> </ul> <p>Docs</p> <ul> <li> <p>Added Using the image visualizer and Using the video visualizer sections to the App user guide</p> </li> <li> <p>Added sections covering merging datasets and batch updates to the dataset user guide page</p> </li> </ul> <p>Zoo</p> <ul> <li>Patched an Open Images issue where <code>classes</code> or <code>attrs</code> requirements were being ignored when loading a dataset with no <code>max_samples</code> requirement</li> </ul>"},{"location":"release-notes/#fiftyone-01121","title":"FiftyOne 0.11.2.1 \u00b6","text":"<p>Released July 31, 2021</p> <p>Zoo</p> <ul> <li> <p>Patched an Open Images issue where label files were not being downloaded when running a <code>load_zoo_dataset()</code> call that does not include <code>classes</code> or <code>attrs</code> options in an environment where Open Images has never been downloaded</p> </li> <li> <p>Patched loading of Cityscapes datasets</p> </li> <li> <p>Patched loading of COCO datasets</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0112","title":"FiftyOne 0.11.2 \u00b6","text":"<p>Released July 27, 2021</p> <p>App</p> <ul> <li> <p>Added support for calling <code>Session.open_tab()</code> from remote Jupyter notebooks</p> </li> <li> <p>Fixed a bug that could cause <code>Session.wait()</code> to exit when the App\u2019s tab is refreshed in the browser</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added a <code>plotly&lt;5</code> requirement, which prevents an issue that may cause callbacks for selection events in interactive plots to not trigger as expected when using Plotly V5</p> </li> <li> <p>Added support for evaluating polygons and instance segmentations to <code>evaluate_detections()</code>. See this page for usage details</p> </li> <li> <p>Added support for creating patch views and evaluation patch views into the frames of video datasets</p> </li> <li> <p>Greatly improved the efficiency of creating evaluation patch views</p> </li> <li> <p>Added support for recursively listing data directories when loading datasets from disk</p> </li> <li> <p>Added support for controlling whether/which object attributes are imported/exported in formats like COCO that support arbitrary object attributes</p> </li> <li> <p>Updated all dataset import/export routines to support/prefer custom object attributes stored directly on <code>Label</code> instances as dynamic fields rather than in the <code>attributes</code> dict</p> </li> <li> <p>The ImageSegmentationDirectory format now supports exporting segmentations defined by <code>Detections</code> with instance masks and <code>Polylines</code></p> </li> <li> <p>Added an <code>objects_to_segmentations()</code> utility for converting <code>Detections</code> with instance fields and <code>Polylines</code> to <code>Segmentation</code> format</p> </li> <li> <p>Added graceful handling of edges cases like empty views and missing labels to all evaluation routines</p> </li> <li> <p>Added improved support for <code>creating</code>, <code>viewing</code>, and <code>dropping</code> dropping sample- and frame-level indexes on datasets</p> </li> <li> <p>Added additional indexes on patch and frames views to enable efficient ID-based queries</p> </li> <li> <p>Added support for gracefully loading and deleting evaluations and brain methods executed in future versions of FiftyOne (e.g., after downgrading your FiftyOne package version)</p> </li> <li> <p>Added an optional <code>progress</code> flag to <code>iter_samples()</code> that renders a progress bar tracking the progress of the iteration</p> </li> <li> <p>Added support for installing FiftyOne on RHEL7 (Red Hat Enterprise Linux)</p> </li> <li> <p>A helpful error message is now raised when a user tries to load a dataset from a future version of FiftyOne without following the downgrade instructions</p> </li> <li> <p>Fixed a bug that prevented FiftyOne from being imported on read-only filesystems</p> </li> <li> <p>Fixed a bug that prevented the proper loading of the Open Images V6 dataset after partial downloads involving only a subset of the available label types</p> </li> </ul> <p>Zoo</p> <ul> <li> <p>Added support for importing license data when loading the COCO-2014 and COCO-2017 datasets from the zoo</p> </li> <li> <p>The inapplicable <code>classes</code> flag will now be ignored when loading the unlabeled test split of COCO-2014 and COCO-2017</p> </li> <li> <p>Improved the partial download behavior of the Open Images V6 dataset when the optional <code>classes</code> and <code>attrs</code> parameters are provided</p> </li> <li> <p>Fixed a bug that prevented Windows users from downloading the Open Images V6 dataset</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0111","title":"FiftyOne 0.11.1 \u00b6","text":"<p>Released June 29, 2021</p> <p>App</p> <ul> <li>Updated the expired Slack community link in the App menu bar</li> </ul>"},{"location":"release-notes/#fiftyone-0110","title":"FiftyOne 0.11.0 \u00b6","text":"<p>Released June 29, 2021</p> <p>News</p> <ul> <li>With support from the COCO team, FiftyOne is now a recommended tool for downloading, visualizing, and evaluating on the COCO dataset! Check out this guide for more details</li> </ul> <p>App</p> <ul> <li>Fixed a bug that prevented <code>sample_id</code> fields from appearing in the App when working with frames and patches views</li> </ul> <p>Core</p> <ul> <li> <p>Added various new parameters to methods like <code>Dataset.from_dir()</code> and <code>SampleCollection.export()</code>, including <code>data_path</code>, <code>labels_path</code>, and <code>export_media</code> that allow for customizing the import and export of datasets. For example, you can now perform labels-only imports and exports</p> </li> <li> <p>Added new <code>Dataset.merge_dir()</code> and <code>Dataset.merge_importer()</code> methods for merging datasets from disk into existing FiftyOne datasets</p> </li> <li> <p>Added support for importing and exporting datasets in YOLOv5 format</p> </li> <li> <p>Updated the <code>GeoJSONDataset</code> dataset type to support both image and video datasets</p> </li> <li> <p>Added support for <code>importing</code> and <code>exporting</code> extra attributes in COCO format via a new <code>extra_attrs</code> parameter</p> </li> </ul> <p>Zoo</p> <ul> <li> <p>Added support for partial downloads and loading segmentations to the COCO-2014 and COCO-2017 datasets</p> </li> <li> <p>When performing partial downloads on the Open Images v6 Dataset involving a subset of the available classes, all labels for matching samples will now be loaded by default</p> </li> </ul> <p>Docs</p> <ul> <li> <p>Added a new page demonstrating how to use FiftyOne to download, visualize, and evaluate on the COCO dataset</p> </li> <li> <p>Added a new page demonstrating how to use FiftyOne to download, visualize, and evaluate on the Open Images dataset</p> </li> </ul>"},{"location":"release-notes/#fiftyone-0100","title":"FiftyOne 0.10.0 \u00b6","text":"<p>Released June 21, 2021</p> <p>News</p> <ul> <li>We\u2019ve collaborated with the PyTorch Lightning team to make it easy to train Lightning Flash tasks on your FiftyOne datasets. Check out this guide for more details</li> </ul> <p>Core</p> <ul> <li>Updated the <code>apply_model()</code> and <code>compute_embeddings()</code> methods to natively support applying Lightning Flash models to FiftyOne datasets!</li> </ul> <p>Docs</p> <ul> <li>Added a new page demonstrating how to use the Lightning Flash integration</li> </ul>"},{"location":"release-notes/#fiftyone-094","title":"FiftyOne 0.9.4 \u00b6","text":"<p>Released June 15, 2021</p> <p>App</p> <ul> <li> <p>Added support for matching samples by ID in the Filters Sidebar</p> </li> <li> <p>Fixed a bug that caused the App to crash when selecting samples with the <code>Color by value</code> setting active</p> </li> <li> <p>Fixed a bug that caused the App to crash on some Windows machines by ensuring the correct MIME type is set for JavaScript files</p> </li> </ul> <p>Core</p> <ul> <li> <p>Improved the performance of importing data into FiftyOne by 2x or more!</p> </li> <li> <p>Added a <code>to_frames()</code> view stage that enables on-the-fly conversion of video datasets into frames views</p> </li> <li> <p>Added <code>last()</code>, <code>head()</code>, and <code>tail()</code> methods to the <code>Frames</code> class</p> </li> <li> <p>Added new <code>exclude_fields()</code>, <code>select_frames()</code>, and <code>match_frames()</code> view stages that enable selecting specific frames of video collections via IDs or filter expressions, respectively</p> </li> <li> <p>Added a new <code>match_labels()</code> view stage that enables matching samples that have specific labels without actually filtering the non-matching labels</p> </li> <li> <p>Added support for exporting image patches using <code>export()</code> by specifying an image classification dataset type and including a spatial <code>label_field</code> that defines the image patches to extract</p> </li> <li> <p>Added support for automatically coercing single label fields like <code>Detection</code> into the corresponding multiple label field type <code>Detections</code> when using <code>export()</code> to export in dataset formats that expect list-type fields</p> </li> <li> <p>Added support for executing an aggregation on multiple fields via the abbreviated syntax <code>ids, filepaths = dataset.values([\"id\", \"filepath\"])</code></p> </li> <li> <p>Exposed the <code>id</code> field of all samples and frames in dataset schemas</p> </li> <li> <p>Added support for merging the elements of list fields via <code>Dataset.merge_samples()</code> and <code>Document.merge()</code></p> </li> <li> <p>Added a number of useful options to <code>Dataset.merge_samples()</code>, including <code>fields</code>, <code>omit_fields</code>, and <code>merge_lists</code></p> </li> <li> <p>Improved the efficiency of <code>Dataset.merge_samples()</code> when the <code>overwrite=False</code> option is provided</p> </li> <li> <p>Added an optional <code>bool</code> flag to the <code>match_tags()</code> view stage that allows for optionally matching samples without the specified tags</p> </li> <li> <p>Added support for computing filehashes via the <code>hashlib</code> module to <code>compute_filehash()</code></p> </li> <li> <p>Updated the <code>import_from_labelbox()</code> method to use the correct label ID (\u201cDataRow ID\u201d, not \u201cID\u201d)</p> </li> <li> <p>Added an optional <code>edges</code> argument to <code>scatterplot()</code> and <code>location_scatterplot()</code> that enables drawing undirected edges between scatterpoints</p> </li> <li> <p>Fixed a bug in <code>limit_labels()</code> that would cause views to contain empty label lists if the source dataset contains None-valued fields</p> </li> <li> <p>Fixed a bug that prevented <code>ViewExpression.contains()</code> from accepting <code>ViewExpression</code> instances as arguments</p> </li> </ul> <p>Zoo</p> <ul> <li> <p>Fixed a string encoding issue that prevented some Windows users from loading the Open Images V6 dataset</p> </li> <li> <p>Updated the vgg16-imagenet-tf1 model (formerly named <code>vgg16-imagenet-tf</code>) to reflect the fact that it only supports TensorFlow 1.X</p> </li> </ul> <p>Docs</p> <ul> <li>Added example usages of <code>to_frames()</code> to the user guide</li> </ul>"},{"location":"release-notes/#fiftyone-093","title":"FiftyOne 0.9.3 \u00b6","text":"<p>Released May 18, 2021</p> <p>App</p> <ul> <li> <p>Fixed an issue that prevented some datasets and views that contain vector or array data (e.g., logits) from properly loading in the App</p> </li> <li> <p>Fixed a bug that prevented loading video datasets in the App in Google Colab environments</p> </li> </ul>"},{"location":"release-notes/#fiftyone-092","title":"FiftyOne 0.9.2 \u00b6","text":"<p>Released May 16, 2021</p> <p>Zoo</p> <ul> <li>Fixed a multiprocessing bug that prevented Mac users running Python 3.8 or later from loading the Open Images V6 dataset</li> </ul>"},{"location":"release-notes/#fiftyone-091","title":"FiftyOne 0.9.1 \u00b6","text":"<p>Released May 12, 2021</p> <p>App</p> <ul> <li>Fixed a bug that caused the App to crash when choosing to <code>Color by value</code></li> </ul>"},{"location":"release-notes/#fiftyone-090","title":"FiftyOne 0.9.0 \u00b6","text":"<p>Released May 12, 2021</p> <p>News</p> <ul> <li>We\u2019ve collaborated with the Open Images Team at Google to make FiftyOne a recommended tool for downloading, visualizing, and evaluating on the Open Images Dataset! Check out this guide for more details</li> </ul> <p>App</p> <ul> <li> <p>Added a <code>Patches</code> action for easy switching to object/evaluation patches views. See this page for usage details</p> </li> <li> <p>Added a <code>Sort by similarity</code> action that enables sorting by similarity to the selected samples/patches. See this page for usage details</p> </li> <li> <p>Added a zoom slider in the top right of the sample grid that adjusts the tile size of the sample grid</p> </li> <li> <p>Added the ability to clear filters for entire field groups, e.g. <code>Labels</code> and <code>Scalars</code>, in the Filters Sidebar</p> </li> <li> <p>Added <code>filepath</code> to the <code>Scalars</code> group in the Filters Sidebar</p> </li> <li> <p>Added a <code>Label tags</code> graphs tab</p> </li> <li> <p>Refreshed numeric, string, and boolean filter styles with improved functionality and interaction</p> </li> <li> <p>Added support for <code>Session.wait()</code> in browser contexts</p> </li> </ul> <p>Brain</p> <ul> <li>Added a <code>compute_similarity()</code> method for indexing samples and object patches by similarity. See this page for usage details</li> </ul> <p>Core</p> <ul> <li> <p>Added support for Open Images-style detection evaluation when using <code>evaluate_detections()</code>. See this page for usage details</p> </li> <li> <p>Added the <code>to_patches()</code> and <code>to_evaluation_patches()</code> view stages for transforming collections into flattened views with respect to labels and evaluations, respectively. See this page for usage details</p> </li> <li> <p>Added support for applying image models to the frames of video datasets when using <code>apply_model()</code>, <code>compute_embeddings()</code>, and <code>compute_patch_embeddings()</code></p> </li> <li> <p>Added full support for embedded documents (e.g. labels) in <code>values()</code> and <code>set_values()</code></p> </li> <li> <p>Added support for passing expressions directly to aggregations</p> </li> <li> <p>Added an optional <code>omit_empty</code> flag to <code>select_labels()</code> and <code>exclude_labels()</code> that controls whether samples with no labels are omitted when filtering</p> </li> <li> <p>Added a <code>Dataset.delete_labels()</code> method for efficiently deleting labels via a variety of natural syntaxes</p> </li> <li> <p>Deprecated <code>Dataset.remove_sample()</code> and <code>Dataset.remove_samples()</code> in favor of a single <code>Dataset.delete_samples()</code> method</p> </li> <li> <p>Brain results and evaluation results that are loaded via <code>load_evaluation_results()</code> <code>load_brain_results()</code> are now cached on the <code>Dataset</code> object in-memory so that subsequent retrievals of the results in the same session will be instant</p> </li> </ul> <p>Zoo</p> <ul> <li>Added Open Images V6 to the dataset zoo!</li> </ul> <p>Docs</p> <ul> <li> <p>Added a new Open Images tutorial</p> </li> <li> <p>Added object patches and evaluation patches sections to the App guide</p> </li> <li> <p>Added a similarity section to the Brain guide</p> </li> <li> <p>Added Open Images evaluation and evaluation patches sections to the evaluation guide</p> </li> <li> <p>Added object patches and evaluation patches sections to the views guide</p> </li> <li> <p>Added example uses of <code>to_patches()</code> and <code>to_evaluation_patches()</code> to the object detection tutorial</p> </li> <li> <p>Added example use of <code>to_patches()</code> to the detection mistakes tutorial</p> </li> <li> <p>Added example use of <code>to_patches()</code> to the adding detections recipe</p> </li> </ul>"},{"location":"release-notes/#fiftyone-080","title":"FiftyOne 0.8.0 \u00b6","text":"<p>Released April 5, 2021</p> <p>App</p> <ul> <li> <p>Added the ability to tag samples and labels directly from the App in both the sample grid (macro) and expanded sample view (micro) with respect to and filters or currently selected samples/labels</p> </li> <li> <p>Added a <code>LABEL TAGS</code> section to the Filters Sidebar to coincide with the introduction of label tags</p> </li> <li> <p>Added label tooltips that display on hover in the expanded sample view</p> </li> <li> <p>Expanded actions to list of button groups in the sample grid and expanded sample view</p> </li> <li> <p>Added support for rendering semantic labels in the new tooltip in the expanded sample view for <code>Segmentation</code> mask values (pixel values) using the new <code>Dataset.mask_targets</code> and <code>Dataset.default_mask_targets</code> fields</p> </li> <li> <p>Fixed hiding, clearing, and only showing selected samples in the samples grid</p> </li> </ul> <p>Brain</p> <ul> <li> <p>Added a <code>compute_visualization()</code> method that uses embeddings and dimensionality reduction methods to generate interactive visualizations of the samples and/or labels in a dataset. Check out this page for details. Features include:</p> </li> <li> <p>Provide your own embeddings, or choose a model from the Model Zoo, or use the provided default model</p> </li> <li> <p>Supported dimensionality reduction methods include UMAP, t-SNE, and PCA</p> </li> <li> <p>Use this capability in a Jupyter notebook and you can interact with the plots to select samples/labels of interest in a connected <code>Session</code></p> </li> <li> <p>Added support for saving brain method results on datasets. Previous brain results can now be loaded at any time via <code>Dataset.load_brain_results()</code></p> </li> <li> <p>Added support for providing a custom <code>Model</code> or model from the Model Zoo to <code>compute_uniqueness()</code></p> </li> </ul> <p>Core</p> <ul> <li> <p>Added a <code>fiftyone.core.plots</code> module that provides a powerful API for visualizing datasets, including interactive plots when used in Jupyter notebooks. See this page for more information. Highlights include:</p> </li> <li> <p><code>plot_confusion_matrix()</code>: an interactive confusion matrix that can be attached to a <code>Session</code> object to visually explore model predictions</p> </li> <li> <p><code>scatterplot()</code>: an interactive scatterplot of 2D or 3D points that can be attached to a <code>Session</code> to explore the samples/labels in a dataset based on their locations in a low-dimensional embedding space</p> </li> <li> <p><code>location_scatterplot()</code>: an interactive scatterplot of a dataset via its <code>GeoLocation</code> coordinates</p> </li> <li> <p>Added <code>GeoLocation</code> and <code>GeoLocations</code> label types that can be used to store arbitrary GeoJSON location data on samples</p> </li> <li> <p>Added the <code>GeoJSONDataset</code> dataset type for importing and exporting datasets in GeoJSON format</p> </li> <li> <p>Added <code>SampleCollection.geo_near()</code> and <code>SampleCollection.geo_within()</code> view stages for querying datasets with location data</p> </li> <li> <p>Upgraded the implementation of the FiftyOneDataset format, which is now 10-100x faster at importing/exporting datasets</p> </li> <li> <p>Added support for generating zip/tar/etc archives to <code>SampleCollection.export()</code> by passing an archive path rather than a directory path</p> </li> <li> <p>Added <code>Dataset.from_archive()</code> and <code>Dataset.add_archive()</code> factory methods for importing datasets stored in archives</p> </li> <li> <p>Added support for saving evaluation results on a dataset. Results can now be loaded at any time via <code>Dataset.load_evaluation_results()</code></p> </li> <li> <p>Added a <code>tags</code> attribute to all <code>Label</code> types that can store a list of string tags for the labels (analogous to the <code>tags</code> attribute of <code>Sample</code>)</p> </li> <li> <p>Added a number of methods for working with sample and label tags:</p> </li> <li> <p><code>SampleCollection.tag_samples()</code></p> </li> <li> <p><code>SampleCollection.untag_samples()</code></p> </li> <li> <p><code>SampleCollection.count_sample_tags()</code></p> </li> <li> <p><code>SampleCollection.tag_labels()</code></p> </li> <li> <p><code>SampleCollection.untag_labels()</code></p> </li> <li> <p><code>SampleCollection.count_label_tags()</code></p> </li> <li> <p>BREAKING CHANGE: Renamed all applicable API components that previously referenced \u201cobjects\u201d to use the more widely applicable term \u201clabels\u201d. Affected attributes, classes, and methods are:</p> </li> <li> <p><code>Session.selected_labels</code> (previously <code>selected_objects</code>)</p> </li> <li> <p><code>SampleCollection.select_labels()</code> (previously <code>select_labels()</code>)</p> </li> <li> <p><code>SampleCollection.select_labels()</code> (previously <code>exclude_labels()</code>)</p> </li> <li> <p><code>SelectLabels</code> (previously <code>SelectObjects</code>)</p> </li> <li> <p><code>ExcludeLabels</code> (previously <code>ExcludeObjects</code>)</p> </li> <li> <p>Added new keyword arguments <code>ids</code>, <code>tags</code>, and <code>fields</code> to <code>SampleCollection.select_labels()</code> and <code>SampleCollection.select_labels()</code> and their corresponding view stages that enable easier-to-use selection of labels by their IDs or tags</p> </li> <li> <p>Added <code>Session.select_labels()</code> for programmatically selecting labels as well a setters for <code>Session.selected</code> and <code>Session.selected_labels</code></p> </li> <li> <p>Added <code>Dataset.classes</code> and <code>Dataset.default_classes</code> fields that enable storing class label lists at the dataset-level that can be automatically used by methods like <code>Dataset.evaluate_classifications()</code> when knowledge of the full schema of a model is required</p> </li> <li> <p>Added <code>Dataset.mask_targets</code> and <code>Dataset.default_mask_targets</code> fields for providing semantic labels for <code>Segmentation</code> mask values to be used in the App\u2019s expanded sample view</p> </li> <li> <p>Improved the runtime of <code>Dataset.merge_samples()</code> by ~100x for image datasets and ~10x for video datasets</p> </li> <li> <p>Added an <code>Dataset.add_collection()</code> method for adding the contents of a <code>SampleCollection</code> to another <code>Dataset</code></p> </li> <li> <p>Added the trigonometric view expressions <code>cos</code>, <code>sin</code>, <code>tan</code>, <code>cosh</code> <code>sinh</code>, <code>tanh</code>, <code>arccos</code>, <code>arcsin</code>, <code>arcan</code> <code>arccosh</code>, <code>arcsinh</code>, and <code>arctanh</code></p> </li> <li> <p>Added a <code>randn</code> expression that can generate Gaussian random numbers</p> </li> <li> <p>Fixed a bug that prevented <code>evaluate_detections()</code> from being able to process video datasets</p> </li> <li> <p>Added support for applying intensive view stages such as sorting to datasets whose database representation exceeds 100MB</p> </li> <li> <p>Fixed schema errors in <code>DatasetView</code> instances that contain selected or excluded fields</p> </li> <li> <p>Fixed copying of <code>DatasetView</code> instances where <code>ViewField</code> is used</p> </li> </ul> <p>Zoo</p> <ul> <li>Added the quickstart-geo dataset to enable quick exploration of location-based datasets</li> </ul> <p>CLI</p> <ul> <li>Removed the <code>--desktop</code> flag from the fiftyone app connect command</li> </ul> <p>Docs</p> <ul> <li> <p>Added a tutorial demonstrating how to use <code>compute_visualization()</code> on image datasets</p> </li> <li> <p>Added an interactive plots page to the user guide</p> </li> <li> <p>Added a Tags and tagging section to the App user guide</p> </li> <li> <p>Added a visualizing embedding section to the Brain user guide</p> </li> </ul>"},{"location":"release-notes/#fiftyone-074","title":"FiftyOne 0.7.4 \u00b6","text":"<p>Released March 2, 2021</p> <p>App</p> <ul> <li> <p>Fixed a bug that prevented <code>Session</code> updates from triggering App updates</p> </li> <li> <p>Fixed hiding labels in the expanded sample view</p> </li> </ul> <p>Brain</p> <ul> <li> <p>Added support for tracking and cleaning up brain runs similar to how evaluations are tracked. See <code>get_brain_info()</code>, <code>list_brain_runs()</code>, <code>load_brain_view()</code>, and <code>delete_brain_run()</code> for details</p> </li> <li> <p>Updated <code>compute_mistakenness()</code> to use FiftyOne\u2019s evaluation framework</p> </li> </ul> <p>Core</p> <ul> <li> <p>Decoupled loading video <code>Sample</code> and <code>SampleView</code> and their frames so the samples are loaded efficiently and frames are only loaded when requested</p> </li> <li> <p>Add a 90 character limit to progress bars in notebook contexts to prevent overflow issues</p> </li> <li> <p>Added low-level utility methods <code>list_datasets()</code> and <code>delete_dataset()</code> for managing a corrupted database</p> </li> <li> <p>Added automatic field generation for <code>labelbox_id_field</code> when using <code>import_from_labelbox()</code></p> </li> </ul> <p>CLI</p> <ul> <li>Added a dataset stats command</li> </ul>"},{"location":"release-notes/#fiftyone-073","title":"FiftyOne 0.7.3 \u00b6","text":"<p>Released February 18, 2021</p> <p>App</p> <ul> <li> <p>Added filtering widgets to the Filters Sidebar for <code>StringFields</code> and <code>BooleanFields</code></p> </li> <li> <p>Added histogram plots for <code>StringFields</code> and <code>BooleanFields</code> in the <code>Scalars</code> tab</p> </li> <li> <p>Moved <code>None</code> selection for <code>StringFields</code> to the input format in the Filters Sidebar</p> </li> <li> <p>Changed <code>None</code> options to only be present when <code>None</code> values exist for a supported <code>Field</code> in the Filters Sidebar</p> </li> <li> <p>Added <code>Color by label</code> support for <code>Classification</code>, <code>Classifications</code>, <code>BooleanField</code>, and <code>StringField</code></p> </li> <li> <p>Added support excluding selected values for a <code>StringField</code> in the Fields Sidebar</p> </li> <li> <p>Various style and interaction improvements in the Filters Sidebar</p> </li> <li> <p>The App will no longer crash when samples whose source media is unsupported or missing are loaded</p> </li> </ul> <p>Core</p> <ul> <li> <p>Added <code>evaluate_classifications()</code>, <code>evaluate_detections()</code>, and <code>evaluate_segmentations()</code> methods that provide support for evaluating various types of labels. See the new evaluation page of the user guide for more details</p> </li> <li> <p>Added <code>one()</code> for retrieving one matched <code>Sample</code> from a <code>Dataset</code> or <code>DatasetView</code></p> </li> <li> <p>Added support for cloning and saving views into video datasets via <code>clone()</code> and <code>save()</code></p> </li> <li> <p>Added support for extracting batches of frame-level and/or array fields via the <code>values()</code> aggregation</p> </li> <li> <p>Added support for setting batches of frame-level and/or array fields via <code>set_values()</code></p> </li> <li> <p>Added support for accessing samples from a <code>Dataset</code> or <code>DatasetView</code> via the <code>dataset[filepath]</code> syntax</p> </li> <li> <p>Added support for passing <code>Sample</code> and any <code>Sample</code> iterable, e.g. <code>DatasetView</code>, to methods like <code>remove_samples()</code>, <code>exclude()</code>, and <code>select()</code></p> </li> <li> <p>Changed the default value for <code>only_matches</code> for <code>filter_classifications()</code>, <code>filter_detections()</code>, <code>filter_field()</code>, <code>filter_labels()</code>, <code>filter_keypoints()</code>, and <code>filter_polylines()</code> from <code>False</code> to <code>True</code></p> </li> <li> <p><code>compute_metadata()</code> will now gracefully skip samples for which media metadata cannot be computed</p> </li> <li> <p>Added a <code>stats()</code> method for listing helpful info about the size of various entities of a dataset</p> </li> </ul> <p>Zoo</p> <ul> <li> <p>Added support for storing logits for many zoo models when using <code>apply_model()</code></p> </li> <li> <p>Default confidence thresholds for zoo models are now stored on a per-model basis rather than as a global default value in <code>apply_model()</code>. All detection models still have a default confidence threshold of 0.3, and all other model types have no default confidence threshold</p> </li> </ul> <p>CLI</p> <ul> <li>Added a migration API to provide better support for downgrading the version of your <code>fiftyone</code> package</li> </ul> <p>Docs</p> <ul> <li> <p>Added a new evaluation page to the user guide that explains how to evaluate various types of models with FiftyOne</p> </li> <li> <p>Removed legacy <code>--index</code> flags from the install instructions from the troubleshooting page which prevented a valid installation</p> </li> </ul>"},{"location":"release-notes/#fiftyone-072","title":"FiftyOne 0.7.2 \u00b6","text":"<p>Released January 28, 2021</p> <p>App</p> <ul> <li> <p>Changed the Filters Sidebar label filters to only return matched samples, i.e., samples with at least one matching label with respect to a filter</p> </li> <li> <p>Fixed a bug in Colab notebooks that allowed for the <code>.ipynb</code> file to grow unnecessarily large</p> </li> <li> <p>Improved plotting of numeric fields in the <code>Scalars</code> tab, including <code>[min, max)</code> ranges for tooltips and integer binning when appropriate\\ \\</p> </li> <li>Fixed a bug that prevented\\ <code>select_fields()</code>\\ and\\ <code>exclude_fields()</code>\\ from being properly respected by the Filters Sidebar\\ \\</li> <li>Fixed a bug that prevented selected samples from being cleared when modifying\\ your view or choosing an option from the select samples dropdown\\ \\</li> <li>Added an <code>AppConfig</code> for configuring options like the color pool to use when\\ drawing <code>Label</code> fields. See this page for\\ more info\\ \\ \\ Core\\ \\</li> <li>Added the <code>MapLabels</code> and\\ <code>SetField</code> view stages\\ \\</li> <li>Added the\\ <code>HistogramValues</code> and\\ <code>Sum</code> aggregations\\ \\</li> <li>Added over a dozen new\\ <code>ViewExpression</code> methods including powerful transformations like\\ <code>map_values()</code>,\\ <code>reduce()</code>, and\\ <code>sort()</code>\\ \\</li> <li>Exposed all <code>Aggregations</code> as\\ single execution methods on the <code>SampleCollection</code> interface, e.g.,\\ <code>distinct()</code>\\ \\</li> <li>Added support for all <code>Label</code> types in\\ <code>filter_labels()</code>\\ \\</li> <li>Added support for generalized field paths (embedded fields, lists, etc) to\\ the <code>Bounds</code>,\\ <code>Count</code>,\\ <code>CountValues</code>, and\\ <code>Distinct</code>\\ aggregations\\ \\</li> <li>Removed the deprecated\\ <code>ConfidenceBounds</code>,\\ <code>CountLabels</code>, and\\ <code>DistinctLabels</code>\\ aggregations\\ \\</li> <li>Removed the redundant\\ <code>match_tag()</code>\\ stage in favor of\\ <code>match_tags()</code>\\ \\</li> <li>Removed <code>AggregationResult</code> classes in favor of returning\\ <code>Aggregation</code> results\\ directly as builtin types\\ \\</li> <li>Added the optional <code>config</code> keyword argument to\\ <code>launch_app()</code> and\\ <code>Session</code> for overriding the default\\ AppConfig.\\ \\ \\ Zoo\\ \\</li> <li>Added a default confidence threshold of <code>0.3</code> when applying models from the\\ Model Zoo via\\ <code>apply_model()</code>,\\ which omits spurious low quality predictions from many models\\ \\ \\ CLI\\ \\</li> <li>Added a fiftyone app config command for\\ inspecting the default App config\\ \\</li> <li>Improved <code>ctrl + c</code> exit handling for CLI commands\\ \\ \\ Docs\\ \\</li> <li>Added a new section to the\\ Configuring FiftyOne guide explaining how to\\ programmatically configure the App\u2019s behavior\\ \\</li> <li>Updated the Dataset views guide to provide a thorough\\ overview of new functionality provided by stages like\\ <code>SetField</code>\\ \\</li> <li>Updated the Aggregations guide to provide a\\ thorough overview and examples of various aggregation functionality,\\ including advanced usage tips\\ \\</li> <li>Added an FAQ section providing instructions for working with\\ remote Jupyter notebooks\\ \\</li> <li>Added code examples to all <code>ViewStage</code> class docstrings and their\\ corresponding sample collection methods, e.g.,\\ <code>map_labels()</code>\\ \\</li> <li>Added code examples to all <code>Aggregation</code> class docs and their corresponding\\ sample collection methods, e.g.,\\ <code>bounds()</code>\\ \\ \\ ## FiftyOne 0.7.1 \u00b6\\ \\ Released January 8, 2021\\ \\ App\\ \\</li> <li>Added automatic screenshotting for notebook environments\\ \\</li> <li>Fixed a bug where the Filters Sidebar statistics would not load for empty\\ views\\ \\</li> <li>Fixed style inconsistencies in Firefox\\ \\ \\ Core\\ \\</li> <li>Added <code>Session.freeze()</code> for\\ manually screenshotting the active App in a notebook environment\\ \\</li> <li>Renamed <code>Dataset.clone_field()</code> to\\ <code>Dataset.clone_sample_field()</code>\\ for consistency with other operations\\ \\</li> <li>Added a\\ <code>Dataset.clone_frame_field()</code>\\ method for cloning frame-level fields of video datasets\\ \\</li> <li>Added\\ <code>DatasetView.clone_sample_field()</code>\\ and\\ <code>DatasetView.clone_frame_field()</code>\\ methods that allow cloning views into sample fields (e.g., after filtering\\ the labels in a list field)\\ \\</li> <li>Added a <code>DatasetView.clone()</code>\\ method for cloning a view as a new dataset\\ \\</li> <li>Added a <code>DatasetView.save()</code>\\ method for saving a view, overwriting the contents of the underlying dataset\\ \\</li> <li>Re-implemented\\ <code>Dataset.clone_sample_field()</code>\\ and\\ <code>Dataset.merge_samples()</code>\\ via efficient DB-only operations\\ \\</li> <li>Added the <code>overwrite</code> keyword argument to the\\ <code>Dataset()</code> constructor\\ \\</li> <li>Added a <code>database_dir</code> option to the\\ FiftyOne Config\\ \\</li> <li>Added a <code>default_app_port</code> option to the\\ FiftyOne Config\\ \\ \\ Zoo\\ \\</li> <li>Added a CenterNet model to\\ the model zoo\\ \\</li> <li>Upgraded the Model Zoo so that many detection models that\\ previously required TensorFlow 1.X can now be used with TensorFlow 2.X\\ \\</li> <li>Added Caltech-256 to the dataset zoo\\ \\</li> <li>Added ImageNet Sample to the dataset zoo\\ \\</li> <li>Caltech-101 is now available natively in the\\ dataset zoo without the TF backend\\ \\</li> <li>KITTI is now available natively in the dataset zoo\\ without the TF backend\\ \\</li> <li>Fixed a bug that prevented ImageNet 2012\\ from loading properly when using the TF backend\\ \\ \\ CLI\\ \\</li> <li>Added support for controlling the error level when\\ applying zoo models\\ \\ \\ Docs\\ \\</li> <li>Added a Dataset Zoo listing that describes all\\ datasets in the zoo\\ \\</li> <li>Added a Model Zoo listing that describes all models\\ in the zoo\\ \\ \\ ## FiftyOne 0.7.0 \u00b6\\ \\ Released December 21, 2020\\ \\ App\\ \\</li> <li>Added web browser support, which is now the default setting\\ \\</li> <li>Added IPython notebook support, e.g. Jupyter and Google\\ Colab\\ \\</li> <li>The desktop App can now be installed as an\\ optional dependency\\ \\</li> <li>Fixed an issue where the App would freeze after filtering labels in the\\ Filters Sidebar\\ \\ \\ Core\\ \\</li> <li>Added a Model Zoo containing over 70 pretrained detection,\\ classification, and segmentation models that you can use to generate\\ predictions and embeddings\\ \\</li> <li>Moved project hosting to pypi.org\\ \\</li> <li>Added the <code>Session.show()</code> method\\ for displaying the App in IPython notebook cells\\ \\</li> <li>Added an in-App feedback form. We would love to hear from you!\\ \\</li> <li>Added Python 3.9 support\\ \\</li> <li>Removed Python 3.5 support\\ \\ \\ CLI\\ \\</li> <li>Added a fiftyone zoo models command that\\ provides access to the model zoo\\ \\</li> <li>Moved the dataset zoo commands to\\ fiftyone zoo datasets (previously they\\ were at <code>fiftyone zoo</code>)\\ \\</li> <li>Added a <code>--desktop</code> flag to commands that launch the App that enables\\ launching the App as a desktop App (rather than a web browser)\\ \\ \\ ## FiftyOne 0.6.6 \u00b6\\ \\ Released November 25, 2020\\ \\ App\\ \\</li> <li>Added a dropdown in the header to change datasets from the App\\ \\</li> <li>Added the ability to refresh the App by clicking the FiftyOne logo in the\\ header\\ \\</li> <li>Fixed a bug the caused numeric (scalar field) range sliders to disappear\\ after changing the default value\\ \\</li> <li>Fixed a bug that prevented the App state from updating appropriately after\\ applying label filters\\ \\ \\ Brain\\ \\</li> <li>Added support for computing mistakenness for detections when using\\ <code>compute_mistakenness()</code>\\ \\ \\ Core\\ \\</li> <li>Fixed a bug that prevented COCO datasets from being loaded from the\\ Dataset Zoo\\ \\ \\ CLI\\ \\</li> <li>Added support for customizing the local port when connecting to the App via\\ the CLI\\ \\</li> <li>Added an <code>--ssh-key</code> option to the <code>app connect</code> command\\ \\ \\ Docs\\ \\</li> <li>Added a tutorial demonstrating how to\\ use <code>compute_mistakenness()</code> to\\ detect label mistakes for detection datasets\\ \\</li> <li>Added questions to the FAQ page:\\ \\</li> <li>Can I launch multiple App instances on a machine?\\ \\</li> <li>Can I connect multiple App instances to the same dataset?\\ \\</li> <li>Can I connect to multiple remote sessions?\\ \\</li> <li>Can I serve multiple remote sessions from a machine?\\ \\ \\ ## FiftyOne 0.6.5 \u00b6\\ \\ Released November 16, 2020\\ \\ App\\ \\</li> <li>Added concurrency to the server which greatly improves loading speeds and\\ time-to-interaction in the Grid, View Bar, and Filters Sidebar for larger\\ datasets and views\\ \\</li> <li>Renamed the Display Options Sidebar to the Filters Sidebar\\ \\</li> <li>Added support for coloring by <code>label</code> value in the Filters Sidebar\\ \\</li> <li>Added support for filtering\\ <code>keypoint</code>,\\ <code>keypoints</code>,\\ <code>polyline</code>,\\ <code>polylines</code> fields by <code>label</code> value\\ in the Filters Sidebar\\ \\</li> <li>Moved plot tabs into an expandable window that can be resized and maximized.\\ This allows for viewing distributions and the sample grid at the same time\\ \\</li> <li>Fixed video loading in the grid and modal for video samples with metadata\\ \\</li> <li>Fixed showing and hiding samples in the select sample menu\\ \\</li> <li>Fixed a memory usage bug in the sample grid\\ \\ \\ Core\\ \\</li> <li>Added Cityscapes and\\ LFW to the\\ Dataset Zoo\\ \\</li> <li>Added support for renaming and deleting embedded document fields of samples\\ via <code>Dataset.rename_sample_field()</code> and\\ <code>Dataset.delete_sample_field()</code>\\ \\</li> <li>Added support for renaming and deleting embedded document fields of frames\\ of video samples via <code>Dataset.rename_frame_field()</code> and\\ <code>Dataset.delete_frame_field()</code>\\ \\</li> <li>Added support for deleting fields and embedded fields of individual samples\\ via <code>Sample.clear_field()</code>\\ and <code>del sample[field_name]</code>\\ \\</li> <li>Added support for deleting fields and embedded fields of frame labels via\\ <code>Frame.clear_field()</code>\\ and <code>del frame[field_name]</code>\\ \\</li> <li>Added support for reading/writing video datasets in JSON format via\\ <code>Dataset.from_json()</code> and\\ <code>SampleCollection.write_json()</code>,\\ respectively\\ \\</li> <li>Added <code>a module</code> for importing and exporting\\ annotations from Scale AI\\ \\</li> <li>Added <code>a module</code> for importing and exporting\\ annotations from Labelbox\\ \\</li> <li>Fixed a bug that prevented\\ <code>Dataset.add_sample()</code> and\\ <code>Dataset.add_samples()</code>\\ from working properly when provided samples that belong to other sample\\ collections\\ \\</li> <li>Fixed a bug that prevented frame labels from being properly cloned when\\ calling <code>Dataset.clone()</code> on\\ video datasets\\ \\ \\ Docs\\ \\</li> <li>Added an Environments page that outlines how\\ to work with local, remote, and cloud data. Includes instructions for AWS,\\ Google Cloud, and Azure\\ \\</li> <li>Add an FAQ page\\ \\ \\ ## FiftyOne 0.6.4 \u00b6\\ \\ Released October 29, 2020\\ \\ App\\ \\</li> <li>Improved page load times for video datasets\\ \\</li> <li>Improved support for frame- and sample-level labels in display options for\\ video datasets\\ \\</li> <li>Added support for all label types in the labels distributions tab\\ \\</li> <li>Added support for selecting and hiding labels in the sample modal\\ \\ \\ Brain\\ \\</li> <li>Added support for computing uniqueness within regions-of-interest specified\\ by a set of detections/polylines when using\\ <code>compute_uniqueness()</code>\\ \\ \\ Core\\ \\</li> <li>Added the\\ <code>filter_labels()</code>\\ view stage, which supersedes the old dedicated per-label-type filtering\\ stages\\ \\</li> <li>Added\\ <code>select_labels()</code>\\ and\\ <code>exclude_labels()</code>\\ to select or exclude labels from a dataset or view\\ \\</li> <li>Added an <code>aggregations framework</code> for\\ computing aggregate values via\\ <code>aggregate()</code>\\ \\</li> <li>Added the\\ <code>selected_labels</code>\\ session attribute, which holds the currently selected labels in the App\\ \\</li> <li>Added support for\\ <code>adding</code>,\\ <code>renaming</code>, and\\ <code>deleting</code>\\ frame-level fields of video datasets\\ \\</li> <li>Added the\\ <code>TorchImagePatchesDataset</code>\\ that emits tensors of patches extracted from images defined by sets of\\ <code>Detections</code> associated with the\\ images\\ \\ \\ ## FiftyOne 0.6.3 \u00b6\\ \\ Released October 20, 2020\\ \\ App\\ \\</li> <li>Added sample-level display options stats, filtering, and toggling for video\\ datasets\\ \\ \\ Core\\ \\</li> <li>Added support for importing\\ and exporting video\\ classification datasets organized as directory trees on disk\\ \\</li> <li>Added BDD100K,\\ HMDB51,\\ and UCF101 to\\ the Dataset Zoo\\ \\</li> <li>Added new versions of COCO that contain\\ instance segmentations to the Dataset Zoo\\ \\</li> <li>Added utilities for selecting labels from datasets via the Python library\\ \\</li> <li>Added a boolean <code>only_matches</code> parameter to all filter stages that enables\\ the user to specify that a view should only contain samples that match the\\ given filter\\ \\</li> <li>Improved performance when ingesting video datasets with frame-level labels\\ \\</li> <li>Added a <code>reencode_videos()</code>\\ utility to re-encode the videos in a sample collection so that they are\\ visualizable in the FiftyOne App\\ \\ \\ ## FiftyOne 0.6.2 \u00b6\\ \\ Released October 15, 2020\\ \\ App\\ \\</li> <li>Improved page and grid load times for video datasets by around 10x\\ \\</li> <li>Added filtering, toggling, and statistics for labels with respect to the\\ frame schema in the display options sidebars for video datasets\\ \\</li> <li>Added margins to the grid view for both image and video datasets\\ \\</li> <li>Fixed list parameter input submission in the view bar\\ \\</li> <li>Fixed an issue causing some label counts to be incorrect after filters are\\ applied\\ \\</li> <li>Added support for using the keyboard to select labels when filtering\\ \\ \\ Brain\\ \\</li> <li><code>compute_uniqueness()</code> and\\ <code>compute_hardness()</code> now support\\ multilabel classification tasks\\ \\ \\ Core\\ \\</li> <li><code>Polyline</code> instances can now represent labels composed of multiple shapes\\ \\</li> <li>Segmentations can now be imported and\\ exported when using\\ COCO Object Detection Format.\\ \\</li> <li>Polylines and keypoints can now be imported and\\ exported when using\\ CVAT image format\\ \\</li> <li>Polylines and keypoints can now be imported and\\ exported when using\\ CVAT video format\\ \\</li> <li>Added support for rendering annotated versions of video samples with their\\ frame labels overlaid via\\ <code>draw_labels()</code>\\ \\</li> <li>Added support for launching quickstarts as\\ remote sessions\\ \\</li> <li>Added <code>Frames.update()</code> and\\ <code>Frames.merge()</code> methods to replace\\ and merge video frames, respectively\\ \\</li> <li>Fixed <code>Dataset.merge_samples()</code>\\ to properly merge the frame-by-frame contents of video samples\\ \\</li> <li>Fixed a bug where <code>sample.copy()</code>\\ would not create a copy of the frames of a video sample\\ \\ \\ ## FiftyOne 0.6.1 \u00b6\\ \\ Released October 7, 2020\\ \\ App\\ \\</li> <li>Added support for visualizing keypoints, polylines, and segmentation masks\\ \\</li> <li>Added autocompletion when selecting <code>SortBy</code> fields in the view bar\\ \\</li> <li>Added support for viewing <code>index</code> fields of <code>Detection</code> labels in the media\\ viewer, if present\\ \\</li> <li>Fixed counting of <code>Classifications</code> fields in the expanded sample view\\ \\</li> <li>Fixed a bug that prevented label filters from fully resetting when a <code>reset</code>\\ or <code>clear</code> button is pressed\\ \\ \\ Core\\ \\</li> <li>Added support for storing <code>keypoints</code>,\\ <code>polylines</code>, and\\ <code>segmentation masks</code> on samples\\ \\</li> <li>Added support for setting an <code>index</code> attribute on <code>Detection</code> instances that\\ defines a unique identifier for an object (e.g., across frames of a video)\\ \\</li> <li>Added support for importing and\\ exporting datasets in\\ YOLOv4 format\\ \\</li> <li>Added support for importing and\\ exporting datasets in\\ CVAT video format\\ \\</li> <li>Added support for importing and\\ exporting video datasets in\\ <code>FiftyOneDataset</code> format\\ \\</li> <li>Added frame field schemas to string representations for video datasets/views\\ \\ \\ CLI\\ \\</li> <li>Added options to\\ fiftyone datasets delete to delete all\\ datasets matching a pattern and all non-persistent datasets\\ \\ \\ Docs\\ \\</li> <li>Added a recipe for merging datasets\\ \\</li> <li>Fixed some table widths and other display issues\\ \\ \\ ## FiftyOne 0.6.0 \u00b6\\ \\ Released October 1, 2020\\ \\ App\\ \\</li> <li>Added support for visualizing video datasets in the App\\ \\ \\ Core\\ \\</li> <li>Added support for storing frame labels on video\\ samples\\ \\</li> <li>Added support for importing and\\ exporting datasets of unlabeled videos\\ \\</li> <li>Added support for importing and\\ exporting labeled video\\ datasets in\\ ETA VideoLabels format.\\ \\</li> <li>Added support for importing and\\ exporting video datasets in\\ custom formats\\ \\</li> <li>Improved the performance of\\ <code>Dataset.rename_sample_field()</code>\\ \\</li> <li>Added support for using disk space when running aggregation pipelines on\\ large datasets\\ \\</li> <li>Added support for automatically creating database indexes when sorting by\\ sample fields, for efficiency\\ \\</li> <li>Fixed issues with serializing vector fields and numpy arrays\\ \\ \\ ## FiftyOne 0.5.6 \u00b6\\ \\ Released September 23, 2020\\ \\ App\\ \\</li> <li>Added autocompletion to view bar stage fields that accept field names (for\\ example, <code>Exists</code>)\\ \\</li> <li>Fixed an issue that would prevent datasets with no numeric labels or scalars\\ from loading in the App\\ \\</li> <li>Fixed an error that could occur when a view included no samples\\ \\</li> <li>Added notifications in the App that are displayed if errors occur on the\\ backend\\ \\</li> <li>Improved keyboard navigation between view bar stages\\ \\ \\ Core\\ \\</li> <li>Added support for loading (possibly-randomized) subsets of datasets when\\ importing them via <code>DatasetImporter</code> instances, or via factory methods such\\ as <code>Dataset.from_dir()</code>\\ \\</li> <li>Added support for optionally skipping unlabeled images when importing image\\ datasets via <code>LabeledImageDatasetImporter</code> instances\\ \\</li> <li>Added a\\ <code>Dataset.merge_samples()</code>\\ method for merging samples in datasets via joining by <code>filepath</code>\\ \\</li> <li>Added a\\ <code>Dataset.rename_sample_field()</code>\\ method for renaming sample fields of datasets\\ \\ \\ ## FiftyOne 0.5.5 \u00b6\\ \\ Released September 15, 2020\\ \\ App\\ \\</li> <li>Added support for filtering samples by numeric fields in the sidebar\\ \\</li> <li>Confidence bounds are now computed for the confidence slider in the label\\ filter - a <code>[0, 1]</code> range is no longer assumed\\ \\</li> <li>Fixed an issue that would cause certain stages to be reevaluated when the view\\ bar was edited\\ \\</li> <li>Improved responsiveness when adding stages in the view bar, filtering, and\\ selecting samples\\ \\</li> <li>Simplified placeholders in the view bar\\ \\</li> <li>Added support for filtering sample JSON in the expanded sample view to match\\ the labels displayed in the media viewer\\ \\</li> <li>Updated the instructions that appear when starting the App before connecting\\ to a session\\ \\ \\ Core\\ \\</li> <li>Added support for <code>Session.wait()</code>\\ for remote sessions, to make starting a remote session from a script easier\\ \\ \\ ## FiftyOne 0.5.4 \u00b6\\ \\ Released September 9, 2020\\ \\ App\\ \\</li> <li>Added support for selecting/excluding samples from the current view in the\\ App by selecting them and then choosing the appropriate option from a sample\\ selection menu\\ \\</li> <li>Added autocomplete when creating new stages in the view bar\\ \\</li> <li>Updated the look-and-feel of the view bar to clarify when a stage and/or the\\ entire view bar are active, and to make the bar more visually consistent with\\ the rest of the App\\ \\</li> <li>Media viewer options are maintained while browsing between samples in\\ expanded sample view\\ \\</li> <li>Improved the look-and-feel of confidence sliders when filtering labels\\ \\</li> <li>Limited floating point numbers to three decimals when rendering them in the\\ media viewer\\ \\</li> <li>Fixed some bugs related to content overflow in the view bar\\ \\ \\ Core\\ \\</li> <li>Added support for exporting <code>Classification</code> labels in dataset formats that\\ expect <code>Detections</code> labels\\ \\</li> <li>Added support for importing/exporting supercategories for datasets in\\ COCO format\\ \\ \\ ## FiftyOne 0.5.3 \u00b6\\ \\ Released September 1, 2020\\ \\ App\\ \\</li> <li>Added support for filtering labels in the expanded sample view\\ \\</li> <li>Added support for displaying detection attributes in the expanded sample view\\ \\</li> <li>Added an option to display confidence when viewing labels in the expanded\\ sample view\\ \\</li> <li>Updated look-and-feel of display options in the expanded sample view to match\\ the main image grid view\\ \\</li> <li>Adopting a default color palette for sample fields in the App that ensures\\ that they are visually distinct\\ \\</li> <li>Fixed a bug that prevented the App from loading empty views\\ \\</li> <li>Fixed a bug that prevented the view bar from using default values for some\\ stage parameters\\ \\ \\ Core\\ \\</li> <li>Added support for checking that a field does not exist via a new boolean\\ parameter of the\\ <code>exists()</code>\\ view stage\\ \\</li> <li>Fixed a bug that prevented FiftyOne from starting for some Windows users\\ \\</li> <li>Fixed a bug that caused\\ <code>take()</code> and\\ <code>shuffle()</code> view\\ stages with random seeds to be regenerated when handing off between the App\\ and Python shell\\ \\ \\ ## FiftyOne 0.5.2 \u00b6\\ \\ Released August 26, 2020\\ \\ App\\ \\</li> <li>Added a label filter to the App that allows you to interactively explore your\\ labels, investigating things like confidence thresholds, individual classes,\\ and more, directly from the App\\ \\</li> <li>Added an App error page with support for refreshing the App if something goes\\ wrong\\ \\</li> <li>The App can now be closed and reopened within the same session\\ \\ \\ Core\\ \\</li> <li>Added a fiftyone quickstart command that\\ downloads a small dataset, launches the App, and prints some suggestions for\\ exploring the dataset\\ \\</li> <li>Added support for multiple simultaneous FiftyOne processes. You can now\\ operate multiple App instances (using different ports), Python shells, and/or\\ CLI processes.\\ \\</li> <li>Added support for automatically expanding labels from multitask formats such\\ as BDDDataset and\\ FiftyOneImageLabelsDataset into\\ separate label fields when importing datasets\\ \\</li> <li>Added support for exporting multiple label fields in supported formats such\\ as BDDDataset and\\ FiftyOneImageLabelsDataset\\ via the <code>export()</code>\\ method\\ \\</li> <li>Added support for filtering fields via the\\ <code>filter_field()</code>\\ method\\ \\</li> <li>Provided a more helpful error message when using the\\ Dataset Zoo with no backend ML framework installed\\ \\</li> <li>Made <code>pycocotools</code> an optional dependency to make installation on Windows\\ easier\\ \\ \\ Docs\\ \\</li> <li>Updated the evaluate object detections\\ tutorial to make it more friendly for execution on CPU-only machines\\ \\</li> <li>Refreshed all App-related media in the docs to reflect the new App design\\ introduced in v0.5.0\\ \\ \\ ## FiftyOne 0.5.1 \u00b6\\ \\ Released August 18, 2020\\ \\ App\\ \\</li> <li>Statistics in the display options sidebar now reflect the current\\ view, not the entire dataset\\ \\</li> <li>Improved image tiling algorithm that prevents single images from filling an\\ entire grid row\\ \\</li> <li>Added support for toggling label visibility within the expanded sample modal\\ \\</li> <li>Improved display of long label and tag names throughout the app\\ \\</li> <li>Enhanced view bar functionality, including keyword arguments, type\\ annotations, error messages, help links, and overall stability improvements\\ \\</li> <li>Added keyboard shortcuts for interacting with the view bar:\\ \\</li> <li><code>DEL</code> and <code>BACKSPACE</code> delete the raised (active) stage\\ \\</li> <li><code>ESC</code> toggles focus on the ViewBar, which activates shortcuts\\ \\</li> <li><code>TAB</code>, <code>ENTER</code>, and <code>ESC</code> submits stage input fields\\ \\</li> <li><code>LEFT</code> and <code>RIGHT ARROW</code> traverses view stages and add-stage buttons\\ \\</li> <li><code>SHIFT + LEFT ARROW</code> and <code>SHIFT + RIGHT ARROW</code> traverse stages\\ \\ \\ Core\\ \\</li> <li>Greatly improved the performance of loading dataset samples from the database\\ \\</li> <li>Added support for <code>renaming</code> and\\ <code>cloning</code> datasets\\ \\</li> <li>Added more string matching operations when\\ querying samples, including\\ <code>starts_with()</code>,\\ <code>ends_with()</code>,\\ <code>contains_str()</code> and\\ <code>matches_str()</code>\\ \\ \\ Docs\\ \\</li> <li>Added a tutorial demonstrating performing error analysis on the\\ Open Images Dataset\\ powered by FiftyOne\\ \\ \\ ## FiftyOne 0.5.0 \u00b6\\ \\ Released August 11, 2020\\ \\ News\\ \\</li> <li>FiftyOne is now open source! Read more about this exciting development\\ in this press release\\ \\ \\ App\\ \\</li> <li>Major design refresh, including a\\ new look-and-feel for the App\\ \\</li> <li>Added view bar that supports constructing dataset views directly in the App\\ \\</li> <li>Redesigned expanded sample view:\\ \\</li> <li>Improved look-and-feel, with modal-style form factor\\ \\</li> <li>Added support for maximizing the media player\\ \\</li> <li>Added support for maximizing the raw sample view\\ \\</li> <li>Added arrow controls to navigate between samples\\ \\ \\ Core\\ \\</li> <li>Added support for importing and\\ exporting FiftyOne datasets via the\\ <code>FiftyOneDataset</code> type\\ \\</li> <li>Added a <code>Dataset.info</code> field that\\ can be used to store dataset-level info in FiftyOne datasets\\ \\</li> <li>Added a <code>shuffle()</code>\\ view stage for randomly shuffling the samples in a dataset\\ \\</li> <li>Upgraded the <code>take()</code>\\ view stage so that each instance of a view maintains a deterministic set of\\ samples\\ \\ \\ ## FiftyOne 0.4.1 \u00b6\\ \\ Released August 4, 2020\\ \\ Core\\ \\</li> <li>Added a powerful <code>fiftyone.core.expressions</code> module for constructing\\ complex DatasetView <code>match()</code>,\\ <code>sort_by()</code>, etc.\\ stages\\ \\</li> <li>Added an\\ <code>evaluate_detections()</code>\\ utility for evaluating object detections in FiftyOne datasets\\ \\</li> <li>Adding support for rendering annotated versions of sample data with their\\ labels overlaid via a\\ <code>draw_labels()</code>\\ method\\ \\ \\ Docs\\ \\</li> <li>Added a tutorial demonstrating\\ object detection evaluation workflows powered by FiftyOne\\ \\</li> <li>Added full documentation for constructing\\ DatasetViews with powerful matching, filtering, and sorting operations\\ \\</li> <li>Added a recipe showing how to render annotated\\ versions of samples with label field(s) overlaid\\ \\</li> <li>Upgraded dataset creation docs\\ that simplify the material and make it easier to find the creation strategy\\ of interest\\ \\</li> <li>Improved layout of tutorials,\\ recipes, and user guide\\ landing pages\\ \\ \\ ## FiftyOne 0.4.0 \u00b6\\ \\ Released July 21, 2020\\ \\ App\\ \\</li> <li>Fixed an issue that could cause launching the App to fail on Windows under\\ Python 3.6 and older\\ \\ \\ Core\\ \\</li> <li>Added support for importing datasets in custom formats via the\\ <code>DatasetImporter</code> interface\\ \\</li> <li>Added support for exporting datasets to disk in custom formats via the\\ <code>DatasetExporter</code> interface\\ \\</li> <li>Added support for parsing individual elements of samples in the\\ <code>SampleParser</code> interface\\ \\</li> <li>Added an option to image loaders in <code>fiftyone.utils.torch</code> to convert\\ images to RGB\\ \\</li> <li>Fixed an issue where\\ <code>Dataset.delete_sample_field()</code>\\ would not permanently delete fields if they were modified after deletion\\ \\</li> <li>Improved the string representation of <code>ViewStage</code> instances\\ \\ \\ Docs\\ \\</li> <li>Added a recipe demonstrating how to\\ convert datasets on disk between common\\ formats\\ \\</li> <li>Added recipes demonstratings how to write your own\\ custom dataset importers,\\ custom dataset exporters, and\\ custom sample parsers\\ \\</li> <li>Added a Configuring FiftyOne page to the User\\ Guide that explains how to customize your FiftyOne Config\\ \\ \\ ## FiftyOne 0.3.0 \u00b6\\ \\ Released June 24, 2020\\ \\ App\\ \\</li> <li>Fixed an issue that could prevent the App from connecting to the FiftyOne\\ backend\\ \\ \\ Core\\ \\</li> <li>Added support for importing and exporting datasets in several common formats:\\ \\</li> <li>COCO: <code>COCODetectionDataset</code>\\ \\</li> <li>VOC: <code>VOCDetectionDataset</code>\\ \\</li> <li>KITTI: <code>KITTIDetectionDataset</code>\\ \\</li> <li>Image classification TFRecords:\\ <code>TFImageClassificationDataset</code>\\ \\</li> <li>TF Object Detection API TFRecords:\\ <code>TFObjectDetectionDataset</code>\\ \\</li> <li>CVAT image: <code>CVATImageDataset</code>\\ \\</li> <li>Berkeley DeepDrive: <code>BDDDataset</code>\\ \\ \\</li> <li>Added <code>Dataset.add_dir()</code> and\\ <code>Dataset.from_dir()</code> to allow\\ for importing datasets on disk in any supported format\\ \\</li> <li>Added a <code>convert_dataset()</code>\\ method to convert between supported dataset formats\\ \\</li> <li>Added support for downloading COCO 2014/2017 through the FiftyOne Dataset Zoo\\ via the Torch backend\\ \\ \\ CLI\\ \\</li> <li>Added <code>fiftyone convert</code> to convert datasets on disk between any supported\\ formats\\ \\</li> <li>Added <code>fiftyone datasets head</code> and <code>fiftyone datasets tail</code> to print the\\ head/tail of datasets\\ \\</li> <li>Added <code>fiftyone datasets stream</code> to stream the samples in a dataset to the\\ terminal with a <code>less</code>-like interface\\ \\</li> <li>Added <code>fiftyone datasets export</code> to export datasets in any available format\\ \\ \\ ## FiftyOne 0.2.1 \u00b6\\ \\ Released June 19, 2020\\ \\ Core\\ \\</li> <li>Added preliminary Windows support\\ \\</li> <li><code>Dataset.add_images_dir()</code>\\ now skips non-images\\ \\</li> <li>Improved performance of adding samples to datasets\\ \\ \\ CLI\\ \\</li> <li>Fixed an issue that could cause port forwarding to hang when initializing a\\ remote session\\ \\ \\ ## FiftyOne 0.2.0 \u00b6\\ \\ Released June 12, 2020\\ \\ App\\ \\</li> <li>Added distribution graphs for label fields\\ \\</li> <li>Fixed an issue causing cached images from previously-loaded datasets to be\\ displayed after loading a new dataset\\ \\ \\ Core\\ \\</li> <li>Added support for persistent datasets\\ \\</li> <li>Added a class-based view stage approach via the <code>ViewStage</code> interface\\ \\</li> <li>Added support for serializing collections as JSON and reading datasets from\\ JSON\\ \\</li> <li>Added support for storing numpy arrays in samples\\ \\</li> <li>Added a config option to control visibility of progress bars\\ \\</li> <li>Added progress reporting to\\ <code>Dataset.add_samples()</code>\\ \\</li> <li>Added a <code>SampleCollection.compute_metadata()</code>\\ method to enable population of the <code>metadata</code> fields of samples\\ \\</li> <li>Improved reliability of shutting down the App and database services\\ \\</li> <li>Improved string representations of <code>Dataset</code> and <code>Sample</code> objects\\ \\ \\ CLI\\ \\</li> <li>Added support for creating datasets and launching the App\\ \\ \\</li> </ul>"},{"location":"api/","title":"Future home of the API docs","text":""},{"location":"cli/","title":"FiftyOne Command-Line Interface (CLI) \u00b6","text":"<p>Installing FiftyOne automatically installs <code>fiftyone</code>, a command-line interface (CLI) for interacting with FiftyOne. This utility provides access to many useful features, including creating and inspecting datasets, visualizing datasets in the App, exporting datasets and converting dataset formats, and downloading datasets from the FiftyOne Dataset Zoo.</p>"},{"location":"cli/#quickstart","title":"Quickstart \u00b6","text":"<p>To see the available top-level commands, type:</p> <pre><code>fiftyone --help\n</code></pre> <p>You can learn more about any available subcommand via:</p> <pre><code>fiftyone &lt;command&gt; --help\n</code></pre> <p>For example, to see your current FiftyOne config, you can execute <code>fiftyone config</code>.</p>"},{"location":"cli/#tab-completion","title":"Tab completion \u00b6","text":"<p>To enable tab completion in <code>bash</code>, add the following line to your <code>~/.bashrc</code>:</p> <pre><code>eval \"$(register-python-argcomplete fiftyone)\"\n</code></pre> <p>To enable tab completion in <code>zsh</code>, add these lines to your <code>~/.zshrc</code>:</p> <pre><code>autoload bashcompinit\nbashcompinit\neval \"$(register-python-argcomplete fiftyone)\"\n</code></pre> <p>To enable tab completion in <code>tcsh</code>, add these lines to your <code>~/.tcshrc</code>:</p> <pre><code>eval `register-python-argcomplete --shell tcsh fiftyone`\n</code></pre>"},{"location":"cli/#fiftyone-cli","title":"FiftyOne CLI \u00b6","text":"<p>The FiftyOne command-line interface.</p> <pre><code>fiftyone [-h] [-v] [--all-help]\n         {quickstart,annotation,brain,evaluation,app,config,constants,convert,datasets,migrate,operators,delegated,plugins,utils,zoo}\n         ...\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  -v, --version         show version info\n  --all-help            show help recursively and exit\n\navailable commands:\n  {quickstart,annotation,brain,evaluation,app,config,constants,convert,datasets,migrate,operators,delegated,plugins,utils,zoo}\n    quickstart          Launch a FiftyOne quickstart.\n    annotation          Tools for working with the FiftyOne annotation API.\n    brain               Tools for working with the FiftyOne Brain.\n    evaluation          Tools for working with the FiftyOne evaluation API.\n    app                 Tools for working with the FiftyOne App.\n    config              Tools for working with your FiftyOne config.\n    constants           Print constants from `fiftyone.constants`.\n    convert             Convert datasets on disk between supported formats.\n    datasets            Tools for working with FiftyOne datasets.\n    migrate             Tools for migrating the FiftyOne database.\n    operators           Tools for working with FiftyOne operators.\n    delegated           Tools for working with FiftyOne delegated operations.\n    plugins             Tools for working with FiftyOne plugins.\n    utils               FiftyOne utilities.\n    zoo                 Tools for working with the FiftyOne Zoo.\n</code></pre>"},{"location":"cli/#fiftyone-quickstart","title":"FiftyOne quickstart \u00b6","text":"<p>Launch a FiftyOne quickstart.</p> <pre><code>fiftyone quickstart [-h] [-v] [-p PORT] [-A ADDRESS] [-r] [-a] [-w WAIT]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  -v, --video           launch the quickstart with a video dataset\n  -p PORT, --port PORT  the port number to use\n  -A ADDRESS, --address ADDRESS\n                        the address (server name) to use\n  -r, --remote          whether to launch a remote App session\n  -w WAIT, --wait WAIT  the number of seconds to wait for a new App\n                        connection before returning if all connections are\n                        lost. If negative, the process will wait forever,\n                        regardless of connections\n</code></pre> <p>Examples</p> <pre><code># Launch the quickstart\nfiftyone quickstart\n</code></pre> <pre><code># Launch the quickstart with a video dataset\nfiftyone quickstart --video\n</code></pre> <pre><code># Launch the quickstart as a remote session\nfiftyone quickstart --remote\n</code></pre>"},{"location":"cli/#fiftyone-config","title":"FiftyOne config \u00b6","text":"<p>Tools for working with your FiftyOne config.</p> <pre><code>fiftyone config [-h] [-l] [FIELD]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  FIELD         a config field to print\n\noptional arguments:\n  -h, --help    show this help message and exit\n  -l, --locate  print the location of your config on disk\n</code></pre> <p>Examples</p> <pre><code># Print your entire config\nfiftyone config\n</code></pre> <pre><code># Print a specific config field\nfiftyone config &lt;field&gt;\n</code></pre> <pre><code># Print the location of your config on disk (if one exists)\nfiftyone config --locate\n</code></pre>"},{"location":"cli/#print-constants","title":"Print constants \u00b6","text":"<p>Print constants from <code>fiftyone.constants</code>.</p> <pre><code>fiftyone constants [-h] [CONSTANT]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  CONSTANT    the constant to print\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre> <p>Examples</p> <pre><code># Print all constants\nfiftyone constants\n</code></pre> <pre><code># Print a specific constant\nfiftyone constants &lt;CONSTANT&gt;\n</code></pre>"},{"location":"cli/#convert-dataset-formats","title":"Convert dataset formats \u00b6","text":"<p>Convert datasets on disk between supported formats.</p> <pre><code>fiftyone convert [-h] --input-type INPUT_TYPE --output-type OUTPUT_TYPE\n                 [--input-dir INPUT_DIR]\n                 [--input-kwargs KEY=VAL [KEY=VAL ...]]\n                 [--output-dir OUTPUT_DIR]\n                 [--output-kwargs KEY=VAL [KEY=VAL ...]] [-o]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  --input-dir INPUT_DIR\n                        the directory containing the dataset\n  --input-kwargs KEY=VAL [KEY=VAL ...]\n                        additional keyword arguments for\n                        `fiftyone.utils.data.convert_dataset(..., input_kwargs=)`\n  --output-dir OUTPUT_DIR\n                        the directory to which to write the output dataset\n  --output-kwargs KEY=VAL [KEY=VAL ...]\n                        additional keyword arguments for\n                        `fiftyone.utils.data.convert_dataset(..., output_kwargs=)`\n  -o, --overwrite       whether to overwrite an existing output directory\n\nrequired arguments:\n  --input-type INPUT_TYPE\n                        the fiftyone.types.Dataset type of the input dataset\n  --output-type OUTPUT_TYPE\n                        the fiftyone.types.Dataset type to output\n</code></pre> <p>Examples</p> <pre><code># Convert an image classification directory tree to TFRecords format\nfiftyone convert \\\n    --input-dir /path/to/image-classification-directory-tree \\\n    --input-type fiftyone.types.ImageClassificationDirectoryTree \\\n    --output-dir /path/for/tf-image-classification-dataset \\\n    --output-type fiftyone.types.TFImageClassificationDataset\n</code></pre> <pre><code># Convert a COCO detection dataset to CVAT image format\nfiftyone convert \\\n    --input-dir /path/to/coco-detection-dataset \\\n    --input-type fiftyone.types.COCODetectionDataset \\\n    --output-dir /path/for/cvat-image-dataset \\\n    --output-type fiftyone.types.CVATImageDataset\n</code></pre> <pre><code># Perform a customized conversion via optional kwargs\nfiftyone convert \\\n    --input-dir /path/to/coco-detection-dataset \\\n    --input-type fiftyone.types.COCODetectionDataset \\\n    --input-kwargs max_samples=100 shuffle=True \\\n    --output-dir /path/for/cvat-image-dataset \\\n    --output-type fiftyone.types.TFObjectDetectionDataset \\\n    --output-kwargs force_rgb=True \\\n    --overwrite\n</code></pre>"},{"location":"cli/#fiftyone-datasets","title":"FiftyOne datasets \u00b6","text":"<p>Tools for working with FiftyOne datasets.</p> <pre><code>fiftyone datasets [-h] [--all-help]\n                  {list,info,create,head,tail,stream,export,delete} ...\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  --all-help            show help recursively and exit\n\navailable commands:\n  {list,info,create,head,tail,stream,export,delete}\n    list                List FiftyOne datasets.\n    info                Print information about FiftyOne datasets.\n    stats               Print stats about FiftyOne datasets on disk.\n    create              Tools for creating FiftyOne datasets.\n    head                Prints the first few samples in a FiftyOne dataset.\n    tail                Prints the last few samples in a FiftyOne dataset.\n    stream              Streams the samples in a FiftyOne dataset.\n    export              Export FiftyOne datasets to disk in supported formats.\n    draw                Writes annotated versions of samples in FiftyOne datasets to disk.\n    rename              Rename FiftyOne datasets.\n    delete              Delete FiftyOne datasets.\n</code></pre>"},{"location":"cli/#list-datasets","title":"List datasets \u00b6","text":"<p>List FiftyOne datasets.</p> <pre><code>fiftyone datasets list [-h] [-p PATT] [-t TAG [TAG ...]]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help        show this help message and exit\n  -p PATT, --glob-patt PATT\n                    an optional glob pattern of dataset names to include\n  -t TAG [TAG ...], --tags TAG [TAG ...]\n                    only show datasets with the given tag(s)\n</code></pre> <p>Examples</p> <pre><code># List available datasets\nfiftyone datasets list\n</code></pre> <pre><code># List datasets matching a given pattern\nfiftyone datasets list --glob-patt 'quickstart-*'\n</code></pre> <pre><code># List datasets with the given tag(s)\nfiftyone datasets list --tags automotive healthcare\n</code></pre>"},{"location":"cli/#print-dataset-information","title":"Print dataset information \u00b6","text":"<p>Print information about FiftyOne datasets.</p> <pre><code>fiftyone datasets info [-h] [-p PATT] [-t TAG [TAG ...]] [-s FIELD] [-r] [NAME]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                  the name of a dataset\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -p PATT, --glob-patt PATT\n                        an optional glob pattern of dataset names to include\n  -t TAG [TAG ...], --tags TAG [TAG ...]\n                        only show datasets with the given tag(s)\n  -s FIELD, --sort-by FIELD\n                        a field to sort the dataset rows by\n  -r, --reverse         whether to print the results in reverse order\n</code></pre> <p>Examples</p> <pre><code># Print basic information about multiple datasets\nfiftyone datasets info\nfiftyone datasets info --glob-patt 'quickstart-*'\nfiftyone datasets info --tags automotive healthcare\nfiftyone datasets info --sort-by created_at\nfiftyone datasets info --sort-by name --reverse\n</code></pre> <pre><code># Print information about a specific dataset\nfiftyone datasets info &lt;name&gt;\n</code></pre>"},{"location":"cli/#print-dataset-stats","title":"Print dataset stats \u00b6","text":"<p>Print stats about FiftyOne datasets on disk.</p> <pre><code>fiftyone datasets stats [-h] [-m] [-c] NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                 the name of the dataset\n\noptional arguments:\n  -h, --help           show this help message and exit\n  -m, --include-media  whether to include stats about the size of the raw\n                       media in the dataset\n  -c, --compressed     whether to return the sizes of collections in their\n                       compressed form on disk\n</code></pre> <p>Examples</p> <pre><code># Print stats about the given dataset on disk\nfiftyone datasets stats &lt;name&gt;\n</code></pre>"},{"location":"cli/#create-datasets","title":"Create datasets \u00b6","text":"<p>Tools for creating FiftyOne datasets.</p> <pre><code>fiftyone datasets create [-h] [-n NAME] [-d DATASET_DIR] [-j JSON_PATH]\n                         [-t TYPE] [-k KEY=VAL [KEY=VAL ...]]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  -n NAME, --name NAME  a name for the dataset\n  -d DATASET_DIR, --dataset-dir DATASET_DIR\n                        the directory containing the dataset\n  -j JSON_PATH, --json-path JSON_PATH\n                        the path to a samples JSON file to load\n  -t TYPE, --type TYPE  the fiftyone.types.Dataset type of the dataset\n  -k KEY=VAL [KEY=VAL ...], --kwargs KEY=VAL [KEY=VAL ...]\n                        additional type-specific keyword arguments for\n                        `fiftyone.core.dataset.Dataset.from_dir()`\n</code></pre> <p>Examples</p> <pre><code># Create a dataset from the given data on disk\nfiftyone datasets create \\\n    --name &lt;name&gt; --dataset-dir &lt;dataset-dir&gt; --type &lt;type&gt;\n</code></pre> <pre><code># Create a dataset from a random subset of the data on disk\nfiftyone datasets create \\\n    --name &lt;name&gt; --dataset-dir &lt;dataset-dir&gt; --type &lt;type&gt; \\\n    --kwargs max_samples=50 shuffle=True\n</code></pre> <pre><code># Create a dataset from the given samples JSON file\nfiftyone datasets create --json-path &lt;json-path&gt;\n</code></pre>"},{"location":"cli/#print-dataset-head","title":"Print dataset head \u00b6","text":"<p>Prints the first few samples in a FiftyOne dataset.</p> <pre><code>fiftyone datasets head [-h] [-n NUM_SAMPLES] NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                  the name of the dataset\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -n NUM_SAMPLES, --num-samples NUM_SAMPLES\n                        the number of samples to print\n</code></pre> <p>Examples</p> <pre><code># Prints the first few samples in a dataset\nfiftyone datasets head &lt;name&gt;\n</code></pre> <pre><code># Prints the given number of samples from the head of a dataset\nfiftyone datasets head &lt;name&gt; --num-samples &lt;num-samples&gt;\n</code></pre>"},{"location":"cli/#print-dataset-tail","title":"Print dataset tail \u00b6","text":"<p>Prints the last few samples in a FiftyOne dataset.</p> <pre><code>fiftyone datasets tail [-h] [-n NUM_SAMPLES] NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                  the name of the dataset\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -n NUM_SAMPLES, --num-samples NUM_SAMPLES\n                        the number of samples to print\n</code></pre> <p>Examples</p> <pre><code># Print the last few samples in a dataset\nfiftyone datasets tail &lt;name&gt;\n</code></pre> <pre><code># Print the given number of samples from the tail of a dataset\nfiftyone datasets tail &lt;name&gt; --num-samples &lt;num-samples&gt;\n</code></pre>"},{"location":"cli/#stream-samples-to-the-terminal","title":"Stream samples to the terminal \u00b6","text":"<p>Stream samples in a FiftyOne dataset to the terminal.</p> <pre><code>fiftyone datasets stream [-h] NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME        the name of the dataset\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre> <p>Examples</p> <pre><code># Stream the samples of the dataset to the terminal\nfiftyone datasets stream &lt;name&gt;\n</code></pre>"},{"location":"cli/#export-datasets","title":"Export datasets \u00b6","text":"<p>Export FiftyOne datasets to disk in supported formats.</p> <pre><code>fiftyone datasets export [-h] [-d EXPORT_DIR] [-j JSON_PATH]\n                         [-f LABEL_FIELD] [-t TYPE]\n                         [--filters KEY=VAL [KEY=VAL ...]]\n                         [-k KEY=VAL [KEY=VAL ...]]\n                         NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                  the name of the dataset to export\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d EXPORT_DIR, --export-dir EXPORT_DIR\n                        the directory in which to export the dataset\n  -j JSON_PATH, --json-path JSON_PATH\n                        the path to export the dataset in JSON format\n  -f LABEL_FIELD, --label-field LABEL_FIELD\n                        the name of the label field to export\n  -t TYPE, --type TYPE  the fiftyone.types.Dataset type in which to export\n  --filters KEY=VAL [KEY=VAL ...]\n                        specific sample tags or class labels to export. To\n                        use sample tags, pass tags as `tags=train,val` and\n                        to use label filters, pass label field and values\n                        as in ground_truth=car,person,dog\n  -k KEY=VAL [KEY=VAL ...], --kwargs KEY=VAL [KEY=VAL ...]\n                        additional type-specific keyword arguments for\n                        `fiftyone.core.collections.SampleCollection.export()`\n</code></pre> <p>Examples</p> <pre><code># Export the dataset to disk in the specified format\nfiftyone datasets export &lt;name&gt; \\\n    --export-dir &lt;export-dir&gt; --type &lt;type&gt; --label-field &lt;label-field&gt;\n</code></pre> <pre><code># Export the dataset to disk in JSON format\nfiftyone datasets export &lt;name&gt; --json-path &lt;json-path&gt;\n</code></pre> <pre><code># Only export cats and dogs from the validation split\nfiftyone datasets export &lt;name&gt; \\\\\n    --filters tags=validation ground_truth=cat,dog \\\\\n    --export-dir &lt;export-dir&gt; --type &lt;type&gt; --label-field ground_truth\n</code></pre> <pre><code># Perform a customized export of a dataset\nfiftyone datasets export &lt;name&gt; \\\n    --type &lt;type&gt; \\\n    --kwargs labels_path=/path/for/labels.json\n</code></pre>"},{"location":"cli/#drawing-labels-on-samples","title":"Drawing labels on samples \u00b6","text":"<p>Renders annotated versions of samples in FiftyOne datasets to disk.</p> <pre><code>fiftyone datasets draw [-h] [-d OUTPUT_DIR] [-f LABEL_FIELDS] NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                  the name of the dataset\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d OUTPUT_DIR, --output-dir OUTPUT_DIR\n                        the directory to write the annotated media\n  -f LABEL_FIELDS, --label-fields LABEL_FIELDS\n                        a comma-separated list of label fields to export\n</code></pre> <p>Examples</p> <pre><code># Write annotated versions of the media in the dataset with the\n# specified label field(s) overlaid to disk\nfiftyone datasets draw &lt;name&gt; \\\n    --output-dir &lt;output-dir&gt; --label-fields &lt;list&gt;,&lt;of&gt;,&lt;fields&gt;\n</code></pre>"},{"location":"cli/#rename-datasets","title":"Rename datasets \u00b6","text":"<p>Rename FiftyOne datasets.</p> <pre><code>fiftyone datasets rename [-h] NAME NEW_NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME        the name of the dataset\n  NEW_NAME    a new name for the dataset\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre> <p>Examples</p> <pre><code># Rename the dataset\nfiftyone datasets rename &lt;old-name&gt; &lt;new-name&gt;\n</code></pre>"},{"location":"cli/#delete-datasets","title":"Delete datasets \u00b6","text":"<p>Delete FiftyOne datasets.</p> <pre><code>fiftyone datasets delete [-h] [-g GLOB_PATT] [--non-persistent]\n                         [NAME [NAME ...]]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                  the dataset name(s) to delete\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -g GLOB_PATT, --glob-patt GLOB_PATT\n                        a glob pattern of datasets to delete\n  --non-persistent      delete all non-persistent datasets\n</code></pre> <p>Examples</p> <pre><code># Delete the datasets with the given name(s)\nfiftyone datasets delete &lt;name1&gt; &lt;name2&gt; ...\n</code></pre> <pre><code># Delete the datasets whose names match the given glob pattern\nfiftyone datasets delete --glob-patt &lt;glob-patt&gt;\n</code></pre> <pre><code># Delete all non-persistent datasets\nfiftyone datasets delete --non-persistent\n</code></pre>"},{"location":"cli/#fiftyone-migrations","title":"FiftyOne migrations \u00b6","text":"<p>Tools for migrating the FiftyOne database.</p> <p>See this page for more information about migrating FiftyOne deployments.</p> <pre><code>fiftyone migrate [-h] [-i] [-a]\n                 [-v VERSION]\n                 [-n DATASET_NAME [DATASET_NAME ...]]\n                 [--error-level LEVEL]\n                 [--verbose]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  -i, --info            whether to print info about the current revisions\n  -a, --all             whether to migrate the database and all datasets\n  -v VERSION, --version VERSION\n                        the revision to migrate to\n  -n DATASET_NAME [DATASET_NAME ...], --dataset-name DATASET_NAME [DATASET_NAME ...]\n                        the name of a specific dataset to migrate\n  --error-level LEVEL   the error level (0=error, 1=warn, 2=ignore) to use\n                        when migrating individual datasets\n  --verbose             whether to log incremental migrations that are performed\n</code></pre> <p>Examples</p> <pre><code># Print information about the current revisions of all datasets\nfiftyone migrate --info\n</code></pre> <pre><code># Migrate the database and all datasets to the current client version\nfiftyone migrate --all\n</code></pre> <pre><code># Migrate to a specific revision\nfiftyone migrate --all --version &lt;VERSION&gt;\n</code></pre> <pre><code># Migrate a specific dataset\nfiftyone migrate ... --dataset-name &lt;DATASET_NAME&gt;\n</code></pre> <pre><code># Update the database version without migrating any existing datasets\nfiftyone migrate\n</code></pre>"},{"location":"cli/#fiftyone-operators","title":"FiftyOne operators \u00b6","text":"<p>Tools for working with FiftyOne operators and panels.</p> <pre><code>fiftyone operators [-h] [--all-help] {list,info} ...\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help   show this help message and exit\n  --all-help   show help recursively and exit\n\navailable commands:\n  {list,info}\n    list       List operators and panels that you've installed locally.\n    info       Prints information about operators and panels that you've installed locally.\n</code></pre>"},{"location":"cli/#list-operators","title":"List operators \u00b6","text":"<p>List operators and panels that you\u2019ve installed locally.</p> <pre><code>fiftyone operators list [-h] [-e] [-d] [-o] [-p] [-n]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  -e, --enabled         only show enabled operators and panels\n  -d, --disabled        only show disabled operators and panels\n  -o, --operators-only  only show operators\n  -p, --panels-only     only show panels\n  -n, --names-only      only show names\n</code></pre> <p>Examples</p> <pre><code># List all available operators and panels\nfiftyone operators list\n</code></pre> <pre><code># List enabled operators and panels\nfiftyone operators list --enabled\n</code></pre> <pre><code># List disabled operators and panels\nfiftyone operators list --disabled\n</code></pre> <pre><code># Only list panels\nfiftyone operators list --panels-only\n</code></pre>"},{"location":"cli/#operator-info","title":"Operator info \u00b6","text":"<p>Prints information about operators and panels that you\u2019ve installed locally.</p> <pre><code>fiftyone operators info [-h] URI\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  URI         the operator or panel URI\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre> <p>Examples</p> <pre><code># Prints information about an operator or panel\nfiftyone operators info &lt;uri&gt;\n</code></pre>"},{"location":"cli/#fiftyone-delegated-operations","title":"FiftyOne delegated operations \u00b6","text":"<p>Tools for working with FiftyOne delegated operations.</p> <pre><code>fiftyone delegated [-h] [--all-help] {launch,list,info,fail,delete,cleanup} ...\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help   show this help message and exit\n  --all-help   show help recursively and exit\n\navailable commands:\n  {launch,list,info,fail,delete,cleanup}\n    launch              Launches a service for running delegated operations.\n    list                List delegated operations.\n    info                Prints information about a delegated operation.\n    fail                Manually mark delegated as failed.\n    delete              Delete delegated operations.\n    cleanup             Cleanup delegated operations.\n</code></pre>"},{"location":"cli/#launch-delegated-service","title":"Launch delegated service \u00b6","text":"<p>Launches a service for running delegated operations.</p> <pre><code>fiftyone delegated launch [-h] [-t TYPE]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  -t TYPE, --type TYPE  the type of service to launch. The default is 'local'\n</code></pre> <p>Examples</p> <pre><code># Launch a local service\nfiftyone delegated launch\n</code></pre>"},{"location":"cli/#list-delegated-operations","title":"List delegated operations \u00b6","text":"<p>List delegated operations.</p> <pre><code>fiftyone delegated list [-h]\n                        [-o OPERATOR]\n                        [-d DATASET]\n                        [-s STATE]\n                        [--sort-by SORT_BY]\n                        [--reverse]\n                        [-l LIMIT]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  -o OPERATOR, --operator OPERATOR\n                        only list operations for this operator\n  -d DATASET, --dataset DATASET\n                        only list operations for this dataset\n  -s STATE, --state STATE\n                        only list operations with this state. Supported\n                        values are ('SCHEDULED', 'QUEUED', 'RUNNING', 'COMPLETED', 'FAILED')\n  --sort-by SORT_BY     how to sort the operations. Supported values are\n                        ('SCHEDULED_AT', 'QUEUED_AT', 'STARTED_AT', COMPLETED_AT', 'FAILED_AT', 'OPERATOR')\n  --reverse             whether to sort in reverse order\n  -l LIMIT, --limit LIMIT\n                        a maximum number of operations to show\n</code></pre> <p>Examples</p> <pre><code># List all delegated operations\nfiftyone delegated list\n</code></pre> <pre><code># List some specific delegated operations\nfiftyone delegated list \\\n    --dataset quickstart \\\n    --operator @voxel51/io/export_samples \\\n    --state COMPLETED \\\n    --sort-by COMPLETED_AT \\\n    --limit 10\n</code></pre>"},{"location":"cli/#delegated-operation-info","title":"Delegated operation info \u00b6","text":"<p>Prints information about a delegated operation.</p> <pre><code>fiftyone delegated info [-h] ID\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  ID          the operation ID\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre> <p>Examples</p> <pre><code># Print information about a delegated operation\nfiftyone delegated info &lt;id&gt;\n</code></pre>"},{"location":"cli/#mark-delegated-operations-as-failed","title":"Mark delegated operations as failed \u00b6","text":"<p>Manually mark delegated operations as failed.</p> <pre><code>fiftyone delegated fail [-h] [IDS ...]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  IDS         an operation ID or list of operation IDs\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre> <p>Examples</p> <pre><code># Manually mark the specified operation(s) as FAILED\nfiftyone delegated fail &lt;id1&gt; &lt;id2&gt; ...\n</code></pre>"},{"location":"cli/#delete-delegated-operations","title":"Delete delegated operations \u00b6","text":"<p>Delete delegated operations.</p> <pre><code>fiftyone delegated delete [-h] [IDS ...]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  IDS         an operation ID or list of operation IDs\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre> <p>Examples</p> <pre><code># Delete the specified operation(s)\nfiftyone delegated delete &lt;id1&gt; &lt;id2&gt; ...\n</code></pre>"},{"location":"cli/#cleanup-delegated-operations","title":"Cleanup delegated operations \u00b6","text":"<p>Cleanup delegated operations.</p> <pre><code>fiftyone delegated cleanup [-h]\n                           [-o OPERATOR]\n                           [-d DATASET]\n                           [-s STATE]\n                           [--orphan]\n                           [--dry-run]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  -o OPERATOR, --operator OPERATOR\n                        cleanup operations for this operator\n  -d DATASET, --dataset DATASET\n                        cleanup operations for this dataset\n  -s STATE, --state STATE\n                        delete operations in this state. Supported values\n                        are ('QUEUED', 'COMPLETED', 'FAILED')\n  --orphan              delete all operations associated with non-existent\n                        datasets\n  --dry-run             whether to print information rather than actually\n                        deleting operations\n</code></pre> <p>Examples</p> <pre><code># Delete all failed operations associated with a given dataset\nfiftyone delegated cleanup --dataset quickstart --state FAILED\n</code></pre> <pre><code># Delete all delegated operations associated with non-existent datasets\nfiftyone delegated cleanup --orphan\n</code></pre> <pre><code># Print information about operations rather than actually deleting them\nfiftyone delegated cleanup --orphan --dry-run\n</code></pre>"},{"location":"cli/#fiftyone-plugins","title":"FiftyOne plugins \u00b6","text":"<p>Tools for working with FiftyOne plugins.</p> <pre><code>fiftyone plugins [-h] [--all-help] {list,info,download,requirements,create,enable,disable,delete} ...\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  --all-help            show help recursively and exit\n\navailable commands:\n  {list,info,download,requirements,create,enable,disable,delete}\n    list                List plugins that you've downloaded or created locally.\n    info                Prints information about plugins that you've downloaded or created\n    download            Download plugins from the web.\n    requirements        Handles package requirements for plugins.\n    create              Creates or initializes a plugin.\n    enable              Enables the given plugin(s).\n    disable             Disables the given plugin(s).\n    delete              Delete plugins from your local machine.\n</code></pre>"},{"location":"cli/#list-plugins","title":"List plugins \u00b6","text":"<p>List plugins that you\u2019ve downloaded or created locally.</p> <pre><code>fiftyone plugins list [-h] [-e] [-d] [-n]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help        show this help message and exit\n  -e, --enabled     only show enabled plugins\n  -d, --disabled    only show disabled plugins\n  -n, --names-only  only show plugin names\n</code></pre> <p>Examples</p> <pre><code># List all locally available plugins\nfiftyone plugins list\n</code></pre> <pre><code># List enabled plugins\nfiftyone plugins list --enabled\n</code></pre> <pre><code># List disabled plugins\nfiftyone plugins list --disabled\n</code></pre>"},{"location":"cli/#plugin-info","title":"Plugin info \u00b6","text":"<p>List plugins that you\u2019ve downloaded or created locally.</p> <pre><code>fiftyone plugins info [-h] NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME        the plugin name\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre> <p>Examples</p> <pre><code># Prints information about a plugin\nfiftyone plugins info &lt;name&gt;\n</code></pre>"},{"location":"cli/#download-plugins","title":"Download plugins \u00b6","text":"<p>Download plugins from the web.</p> <p>When downloading plugins from GitHub, you can provide any of the following formats:</p> <ul> <li> <p>a GitHub repo URL like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;</code></p> </li> <li> <p>a GitHub ref like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/tree/&lt;branch&gt;</code> or <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/commit/&lt;commit&gt;</code></p> </li> <li> <p>a GitHub ref string like <code>&lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;]</code></p> </li> </ul> <p>Note</p> <p>To download from a private GitHub repository that you have access to, provide your GitHub personal access token by setting the <code>GITHUB_TOKEN</code> environment variable.</p> <pre><code>fiftyone plugins download [-h] [-n [PLUGIN_NAMES ...]] [-o] URL_OR_GH_REPO\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  URL_OR_GH_REPO        A URL or &lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;] of a GitHub repository\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -n [PLUGIN_NAMES ...], --plugin-names [PLUGIN_NAMES ...]\n                        a plugin name or list of plugin names to download\n  -o, --overwrite       whether to overwrite existing plugins\n</code></pre> <p>Examples</p> <pre><code># Download plugins from a GitHub repository URL\nfiftyone plugins download &lt;github-repo-url&gt;\n</code></pre> <pre><code># Download plugins by specifying the GitHub repository details\nfiftyone plugins download &lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;]\n</code></pre> <pre><code># Download specific plugins from a URL\nfiftyone plugins download &lt;url&gt; --plugin-names &lt;name1&gt; &lt;name2&gt; &lt;name3&gt;\n</code></pre>"},{"location":"cli/#plugin-requirements","title":"Plugin requirements \u00b6","text":"<p>Handles package requirements for plugins.</p> <pre><code>fiftyone plugins requirements [-h] [-p] [-i] [-e] [--error-level LEVEL] NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                 the plugin name\n\noptional arguments:\n  -h, --help           show this help message and exit\n  -p, --print          print the requirements for the plugin\n  -i, --install        install any requirements for the plugin\n  -e, --ensure         ensure the requirements for the plugin are satisfied\n  --error-level LEVEL  the error level (0=error, 1=warn, 2=ignore) to use when installing or ensuring plugin requirements\n</code></pre> <p>Examples</p> <pre><code># Print requirements for a plugin\nfiftyone plugins requirements &lt;name&gt; --print\n</code></pre> <pre><code># Install any requirements for the plugin\nfiftyone plugins requirements &lt;name&gt; --install\n</code></pre> <pre><code># Ensures that the requirements for the plugin are satisfied\nfiftyone plugins requirements &lt;name&gt; --ensure\n</code></pre>"},{"location":"cli/#create-plugins","title":"Create plugins \u00b6","text":"<p>Creates or initializes a plugin.</p> <pre><code>fiftyone plugins create [-h]\n                        [-f [FILES ...]]\n                        [-d OUTDIR]\n                        [--label LABEL]\n                        [--description DESCRIPTION]\n                        [--version VERSION]\n                        [-o]\n                        [--kwargs KEY=VAL [KEY=VAL ...]]\n                        [NAME ...]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                  the plugin name(s)\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -f [FILES ...], --from-files [FILES ...]\n                        a directory or list of explicit filepaths to include in the plugin\n  -d OUTDIR, --outdir OUTDIR\n                        a directory in which to create the plugin\n  --label LABEL         a display name for the plugin\n  --description DESCRIPTION\n                        a description for the plugin\n  --version VERSION     an optional FiftyOne version requirement for the plugin\n  -o, --overwrite       whether to overwrite existing plugins\n  --kwargs KEY=VAL [KEY=VAL ...]\n                        additional keyword arguments to include in the plugin definition\n</code></pre> <p>Examples</p> <pre><code># Initialize a new plugin\nfiftyone plugins create &lt;name&gt;\n</code></pre> <pre><code># Create a plugin from existing files\nfiftyone plugins create \\\n    &lt;name&gt; \\\n    --from-files /path/to/dir \\\n    --label &lt;label&gt; \\\n    --description &lt;description&gt;\n</code></pre>"},{"location":"cli/#enable-plugins","title":"Enable plugins \u00b6","text":"<p>Enables the given plugin(s).</p> <pre><code>fiftyone plugins enable [-h] [-a] [NAME ...]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME        the plugin name(s)\n\noptional arguments:\n  -h, --help  show this help message and exit\n  -a, --all   whether to enable all plugins\n</code></pre> <p>Examples</p> <pre><code># Enable a plugin\nfiftyone plugins enable &lt;name&gt;\n</code></pre> <pre><code># Enable multiple plugins\nfiftyone plugins enable &lt;name1&gt; &lt;name2&gt; ...\n</code></pre> <pre><code># Enable all plugins\nfiftyone plugins enable --all\n</code></pre>"},{"location":"cli/#disable-plugins","title":"Disable plugins \u00b6","text":"<p>Disables the given plugin(s).</p> <pre><code>fiftyone plugins disable [-h] [-a] [NAME ...]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME        the plugin name(s)\n\noptional arguments:\n  -h, --help  show this help message and exit\n  -a, --all   whether to disable all plugins\n</code></pre> <p>Examples</p> <pre><code># Disable a plugin\nfiftyone plugins disable &lt;name&gt;\n</code></pre> <pre><code># Disable multiple plugins\nfiftyone plugins disable &lt;name1&gt; &lt;name2&gt; ...\n</code></pre> <pre><code># Disable all plugins\nfiftyone plugins disable --all\n</code></pre>"},{"location":"cli/#delete-plugins","title":"Delete plugins \u00b6","text":"<p>Delete plugins from your local machine.</p> <pre><code>fiftyone plugins delete [-h] [-a] [NAME ...]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME        the plugin name(s)\n\noptional arguments:\n  -h, --help  show this help message and exit\n  -a, --all   whether to delete all plugins\n</code></pre> <p>Examples</p> <pre><code># Delete a plugin from local disk\nfiftyone plugins delete &lt;name&gt;\n</code></pre> <pre><code># Delete multiple plugins from local disk\nfiftyone plugins delete &lt;name1&gt; &lt;name2&gt; ...\n</code></pre> <pre><code># Delete all plugins from local disk\nfiftyone plugins delete --all\n</code></pre>"},{"location":"cli/#fiftyone-utilities","title":"FiftyOne utilities \u00b6","text":"<p>FiftyOne utilities.</p> <pre><code>fiftyone utils [-h] [--all-help]\n               {compute-metadata,transform-images,transform-videos} ...\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  --all-help            show help recursively and exit\n\navailable commands:\n  {compute-metadata,transform-images,transform-videos}\n    compute-metadata    Populates the `metadata` field of all samples in the dataset.\n    transform-images    Transforms the images in a dataset per the specified parameters.\n    transform-videos    Transforms the videos in a dataset per the specified parameters.\n</code></pre>"},{"location":"cli/#compute-metadata","title":"Compute metadata \u00b6","text":"<p>Populates the <code>metadata</code> field of all samples in the dataset.</p> <pre><code>fiftyone utils compute-metadata [-h] [-o] [-n NUM_WORKERS] [-s] DATASET_NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                  the name of the dataset\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -o, --overwrite       whether to overwrite existing metadata\n  -n NUM_WORKERS, --num-workers NUM_WORKERS\n                        a suggested number of worker processes to use\n  -s, --skip-failures   whether to gracefully continue without raising an\n                        error if metadata cannot be computed for a sample\n</code></pre> <p>Examples</p> <pre><code># Populate all missing `metadata` sample fields\nfiftyone utils compute-metadata &lt;dataset-name&gt;\n</code></pre> <pre><code># (Re)-populate the `metadata` field for all samples\nfiftyone utils compute-metadata &lt;dataset-name&gt; --overwrite\n</code></pre>"},{"location":"cli/#transform-images","title":"Transform images \u00b6","text":"<p>Transforms the images in a dataset per the specified parameters.</p> <pre><code>fiftyone utils transform-images [-h] [--size SIZE] [--min-size MIN_SIZE]\n                                [--max-size MAX_SIZE] [-i INTERPOLATION]\n                                [-e EXT] [-f] [--media-field MEDIA_FIELD]\n                                [--output-field OUTPUT_FIELD]\n                                [-o OUTPUT_DIR] [-r REL_DIR]\n                                [--no-update-filepaths]\n                                [-d] [-n NUM_WORKERS] [-s]\n                                DATASET_NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  DATASET_NAME          the name of the dataset\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --size SIZE           a `width,height` for each image. A dimension can be\n                        -1 if no constraint should be applied\n  --min-size MIN_SIZE   a minimum `width,height` for each image. A dimension\n                        can be -1 if no constraint should be applied\n  --max-size MAX_SIZE   a maximum `width,height` for each image. A dimension\n                        can be -1 if no constraint should be applied\n  -i INTERPOLATION, --interpolation INTERPOLATION\n                        an optional `interpolation` argument for `cv2.resize()`\n  -e EXT, --ext EXT     an image format to convert to (e.g., '.png' or '.jpg')\n  -f, --force-reencode  whether to re-encode images whose parameters already\n                        meet the specified values\n  --media-field MEDIA_FIELD\n                        the input field containing the image paths to\n                        transform\n  --output-field OUTPUT_FIELD\n                        an optional field in which to store the paths to\n                        the transformed images. By default, `media_field`\n                        is updated in-place\n  -o OUTPUT_DIR, --output-dir OUTPUT_DIR\n                        an optional output directory in which to write the\n                        transformed images. If none is provided, the images\n                        are updated in-place\n  -r REL_DIR, --rel-dir REL_DIR\n                        an optional relative directory to strip from each\n                        input filepath to generate a unique identifier that\n                        is joined with `output_dir` to generate an output\n                        path for each image\n  --no-update-filepaths\n                        whether to store the output filepaths on the sample\n                        collection\n  -d, --delete-originals\n                        whether to delete the original images after transforming\n  -n NUM_WORKERS, --num-workers NUM_WORKERS\n                        a suggested number of worker processes to use\n  -s, --skip-failures   whether to gracefully continue without raising an\n                        error if an image cannot be transformed\n</code></pre> <p>Examples</p> <pre><code># Convert the images in the dataset to PNGs\nfiftyone utils transform-images &lt;dataset-name&gt; --ext .png --delete-originals\n</code></pre> <pre><code># Ensure that no images in the dataset exceed 1920 x 1080\nfiftyone utils transform-images &lt;dataset-name&gt; --max-size 1920,1080\n</code></pre>"},{"location":"cli/#transform-videos","title":"Transform videos \u00b6","text":"<p>Transforms the videos in a dataset per the specified parameters.</p> <pre><code>fiftyone utils transform-videos [-h] [--fps FPS] [--min-fps MIN_FPS]\n                                [--max-fps MAX_FPS] [--size SIZE]\n                                [--min-size MIN_SIZE] [--max-size MAX_SIZE]\n                                [-r] [-f]\n                                [--media-field MEDIA_FIELD]\n                                [--output-field OUTPUT_FIELD]\n                                [--output-dir OUTPUT_DIR]\n                                [--rel-dir REL_DIR]\n                                [--no-update-filepaths]\n                                [-d] [-s] [-v]\n                                DATASET_NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  DATASET_NAME          the name of the dataset\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --fps FPS             a frame rate at which to resample the videos\n  --min-fps MIN_FPS     a minimum frame rate. Videos with frame rate below\n                        this value are upsampled\n  --max-fps MAX_FPS     a maximum frame rate. Videos with frame rate exceeding\n                        this value are downsampled\n  --size SIZE           a `width,height` for each frame. A dimension can be -1\n                        if no constraint should be applied\n  --min-size MIN_SIZE   a minimum `width,height` for each frame. A dimension\n                        can be -1 if no constraint should be applied\n  --max-size MAX_SIZE   a maximum `width,height` for each frame. A dimension\n                        can be -1 if no constraint should be applied\n  -r, --reencode        whether to re-encode the videos as H.264 MP4s\n  -f, --force-reencode  whether to re-encode videos whose parameters already\n                        meet the specified values\n  --media-field MEDIA_FIELD\n                        the input field containing the video paths to\n                        transform\n  --output-field OUTPUT_FIELD\n                        an optional field in which to store the paths to\n                        the transformed videos. By default, `media_field`\n                        is updated in-place\n  --output-dir OUTPUT_DIR\n                        an optional output directory in which to write the\n                        transformed videos. If none is provided, the videos\n                        are updated in-place\n  --rel-dir REL_DIR     an optional relative directory to strip from each\n                        input filepath to generate a unique identifier that\n                        is joined with `output_dir` to generate an output\n                        path for each video\n  --no-update-filepaths\n                        whether to store the output filepaths on the sample\n                        collection\n  -d, --delete-originals\n                        whether to delete the original videos after transforming\n  -s, --skip-failures   whether to gracefully continue without raising an\n                        error if a video cannot be transformed\n  -v, --verbose         whether to log the `ffmpeg` commands that are executed\n</code></pre> <p>Examples</p> <pre><code># Re-encode the videos in the dataset as H.264 MP4s\nfiftyone utils transform-videos &lt;dataset-name&gt; --reencode\n</code></pre> <pre><code># Ensure that no videos in the dataset exceed 1920 x 1080 and 30fps\nfiftyone utils transform-videos &lt;dataset-name&gt; \\\n    --max-size 1920,1080 --max-fps 30.0\n</code></pre>"},{"location":"cli/#fiftyone-annotation","title":"FiftyOne Annotation \u00b6","text":"<p>Tools for working with the FiftyOne annotation API.</p> <pre><code>fiftyone annotation [-h] [--all-help] {config} ...\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  --all-help            show help recursively and exit\n\navailable commands:\n  {config}\n    config              Tools for working with your FiftyOne annotation config.\n</code></pre>"},{"location":"cli/#annotation-config","title":"Annotation Config \u00b6","text":"<p>Tools for working with your FiftyOne annotation config.</p> <pre><code>fiftyone annotation config [-h] [-l] [FIELD]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  FIELD         an annotation config field to print\n\noptional arguments:\n  -h, --help    show this help message and exit\n  -l, --locate  print the location of your annotation config on disk\n</code></pre> <p>Examples</p> <pre><code># Print your entire annotation config\nfiftyone annotation config\n</code></pre> <pre><code># Print a specific annotation config field\nfiftyone annotation config &lt;field&gt;\n</code></pre> <pre><code># Print the location of your annotation config on disk (if one exists)\nfiftyone annotation config --locate\n</code></pre>"},{"location":"cli/#fiftyone-app","title":"FiftyOne App \u00b6","text":"<p>Tools for working with the FiftyOne App.</p> <pre><code>fiftyone app [-h] [--all-help] {config,launch,view,connect} ...\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  --all-help            show help recursively and exit\n\navailable commands:\n  {config,launch,view,connect}\n    config              Tools for working with your App config.\n    launch              Launch the FiftyOne App.\n    view                View datasets in the App without persisting them to the database.\n    connect             Connect to a remote FiftyOne App.\n</code></pre>"},{"location":"cli/#app-config","title":"App Config \u00b6","text":"<p>Tools for working with your FiftyOne App config.</p> <pre><code>fiftyone app config [-h] [-l] [FIELD]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  FIELD         an App config field to print\n\noptional arguments:\n  -h, --help    show this help message and exit\n  -l, --locate  print the location of your App config on disk\n</code></pre> <p>Examples</p> <pre><code># Print your entire App config\nfiftyone app config\n</code></pre> <pre><code># Print a specific App config field\nfiftyone app config &lt;field&gt;\n</code></pre> <pre><code># Print the location of your App config on disk (if one exists)\nfiftyone app config --locate\n</code></pre>"},{"location":"cli/#launch-the-app","title":"Launch the App \u00b6","text":"<p>Launch the FiftyOne App.</p> <pre><code>fiftyone app launch [-h] [-p PORT] [-A ADDRESS] [-b BROWSER] [-r] [-a] [-w WAIT] [NAME]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                  the name of a dataset to open\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -p PORT, --port PORT  the port number to use\n  -A ADDRESS, --address ADDRESS\n                        the address (server name) to use\n  -r, --remote          whether to launch a remote App session\n  -b BROWSER, --browser BROWSER\n                        the browser to use to open the App\n  -w WAIT, --wait WAIT  the number of seconds to wait for a new App\n                        connection before returning if all connections are\n                        lost. If negative, the process will wait forever,\n                        regardless of connections\n</code></pre> <p>Examples</p> <pre><code># Launch the App\nfiftyone app launch\n</code></pre> <pre><code># Launch the App with the given dataset loaded\nfiftyone app launch &lt;name&gt;\n</code></pre> <pre><code># Launch a remote App session\nfiftyone app launch ... --remote\n</code></pre> <pre><code># Launch an App session with a specific browser\nfiftyone app launch ... --browser &lt;name&gt;\n</code></pre>"},{"location":"cli/#view-datasets-in-app","title":"View datasets in App \u00b6","text":"<p>View datasets in the FiftyOne App without persisting them to the database.</p> <pre><code>fiftyone app view [-h] [-n NAME] [-d DATASET_DIR] [-t TYPE] [-z NAME]\n                  [-s SPLITS [SPLITS ...]] [--images-dir IMAGES_DIR]\n                  [--images-patt IMAGES_PATT] [--videos-dir VIDEOS_DIR]\n                  [--videos-patt VIDEOS_PATT] [-j JSON_PATH] [-p PORT]\n                  [-A ADDRESS] [-r] [-a] [-w WAIT]\n                  [-k KEY=VAL [KEY=VAL ...]]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  -n NAME, --name NAME  a name for the dataset\n  -d DATASET_DIR, --dataset-dir DATASET_DIR\n                        the directory containing the dataset to view\n  -t TYPE, --type TYPE  the fiftyone.types.Dataset type of the dataset\n  -z NAME, --zoo-dataset NAME\n                        the name of a zoo dataset to view\n  -s SPLITS [SPLITS ...], --splits SPLITS [SPLITS ...]\n                        the dataset splits to load\n  --images-dir IMAGES_DIR\n                        the path to a directory of images\n  --images-patt IMAGES_PATT\n                        a glob pattern of images\n  --videos-dir VIDEOS_DIR\n                        the path to a directory of videos\n  --videos-patt VIDEOS_PATT\n                        a glob pattern of videos\n  -j JSON_PATH, --json-path JSON_PATH\n                        the path to a samples JSON file to view\n  -p PORT, --port PORT  the port number to use\n  -A ADDRESS, --address ADDRESS\n                        the address (server name) to use\n  -r, --remote          whether to launch a remote App session\n  -w WAIT, --wait WAIT  the number of seconds to wait for a new App\n                        connection before returning if all connections are\n                        lost. If negative, the process will wait forever,\n                        regardless of connections\n  -k KEY=VAL [KEY=VAL ...], --kwargs KEY=VAL [KEY=VAL ...]\n                        additional type-specific keyword arguments for\n                        `fiftyone.core.dataset.Dataset.from_dir()`\n</code></pre> <p>Examples</p> <pre><code># View a dataset stored on disk in the App\nfiftyone app view --dataset-dir &lt;dataset-dir&gt; --type &lt;type&gt;\n</code></pre> <pre><code># View a zoo dataset in the App\nfiftyone app view --zoo-dataset &lt;name&gt; --splits &lt;split1&gt; ...\n</code></pre> <pre><code># View a directory of images in the App\nfiftyone app view --images-dir &lt;images-dir&gt;\n</code></pre> <pre><code># View a glob pattern of images in the App\nfiftyone app view --images-patt &lt;images-patt&gt;\n</code></pre> <pre><code># View a directory of videos in the App\nfiftyone app view --videos-dir &lt;videos-dir&gt;\n</code></pre> <pre><code># View a glob pattern of videos in the App\nfiftyone app view --videos-patt &lt;videos-patt&gt;\n</code></pre> <pre><code># View a dataset stored in JSON format on disk in the App\nfiftyone app view --json-path &lt;json-path&gt;\n</code></pre> <pre><code># View the dataset in a remote App session\nfiftyone app view ... --remote\n</code></pre> <pre><code># View a random subset of the data stored on disk in the App\nfiftyone app view ... --kwargs max_samples=50 shuffle=True\n</code></pre>"},{"location":"cli/#connect-to-remote-app","title":"Connect to remote App \u00b6","text":"<p>Connect to a remote FiftyOne App in your web browser.</p> <pre><code>fiftyone app connect [-h] [-d DESTINATION] [-p PORT] [-A ADDRESS] [-l PORT]\n                     [-i KEY]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  -d DESTINATION, --destination DESTINATION\n                        the destination to connect to, e.g., [username@]hostname\n  -p PORT, --port PORT  the remote port to connect to\n  -l PORT, --local-port PORT\n                        the local port to use to serve the App\n  -i KEY, --ssh-key KEY\n                        optional ssh key to use to login\n</code></pre> <p>Examples</p> <pre><code># Connect to a remote App with port forwarding already configured\nfiftyone app connect\n</code></pre> <pre><code># Connect to a remote App session\nfiftyone app connect --destination &lt;destination&gt; --port &lt;port&gt;\n</code></pre> <pre><code># Connect to a remote App session using an ssh key\nfiftyone app connect ... --ssh-key &lt;path/to/key&gt;\n</code></pre> <pre><code># Connect to a remote App using a custom local port\nfiftyone app connect ... --local-port &lt;port&gt;\n</code></pre>"},{"location":"cli/#fiftyone-brain","title":"FiftyOne Brain \u00b6","text":"<p>Tools for working with the FiftyOne Brain.</p> <pre><code>fiftyone brain [-h] [--all-help] {config} ...\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  --all-help            show help recursively and exit\n\navailable commands:\n  {config}\n    config              Tools for working with your FiftyOne Brain config.\n</code></pre>"},{"location":"cli/#brain-config","title":"Brain Config \u00b6","text":"<p>Tools for working with your FiftyOne Brain config.</p> <pre><code>fiftyone brain config [-h] [-l] [FIELD]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  FIELD         a brain config field to print\n\noptional arguments:\n  -h, --help    show this help message and exit\n  -l, --locate  print the location of your brain config on disk\n</code></pre> <p>Examples</p> <pre><code># Print your entire brain config\nfiftyone brain config\n</code></pre> <pre><code># Print a specific brain config field\nfiftyone brain config &lt;field&gt;\n</code></pre> <pre><code># Print the location of your brain config on disk (if one exists)\nfiftyone brain config --locate\n</code></pre>"},{"location":"cli/#fiftyone-evaluation","title":"FiftyOne Evaluation \u00b6","text":"<p>Tools for working with the FiftyOne evaluation API.</p> <pre><code>fiftyone evaluation [-h] [--all-help] {config} ...\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  --all-help            show help recursively and exit\n\navailable commands:\n  {config}\n    config              Tools for working with your FiftyOne evaluation config.\n</code></pre>"},{"location":"cli/#evaluation-config","title":"Evaluation Config \u00b6","text":"<p>Tools for working with your FiftyOne evaluation config.</p> <pre><code>fiftyone evaluation config [-h] [-l] [FIELD]\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  FIELD         an evaluation config field to print\n\noptional arguments:\n  -h, --help    show this help message and exit\n  -l, --locate  print the location of your evaluation config on disk\n</code></pre> <p>Examples</p> <pre><code># Print your entire evaluation config\nfiftyone evaluation config\n</code></pre> <pre><code># Print a specific evaluation config field\nfiftyone evaluation config &lt;field&gt;\n</code></pre> <pre><code># Print the location of your evaluation config on disk (if one exists)\nfiftyone evaluation config --locate\n</code></pre>"},{"location":"cli/#fiftyone-zoo","title":"FiftyOne Zoo \u00b6","text":"<p>Tools for working with the FiftyOne Zoo.</p> <pre><code>fiftyone zoo [-h] [--all-help] {datasets,models} ...\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help         show this help message and exit\n  --all-help         show help recursively and exit\n\navailable commands:\n  {datasets,models}\n    datasets         Tools for working with the FiftyOne Dataset Zoo.\n    models           Tools for working with the FiftyOne Model Zoo.\n</code></pre>"},{"location":"cli/#fiftyone-dataset-zoo","title":"FiftyOne Dataset Zoo \u00b6","text":"<p>Tools for working with the FiftyOne Dataset Zoo.</p> <pre><code>fiftyone zoo datasets [-h] [--all-help]\n                      {list,find,info,download,load,delete} ...\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  --all-help            show help recursively and exit\n\navailable commands:\n  {list,find,info,download,load,delete}\n    list                List datasets in the FiftyOne Dataset Zoo.\n    find                Locate a downloaded zoo dataset on disk.\n    info                Print information about datasets in the FiftyOne Dataset Zoo.\n    download            Download zoo datasets.\n    load                Load zoo datasets as persistent FiftyOne datasets.\n    delete              Deletes the local copy of the zoo dataset on disk.\n</code></pre>"},{"location":"cli/#list-datasets-in-zoo","title":"List datasets in zoo \u00b6","text":"<p>List datasets in the FiftyOne Dataset Zoo.</p> <pre><code>fiftyone zoo datasets list [-h] [-n] [-d] [-s SOURCE] [-t TAGS]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  -n, --names-only      only show dataset names\n  -d, --downloaded-only\n                        only show datasets that have been downloaded\n  -s SOURCE, --source SOURCE\n                        only show datasets available from the specified source\n  -t TAGS, --tags TAGS  only show datasets with the specified tag or list,of,tags\n</code></pre> <p>Examples</p> <pre><code># List available datasets\nfiftyone zoo datasets list\n</code></pre> <pre><code># List available dataset names\nfiftyone zoo datasets list --names-only\n</code></pre> <pre><code># List downloaded datasets\nfiftyone zoo datasets list --downloaded-only\n</code></pre> <pre><code># List available datasets from the given source\nfiftyone zoo datasets list --source &lt;source&gt;\n</code></pre> <pre><code># List available datasets with the given tag\nfiftyone zoo datasets list --tags &lt;tag&gt;\n</code></pre>"},{"location":"cli/#find-zoo-datasets-on-disk","title":"Find zoo datasets on disk \u00b6","text":"<p>Locate a downloaded zoo dataset on disk.</p> <pre><code>fiftyone zoo datasets find [-h] [-s SPLIT] NAME_OR_URL\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME_OR_URL           the name or remote location of the dataset\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -s SPLIT, --split SPLIT\n</code></pre> <p>Examples</p> <pre><code># Print the location of a downloaded zoo dataset on disk\nfiftyone zoo datasets find &lt;name&gt;\n</code></pre> <pre><code># Print the location of a remotely-sourced zoo dataset on disk\nfiftyone zoo datasets find https://github.com/&lt;user&gt;/&lt;repo&gt;\nfiftyone zoo datasets find &lt;url&gt;\n</code></pre> <pre><code># Print the location of a specific split of a dataset\nfiftyone zoo datasets find &lt;name&gt; --split &lt;split&gt;\n</code></pre>"},{"location":"cli/#show-zoo-dataset-info","title":"Show zoo dataset info \u00b6","text":"<p>Print information about datasets in the FiftyOne Dataset Zoo.</p> <pre><code>fiftyone zoo datasets info [-h] NAME_OR_URL\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME_OR_URL           the name or remote location of the dataset\n\noptional arguments:\n  -h, --help            show this help message and exit\n</code></pre> <p>Examples</p> <pre><code># Print information about a zoo dataset\nfiftyone zoo datasets info &lt;name&gt;\n</code></pre> <pre><code># Print information about a remote zoo dataset\nfiftyone zoo datasets info https://github.com/&lt;user&gt;/&lt;repo&gt;\nfiftyone zoo datasets info &lt;url&gt;\n</code></pre>"},{"location":"cli/#download-zoo-datasets","title":"Download zoo datasets \u00b6","text":"<p>Download zoo datasets.</p> <p>When downloading remotely-sourced zoo datasets, you can provide any of the following formats:</p> <ul> <li> <p>a GitHub repo URL like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;</code></p> </li> <li> <p>a GitHub ref like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/tree/&lt;branch&gt;</code> or <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/commit/&lt;commit&gt;</code></p> </li> <li> <p>a GitHub ref string like <code>&lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;]</code></p> </li> <li> <p>a publicly accessible URL of an archive (eg zip or tar) file</p> </li> </ul> <p>Note</p> <p>To download from a private GitHub repository that you have access to, provide your GitHub personal access token by setting the <code>GITHUB_TOKEN</code> environment variable.</p> <pre><code>fiftyone zoo datasets download [-h] [-s SPLITS [SPLITS ...]]\n                               [-k KEY=VAL [KEY=VAL ...]]\n                               NAME_OR_URL\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME_OR_URL           the name or remote location of the dataset\n\noptional arguments:\n\n  -h, --help            show this help message and exit\n  -s SPLITS [SPLITS ...], --splits SPLITS [SPLITS ...]\n                        the dataset splits to download\n  -k KEY=VAL [KEY=VAL ...], --kwargs KEY=VAL [KEY=VAL ...]\n                        optional dataset-specific keyword arguments for\n                        `fiftyone.zoo.download_zoo_dataset()`\n</code></pre> <p>Examples</p> <pre><code># Download a zoo dataset\nfiftyone zoo datasets download &lt;name&gt;\n</code></pre> <pre><code># Download a remotely-sourced zoo dataset\nfiftyone zoo datasets download https://github.com/&lt;user&gt;/&lt;repo&gt;\nfiftyone zoo datasets download &lt;url&gt;\n</code></pre> <pre><code># Download the specified split(s) of a zoo dataset\nfiftyone zoo datasets download &lt;name&gt; --splits &lt;split1&gt; ...\n</code></pre> <pre><code># Download a zoo dataset that requires extra keyword arguments\nfiftyone zoo datasets download &lt;name&gt; \\\n    --kwargs source_dir=/path/to/source/files\n</code></pre>"},{"location":"cli/#load-zoo-datasets","title":"Load zoo datasets \u00b6","text":"<p>Load zoo datasets as persistent FiftyOne datasets.</p> <p>When loading remotely-sourced zoo datasets, you can provide any of the following formats:</p> <ul> <li> <p>a GitHub repo URL like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;</code></p> </li> <li> <p>a GitHub ref like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/tree/&lt;branch&gt;</code> or <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/commit/&lt;commit&gt;</code></p> </li> <li> <p>a GitHub ref string like <code>&lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;]</code></p> </li> <li> <p>a publicly accessible URL of an archive (eg zip or tar) file</p> </li> </ul> <p>Note</p> <p>To download from a private GitHub repository that you have access to, provide your GitHub personal access token by setting the <code>GITHUB_TOKEN</code> environment variable.</p> <pre><code>fiftyone zoo datasets load [-h] [-s SPLITS [SPLITS ...]]\n                           [-n DATASET_NAME] [-k KEY=VAL [KEY=VAL ...]]\n                           NAME_OR_URL\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME_OR_URL           the name or remote location of the dataset\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -s SPLITS [SPLITS ...], --splits SPLITS [SPLITS ...]\n                        the dataset splits to load\n  -n DATASET_NAME, --dataset-name DATASET_NAME\n                        a custom name to give the FiftyOne dataset\n  -k KEY=VAL [KEY=VAL ...], --kwargs KEY=VAL [KEY=VAL ...]\n                        additional dataset-specific keyword arguments for\n                        `fiftyone.zoo.load_zoo_dataset()`\n</code></pre> <p>Examples</p> <pre><code># Load the zoo dataset with the given name\nfiftyone zoo datasets load &lt;name&gt;\n</code></pre> <pre><code># Load a remotely-sourced zoo dataset\nfiftyone zoo datasets load https://github.com/&lt;user&gt;/&lt;repo&gt;\nfiftyone zoo datasets load &lt;url&gt;\n</code></pre> <pre><code># Load the specified split(s) of a zoo dataset\nfiftyone zoo datasets load &lt;name&gt; --splits &lt;split1&gt; ...\n</code></pre> <pre><code># Load a zoo dataset with a custom name\nfiftyone zoo datasets load &lt;name&gt; --dataset-name &lt;dataset-name&gt;\n</code></pre> <pre><code># Load a zoo dataset that requires custom keyword arguments\nfiftyone zoo datasets load &lt;name&gt; \\\n    --kwargs source_dir=/path/to/source_files\n</code></pre> <pre><code># Load a random subset of a zoo dataset\nfiftyone zoo datasets load &lt;name&gt; \\\n    --kwargs max_samples=50 shuffle=True\n</code></pre>"},{"location":"cli/#delete-zoo-datasets","title":"Delete zoo datasets \u00b6","text":"<p>Deletes the local copy of the zoo dataset on disk.</p> <pre><code>fiftyone zoo datasets delete [-h] [-s SPLIT] NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                  the name of the dataset\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -s SPLIT, --split SPLIT\n                        a dataset split\n</code></pre> <p>Examples</p> <pre><code># Delete an entire zoo dataset from disk\nfiftyone zoo datasets delete &lt;name&gt;\n</code></pre> <pre><code># Delete a specific split of a zoo dataset from disk\nfiftyone zoo datasets delete &lt;name&gt; --split &lt;split&gt;\n</code></pre>"},{"location":"cli/#fiftyone-model-zoo","title":"FiftyOne Model Zoo \u00b6","text":"<p>Tools for working with the FiftyOne Model Zoo.</p> <pre><code>fiftyone zoo models [-h] [--all-help]\n                    {list,find,info,requirements,download,apply,embed,delete,list-sources,register-source,delete-source}\n                    ...\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  --all-help            show help recursively and exit\n\navailable commands:\n  {list,find,info,requirements,download,apply,embed,delete,register-source,delete-source}\n    list                List models in the FiftyOne Model Zoo.\n    find                Locate the downloaded zoo model on disk.\n    info                Print information about models in the FiftyOne Model Zoo.\n    requirements        Handles package requirements for zoo models.\n    download            Download zoo models.\n    apply               Apply zoo models to datasets.\n    embed               Generate embeddings for datasets with zoo models.\n    delete              Deletes the local copy of the zoo model on disk.\n    list-sources        Lists remote zoo model sources that are registered locally.\n    register-source     Registers a remote source of zoo models.\n    delete-source       Deletes the remote source and all downloaded models associated with it.\n</code></pre>"},{"location":"cli/#list-models-in-zoo","title":"List models in zoo \u00b6","text":"<p>List models in the FiftyOne Model Zoo.</p> <pre><code>fiftyone zoo models list [-h] [-n] [-d] [-t TAGS] [-s SOURCE]\n</code></pre> <p>Arguments</p> <pre><code>optional arguments:\n  -h, --help            show this help message and exit\n  -n, --names-only      only show model names\n  -d, --downloaded-only\n                        only show models that have been downloaded\n  -t TAGS, --tags TAGS  only show models with the specified tag or list,of,tags\n  -s SOURCE, --source SOURCE\n                        only show models available from the specified remote source\n</code></pre> <p>Examples</p> <pre><code># List available models\nfiftyone zoo models list\n</code></pre> <pre><code># List available models (names only)\nfiftyone zoo models list --names-only\n</code></pre> <pre><code># List downloaded models\nfiftyone zoo models list --downloaded-only\n</code></pre> <pre><code># List available models with the given tag\nfiftyone zoo models list --tags &lt;tag&gt;\n</code></pre> <pre><code># List available models from the given remote source\nfiftyone zoo models list --source &lt;source&gt;\n</code></pre>"},{"location":"cli/#find-zoo-models-on-disk","title":"Find zoo models on disk \u00b6","text":"<p>Locate the downloaded zoo model on disk.</p> <pre><code>fiftyone zoo models find [-h] NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                  the name of the model\n\noptional arguments:\n  -h, --help            show this help message and exit\n</code></pre> <p>Examples</p> <pre><code># Print the location of the downloaded zoo model on disk\nfiftyone zoo models find &lt;name&gt;\n</code></pre>"},{"location":"cli/#show-zoo-model-info","title":"Show zoo model info \u00b6","text":"<p>Print information about models in the FiftyOne Model Zoo.</p> <pre><code>fiftyone zoo models info [-h] NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                  the name of the model\n\noptional arguments:\n  -h, --help            show this help message and exit\n</code></pre> <p>Examples</p> <pre><code># Print information about a zoo model\nfiftyone zoo models info &lt;name&gt;\n</code></pre>"},{"location":"cli/#zoo-model-requirements","title":"Zoo model requirements \u00b6","text":"<p>Handles package requirements for zoo models.</p> <pre><code>fiftyone zoo models requirements [-h] [-p] [-i] [-e]\n                                 [--error-level LEVEL]\n                                 NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME                 the name of the model\n\noptional arguments:\n  -h, --help           show this help message and exit\n  -p, --print          print the requirements for the zoo model\n  -i, --install        install any requirements for the zoo model\n  -e, --ensure         ensure the requirements for the zoo model are satisfied\n  --error-level LEVEL  the error level (0=error, 1=warn, 2=ignore) to use\n                       when installing or ensuring model requirements\n</code></pre> <p>Examples</p> <pre><code># Print requirements for a zoo model\nfiftyone zoo models requirements &lt;name&gt; --print\n</code></pre> <pre><code># Install any requirements for the zoo model\nfiftyone zoo models requirements &lt;name&gt; --install\n</code></pre> <pre><code># Ensures that the requirements for the zoo model are satisfied\nfiftyone zoo models requirements &lt;name&gt; --ensure\n</code></pre>"},{"location":"cli/#download-zoo-models","title":"Download zoo models \u00b6","text":"<p>Download zoo models.</p> <p>When downloading remotely-sourced zoo models, you can provide any of the following:</p> <ul> <li> <p>a GitHub repo URL like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;</code></p> </li> <li> <p>a GitHub ref like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/tree/&lt;branch&gt;</code> or <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/commit/&lt;commit&gt;</code></p> </li> <li> <p>a GitHub ref string like <code>&lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;]</code></p> </li> </ul> <p>Note</p> <p>To download from a private GitHub repository that you have access to, provide your GitHub personal access token by setting the <code>GITHUB_TOKEN</code> environment variable.</p> <pre><code>fiftyone zoo models download [-h] [-n MODEL_NAME] [-o] NAME_OR_URL\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME_OR_URL           the name or remote location of the model\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -n MODEL_NAME, --model-name MODEL_NAME\n                        the specific model to download, if `name_or_url` is\n                        a remote source\n  -o, --overwrite       whether to overwrite any existing model files\n</code></pre> <p>Examples</p> <pre><code># Download a zoo model\nfiftyone zoo models download &lt;name&gt;\n</code></pre> <pre><code># Download a remotely-sourced zoo model\nfiftyone zoo models download https://github.com/&lt;user&gt;/&lt;repo&gt; \\\n    --model-name &lt;name&gt;\nfiftyone zoo models download &lt;url&gt; --model-name &lt;name&gt;\n</code></pre>"},{"location":"cli/#apply-zoo-models-to-datasets","title":"Apply zoo models to datasets \u00b6","text":"<p>Apply zoo models to datasets.</p> <p>When applying remotely-sourced zoo models, you can provide any of the following formats:</p> <ul> <li> <p>a GitHub repo URL like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;</code></p> </li> <li> <p>a GitHub ref like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/tree/&lt;branch&gt;</code> or <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/commit/&lt;commit&gt;</code></p> </li> <li> <p>a GitHub ref string like <code>&lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;]</code></p> </li> <li> <p>a publicly accessible URL of an archive (eg zip or tar) file</p> </li> </ul> <p>Note</p> <p>To download from a private GitHub repository that you have access to, provide your GitHub personal access token by setting the <code>GITHUB_TOKEN</code> environment variable.</p> <pre><code>fiftyone zoo models apply [-h] [-n MODEL_NAME] [-b BATCH_SIZE] [-t THRESH]\n                          [-l] [-i] [--error-level LEVEL]\n                          NAME_OR_URL DATASET_NAME LABEL_FIELD\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME_OR_URL           the name or remote location of the zoo model\n  DATASET_NAME          the name of the FiftyOne dataset to process\n  LABEL_FIELD           the name of the field in which to store the predictions\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -n MODEL_NAME, --model-name MODEL_NAME\n                        the specific model to apply, if `name_or_url` is a\n                        remote source\n  -b BATCH_SIZE, --batch-size BATCH_SIZE\n                        an optional batch size to use during inference\n  -t THRESH, --confidence-thresh THRESH\n                        an optional confidence threshold to apply to any\n                        applicable labels generated by the model\n  -l, --store-logits    store logits for the predictions\n  -i, --install         install any requirements for the zoo model\n  --error-level LEVEL   the error level (0=error, 1=warn, 2=ignore) to use\n                        when installing or ensuring model requirements\n</code></pre> <p>Examples</p> <pre><code># Apply a zoo model to a dataset\nfiftyone zoo models apply &lt;model-name&gt; &lt;dataset-name&gt; &lt;label-field&gt;\n</code></pre> <pre><code># Apply a remotely-sourced zoo model to a dataset\nfiftyone zoo models apply https://github.com/&lt;user&gt;/&lt;repo&gt; \\\n    &lt;dataset-name&gt; &lt;label-field&gt; --model-name &lt;model-name&gt;\nfiftyone zoo models apply &lt;url&gt; \\\n    &lt;dataset-name&gt; &lt;label-field&gt; --model-name &lt;model-name&gt;\n</code></pre> <pre><code># Apply a zoo classifier with some customized parameters\nfiftyone zoo models apply \\\n    &lt;model-name&gt; &lt;dataset-name&gt; &lt;label-field&gt; \\\n    --confidence-thresh 0.7 \\\n    --store-logits \\\n    --batch-size 32\n</code></pre>"},{"location":"cli/#generate-embeddings-with-zoo-models","title":"Generate embeddings with zoo models \u00b6","text":"<p>Generate embeddings for datasets with zoo models.</p> <p>When applying remotely-sourced zoo models, you can provide any of the following formats:</p> <ul> <li> <p>a GitHub repo URL like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;</code></p> </li> <li> <p>a GitHub ref like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/tree/&lt;branch&gt;</code> or <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/commit/&lt;commit&gt;</code></p> </li> <li> <p>a GitHub ref string like <code>&lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;]</code></p> </li> <li> <p>a publicly accessible URL of an archive (eg zip or tar) file</p> </li> </ul> <p>Note</p> <p>To download from a private GitHub repository that you have access to, provide your GitHub personal access token by setting the <code>GITHUB_TOKEN</code> environment variable.</p> <pre><code>fiftyone zoo models embed [-h] [-n MODEL_NAME] [-b BATCH_SIZE] [-i]\n                          [--error-level LEVEL]\n                          NAME_OR_URL DATASET_NAME EMBEDDINGS_FIELD\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME_OR_URL           the name or remote location of the zoo model\n  DATASET_NAME          the name of the FiftyOne dataset to process\n  EMBEDDINGS_FIELD      the name of the field in which to store the embeddings\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -n MODEL_NAME, --model-name MODEL_NAME\n                        the specific model to apply, if `name_or_url` is a\n                        remote source\n  -b BATCH_SIZE, --batch-size BATCH_SIZE\n                        an optional batch size to use during inference\n  -i, --install         install any requirements for the zoo model\n  --error-level LEVEL   the error level (0=error, 1=warn, 2=ignore) to use\n                        when installing or ensuring model requirements\n</code></pre> <p>Examples</p> <pre><code># Generate embeddings for a dataset with a zoo model\nfiftyone zoo models embed &lt;model-name&gt; &lt;dataset-name&gt; &lt;embeddings-field&gt;\n</code></pre> <pre><code># Generate embeddings for a dataset with a remotely-sourced zoo model\nfiftyone zoo models embed https://github.com/&lt;user&gt;/&lt;repo&gt; \\\n    &lt;dataset-name&gt; &lt;embeddings-field&gt; --model-name &lt;model-name&gt;\nfiftyone zoo models embed &lt;url&gt; \\\n    &lt;dataset-name&gt; &lt;embeddings-field&gt; --model-name &lt;model-name&gt;\n</code></pre>"},{"location":"cli/#delete-zoo-models","title":"Delete zoo models \u00b6","text":"<p>Deletes the local copy of the zoo model on disk.</p> <pre><code>fiftyone zoo models delete [-h] NAME\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  NAME        the name of the model\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre> <p>Examples</p> <pre><code># Delete the zoo model from disk\nfiftyone zoo models delete &lt;name&gt;\n</code></pre>"},{"location":"cli/#list-zoo-model-sources","title":"List zoo model sources \u00b6","text":"<p>Lists remote zoo model sources that are registered locally.</p> <pre><code>fiftyone zoo models list-sources [-h]\n</code></pre> <p>Examples</p> <pre><code># Lists the registered remote zoo model sources\nfiftyone zoo models list-sources\n</code></pre>"},{"location":"cli/#register-zoo-model-sources","title":"Register zoo model sources \u00b6","text":"<p>Registers a remote source of zoo models.</p> <p>You can provide any of the following formats:</p> <ul> <li> <p>a GitHub repo URL like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;</code></p> </li> <li> <p>a GitHub ref like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/tree/&lt;branch&gt;</code> or <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/commit/&lt;commit&gt;</code></p> </li> <li> <p>a GitHub ref string like <code>&lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;]</code></p> </li> <li> <p>a publicly accessible URL of an archive (eg zip or tar) file</p> </li> </ul> <p>Note</p> <p>To download from a private GitHub repository that you have access to, provide your GitHub personal access token by setting the <code>GITHUB_TOKEN</code> environment variable.</p> <pre><code>fiftyone zoo models register-source [-h] [-o] URL_OR_GH_REPO\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  URL_OR_GH_REPO   the remote source to register\n\noptional arguments:\n  -h, --help       show this help message and exit\n  -o, --overwrite  whether to overwrite any existing files\n</code></pre> <p>Examples</p> <pre><code># Register a remote zoo model source\nfiftyone zoo models register-source https://github.com/&lt;user&gt;/&lt;repo&gt;\nfiftyone zoo models register-source &lt;url&gt;\n</code></pre>"},{"location":"cli/#delete-zoo-model-sources","title":"Delete zoo model sources \u00b6","text":"<p>Deletes the remote source and all downloaded models associated with it.</p> <p>You can provide any of the following formats:</p> <ul> <li> <p>a GitHub repo URL like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;</code></p> </li> <li> <p>a GitHub ref like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/tree/&lt;branch&gt;</code> or <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/commit/&lt;commit&gt;</code></p> </li> <li> <p>a GitHub ref string like <code>&lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;]</code></p> </li> <li> <p>a publicly accessible URL of an archive (eg zip or tar) file</p> </li> </ul> <pre><code>fiftyone zoo models delete-source [-h] URL_OR_GH_REPO\n</code></pre> <p>Arguments</p> <pre><code>positional arguments:\n  URL_OR_GH_REPO   the remote source to delete\n\noptional arguments:\n  -h, --help       show this help message and exit\n</code></pre> <p>Examples</p> <pre><code># Delete a remote zoo model source\nfiftyone zoo models delete-source https://github.com/&lt;user&gt;/&lt;repo&gt;\nfiftyone zoo models delete-source &lt;url&gt;\n</code></pre>"},{"location":"data/","title":"Datasets and Models with FiftyOne","text":"<p>This page introduces the section. Talks about how to bring in data and models in general but then also refers to some of the suggested ways to do it like zoos, hugging face, integrations...</p>"},{"location":"data/#datasets","title":"Datasets","text":"<p>Our conceptual documentation on Datasets covers many different ways to bring your data into FiftyOne, it is a good idea to undestand those concepts before you go any further with bringing in data. </p>"},{"location":"data/#models","title":"Models","text":"<p>We need a discussion on the different ways to work with models in FiftyOne - from zoo, to integrations, to use your own but then add the Label you want back to your data</p>"},{"location":"data/hugging_face_data/","title":"Using Datasets from Hugging Faces","text":""},{"location":"data/hugging_face_data/#voxel51-workspace","title":"Voxel51 workspace","text":""},{"location":"data/hugging_face_data/#other-workspaces","title":"Other Workspaces","text":""},{"location":"data/dataset_zoo/","title":"FiftyOne Dataset Zoo \u00b6","text":"<p>The FiftyOne Dataset Zoo provides a powerful interface for downloading datasets and loading them into FiftyOne.</p> <p>It provides native access to dozens of popular benchmark datasets, and it also supports downloading arbitrary public or private datasets whose download/preparation methods are provided via GitHub repositories or URLs.</p>"},{"location":"data/dataset_zoo/#built-in-datasets","title":"Built-in datasets \u00b6","text":"<p>The Dataset Zoo provides built-in access to dozens of datasets that you can load into FiftyOne with a single command.</p> <p>Explore the datasets in the zoo</p>"},{"location":"data/dataset_zoo/#remotely-sourced-datasets","title":"Remotely-sourced datasets \u00b6","text":"<p>The Dataset Zoo also supports loading datasets whose download/preparation methods are provided via GitHub repositories or URLs.</p> <p>Learn how to download remote datasets</p>"},{"location":"data/dataset_zoo/#api-reference","title":"API reference \u00b6","text":"<p>The Dataset Zoo can be accessed via the Python library and the CLI. Consult the API reference below to see how to download, load, and manage zoo datasets.</p> <p>Check out the API reference</p>"},{"location":"data/dataset_zoo/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>Methods for working with the Dataset Zoo are conveniently exposed via the Python library and the CLI. The basic recipe for loading a zoo dataset and visualizing it in the App is shown below.</p> <p></p>"},{"location":"data/dataset_zoo/api/","title":"Dataset Zoo API \u00b6","text":"<p>You can interact with the Dataset Zoo either via the Python library or the CLI:</p>"},{"location":"data/dataset_zoo/api/#listing-zoo-datasets","title":"Listing zoo datasets \u00b6","text":""},{"location":"data/dataset_zoo/api/#getting-information-about-zoo-datasets","title":"Getting information about zoo datasets \u00b6","text":""},{"location":"data/dataset_zoo/api/#downloading-zoo-datasets","title":"Downloading zoo datasets \u00b6","text":""},{"location":"data/dataset_zoo/api/#loading-zoo-datasets","title":"Loading zoo datasets \u00b6","text":""},{"location":"data/dataset_zoo/api/#loading-zoo-datasets-with-manual-downloads","title":"Loading zoo datasets with manual downloads \u00b6","text":"<p>Some zoo datasets such as <code>BDD100K</code> and <code>Cityscapes</code> require that you create accounts on a website and manually download the source files. In such cases, the <code>ZooDataset</code> class will provide additional argument(s) that let you specify the paths to these files that you have manually downloaded on disk.</p> <p>You can load these datasets into FiftyOne by first calling <code>download_zoo_dataset()</code> with the appropriate keyword arguments (which are passed to the underlying <code>ZooDataset</code> constructor) to wrangle the raw download into FiftyOne format, and then calling <code>load_zoo_dataset()</code> or using fiftyone zoo datasets load to load the dataset into FiftyOne.</p> <p>For example, the following snippet shows how to load the BDD100K dataset from the zoo:</p> <pre><code>import fiftyone.zoo as foz\n\n# First parse the manually downloaded files in `source_dir`\nfoz.download_zoo_dataset(\n    \"bdd100k\", source_dir=\"/path/to/dir-with-bdd100k-files\"\n)\n\n# Now load into FiftyOne\ndataset = foz.load_zoo_dataset(\"bdd100k\", split=\"validation\")\n</code></pre>"},{"location":"data/dataset_zoo/api/#controlling-where-zoo-datasets-are-downloaded","title":"Controlling where zoo datasets are downloaded \u00b6","text":"<p>By default, zoo datasets are downloaded into subdirectories of <code>fiftyone.config.dataset_zoo_dir</code> corresponding to their names.</p> <p>You can customize this backend by modifying the <code>dataset_zoo_dir</code> setting of your FiftyOne config.</p>"},{"location":"data/dataset_zoo/api/#deleting-zoo-datasets","title":"Deleting zoo datasets \u00b6","text":""},{"location":"data/dataset_zoo/api/#adding-datasets-to-the-zoo","title":"Adding datasets to the zoo \u00b6","text":"<p>We frequently add new built-in datasets to the Dataset Zoo, which will automatically become accessible to you when you update your FiftyOne package.</p> <p>Note</p> <p>FiftyOne is open source! You are welcome to contribute datasets to the public dataset zoo by submitting a pull request to the GitHub repository.</p> <p>You can also add your own datasets to your local dataset zoo, enabling you to work with these datasets via the <code>fiftyone.zoo.datasets</code> package and the CLI using the same syntax that you would with publicly available datasets.</p> <p>To add dataset(s) to your local zoo, you simply write a JSON manifest file in the format below to tell FiftyOne about the dataset. For example, the manifest below adds a second copy of the <code>quickstart</code> dataset to the zoo under the alias <code>quickstart-copy</code>:</p> <pre><code>{\n    \"custom\": {\n        \"quickstart-copy\": \"fiftyone.zoo.datasets.base.QuickstartDataset\"\n    }\n}\n</code></pre> <p>In the above, <code>custom</code> specifies the source of the dataset, which can be an arbitrary string and simply controls the column of the fiftyone zoo datasets list listing in which the dataset is annotated; <code>quickstart-copy</code> is the name of the new dataset; and <code>fiftyone.zoo.datasets.base.QuickstartDataset</code> is the fully-qualified class name of the <code>ZooDataset class</code> for the dataset, which specifies how to download and load the dataset into FiftyOne. This class can be defined anywhere that is importable at runtime in your environment.</p> <p>Finally, expose your new dataset(s) to FiftyOne by adding your manifest to the <code>dataset_zoo_manifest_paths</code> parameter of your FiftyOne config. One way to do this is to set the <code>FIFTYONE_DATASET_ZOO_MANIFEST_PATHS</code> environment variable:</p> <pre><code>export FIFTYONE_DATASET_ZOO_MANIFEST_PATHS=/path/to/custom/manifest.json\n</code></pre> <p>Now you can access the <code>quickstart-copy</code> dataset as you would any other zoo dataset:</p> <pre><code># Will contain `quickstart-copy`\nfiftyone zoo datasets list\n\n# Load custom dataset into FiftyOne\nfiftyone zoo datasets load quickstart-copy\n</code></pre>"},{"location":"data/dataset_zoo/api/#customizing-your-ml-backend","title":"Customizing your ML backend \u00b6","text":"<p>Behind the scenes, FiftyOne uses either TensorFlow Datasets or TorchVision Datasets libraries to download and wrangle some zoo datasets, depending on which ML library you have installed. In order to load datasets using TF, you must have the tensorflow-datasets package installed on your machine. In order to load datasets using PyTorch, you must have the torch and torchvision packages installed.</p> <p>Note that the ML backends may expose different datasets.</p> <p>For datasets that require an ML backend, FiftyOne will use whichever ML backend is necessary to download the requested zoo dataset. If a dataset is available through both backends, it will use the backend specified by the <code>fo.config.default_ml_backend</code> setting in your FiftyOne config.</p> <p>You can customize this backend by modifying the <code>default_ml_backend</code> setting of your FiftyOne config.</p>"},{"location":"data/dataset_zoo/datasets/","title":"Built-In Zoo Datasets \u00b6","text":"<p>This page lists all of the natively available datasets in the FiftyOne Dataset Zoo.</p> <p>Check out the API reference for complete instructions for using the Dataset Zoo.</p> <p>Note</p> <p>Some datasets are loaded via the TorchVision Datasets or TensorFlow Datasets packages under the hood.</p> <p>If you do not have a suitable package installed when attempting to download a zoo dataset, you\u2019ll see an error message that will help you install one.</p> Dataset name Tags ActivityNet 100 video, classification, action-recognition, temporal-detection ActivityNet 200 video, classification, action-recognition, temporal-detection BDD100K image, multilabel, automotive, manual Caltech-101 image, classification Caltech-256 image, classification CIFAR-10 image, classification CIFAR-100 image, classification Cityscapes image, multilabel, automotive, manual COCO-2014 image, detection, segmentation COCO-2017 image, detection, segmentation Fashion MNIST image, classification Families in the Wild image, classification HMDB51 video, action-recognition ImageNet 2012 image, classification, manual ImageNet Sample image, classification Kinetics 400 video, classification, action-recognition Kinetics 600 video, classification, action-recognition Kinetics 700 video, classification, action-recognition Kinetics 700-2020 video, classification, action-recognition KITTI image, detection KITTI Multiview image, point-cloud, detection Labeled Faces in the Wild image, classification, facial-recognition MNIST image, classification Open Images V6 image, classification, detection, segmentation, relationships Open Images V7 image, classification, detection, segmentation, keypoints, relationships Places image, classification Quickstart image, quickstart Quickstart Geo image, location, quickstart Quickstart Video video, quickstart Quickstart Groups image, point-cloud, quickstart Quickstart 3D 3d, point-cloud, mesh, quickstart Sama-COCO image, detection, segmentation UCF101 video, action-recognition VOC-2007 image, detection VOC-2012 image, detection"},{"location":"data/dataset_zoo/datasets/#activitynet-100","title":"ActivityNet 100 \u00b6","text":"<p>ActivityNet is a large-scale video dataset for human activity understanding supporting the tasks of global video classification, trimmed activity classification, and temporal activity detection.</p> <p>This version contains videos and temporal activity detections for the 100 class version of the dataset.</p> <p>Note</p> <p>Check out this guide for more details on using FiftyOne to work with ActivityNet.</p> <p>Notes</p> <ul> <li> <p>ActivityNet 100 and 200 differ in the number of activity classes and videos per split</p> </li> <li> <p>Partial downloads will download videos (if still available) from YouTube</p> </li> <li> <p>Full splits can be loaded by first downloading the official source files from the ActivityNet maintainers</p> </li> <li> <p>The test set does not have annotations</p> </li> </ul> <p>Details</p> <ul> <li> <p>Dataset name: <code>activitynet-100</code></p> </li> <li> <p>Dataset source: http://activity-net.org/index.html</p> </li> <li> <p>Dataset size: 223 GB</p> </li> <li> <p>Tags: <code>video, classification, action-recognition, temporal-detection</code></p> </li> <li> <p>Supported splits: <code>train, validation, test</code></p> </li> <li> <p>ZooDataset class: <code>ActivityNet100Dataset</code></p> </li> </ul> <p>Full split stats</p> <ul> <li> <p>Train split: 4,819 videos (7,151 instances)</p> </li> <li> <p>Test split: 2,480 videos (labels withheld)</p> </li> <li> <p>Validation split: 2,383 videos (3,582 instances)</p> </li> </ul> <p>Partial downloads</p> <p>FiftyOne provides parameters that can be used to efficiently download specific subsets of the ActivityNet dataset to suit your needs. When new subsets are specified, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from YouTube.</p> <p>The following parameters are available to configure a partial download of ActivityNet 100 by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If none are provided, all available splits are loaded</p> </li> <li> <p>source_dir ( None): the directory containing the manually downloaded ActivityNet files used to avoid downloading videos from YouTube</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded</p> </li> <li> <p>max_duration ( None): only videos with a duration in seconds that is less than or equal to the <code>max_duration</code> will be downloaded. By default, all videos are downloaded</p> </li> <li> <p>copy_files ( True): whether to move (False) or create copies (True) of the source files when populating <code>dataset_dir</code>. This is only relevant when a <code>source_dir</code> is provided</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual videos. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>classes</code> are also specified, only up to the number of samples that contain at least one specified class will be loaded. By default, all matching samples are loaded</p> </li> </ul> <p>Note</p> <p>See <code>ActivityNet100Dataset</code> and <code>ActivityNetDatasetImporter</code> for complete descriptions of the optional keyword arguments that you can pass to <code>load_zoo_dataset()</code>.</p> <p>Full split downloads</p> <p>Many videos have been removed from YouTube since the creation of ActivityNet. As a result, if you do not specify any partial download parameters defined in the previous section, you must first download the official source files from the ActivityNet maintainers in order to load a full split into FiftyOne.</p> <p>To download the source files, you must fill out this form.</p> <p>Refer to this page to see how to load full splits by passing the <code>source_dir</code> parameter to <code>load_zoo_dataset()</code>.</p> <p>Example usage</p> <p>Note</p> <p>In order to work with video datasets, you\u2019ll need to have ffmpeg installed.</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#activitynet-200","title":"ActivityNet 200 \u00b6","text":"<p>ActivityNet is a large-scale video dataset for human activity understanding supporting the tasks of global video classification, trimmed activity classification, and temporal activity detection.</p> <p>This version contains videos and temporal activity detections for the 200 class version of the dataset.</p> <p>Note</p> <p>Check out this guide for more details on using FiftyOne to work with ActivityNet.</p> <p>Notes</p> <ul> <li> <p>ActivityNet 200 is a superset of ActivityNet 100</p> </li> <li> <p>ActivityNet 100 and 200 differ in the number of activity classes and videos per split</p> </li> <li> <p>Partial downloads will download videos (if still available) from YouTube</p> </li> <li> <p>Full splits can be loaded by first downloading the official source files from the ActivityNet maintainers</p> </li> <li> <p>The test set does not have annotations</p> </li> </ul> <p>Details</p> <ul> <li> <p>Dataset name: <code>activitynet-200</code></p> </li> <li> <p>Dataset source: http://activity-net.org/index.html</p> </li> <li> <p>Dataset size: 500 GB</p> </li> <li> <p>Tags: <code>video, classification, action-recognition, temporal-detection</code></p> </li> <li> <p>Supported splits: <code>train, validation, test</code></p> </li> <li> <p>ZooDataset class: <code>ActivityNet200Dataset</code></p> </li> </ul> <p>Full split stats</p> <ul> <li> <p>Train split: 10,024 videos (15,410 instances)</p> </li> <li> <p>Test split: 5,044 videos (labels withheld)</p> </li> <li> <p>Validation split: 4,926 videos (7,654 instances)</p> </li> </ul> <p>Partial downloads</p> <p>FiftyOne provides parameters that can be used to efficiently download specific subsets of the ActivityNet dataset to suit your needs. When new subsets are specified, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from YouTube.</p> <p>The following parameters are available to configure a partial download of ActivityNet 200 by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If none are provided, all available splits are loaded</p> </li> <li> <p>source_dir ( None): the directory containing the manually downloaded ActivityNet files used to avoid downloading videos from YouTube</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded</p> </li> <li> <p>max_duration ( None): only videos with a duration in seconds that is less than or equal to the <code>max_duration</code> will be downloaded. By default, all videos are downloaded</p> </li> <li> <p>copy_files ( True): whether to move (False) or create copies (True) of the source files when populating <code>dataset_dir</code>. This is only relevant when a <code>source_dir</code> is provided</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual videos. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>classes</code> are also specified, only up to the number of samples that contain at least one specified class will be loaded. By default, all matching samples are loaded</p> </li> </ul> <p>Note</p> <p>See <code>ActivityNet200Dataset</code> and <code>ActivityNetDatasetImporter</code> for complete descriptions of the optional keyword arguments that you can pass to <code>load_zoo_dataset()</code>.</p> <p>Full split downloads</p> <p>Many videos have been removed from YouTube since the creation of ActivityNet. As a result, if you do not specify any partial download parameters defined in the previous section, you must first download the official source files from the ActivityNet maintainers in order to load a full split into FiftyOne.</p> <p>To download the source files, you must fill out this form.</p> <p>Refer to this page to see how to load full splits by passing the <code>source_dir</code> parameter to <code>load_zoo_dataset()</code>.</p> <p>Example usage</p> <p>Note</p> <p>In order to work with video datasets, you\u2019ll need to have ffmpeg installed.</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#bdd100k","title":"BDD100K \u00b6","text":"<p>The Berkeley Deep Drive (BDD) dataset is one of the largest and most diverse video datasets for autonomous vehicles.</p> <p>The BDD100K dataset contains 100,000 video clips collected from more than 50,000 rides covering New York, San Francisco Bay Area, and other regions. The dataset contains diverse scene types such as city streets, residential areas, and highways. Furthermore, the videos were recorded in diverse weather conditions at different times of the day.</p> <p>The videos are split into training (70K), validation (10K) and testing (20K) sets. Each video is 40 seconds long with 720p resolution and a frame rate of 30fps. The frame at the 10th second of each video is annotated for image classification, detection, and segmentation tasks.</p> <p>This version of the dataset contains only the 100K images extracted from the videos as described above, together with the image classification, detection, and segmentation labels.</p> <p>Note</p> <p>In order to load the BDD100K dataset, you must download the source data manually. The directory should be organized in the following format:</p> <pre><code>source_dir/\n    labels/\n        bdd100k_labels_images_train.json\n        bdd100k_labels_images_val.json\n    images/\n        100k/\n            train/\n            test/\n            val/\n</code></pre> <p>You can register at https://bdd-data.berkeley.edu in order to get links to download the data.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>bdd100k</code></p> </li> <li> <p>Dataset source: https://bdd-data.berkeley.edu</p> </li> <li> <p>Dataset size: 7.10 GB</p> </li> <li> <p>Tags: <code>image, multilabel, automotive, manual</code></p> </li> <li> <p>Supported splits: <code>train, validation, test</code></p> </li> <li> <p>ZooDataset class: <code>BDD100KDataset</code></p> </li> </ul> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#caltech-101","title":"Caltech-101 \u00b6","text":"<p>The Caltech-101 dataset of images.</p> <p>The dataset consists of pictures of objects belonging to 101 classes, plus one background clutter class ( <code>BACKGROUND_Google</code>). Each image is labelled with a single object.</p> <p>Each class contains roughly 40 to 800 images, totalling around 9,000 images. Images are of variable sizes, with typical edge lengths of 200-300 pixels. This version contains image-level labels only.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>caltech101</code></p> </li> <li> <p>Dataset source: https://data.caltech.edu/records/mzrjq-6wc02</p> </li> <li> <p>Dataset size: 138.60 MB</p> </li> <li> <p>Tags: <code>image, classification</code></p> </li> <li> <p>Supported splits: <code>N/A</code></p> </li> <li> <p>ZooDataset class: <code>Caltech101Dataset</code></p> </li> </ul> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#caltech-256","title":"Caltech-256 \u00b6","text":"<p>The Caltech-256 dataset of images.</p> <p>The dataset consists of pictures of objects belonging to 256 classes, plus one background clutter class ( <code>clutter</code>). Each image is labelled with a single object.</p> <p>Each class contains between 80 and 827 images, totalling 30,607 images. Images are of variable sizes, with typical edge lengths of 80-800 pixels.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>caltech256</code></p> </li> <li> <p>Dataset source: https://data.caltech.edu/records/nyy15-4j048</p> </li> <li> <p>Dataset size: 1.16 GB</p> </li> <li> <p>Tags: <code>image, classification</code></p> </li> <li> <p>Supported splits: <code>N/A</code></p> </li> <li> <p>ZooDataset class: <code>Caltech256Dataset</code></p> </li> </ul> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#cifar-10","title":"CIFAR-10 \u00b6","text":"<p>The CIFAR-10 dataset of images.</p> <p>The dataset consists of 60,000 32 x 32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>cifar10</code></p> </li> <li> <p>Dataset source: https://www.cs.toronto.edu/~kriz/cifar.html</p> </li> <li> <p>Dataset size: 132.40 MB</p> </li> <li> <p>Tags: <code>image, classification</code></p> </li> <li> <p>Supported splits: <code>train, test</code></p> </li> <li> <p>ZooDataset classes:</p> </li> <li> <p><code>CIFAR10Dataset</code> (TF backend)</p> </li> <li> <p><code>CIFAR10Dataset</code> (Torch backend)</p> </li> </ul> <p>Note</p> <p>You must have the Torch or TensorFlow backend(s) installed to load this dataset.</p> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#cifar-100","title":"CIFAR-100 \u00b6","text":"<p>The CIFAR-100 dataset of images.</p> <p>The dataset consists of 60,000 32 x 32 color images in 100 classes, with 600 images per class. There are 50,000 training images and 10,000 test images.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>cifar100</code></p> </li> <li> <p>Dataset source: https://www.cs.toronto.edu/~kriz/cifar.html</p> </li> <li> <p>Dataset size: 132.03 MB</p> </li> <li> <p>Tags: <code>image, classification</code></p> </li> <li> <p>Supported splits: <code>train, test</code></p> </li> <li> <p>ZooDataset classes:</p> </li> <li> <p><code>CIFAR100Dataset</code> (TF backend)</p> </li> <li> <p><code>CIFAR100Dataset</code> (Torch backend)</p> </li> </ul> <p>Note</p> <p>You must have the Torch or TensorFlow backend(s) installed to load this dataset.</p> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#cityscapes","title":"Cityscapes \u00b6","text":"<p>Cityscapes is a large-scale dataset that contains a diverse set of stereo video sequences recorded in street scenes from 50 different cities, with high quality pixel-level annotations of 5,000 frames in addition to a larger set of 20,000 weakly annotated frames.</p> <p>The dataset is intended for:</p> <ul> <li> <p>Assessing the performance of vision algorithms for major tasks of semantic urban scene understanding: pixel-level, instance-level, and panoptic semantic labeling</p> </li> <li> <p>Supporting research that aims to exploit large volumes of (weakly) annotated data, e.g. for training deep neural networks</p> </li> </ul> <p>Note</p> <p>In order to load the Cityscapes dataset, you must download the source data manually. The directory should be organized in the following format:</p> <pre><code>source_dir/\n    leftImg8bit_trainvaltest.zip\n    gtFine_trainvaltest.zip             # optional\n    gtCoarse.zip                        # optional\n    gtBbox_cityPersons_trainval.zip     # optional\n</code></pre> <p>You can register at https://www.cityscapes-dataset.com/register in order to get links to download the data.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>cityscapes</code></p> </li> <li> <p>Dataset source: https://www.cityscapes-dataset.com</p> </li> <li> <p>Dataset size: 11.80 GB</p> </li> <li> <p>Tags: <code>image, multilabel, automotive, manual</code></p> </li> <li> <p>Supported splits: <code>train, validation, test</code></p> </li> <li> <p>ZooDataset class: <code>CityscapesDataset</code></p> </li> </ul> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#coco-2014","title":"COCO-2014 \u00b6","text":"<p>COCO is a large-scale object detection, segmentation, and captioning dataset.</p> <p>This version contains images, bounding boxes, and segmentations for the 2014 version of the dataset.</p> <p>Note</p> <p>With support from the COCO team, FiftyOne is a recommended tool for downloading, visualizing, and evaluating on the COCO dataset!</p> <p>Check out this guide for more details on using FiftyOne to work with COCO.</p> <p>Notes</p> <ul> <li> <p>COCO defines 91 classes but the data only uses 80 classes</p> </li> <li> <p>Some images from the train and validation sets don\u2019t have annotations</p> </li> <li> <p>The test set does not have annotations</p> </li> <li> <p>COCO 2014 and 2017 use the same images, but the splits are different</p> </li> </ul> <p>Details</p> <ul> <li> <p>Dataset name: <code>coco-2014</code></p> </li> <li> <p>Dataset source: http://cocodataset.org/#home</p> </li> <li> <p>Dataset size: 37.57 GB</p> </li> <li> <p>Tags: <code>image, detection, segmentation</code></p> </li> <li> <p>Supported splits: <code>train, validation, test</code></p> </li> <li> <p>ZooDataset class: <code>COCO2014Dataset</code></p> </li> </ul> <p>Full split stats</p> <ul> <li> <p>Train split: 82,783 images</p> </li> <li> <p>Test split: 40,775 images</p> </li> <li> <p>Validation split: 40,504 images</p> </li> </ul> <p>Partial downloads</p> <p>FiftyOne provides parameters that can be used to efficiently download specific subsets of the COCO dataset to suit your needs. When new subsets are specified, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from the web.</p> <p>The following parameters are available to configure a partial download of COCO-2014 by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If neither is provided, all available splits are loaded</p> </li> <li> <p>label_types ( None): a label type or list of label types to load. Supported values are <code>(\"detections\", \"segmentations\")</code>. By default, only detections are loaded</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded</p> </li> <li> <p>image_ids ( None): a list of specific image IDs to load. The IDs can be specified either as <code>&lt;split&gt;/&lt;image-id&gt;</code> strings or <code>&lt;image-id&gt;</code> ints of strings. Alternatively, you can provide the path to a TXT (newline-separated), JSON, or CSV file containing the list of image IDs to load in either of the first two formats</p> </li> <li> <p>include_id ( False): whether to include the COCO ID of each sample in the loaded labels</p> </li> <li> <p>include_license ( False): whether to include the COCO license of each sample in the loaded labels, if available. The supported values are:</p> </li> <li> <p><code>\"False\"</code> (default): don\u2019t load the license</p> </li> <li> <p><code>True</code>/ <code>\"name\"</code>: store the string license name</p> </li> <li> <p><code>\"id\"</code>: store the integer license ID</p> </li> <li> <p><code>\"url\"</code>: store the license URL</p> </li> <li> <p>only_matching ( False): whether to only load labels that match the <code>classes</code> or <code>attrs</code> requirements that you provide (True), or to load all labels for samples that match the requirements (False)</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual images. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>label_types</code> and/or <code>classes</code> are also specified, first priority will be given to samples that contain all of the specified label types and/or classes, followed by samples that contain at least one of the specified labels types or classes. The actual number of samples loaded may be less than this maximum value if the dataset does not contain sufficient samples matching your requirements</p> </li> </ul> <p>Note</p> <p>See <code>COCO2014Dataset</code> and <code>COCODetectionDatasetImporter</code> for complete descriptions of the optional keyword arguments that you can pass to <code>load_zoo_dataset()</code>.</p> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#coco-2017","title":"COCO-2017 \u00b6","text":"<p>COCO is a large-scale object detection, segmentation, and captioning dataset.</p> <p>This version contains images, bounding boxes, and segmentations for the 2017 version of the dataset.</p> <p>Note</p> <p>With support from the COCO team, FiftyOne is a recommended tool for downloading, visualizing, and evaluating on the COCO dataset!</p> <p>Check out this guide for more details on using FiftyOne to work with COCO.</p> <p>Notes</p> <ul> <li> <p>COCO defines 91 classes but the data only uses 80 classes</p> </li> <li> <p>Some images from the train and validation sets don\u2019t have annotations</p> </li> <li> <p>The test set does not have annotations</p> </li> <li> <p>COCO 2014 and 2017 use the same images, but the splits are different</p> </li> </ul> <p>Details</p> <ul> <li> <p>Dataset name: <code>coco-2017</code></p> </li> <li> <p>Dataset source: http://cocodataset.org/#home</p> </li> <li> <p>Dataset size: 25.20 GB</p> </li> <li> <p>Tags: <code>image, detection, segmentation</code></p> </li> <li> <p>Supported splits: <code>train, validation, test</code></p> </li> <li> <p>ZooDataset class: <code>COCO2017Dataset</code></p> </li> </ul> <p>Full split stats</p> <ul> <li> <p>Train split: 118,287 images</p> </li> <li> <p>Test split: 40,670 images</p> </li> <li> <p>Validation split: 5,000 images</p> </li> </ul> <p>Partial downloads</p> <p>FiftyOne provides parameters that can be used to efficiently download specific subsets of the COCO dataset to suit your needs. When new subsets are specified, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from the web.</p> <p>The following parameters are available to configure a partial download of COCO-2017 by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If neither is provided, all available splits are loaded</p> </li> <li> <p>label_types ( None): a label type or list of label types to load. Supported values are <code>(\"detections\", \"segmentations\")</code>. By default, only detections are loaded</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded</p> </li> <li> <p>image_ids ( None): a list of specific image IDs to load. The IDs can be specified either as <code>&lt;split&gt;/&lt;image-id&gt;</code> strings or <code>&lt;image-id&gt;</code> ints of strings. Alternatively, you can provide the path to a TXT (newline-separated), JSON, or CSV file containing the list of image IDs to load in either of the first two formats</p> </li> <li> <p>include_id ( False): whether to include the COCO ID of each sample in the loaded labels</p> </li> <li> <p>include_license ( False): whether to include the COCO license of each sample in the loaded labels, if available. The supported values are:</p> </li> <li> <p><code>\"False\"</code> (default): don\u2019t load the license</p> </li> <li> <p><code>True</code>/ <code>\"name\"</code>: store the string license name</p> </li> <li> <p><code>\"id\"</code>: store the integer license ID</p> </li> <li> <p><code>\"url\"</code>: store the license URL</p> </li> <li> <p>only_matching ( False): whether to only load labels that match the <code>classes</code> or <code>attrs</code> requirements that you provide (True), or to load all labels for samples that match the requirements (False)</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual images. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>label_types</code> and/or <code>classes</code> are also specified, first priority will be given to samples that contain all of the specified label types and/or classes, followed by samples that contain at least one of the specified labels types or classes. The actual number of samples loaded may be less than this maximum value if the dataset does not contain sufficient samples matching your requirements</p> </li> </ul> <p>Note</p> <p>See <code>COCO2017Dataset</code> and <code>COCODetectionDatasetImporter</code> for complete descriptions of the optional keyword arguments that you can pass to <code>load_zoo_dataset()</code>.</p> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#fashion-mnist","title":"Fashion MNIST \u00b6","text":"<p>The Fashion-MNIST database of Zalando\u2019s fashion article images.</p> <p>The dataset consists of 70,000 28 x 28 grayscale images in 10 classes. There are 60,000 training images and 10,000 test images.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>fashion-mnist</code></p> </li> <li> <p>Dataset source: https://github.com/zalandoresearch/fashion-mnist</p> </li> <li> <p>Dataset size: 36.42 MB</p> </li> <li> <p>Tags: <code>image, classification</code></p> </li> <li> <p>Supported splits: <code>train, test</code></p> </li> <li> <p>ZooDataset classes:</p> </li> <li> <p><code>FashionMNISTDataset</code> (TF backend)</p> </li> <li> <p><code>FashionMNISTDataset</code> (Torch backend)</p> </li> </ul> <p>Note</p> <p>You must have the Torch or TensorFlow backend(s) installed to load this dataset.</p> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#families-in-the-wild","title":"Families in the Wild \u00b6","text":"<p>Families in the Wild is a public benchmark for recognizing families via facial images. The dataset contains over 26,642 images of 5,037 faces collected from 978 families. A unique Family ID (FID) is assigned per family, ranging from F0001-F1018 (i.e., some families were merged or removed since its first release in 2016). The dataset is a continued work in progress. Any contributions are both welcome and appreciated!</p> <p>Faces were cropped from imagery using the five-point face detector MTCNN from various phototypes (i.e., mostly family photos, along with several profile pics of individuals (facial shots). The number of members per family varies from 3-to-26, with the number of faces per subject ranging from 1 to &gt;10.</p> <p>Various levels and types of labels are associated with samples in this dataset. Family-level labels contain a list of members, each assigned a member ID (MID) unique to that respective family (e.g., F0011.MID2 refers to member 2 of family 11). Each member has annotations specifying gender and relationship to all other members in that respective family.</p> <p>The relationships in FIW are:</p> <pre><code>=====  =====\n  ID    Type\n=====  =====\n    0  not related or self\n    1  child\n    2  sibling\n    3  grandchild\n    4  parent\n    5  spouse\n    6  grandparent\n    7  great grandchild\n    8  great grandparent\n    9  TBD\n=====  =====\n</code></pre> <p>Within FiftyOne, each sample corresponds to a single face image and contains primitive labels of the Family ID, Member ID, etc. The relationship labels are stored as multi-label classifications, where each classification represents one relationship that the member has with another member in the family. The number of relationships will differ from one person to the next, but all faces of one person will have the same relationship labels.</p> <p>Additionally, the labels for the Kinship Verification task are also loaded into this dataset through FiftyOne. These labels are stored as classifications just like relationships, but the labels of kinship differ from those defined above. For example, rather than Parent, the label might be <code>fd</code> representing a Father-Daughter kinship or <code>md</code> for Mother-Daughter.</p> <p>In order to make it easier to browse the dataset in the FiftyOne App, each sample also contains a <code>face_id</code> field containing a unique integer for each face of a member, always starting at 0. This allows you to filter the <code>face_id</code> field to 0 in the App to show only a single image of each person.</p> <p>For your reference, the relationship labels are stored in disk in a matrix that provides the relationship of each member with other members of the family as well as names and genders. The i-th rows represent the i-th family member\u2019s relationship to the j-th other members.</p> <p>For example, <code>FID0001.csv</code> contains:</p> <pre><code>MID     1     2     3     Name    Gender\n 1      0     4     5     name1     f\n 2      1     0     1     name2     f\n 3      5     4     0     name3     m\n</code></pre> <p>Here we have three family members, as listed under the MID column (far-left). Each MID reads across its row. We can see that MID1 is related to MID2 by 4 -&gt; 1 (Parent -&gt; Child), which of course can be viewed as the inverse, i.e., MID2 -&gt; MID1 is 1 -&gt; 4. It can also be seen that MID1 and MID3 are spouses of one another, i.e., 5 -&gt; 5.</p> <p>Note</p> <p>The spouse label will likely be removed in future version of this dataset. It serves no value to the problem of kinship.</p> <p>For more information on the data (e.g., statistics, task evaluations, benchmarks, and more), see the recent journal:</p> <pre><code>Robinson, JP, M. Shao, and Y. Fu. \"Survey on the Analysis and Modeling of\nVisual Kinship: A Decade in the Making.\" IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (PAMI), 2021.\n</code></pre> <p>Details</p> <ul> <li> <p>Dataset name: <code>fiw</code></p> </li> <li> <p>Dataset source: https://web.northeastern.edu/smilelab/fiw/</p> </li> <li> <p>Dataset size: 173.00 MB</p> </li> <li> <p>Tags: <code>image, kinship, verification, classification, search-and-retrieval, facial-recognition</code></p> </li> <li> <p>Supported splits: <code>test, val, train</code></p> </li> <li> <p>ZooDataset class: <code>FIWDataset</code></p> </li> </ul> <p>Note</p> <p>For your convenience, FiftyOne provides <code>get_pairwise_labels()</code> and <code>get_identifier_filepaths_map()</code> utilities for FIW.</p> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#hmbd51","title":"HMBD51 \u00b6","text":"<p>HMDB51 is an action recognition dataset containing a total of 6,766 clips distributed across 51 action classes.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>hmdb51</code></p> </li> <li> <p>Dataset source: https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database</p> </li> <li> <p>Dataset size: 2.16 GB</p> </li> <li> <p>Tags: <code>video, action-recognition</code></p> </li> <li> <p>Supported splits: <code>train, test, other</code></p> </li> <li> <p>ZooDataset class: <code>HMDB51Dataset</code></p> </li> </ul> <p>Example usage</p> <p>Note</p> <p>In order to work with video datasets, you\u2019ll need to have ffmpeg installed.</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#imagenet-2012","title":"ImageNet 2012 \u00b6","text":"<p>The ImageNet 2012 dataset.</p> <p>ImageNet, as known as ILSVRC 2012, is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a \u201csynonym set\u201d or \u201csynset\u201d. There are more than 100,000 synsets in WordNet, majority of them are nouns (80,000+). ImageNet provides on average 1,000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated. In its completion, we hope ImageNet will offer tens of millions of cleanly sorted images for most of the concepts in the WordNet hierarchy.</p> <p>Note that labels were never publicly released for the test set, so only the training and validation sets are provided.</p> <p>Note</p> <p>In order to load the ImageNet dataset, you must download the source data manually. The directory should be organized in the following format:</p> <pre><code>source_dir/\n    ILSVRC2012_devkit_t12.tar.gz    # both splits\n    ILSVRC2012_img_train.tar        # train split\n    ILSVRC2012_img_val.tar          # validation split\n</code></pre> <p>You can register at http://www.image-net.org/download-images in order to get links to download the data.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>imagenet-2012</code></p> </li> <li> <p>Dataset source: http://image-net.org</p> </li> <li> <p>Dataset size: 144.02 GB</p> </li> <li> <p>Tags: <code>image, classification, manual</code></p> </li> <li> <p>Supported splits: <code>train, validation</code></p> </li> <li> <p>ZooDataset classes:</p> </li> <li> <p><code>ImageNet2012Dataset</code> (TF backend)</p> </li> <li> <p><code>ImageNet2012Dataset</code> (Torch backend)</p> </li> </ul> <p>Note</p> <p>You must have the Torch or TensorFlow backend(s) installed to load this dataset.</p> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#imagenet-sample","title":"ImageNet Sample \u00b6","text":"<p>A small sample of images from the ImageNet 2012 dataset.</p> <p>The dataset contains 1,000 images, one randomly chosen from each class of the validation split of the ImageNet 2012 dataset.</p> <p>These images are provided according to the terms below.</p> <pre><code>You have been granted access for non-commercial research/educational\nuse. By accessing the data, you have agreed to the following terms.\n\nYou (the \"Researcher\") have requested permission to use the ImageNet\ndatabase (the \"Database\") at Princeton University and Stanford\nUniversity. In exchange for such permission, Researcher hereby agrees\nto the following terms and conditions:\n\n1.  Researcher shall use the Database only for non-commercial research\n    and educational purposes.\n2.  Princeton University and Stanford University make no\n    representations or warranties regarding the Database, including but\n    not limited to warranties of non-infringement or fitness for a\n    particular purpose.\n3.  Researcher accepts full responsibility for his or her use of the\n    Database and shall defend and indemnify Princeton University and\n    Stanford University, including their employees, Trustees, officers\n    and agents, against any and all claims arising from Researcher's\n    use of the Database, including but not limited to Researcher's use\n    of any copies of copyrighted images that he or she may create from\n    the Database.\n4.  Researcher may provide research associates and colleagues with\n    access to the Database provided that they first agree to be bound\n    by these terms and conditions.\n5.  Princeton University and Stanford University reserve the right to\n    terminate Researcher's access to the Database at any time.\n6.  If Researcher is employed by a for-profit, commercial entity,\n    Researcher's employer shall also be bound by these terms and\n    conditions, and Researcher hereby represents that he or she is\n    fully authorized to enter into this agreement on behalf of such\n    employer.\n7.  The law of the State of New Jersey shall apply to all disputes\n    under this agreement.\n</code></pre> <p>Details</p> <ul> <li> <p>Dataset name: <code>imagenet-sample</code></p> </li> <li> <p>Dataset source: http://image-net.org</p> </li> <li> <p>Dataset size: 98.26 MB</p> </li> <li> <p>Tags: <code>image, classification</code></p> </li> <li> <p>Supported splits: <code>N/A</code></p> </li> <li> <p>ZooDataset class: <code>ImageNetSampleDataset</code></p> </li> </ul> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#kinetics-400","title":"Kinetics 400 \u00b6","text":"<p>Kinetics is a collection of large-scale, high-quality datasets of URL links of up to 650,000 video clips that cover 400/600/700 human action classes, depending on the dataset version. The videos include human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Each action class has at least 400/600/700 video clips. Each clip is human annotated with a single action class and lasts around 10 seconds.</p> <p>This dataset contains videos and action classifications for the 400 class version of the dataset.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>kinetics-400</code></p> </li> <li> <p>Dataset source: https://deepmind.com/research/open-source/kinetics</p> </li> <li> <p>Dataset size: 456 GB</p> </li> <li> <p>Tags: <code>video, classification, action-recognition</code></p> </li> <li> <p>Supported splits: <code>train, test, validation</code></p> </li> <li> <p>ZooDataset class: <code>Kinetics400Dataset</code></p> </li> </ul> <p>Original split stats:</p> <ul> <li> <p>Train split: 219,782 videos</p> </li> <li> <p>Test split: 35,357 videos</p> </li> <li> <p>Validation split: 18,035 videos</p> </li> </ul> <p>CVDF split stats:</p> <ul> <li> <p>Train split: 246,534 videos</p> </li> <li> <p>Test split: 39,805 videos</p> </li> <li> <p>Validation split: 19,906 videos</p> </li> </ul> <p>Dataset size:</p> <ul> <li> <p>Train split: 370 GB</p> </li> <li> <p>Test split: 56 GB</p> </li> <li> <p>Validation split: 30 GB</p> </li> </ul> <p>Partial downloads</p> <p>Kinetics is a massive dataset, so FiftyOne provides parameters that can be used to efficiently download specific subsets of the dataset to suit your needs. When new subsets are specified, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from the web.</p> <p>Kinetics videos were originally only accessible from YouTube. Over time, some videos have become unavailable so the CVDF have hosted the Kinetics dataset on AWS.</p> <p>If you are partially downloading the dataset through FiftyOne, the specific videos of interest will be downloaded from YouTube, if necessary. However, when you load an entire split, the CVDF-provided files will be downloaded from AWS.</p> <p>The following parameters are available to configure a partial download of Kinetics by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If neither is provided, all available splits are loaded</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual videos. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>classes</code> are also specified, only up to the number of samples that contain at least one specified class will be loaded. By default, all matching samples are loaded</p> </li> </ul> <p>Note</p> <p>Unlike other versions, Kinteics 400 does not have zips available by class so whenever either <code>classes</code> or <code>max_samples</code> is provided, videos will be downloaded from YouTube.</p> <p>Example usage</p> <p>Note</p> <p>In order to work with video datasets, you\u2019ll need to have ffmpeg installed.</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#kinetics-600","title":"Kinetics 600 \u00b6","text":"<p>Kinetics is a collection of large-scale, high-quality datasets of URL links of up to 650,000 video clips that cover 400/600/700 human action classes, depending on the dataset version. The videos include human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Each action class has at least 400/600/700 video clips. Each clip is human annotated with a single action class and lasts around 10 seconds.</p> <p>This dataset contains videos and action classifications for the 600 class version of the dataset.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>kinetics-600</code></p> </li> <li> <p>Dataset source: https://deepmind.com/research/open-source/kinetics</p> </li> <li> <p>Dataset size: 779 GB</p> </li> <li> <p>Tags: <code>video, classification, action-recognition</code></p> </li> <li> <p>Supported splits: <code>train, test, validation</code></p> </li> <li> <p>ZooDataset class: <code>Kinetics600Dataset</code></p> </li> </ul> <p>Original split stats:</p> <ul> <li> <p>Train split: 370,582 videos</p> </li> <li> <p>Test split: 56,618 videos</p> </li> <li> <p>Validation split: 28,313 videos</p> </li> </ul> <p>CVDF split stats:</p> <ul> <li> <p>Train split: 427,549 videos</p> </li> <li> <p>Test split: 72,924 videos</p> </li> <li> <p>Validation split: 29,793 videos</p> </li> </ul> <p>Dataset size:</p> <ul> <li> <p>Train split: 648 GB</p> </li> <li> <p>Test split: 88 GB</p> </li> <li> <p>Validation split: 43 GB</p> </li> </ul> <p>Partial downloads</p> <p>Kinetics is a massive dataset, so FiftyOne provides parameters that can be used to efficiently download specific subsets of the dataset to suit your needs. When new subsets are specified, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from the web.</p> <p>Kinetics videos were originally only accessible from YouTube. Over time, some videos have become unavailable so the CVDF have hosted the Kinetics dataset on AWS.</p> <p>If you are partially downloading the dataset through FiftyOne, the specific videos of interest will be downloaded from YouTube, if necessary. However, when you load an entire split, the CVDF-provided files will be downloaded from AWS.</p> <p>The following parameters are available to configure a partial download of Kinetics by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If neither is provided, all available splits are loaded</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual videos. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>classes</code> are also specified, only up to the number of samples that contain at least one specified class will be loaded. By default, all matching samples are loaded</p> </li> </ul> <p>Example usage</p> <p>Note</p> <p>In order to work with video datasets, you\u2019ll need to have ffmpeg installed.</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#kinetics-700","title":"Kinetics 700 \u00b6","text":"<p>Kinetics is a collection of large-scale, high-quality datasets of URL links of up to 650,000 video clips that cover 400/600/700 human action classes, depending on the dataset version. The videos include human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Each action class has at least 400/600/700 video clips. Each clip is human annotated with a single action class and lasts around 10 seconds.</p> <p>This dataset contains videos and action classifications for the 700 class version of the dataset.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>kinetics-700</code></p> </li> <li> <p>Dataset source: https://deepmind.com/research/open-source/kinetics</p> </li> <li> <p>Dataset size: 710 GB</p> </li> <li> <p>Tags: <code>video, classification, action-recognition</code></p> </li> <li> <p>Supported splits: <code>train, test, validation</code></p> </li> <li> <p>ZooDataset class: <code>Kinetics700Dataset</code></p> </li> </ul> <p>Split stats:</p> <ul> <li> <p>Train split: 529,046 videos</p> </li> <li> <p>Test split: 67,446 videos</p> </li> <li> <p>Validation split: 33,925 videos</p> </li> </ul> <p>Dataset size</p> <ul> <li> <p>Train split: 603 GB</p> </li> <li> <p>Test split: 59 GB</p> </li> <li> <p>Validation split: 48 GB</p> </li> </ul> <p>Partial downloads</p> <p>Kinetics is a massive dataset, so FiftyOne provides parameters that can be used to efficiently download specific subsets of the dataset to suit your needs. When new subsets are specified, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from the web.</p> <p>Kinetics videos were originally only accessible from YouTube. Over time, some videos have become unavailable so the CVDF have hosted the Kinetics dataset on AWS.</p> <p>If you are partially downloading the dataset through FiftyOne, the specific videos of interest will be downloaded from YouTube, if necessary. However, when you load an entire split, the CVDF-provided files will be downloaded from AWS.</p> <p>The following parameters are available to configure a partial download of Kinetics by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If neither is provided, all available splits are loaded</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual videos. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>classes</code> are also specified, only up to the number of samples that contain at least one specified class will be loaded. By default, all matching samples are loaded</p> </li> </ul> <p>Example usage</p> <p>Note</p> <p>In order to work with video datasets, you\u2019ll need to have ffmpeg installed.</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#kinetics-700-2020","title":"Kinetics 700-2020 \u00b6","text":"<p>Kinetics is a collection of large-scale, high-quality datasets of URL links of up to 650,000 video clips that cover 400/600/700 human action classes, depending on the dataset version. The videos include human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Each action class has at least 400/600/700 video clips. Each clip is human annotated with a single action class and lasts around 10 seconds.</p> <p>This version contains videos and action classifications for the 700 class version of the dataset that was updated with new videos in 2020. This dataset is a superset of Kinetics 700.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>kinetics-700-2020</code></p> </li> <li> <p>Dataset source: https://deepmind.com/research/open-source/kinetics</p> </li> <li> <p>Dataset size: 710 GB</p> </li> <li> <p>Tags: <code>video, classification, action-recognition</code></p> </li> <li> <p>Supported splits: <code>train, test, validation</code></p> </li> <li> <p>ZooDataset class: <code>Kinetics7002020Dataset</code></p> </li> </ul> <p>Original split stats:</p> <ul> <li> <p>Train split: 542,352 videos</p> </li> <li> <p>Test split: 67,433 videos</p> </li> <li> <p>Validation split: 34,125 videos</p> </li> </ul> <p>CVDF split stats:</p> <ul> <li> <p>Train split: 534,073 videos</p> </li> <li> <p>Test split: 64,260 videos</p> </li> <li> <p>Validation split: 33,914 videos</p> </li> </ul> <p>Dataset size</p> <ul> <li> <p>Train split: 603 GB</p> </li> <li> <p>Test split: 59 GB</p> </li> <li> <p>Validation split: 48 GB</p> </li> </ul> <p>Partial downloads</p> <p>Kinetics is a massive dataset, so FiftyOne provides parameters that can be used to efficiently download specific subsets of the dataset to suit your needs. When new subsets are specified, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from the web.</p> <p>Kinetics videos were originally only accessible from YouTube. Over time, some videos have become unavailable so the CVDF have hosted the Kinetics dataset on AWS.</p> <p>If you are partially downloading the dataset through FiftyOne, the specific videos of interest will be downloaded from YouTube, if necessary. However, when you load an entire split, the CVDF-provided files will be downloaded from AWS.</p> <p>The following parameters are available to configure a partial download of Kinetics by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If neither is provided, all available splits are loaded</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual videos. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>classes</code> are also specified, only up to the number of samples that contain at least one specified class will be loaded. By default, all matching samples are loaded</p> </li> </ul> <p>Example usage</p> <p>Note</p> <p>In order to work with video datasets, you\u2019ll need to have ffmpeg installed.</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#kitti","title":"KITTI \u00b6","text":"<p>KITTI contains a suite of vision tasks built using an autonomous driving platform.</p> <p>This dataset contains the left camera images and the associated 2D object detections.</p> <p>The training split contains 7,481 annotated images, and the test split contains 7,518 unlabeled images.</p> <p>A full description of the annotations can be found in the README of the object development kit on the KITTI homepage.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>kitti</code></p> </li> <li> <p>Dataset source: http://www.cvlibs.net/datasets/kitti</p> </li> <li> <p>Dataset size: 12.57 GB</p> </li> <li> <p>Tags: <code>image, detection</code></p> </li> <li> <p>Supported splits: <code>train, test</code></p> </li> <li> <p>ZooDataset class: <code>KITTIDataset</code></p> </li> </ul> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#kitti-multiview","title":"KITTI Multiview \u00b6","text":"<p>KITTI contains a suite of vision tasks built using an autonomous driving platform.</p> <p>This dataset contains the following multiview data for each scene:</p> <ul> <li> <p>Left camera images annotated with 2D object detections</p> </li> <li> <p>Right camera images annotated with 2D object detections</p> </li> <li> <p>Velodyne LIDAR point clouds annotated with 3D object detections</p> </li> </ul> <p>The training split contains 7,481 annotated scenes, and the test split contains 7,518 unlabeled scenes.</p> <p>A full description of the annotations can be found in the README of the object development kit on the KITTI homepage.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>kitti-multiview</code></p> </li> <li> <p>Dataset source: http://www.cvlibs.net/datasets/kitti</p> </li> <li> <p>Dataset size: 53.34 GB</p> </li> <li> <p>Tags: <code>image, point-cloud, detection</code></p> </li> <li> <p>Supported splits: <code>train, test</code></p> </li> <li> <p>ZooDataset class: <code>KITTIMultiviewDataset</code></p> </li> </ul> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#labeled-faces-in-the-wild","title":"Labeled Faces in the Wild \u00b6","text":"<p>Labeled Faces in the Wild is a public benchmark for face verification, also known as pair matching.</p> <p>The dataset contains 13,233 images of 5,749 people\u2019s faces collected from the web. Each face has been labeled with the name of the person pictured. 1,680 of the people pictured have two or more distinct photos in the data set. The only constraint on these faces is that they were detected by the Viola-Jones face detector.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>lfw</code></p> </li> <li> <p>Dataset source: http://vis-www.cs.umass.edu/lfw</p> </li> <li> <p>Dataset size: 173.00 MB</p> </li> <li> <p>Tags: <code>image, classification, facial-recognition</code></p> </li> <li> <p>Supported splits: <code>test, train</code></p> </li> <li> <p>ZooDataset class: <code>LabeledFacesInTheWildDataset</code></p> </li> </ul> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#mnist","title":"MNIST \u00b6","text":"<p>The MNIST database of handwritten digits.</p> <p>The dataset consists of 70,000 28 x 28 grayscale images in 10 classes. There are 60,000 training images and 10,000 test images.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>mnist</code></p> </li> <li> <p>Dataset source: http://yann.lecun.com/exdb/mnist</p> </li> <li> <p>Dataset size: 21.00 MB</p> </li> <li> <p>Tags: <code>image, classification</code></p> </li> <li> <p>Supported splits: <code>train, test</code></p> </li> <li> <p>ZooDataset classes:</p> </li> <li> <p><code>MNISTDataset</code> (TF backend)</p> </li> <li> <p><code>MNISTDataset</code> (Torch backend)</p> </li> </ul> <p>Note</p> <p>You must have the Torch or TensorFlow backend(s) installed to load this dataset.</p> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#open-images-v6","title":"Open Images V6 \u00b6","text":"<p>Open Images V6 is a dataset of ~9 million images, roughly 2 million of which are annotated and available via this zoo dataset.</p> <p>The dataset contains annotations for classification, detection, segmentation, and visual relationship tasks for the 600 boxable classes.</p> <p>Note</p> <p>We\u2019ve collaborated with the Open Images Team at Google to make FiftyOne a recommended tool for downloading, visualizing, and evaluating on the Open Images Dataset!</p> <p>Check out this guide for more details on using FiftyOne to work with Open Images.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>open-images-v6</code></p> </li> <li> <p>Dataset source: https://storage.googleapis.com/openimages/web/index.html</p> </li> <li> <p>Dataset size: 561 GB</p> </li> <li> <p>Tags: <code>image, detection, segmentation, classification</code></p> </li> <li> <p>Supported splits: <code>train, test, validation</code></p> </li> <li> <p>ZooDataset class: <code>OpenImagesV6Dataset</code></p> </li> </ul> <p>Notes</p> <ul> <li> <p>Not all images contain all types of labels</p> </li> <li> <p>All images have been rescaled so that their largest side is at most 1024 pixels</p> </li> </ul> <p>Full split stats</p> <ul> <li> <p>Train split: 1,743,042 images (513 GB)</p> </li> <li> <p>Test split: 125,436 images (36 GB)</p> </li> <li> <p>Validation split: 41,620 images (12 GB)</p> </li> </ul> <p>Partial downloads</p> <p>Open Images is a massive dataset, so FiftyOne provides parameters that can be used to efficiently download specific subsets of the dataset to suit your needs. When new subsets are specified, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from the web.</p> <p>The following parameters are available to configure a partial download of Open Images V6 by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If neither is provided, all available splits are loaded</p> </li> <li> <p>label_types ( None): a label type or list of label types to load. Supported values are <code>(\"detections\", \"classifications\", \"relationships\", \"segmentations\")</code>. By default, all labels types are loaded</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded. You can use <code>get_classes()</code> and <code>get_segmentation_classes()</code> to see the available classes and segmentation classes, respectively</p> </li> <li> <p>attrs ( None): a string or list of strings specifying required relationship attributes to load. This parameter is only applicable if <code>label_types</code> contains <code>\"relationships\"</code>. If provided, only samples containing at least one instance of a specified attribute will be loaded. You can use <code>get_attributes()</code> to see the available attributes</p> </li> <li> <p>image_ids ( None): a list of specific image IDs to load. The IDs can be specified either as <code>&lt;split&gt;/&lt;image-id&gt;</code> or <code>&lt;image-id&gt;</code> strings. Alternatively, you can provide the path to a TXT (newline-separated), JSON, or CSV file containing the list of image IDs to load in either of the first two formats</p> </li> <li> <p>include_id ( True): whether to include the Open Images ID of each sample in the loaded labels</p> </li> <li> <p>only_matching ( False): whether to only load labels that match the <code>classes</code> or <code>attrs</code> requirements that you provide (True), or to load all labels for samples that match the requirements (False)</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual images. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>label_types</code>, <code>classes</code>, and/or <code>attrs</code> are also specified, first priority will be given to samples that contain all of the specified label types, classes, and/or attributes, followed by samples that contain at least one of the specified labels types or classes. The actual number of samples loaded may be less than this maximum value if the dataset does not contain sufficient samples matching your requirements</p> </li> </ul> <p>Note</p> <p>See <code>OpenImagesV6Dataset</code> and <code>OpenImagesV6DatasetImporter</code> for complete descriptions of the optional keyword arguments that you can pass to <code>load_zoo_dataset()</code>.</p> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#open-images-v7","title":"Open Images V7 \u00b6","text":"<p>Open Images V7 is a dataset of ~9 million images, roughly 2 million of which are annotated and available via this zoo dataset.</p> <p>The dataset contains annotations for classification, detection, segmentation, keypoints, and visual relationship tasks for the 600 boxable classes.</p> <p>Note</p> <p>We\u2019ve collaborated with the Open Images Team at Google to make FiftyOne a recommended tool for downloading, visualizing, and evaluating on the Open Images Dataset!</p> <p>Check out this guide for more details on using FiftyOne to work with Open Images.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>open-images-v7</code></p> </li> <li> <p>Dataset source: https://storage.googleapis.com/openimages/web/index.html</p> </li> <li> <p>Dataset size: 561 GB</p> </li> <li> <p>Tags: <code>image, detection, segmentation, classification, keypoint</code></p> </li> <li> <p>Supported splits: <code>train, test, validation</code></p> </li> <li> <p>ZooDataset class: <code>OpenImagesV7Dataset</code></p> </li> </ul> <p>Notes</p> <ul> <li> <p>Not all images contain all types of labels</p> </li> <li> <p>All images have been rescaled so that their largest side is at most 1024 pixels</p> </li> </ul> <p>Full split stats</p> <ul> <li> <p>Train split: 1,743,042 images (513 GB)</p> </li> <li> <p>Test split: 125,436 images (36 GB)</p> </li> <li> <p>Validation split: 41,620 images (12 GB)</p> </li> </ul> <p>Partial downloads</p> <p>Open Images is a massive dataset, so FiftyOne provides parameters that can be used to efficiently download specific subsets of the dataset to suit your needs. When new subsets are specified, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from the web.</p> <p>The following parameters are available to configure a partial download of Open Images V7 by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If neither is provided, all available splits are loaded</p> </li> <li> <p>label_types ( None): a label type or list of label types to load. Supported values are <code>(\"detections\", \"classifications\", \"relationships\", \"points\", segmentations\")</code>. By default, all labels types are loaded</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded. You can use <code>get_classes()</code> and <code>get_segmentation_classes()</code> to see the available classes and segmentation classes, respectively</p> </li> <li> <p>attrs ( None): a string or list of strings specifying required relationship attributes to load. This parameter is only applicable if <code>label_types</code> contains <code>\"relationships\"</code>. If provided, only samples containing at least one instance of a specified attribute will be loaded. You can use <code>get_attributes()</code> to see the available attributes</p> </li> <li> <p>image_ids ( None): a list of specific image IDs to load. The IDs can be specified either as <code>&lt;split&gt;/&lt;image-id&gt;</code> or <code>&lt;image-id&gt;</code> strings. Alternatively, you can provide the path to a TXT (newline-separated), JSON, or CSV file containing the list of image IDs to load in either of the first two formats</p> </li> <li> <p>include_id ( True): whether to include the Open Images ID of each sample in the loaded labels</p> </li> <li> <p>only_matching ( False): whether to only load labels that match the <code>classes</code> or <code>attrs</code> requirements that you provide (True), or to load all labels for samples that match the requirements (False)</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual images. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>label_types</code>, <code>classes</code>, and/or <code>attrs</code> are also specified, first priority will be given to samples that contain all of the specified label types, classes, and/or attributes, followed by samples that contain at least one of the specified labels types or classes. The actual number of samples loaded may be less than this maximum value if the dataset does not contain sufficient samples matching your requirements</p> </li> </ul> <p>Note</p> <p>See <code>OpenImagesV7Dataset</code> and <code>OpenImagesV7DatasetImporter</code> for complete descriptions of the optional keyword arguments that you can pass to <code>load_zoo_dataset()</code>.</p> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#places","title":"Places \u00b6","text":"<p>Places is a scene recognition dataset of 10 million images comprising ~400 unique scene categories.</p> <p>The images are labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>places</code></p> </li> <li> <p>Dataset source: http://places2.csail.mit.edu/download-private.html</p> </li> <li> <p>Dataset size: 29 GB</p> </li> <li> <p>Tags: <code>image, classification</code></p> </li> <li> <p>Supported splits: <code>train, validation, test</code></p> </li> <li> <p>ZooDataset classes: <code>PlacesDataset</code></p> </li> </ul> <p>Full split stats</p> <ul> <li> <p>Train split: 1,803,460 images, with between 3,068 and 5,000 per category</p> </li> <li> <p>Test split: 328,500 images, with 900 images per category</p> </li> <li> <p>Validation split: 36,500 images, with 100 images per category</p> </li> </ul> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#quickstart","title":"Quickstart \u00b6","text":"<p>A small dataset with ground truth bounding boxes and predictions.</p> <p>The dataset consists of 200 images from the validation split of COCO-2017, with model predictions generated by an out-of-the-box Faster R-CNN model from torchvision.models.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>quickstart</code></p> </li> <li> <p>Dataset size: 23.40 MB</p> </li> <li> <p>Tags: <code>image, quickstart</code></p> </li> <li> <p>Supported splits: <code>N/A</code></p> </li> <li> <p>ZooDataset class: <code>QuickstartDataset</code></p> </li> </ul> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#quickstart-geo","title":"Quickstart Geo \u00b6","text":"<p>A small dataset with geolocation data.</p> <p>The dataset consists of 500 images from the validation split of the BDD100K dataset in the New York City area with object detections and GPS timestamps.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>quickstart-geo</code></p> </li> <li> <p>Dataset size: 33.50 MB</p> </li> <li> <p>Tags: <code>image, location, quickstart</code></p> </li> <li> <p>Supported splits: <code>N/A</code></p> </li> <li> <p>ZooDataset class: <code>QuickstartGeoDataset</code></p> </li> </ul> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#quickstart-video","title":"Quickstart Video \u00b6","text":"<p>A small video dataset with dense annotations.</p> <p>The dataset consists of 10 video segments with dense object detections generated by human annotators.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>quickstart-video</code></p> </li> <li> <p>Dataset size: 35.20 MB</p> </li> <li> <p>Tags: <code>video, quickstart</code></p> </li> <li> <p>Supported splits: <code>N/A</code></p> </li> <li> <p>ZooDataset class: <code>QuickstartVideoDataset</code></p> </li> </ul> <p>Example usage</p> <p>Note</p> <p>In order to work with video datasets, you\u2019ll need to have ffmpeg installed.</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#quickstart-groups","title":"Quickstart Groups \u00b6","text":"<p>A small dataset with grouped image and point cloud data.</p> <p>The dataset consists of 200 scenes from the train split of the KITTI dataset, each containing left camera, right camera, point cloud, and 2D/3D object annotation data.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>quickstart-groups</code></p> </li> <li> <p>Dataset size: 516.3 MB</p> </li> <li> <p>Tags: <code>image, point-cloud, quickstart</code></p> </li> <li> <p>Supported splits: <code>N/A</code></p> </li> <li> <p>ZooDataset class: <code>QuickstartGroupsDataset</code></p> </li> </ul> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#quickstart-3d","title":"Quickstart 3D \u00b6","text":"<p>A small 3D dataset with meshes, point clouds, and oriented bounding boxes.</p> <p>The dataset consists of 200 3D mesh samples from the test split of the ModelNet40 dataset, with point clouds generated using a Poisson disk sampling method, and oriented bounding boxes generated based on the convex hull.</p> <p>Objects have been rescaled and recentered from the original dataset.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>quickstart-3d</code></p> </li> <li> <p>Dataset size: 215.7 MB</p> </li> <li> <p>Tags: <code>3d, point-cloud, mesh, quickstart</code></p> </li> <li> <p>Supported splits: <code>N/A</code></p> </li> <li> <p>ZooDataset class: <code>Quickstart3DDataset</code></p> </li> </ul> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#sama-coco","title":"Sama-COCO \u00b6","text":"<p>Sama-COCO is a relabeling of COCO-2017 and is a large-scale object detection and segmentation dataset. Masks in Sama-COCO are tighter and many crowd instances have been decomposed into their components.</p> <p>This version contains images from the COCO-2017 version of the dataset, as well as annotations in the form of bounding boxes, and segmentation masks provided by Sama.</p> <p>Notes</p> <ul> <li> <p>Sama-COCO defines 91 classes but the data only uses 80 classes (like COCO-2017)</p> </li> <li> <p>Some images from the train and validation sets don\u2019t have annotations</p> </li> <li> <p>The test set does not have annotations</p> </li> <li> <p>Sama-COCO has identical splits to COCO-2017</p> </li> </ul> <p>Details</p> <ul> <li> <p>Dataset name: <code>sama-coco</code></p> </li> <li> <p>Dataset source: https://www.sama.com/sama-coco-dataset/</p> </li> <li> <p>Dataset size: 25.67 GB</p> </li> <li> <p>Tags: <code>image, detection, segmentation</code></p> </li> <li> <p>Supported splits: <code>train, validation, test</code></p> </li> <li> <p>ZooDataset class: <code>SamaCOCODataset</code></p> </li> </ul> <p>Full split stats</p> <ul> <li> <p>Train split: 118,287 images</p> </li> <li> <p>Test split: 40,670 images</p> </li> <li> <p>Validation split: 5,000 images</p> </li> </ul> <p>Partial downloads</p> <p>FiftyOne provides parameters that can be used to efficiently download specific subsets of the Sama-COCO dataset to suit your needs. When new subsets are specified, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from the web.</p> <p>The following parameters are available to configure a partial download of Sama-COCO by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If neither is provided, all available splits are loaded</p> </li> <li> <p>label_types ( None): a label type or list of label types to load. Supported values are <code>(\"detections\", \"segmentations\")</code>. By default, only detections are loaded</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded</p> </li> <li> <p>image_ids ( None): a list of specific image IDs to load. The IDs can be specified either as <code>&lt;split&gt;/&lt;image-id&gt;</code> strings or <code>&lt;image-id&gt;</code> ints of strings. Alternatively, you can provide the path to a TXT (newline-separated), JSON, or CSV file containing the list of image IDs to load in either of the first two formats</p> </li> <li> <p>include_id ( False): whether to include the COCO ID of each sample in the loaded labels</p> </li> <li> <p>include_license ( False): whether to include the COCO license of each sample in the loaded labels, if available. The supported values are:</p> </li> <li> <p><code>\"False\"</code> (default): don\u2019t load the license</p> </li> <li> <p><code>True</code>/ <code>\"name\"</code>: store the string license name</p> </li> <li> <p><code>\"id\"</code>: store the integer license ID</p> </li> <li> <p><code>\"url\"</code>: store the license URL</p> </li> <li> <p>only_matching ( False): whether to only load labels that match the <code>classes</code> or <code>attrs</code> requirements that you provide (True), or to load all labels for samples that match the requirements (False)</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual images. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>label_types</code> and/or <code>classes</code> are also specified, first priority will be given to samples that contain all of the specified label types and/or classes, followed by samples that contain at least one of the specified labels types or classes. The actual number of samples loaded may be less than this maximum value if the dataset does not contain sufficient samples matching your requirements</p> </li> </ul> <p>Note</p> <p>See <code>SamaCOCODataset</code> and <code>COCODetectionDatasetImporter</code> for complete descriptions of the optional keyword arguments that you can pass to <code>load_zoo_dataset()</code>.</p> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#ucf101","title":"UCF101 \u00b6","text":"<p>UCF101 is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. This data set is an extension of UCF50 data set which has 50 action categories.</p> <p>With 13,320 videos from 101 action categories, UCF101 gives the largest diversity in terms of actions and with the presence of large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions, etc, it is the most challenging data set to date. As most of the available action recognition data sets are not realistic and are staged by actors, UCF101 aims to encourage further research into action recognition by learning and exploring new realistic action categories.</p> <p>The videos in 101 action categories are grouped into 25 groups, where each group can consist of 4-7 videos of an action. The videos from the same group may share some common features, such as similar background, similar viewpoint, etc.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>ucf101</code></p> </li> <li> <p>Dataset source: https://www.crcv.ucf.edu/research/data-sets/ucf101</p> </li> <li> <p>Dataset size: 6.48 GB</p> </li> <li> <p>Tags: <code>video, action-recognition</code></p> </li> <li> <p>Supported splits: <code>train, test</code></p> </li> <li> <p>ZooDataset class: <code>UCF101Dataset</code></p> </li> </ul> <p>Example usage</p> <p>Note</p> <p>In order to work with video datasets, you\u2019ll need to have ffmpeg installed.</p> <p>Also, if you don\u2019t already have a utility to uncompress <code>.rar</code> archives, you may need to install one. For example, on macOS:</p> <pre><code>brew install rar\n</code></pre> <p></p>"},{"location":"data/dataset_zoo/datasets/#voc-2007","title":"VOC-2007 \u00b6","text":"<p>The dataset for the PASCAL Visual Object Classes Challenge 2007 (VOC2007) for the detection competition.</p> <p>A total of 9,963 images are included in this dataset, where each image contains a set of objects, out of 20 different classes, making a total of 24,640 annotated objects.</p> <p>Note that, as per the official dataset, the test set of VOC2007 does not contain annotations.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>voc-2007</code></p> </li> <li> <p>Dataset source: http://host.robots.ox.ac.uk/pascal/VOC/voc2007</p> </li> <li> <p>Dataset size: 868.85 MB</p> </li> <li> <p>Tags: <code>image, detection</code></p> </li> <li> <p>Supported splits: <code>train, validation, test</code></p> </li> <li> <p>ZooDataset classes:</p> </li> <li> <p><code>VOC2007Dataset</code> (TF backend)</p> </li> <li> <p><code>VOC2007Dataset</code> (Torch backend)</p> </li> </ul> <p>Note</p> <p>The <code>test</code> split is only available via the TensorFlow backend.</p> <p>Note</p> <p>You must have the Torch or TensorFlow backend(s) installed to load this dataset.</p> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/datasets/#voc-2012","title":"VOC-2012 \u00b6","text":"<p>The dataset for the PASCAL Visual Object Classes Challenge 2012 (VOC2012) for the detection competition.</p> <p>A total of 11540 images are included in this dataset, where each image contains a set of objects, out of 20 different classes, making a total of 27450 annotated objects.</p> <p>Note that, as per the official dataset, the test set of VOC2012 does not contain annotations.</p> <p>Details</p> <ul> <li> <p>Dataset name: <code>voc-2012</code></p> </li> <li> <p>Dataset source: http://host.robots.ox.ac.uk/pascal/VOC/voc2012</p> </li> <li> <p>Dataset size: 3.59 GB</p> </li> <li> <p>Tags: <code>image, detection</code></p> </li> <li> <p>Supported splits: <code>train, validation, test</code></p> </li> <li> <p>ZooDataset classes:</p> </li> <li> <p><code>VOC2012Dataset</code> (TF backend)</p> </li> <li> <p><code>VOC2012Dataset</code> (Torch backend)</p> </li> </ul> <p>Note</p> <p>The <code>test</code> split is only available via the TensorFlow backend.</p> <p>Note</p> <p>You must have the Torch or TensorFlow backend(s) installed to load this dataset.</p> <p>Example usage</p> <p></p>"},{"location":"data/dataset_zoo/remote/","title":"Remotely-Sourced Zoo Datasets \u00b6","text":"<p>This page describes how to work with and create zoo datasets whose download/preparation methods are hosted via GitHub repositories or public URLs.</p> <p>Note</p> <p>To download from a private GitHub repository that you have access to, provide your GitHub personal access token by setting the <code>GITHUB_TOKEN</code> environment variable.</p>"},{"location":"data/dataset_zoo/remote/#working-with-remotely-sourced-datasets","title":"Working with remotely-sourced datasets \u00b6","text":"<p>Working with remotely-sourced zoo datasets is just like built-in zoo datasets, as both varieties support the full zoo API.</p> <p>When specifying remote sources, you can provide any of the following:</p> <ul> <li> <p>A GitHub repo URL like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;</code></p> </li> <li> <p>A GitHub ref like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/tree/&lt;branch&gt;</code> or <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/commit/&lt;commit&gt;</code></p> </li> <li> <p>A GitHub ref string like <code>&lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;]</code></p> </li> <li> <p>A publicly accessible URL of an archive (eg zip or tar) file</p> </li> </ul> <p>Here\u2019s the basic recipe for working with remotely-sourced zoo datasets:</p>"},{"location":"data/dataset_zoo/remote/#creating-remotely-sourced-datasets","title":"Creating remotely-sourced datasets \u00b6","text":"<p>A remotely-sourced dataset is defined by a directory with the following contents:</p> <pre><code>fiftyone.yml\n__init__.py\n    def download_and_prepare(dataset_dir, split=None, **kwargs):\n        pass\n\n    def load_dataset(dataset, dataset_dir, split=None, **kwargs):\n        pass\n</code></pre> <p>Each component is described in detail below.</p> <p>Note</p> <p>By convention, datasets also contain an optional <code>README.md</code> file that provides additional information about the dataset and example syntaxes for downloading and working with it.</p>"},{"location":"data/dataset_zoo/remote/#fiftyoneyml","title":"fiftyone.yml \u00b6","text":"<p>The dataset\u2019s <code>fiftyone.yml</code> or <code>fiftyone.yaml</code> file defines relevant metadata about the dataset:</p> Field Required? Description <code>name</code> yes The name of the dataset. Once you\u2019ve downloaded all or part of aremotely-sourced zoo dataset, it will subsequently appear as an availablezoo dataset under this name when using thezoo API <code>type</code> Declare that the directory defines a <code>dataset</code>. This can be omitted forbackwards compatibility, but it is recommended to specify this <code>author</code> The author of the dataset <code>version</code> The version of the dataset <code>url</code> The source (eg GitHub repository) where the directory containing this fileis hosted <code>source</code> The original source of the dataset <code>license</code> The license under which the dataset is distributed <code>description</code> A brief description of the dataset <code>fiftyone.version</code> A semver version specifier (or <code>*</code>) describing the requiredFiftyOne version for the dataset to load properly <code>supports_partial_downloads</code> Specify <code>true</code> or <code>false</code> whether parts of the dataset can bedownloaded/loaded by providing <code>kwargs</code> to<code>download_zoo_dataset()</code>or <code>load_zoo_dataset()</code> asdescribed here. If omitted,this is assumed to be <code>false</code> <code>tags</code> A list of tags for the dataset. Useful in conjunction with<code>list_zoo_datasets()</code> <code>splits</code> A list of the dataset\u2019s supported splits. This should be omitted if thedataset does not contain splits <code>size_samples</code> The totaal number of samples in the dataset, or a list of per-split sizes <p>Here are two example dataset YAML files:</p>"},{"location":"data/dataset_zoo/remote/#download-and-prepare","title":"Download and prepare \u00b6","text":"<p>All dataset\u2019s <code>__init__.py</code> files must define a <code>download_and_prepare()</code> method with the signature below:</p> <pre><code>def download_and_prepare(dataset_dir, split=None, **kwargs):\n    \"\"\"Downloads the dataset and prepares it for loading into FiftyOne.\n\n    Args:\n        dataset_dir: the directory in which to construct the dataset\n        split (None): a specific split to download, if the dataset supports\n            splits. The supported split values are defined by the dataset's\n            YAML file\n        **kwargs: optional keyword arguments that your dataset can define to\n            configure what/how the download is performed\n\n    Returns:\n        a tuple of\n\n        -   ``dataset_type``: a ``fiftyone.types.Dataset`` type that the\n            dataset is stored in locally, or None if the dataset provides\n            its own ``load_dataset()`` method\n        -   ``num_samples``: the total number of downloaded samples for the\n            dataset or split\n        -   ``classes``: a list of classes in the dataset, or None if not\n            applicable\n    \"\"\"\n\n    # Download files and organize them in `dataset_dir`\n    ...\n\n    # Define how the data is stored\n    dataset_type = fo.types.ImageClassificationDirectoryTree\n    dataset_type = None  # custom ``load_dataset()`` method\n\n    # Indicate how many samples have been downloaded\n    # May be less than the total size if partial downloads have been used\n    num_samples = 10000\n\n    # Optionally report what classes exist in the dataset\n    classes = None\n    classes = [\"cat\", \"dog\", ...]\n\n    return dataset_type, num_samples, classes\n</code></pre> <p>This method is called under-the-hood when a user calls <code>download_zoo_dataset()</code> or <code>load_zoo_dataset()</code>, and its job is to download any relevant files from the web and organize and/or prepare them as necessary into a format that\u2019s ready to be loaded into a FiftyOne dataset.</p> <p>The <code>dataset_type</code> that <code>download_and_prepare()</code> returns defines how it the dataset is ultimately loaded into FiftyOne:</p> <ul> <li>Built-in importer: in many cases, FiftyOne already contains a built-in importer that can be leveraged to load data on disk into FiftyOne. Remotely-sourced datasets can take advantage of this by simply returning the appropriate <code>dataset_type</code> from <code>download_and_prepare()</code>, which is then used to load the data into FiftyOne as follows:</li> </ul> <pre><code># If the dataset has splits, `dataset_dir` will be the split directory\ndataset_importer_cls = dataset_type.get_dataset_importer_cls()\ndataset_importer = dataset_importer_cls(dataset_dir=dataset_dir, **kwargs)\n\ndataset.add_importer(dataset_importer, **kwargs)\n</code></pre> <ul> <li>Custom loader: if <code>dataset_type=None</code> is returned, then <code>__init__.py</code> must also contain a <code>load_dataset()</code> method as described below that handles loading the data into FiftyOne as follows:</li> </ul> <pre><code>load_dataset(dataset, dataset_dir, **kwargs)\n</code></pre>"},{"location":"data/dataset_zoo/remote/#load-dataset","title":"Load dataset \u00b6","text":"<p>Datasets that don\u2019t use a built-in importer must also define a <code>load_dataset()</code> method in their <code>__init__.py</code> with the signature below:</p> <pre><code>def load_dataset(dataset, dataset_dir, split=None, **kwargs):\n    \"\"\"Loads the dataset into the given FiftyOne dataset.\n\n    Args:\n        dataset: a :class:`fiftyone.core.dataset.Dataset` to which to import\n        dataset_dir: the directory to which the dataset was downloaded\n        split (None): a split to load. The supported values are\n            ``(\"train\", \"validation\", \"test\")``\n        **kwargs: optional keyword arguments that your dataset can define to\n            configure what/how the load is performed\n    \"\"\"\n\n    # Load data into samples\n    samples = [...]\n\n    # Add samples to the dataset\n    dataset.add_samples(samples)\n</code></pre> <p>This method\u2019s job is to load the filepaths and any relevant labels into <code>Sample</code> objects and then call <code>add_samples()</code> or a similar method to add them to the provided <code>Dataset</code>.</p>"},{"location":"data/dataset_zoo/remote/#partial-downloads","title":"Partial downloads \u00b6","text":"<p>Remotely-sourced datasets can support partial downloads, which is useful for a variety of reasons, including:</p> <ul> <li> <p>A dataset may contain labels for multiple task types but the user is only interested in a subset of them</p> </li> <li> <p>The dataset may be very large and the user only wants to download a small subset of the samples to get familiar with the dataset</p> </li> </ul> <p>Datasets that support partial downloads should declare this in their fiftyone.yml:</p> <pre><code>supports_partial_downloads: true\n</code></pre> <p>The partial download behavior itself is defined via <code>**kwargs</code> in the dataset\u2019s <code>__init__.py</code> methods:</p> <pre><code>def download_and_prepare(dataset_dir, split=None, **kwargs):\n    pass\n\ndef load_dataset(dataset, dataset_dir, split=None, **kwargs):\n    pass\n</code></pre> <p>When <code>download_zoo_dataset(url, ..., **kwargs)</code> is called, any <code>kwargs</code> declared by <code>download_and_prepare()</code> are passed through to it.</p> <p>When <code>load_zoo_dataset(name_or_url, ..., **kwargs)</code> is called, any <code>kwargs</code> declared by <code>download_and_prepare()</code> and <code>load_dataset()</code> are passed through to them, respectively.</p> <p>Note</p> <p>Check out voxel51/coco-2017 for an example of a remotely-sourced dataset that supports partial downloads.</p>"},{"location":"faq/","title":"Frequently Asked Questions \u00b6","text":""},{"location":"faq/#can-i-open-the-fiftyone-app-in-a-browser","title":"Can I open the FiftyOne App in a browser? \u00b6","text":"<p>Yes! In fact, this is the default behavior. Unless you\u2019re working in a notebook, the App will open in your default web browser whenever you call <code>launch_app()</code> .</p> <p>Check out the environments guide to see how to use FiftyOne in all common local, remote, cloud, and notebook environments.</p>"},{"location":"faq/#which-web-browsers-does-the-fiftyone-app-support","title":"Which web browsers does the FiftyOne App support? \u00b6","text":"<p>The FiftyOne App fully supports Chrome, Firefox, and Safari.</p> <p>You may find success using browsers like Edge, Opera, or Chromium, but your mileage will vary. Internet Explorer is explicitly unsupported at this time.</p>"},{"location":"faq/#why-isnt-the-app-opening-not-connected-to-a-session","title":"Why isn\u2019t the App opening? Not connected to a session? \u00b6","text":"<p>When you call <code>fo.launch_app()</code> to launch the FiftyOne App, the App will launch asynchronously and return control to your Python process. The App will then remain connected until the process exits.</p> <p>If you are using the App in a script, you should use <code>session.wait()</code> to block execution until you close it manually:</p> <pre><code># Launch the App\nsession = fo.launch_app(...)\n\n# (Perform any additional operations here)\n\n# Blocks execution until the App is closed\nsession.wait()\n</code></pre> <p>If you launch the App in a script without including <code>session.wait()</code>, the App\u2019s connection will close when the script exits, and you will see a message like \u201cIt looks like you are not connected to a session\u201d in the browser tab that was opened.</p>"},{"location":"faq/#why-cant-i-open-the-app-from-a-script-on-windows","title":"Why can\u2019t I open the App from a script on Windows? \u00b6","text":"<p>If you are a Windows user launching the FiftyOne App from a script, you should use the pattern below to avoid multiprocessing issues, since the App is served via a separate process:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.load_dataset(...)\n\nif __name__ == \"__main__\":\n    # Ensures that the App processes are safely launched on Windows\n    session = fo.launch_app(dataset)\n    session.wait()\n</code></pre> <p>See this section for more details.</p>"},{"location":"faq/#can-i-use-fiftyone-in-a-notebook","title":"Can I use FiftyOne in a notebook? \u00b6","text":"<p>Yes! FiftyOne supports Jupyter Notebooks, Google Colab Notebooks, Databricks Notebooks, and SageMaker Notebooks.</p> <p>All the usual FiftyOne commands can be run in notebook environments, and the App will launch/update in the output of your notebook cells!</p> <p>Check out the notebook environment guide for more information about running FiftyOne in notebooks.</p>"},{"location":"faq/#why-isnt-the-app-loading-in-my-cloud-notebook","title":"Why isn\u2019t the App loading in my cloud notebook? \u00b6","text":"<p>Except for Google Colab and Databricks which have built-in App configuration, when working in a cloud notebook a proxy_url should be set in your FiftyOne App config.</p>"},{"location":"faq/#can-i-use-fiftyone-in-a-remote-notebook","title":"Can I use FiftyOne in a remote notebook? \u00b6","text":"<p>Yes! It is possible to work with a Jupyter notebook in your local browser that is served from a remote machine.</p> <p>Refer to this section of the environment guide for instructions to achieve this.</p>"},{"location":"faq/#can-i-restrict-access-to-my-remote-app-instance","title":"Can I restrict access to my remote App instance? \u00b6","text":"<p>By default, remote App sessions will listen to any connection to their ports. However, if desired, you can restrict access to an App session to a particular IP address or hostname by following these instructions.</p>"},{"location":"faq/#why-arent-plots-appearing-in-my-notebook","title":"Why aren\u2019t plots appearing in my notebook? \u00b6","text":"<p>If you are trying to view plots in a Jupyter notebook but nothing appears after you call <code>plot.show()</code>, then you likely need to follow these instructions to install the proper packages and/or Jupyter notebook extensions.</p> <p>If the proper packages are installed but plots are still not displaying, try including the following commands in your notebook before creating any plots:</p> <pre><code># Ensure that plotly.js is downloaded\nimport plotly.offline as po\npo.init_notebook_mode(connected=True)\n</code></pre>"},{"location":"faq/#can-i-access-data-stored-on-a-remote-server","title":"Can I access data stored on a remote server? \u00b6","text":"<p>Yes! If you install FiftyOne on both your remote server and local machine, then you can load a dataset remotely and then explore it via an App session on your local machine.</p>"},{"location":"faq/#can-i-access-data-stored-in-the-cloud","title":"Can I access data stored in the cloud? \u00b6","text":"<p>Yes! Check out FiftyOne Teams.</p>"},{"location":"faq/#what-operating-systems-does-fiftyone-support","title":"What operating systems does FiftyOne support? \u00b6","text":"<p>FiftyOne officially supports the latest versions of MacOS and Windows, as well as Amazon Linux 2 and 2023, Debian 9+ (x86_64 only), Ubuntu 18.04+, and RHEL/CentOS 7+.</p> <p>Note</p> <p>If installing on Ubuntu 22.04+, Debian, or RHEL/CentOS, <code>fiftyone-db==0.4.3</code> must be requested.</p> <pre><code>pip install fiftyone-db==0.4.3 fiftyone\n</code></pre>"},{"location":"faq/#what-image-file-types-are-supported","title":"What image file types are supported? \u00b6","text":"<p>In general, FiftyOne supports all image types supported by your browser, which includes standard image types like JPEG, PNG, and BMP.</p> <p>Some browsers like Safari natively support other image types such as TIFF, while others do not. You may be able to install a browser extension to work with additional image types, but Voxel51 does not currently recommend any such extensions in particular.</p>"},{"location":"faq/#what-video-file-types-are-supported","title":"What video file types are supported? \u00b6","text":"<p>Core methods that process videos can generally handle any codec supported by FFmpeg.</p> <p>The App can play any video codec that is supported by HTML5 video on your browser, including MP4 (H.264), WebM, and Ogg. If you try to view a video with an unsupported codec in the App, you will be prompted to use the <code>reencode_videos()</code> utility method to re-encode the source video so it is viewable in the App.</p> <p>Note</p> <p>You must install FFmpeg in order to work with video datasets in FiftyOne. See this page for installation instructions.</p>"},{"location":"faq/#what-label-types-are-supported","title":"What label types are supported? \u00b6","text":"<p>FiftyOne provides support for all of the following label types for both image and video datasets:</p> <ul> <li> <p>Classifications</p> </li> <li> <p>Multilabel classifications</p> </li> <li> <p>Object detections</p> </li> <li> <p>Instance segmentations</p> </li> <li> <p>Polylines and polygons</p> </li> <li> <p>Keypoints</p> </li> <li> <p>Semantic segmentations</p> </li> <li> <p>Geolocation data</p> </li> </ul> <p>Check out this guide for simple recipes to load labels in these formats.</p>"},{"location":"faq/#what-happened-to-my-datasets-from-previous-sessions","title":"What happened to my datasets from previous sessions? \u00b6","text":"<p>By default, datasets are non-persistent, which means they are deleted from the database whenever you exit (all) Python sessions in which you\u2019ve imported FiftyOne.</p> <p>To make a dataset persistent, set its <code>persistent</code> property to <code>True</code>:</p> <pre><code>import fiftyone as fo\n\n# This dataset will be deleted when you exit Python\ndataset = fo.Dataset(\"test\")\n\n# Now the dataset is permanent\ndataset.persistent = True\n</code></pre> <p>See this page for more details about dataset persistence.</p> <p>Note</p> <p>FiftyOne does not store the raw data in datasets directly (only the labels), so your source files on disk are never deleted!</p>"},{"location":"faq/#why-didnt-changes-to-my-dataset-save","title":"Why didn\u2019t changes to my dataset save? \u00b6","text":"<p>Although adding samples to datasets immediately writes them to the database, remember that any edits that you make to a sample or its frame labels will not be written to the database until you call <code>sample.save()</code>.</p> <p>Similarly, setting the properties of a <code>Dataset</code> object will be immediately saved, but you must call <code>dataset.save()</code> whenever you edit fields such as <code>info</code> or <code>classes</code> in-place.</p> <p>Refer to this section for more details about modifying samples and this section for more details about storing dataset-level information.</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset(...)\nnew_samples = [...]\n\n# Setting a property is automatically saved\ndataset.persistent = True\n\ndataset.info[\"hello\"] = \"world\"\ndataset.save()  # don't forget this!\n\n# Added samples are automatically saved\ndataset.add_samples(new_samples)\n\nfor sample in dataset:\n    sample[\"field\"] = 51\n    sample.save()  # don't forget this!\n</code></pre>"},{"location":"faq/#can-i-share-a-dataset-with-someone-else","title":"Can I share a dataset with someone else? \u00b6","text":"<p>Yes! Here\u2019s a couple options:</p> <p>Option 1: Export and share</p> <p>You can easily export a dataset in one line of code, zip it, and share the zip with your collaborator, who can then load it in a few lines of code.</p> <p>Option 2: Sharing a remote session</p> <p>Alternatively, see this FAQ for instructions on launching a remote session and inviting collaborator(s) to connect to it from their local machines.</p>"},{"location":"faq/#can-i-use-fiftyone-in-multiple-shells","title":"Can I use FiftyOne in multiple shells? \u00b6","text":"<p>Yes! Any changes you make to a dataset or its samples in one shell will be reflected in the other shells whenever you access that dataset. You can also launch multiple App instances.</p> <p>Working with the same dataset in multiple shells simultaneously is generally seamless, even if you are editing the dataset, as the <code>Dataset</code> class does not store its <code>Sample</code> objects in-memory, it loads them from the database only when they are requested. Therefore, if you add or modify a <code>Sample</code> in one shell, you will immediately have access to the updates the next time you request that <code>Sample</code> in other shells.</p> <p>The one exception to this rule is that <code>Dataset</code> and <code>Sample</code> objects themselves are singletons, so if you hold references to these objects in-memory, they will not be automatically updated by re-accessing them, since the existing instances will be returned back to you.</p> <p>If a dataset may have been changed by another process, you can always manually call <code>Dataset.reload()</code> to reload the <code>Dataset</code> object and all in-memory <code>Sample</code> instances that belong to it.</p>"},{"location":"faq/#can-i-launch-multiple-app-instances-on-a-machine","title":"Can I launch multiple App instances on a machine? \u00b6","text":"<p>Yes! Simply specify a different <code>port</code> for each App instance that you create.</p>"},{"location":"faq/#can-i-connect-multiple-app-instances-to-the-same-dataset","title":"Can I connect multiple App instances to the same dataset? \u00b6","text":"<p>Yes, multiple App instances can be connected to the same <code>Dataset</code> via remote sessions.</p> <p>Note</p> <p>Keep in mind that all users must have ssh access to the system from which the remote session(s) are launched in order to connect to them.</p> <p>You can achieve multiple connections in two ways:</p> <p>Option 1: Same dataset, multiple sessions</p> <p>The typical way to connect multiple App instances to the same dataset is to create a separate remote session instance on the machine that houses the <code>Dataset</code> of interest for each local App instance that you want to create. See this FAQ for instructions on doing this.</p> <p>Option 2: Same dataset, same session</p> <p>Another option is to connect multiple App instances to a single remote session.</p> <p>First, create a remote session on the system that houses the <code>Dataset</code> using either the CLI or Python:</p> <p>Then one or more users can use the CLI on their local machine to connect to the remote session.</p> <p>Note</p> <p>When multiple App instances are connected to the same <code>Session</code>, any actions taken that affect the session (e.g., loading a view) will be reflected in all connected App instances.</p>"},{"location":"faq/#can-i-connect-to-multiple-remote-sessions","title":"Can I connect to multiple remote sessions? \u00b6","text":"<p>Yes, you can launch multiple instances of the App locally, each connected to a different remote session.</p> <p>The key here is to specify a different local port for each App instance that you create.</p> <p>Suppose you are connecting to multiple remote <code>Session</code> instances that were created on different remote systems (e.g., an EC2 instance and a remote server that you own), using commands similar to:</p> <p>On your local machine, you can connect to these remote sessions using a different local port <code>XXXX</code> and <code>YYYY</code> for each.</p> <p>If you do not have FiftyOne installed on your local machine, open a new terminal window on your local machine and execute the following command to setup port forwarding to connect to your remote sessions:</p> <pre><code>ssh -N -L XXXX:localhost:RRRR1 [&lt;username1&gt;@]&lt;hostname1&gt;\n# Then open `http://localhost:XXXX` in your web browser\n</code></pre> <pre><code>ssh -N -L YYYY:localhost:RRRR2 [&lt;username2&gt;@]&lt;hostname2&gt;\n# Then open `http://localhost:YYYY` in your web browser\n</code></pre> <p>In the above, <code>[&lt;username#&gt;@]&lt;hostname#&gt;</code> refers to a remote machine and <code>RRRR#</code> is the remote port that you used for the remote session.</p> <p>Alternatively, if you have FiftyOne installed on your local machine, you can use the CLI to automatically configure port forwarding and open the App in your browser as follows:</p> <pre><code># Connect to first remote session\nfiftyone app connect \\\n    --destination [&lt;username1&gt;@]&lt;hostname1&gt; \\\n    --port RRRR1\n    --local-port XXXX\n</code></pre> <pre><code># Connect to second remote session\nfiftyone app connect \\\n    --destination [&lt;username2&gt;@]&lt;hostname2&gt; \\\n    --port RRRR2\n    --local-port YYYY\n</code></pre> <p>Note</p> <p>You can also serve multiple remote sessions from the same machine.</p>"},{"location":"faq/#can-i-serve-multiple-remote-sessions-from-a-machine","title":"Can I serve multiple remote sessions from a machine? \u00b6","text":"<p>Yes, you can create multiple remote sessions on the same remote machine by specifying different ports for each <code>Session</code> that you create:</p> <p>On your local machine(s), you can now connect to the remote sessions. Connections can be set up using port forwarding in the following way:</p> <pre><code>ssh -N -L WWWW:localhost:XXXX [&lt;username&gt;@]&lt;hostname&gt;\n# Then open `http://localhost:WWWW` in your web browser\n</code></pre> <pre><code>ssh -N -L ZZZZ:localhost:YYYY [&lt;username&gt;@]&lt;hostname&gt;\n# Then open `http://localhost:ZZZZ` in your web browser\n</code></pre> <p>In the above, <code>[&lt;username&gt;@]&lt;hostname&gt;</code> refers to your remote machine, and <code>WWWW</code> and <code>ZZZZ</code> are any 4 digit ports on your local machine(s).</p> <p>Alternatively, if you have FiftyOne installed on your local machine, you can use the CLI to automatically configure port forwarding and open the App in your browser as follows:</p> <pre><code># On a local machine\n\n# Connect to first remote session\nfiftyone app connect \\\n    --destination [&lt;username&gt;@]&lt;hostname&gt; \\\n    --port XXXX \\\n    --local-port WWWW\n</code></pre> <pre><code># On a local machine\n\n# Connect to second remote session\nfiftyone app connect \\\n    --destination [&lt;username&gt;@]&lt;hostname&gt; \\\n    --port YYYY \\\n    --local-port ZZZZ\n</code></pre>"},{"location":"faq/#can-i-use-my-own-mongodb-database","title":"Can I use my own MongoDB database? \u00b6","text":"<p>Yes, you can configure FiftyOne to connect to your own MongoDB instance by setting the <code>database_uri</code> property of your FiftyOne config. Refer to this page for more information.</p>"},{"location":"faq/#too-many-open-files-in-system","title":"Too many open files in system? \u00b6","text":"<p>If you are a MacOS user and see a \u201ctoo many open files in system\u201d error when performing import/export operations with FiftyOne, then you likely need to increase the open files limit for your OS.</p> <p>Following the instructions in this post should resolve the issue for you.</p>"},{"location":"faq/#can-i-downgrade-to-an-older-version-of-fiftyone","title":"Can I downgrade to an older version of FiftyOne? \u00b6","text":"<p>Certainly, refer to these instructions.</p>"},{"location":"faq/#are-the-brain-methods-open-source","title":"Are the Brain methods open source? \u00b6","text":"<p>Yes, the FiftyOne Brain methods are open source.</p> <p>Check out the Brain documentation for detailed instructions on using the various Brain methods.</p>"},{"location":"faq/#does-fiftyone-track-me","title":"Does FiftyOne track me? \u00b6","text":"<p>FiftyOne tracks anonymous UUID-based usage of the App by default. We are a small team building an open source project, and basic knowledge of how users are engaging with the project is critical to informing the roadmap of the project.</p> <p>Note</p> <p>You can disable tracking by setting the <code>do_not_track</code> flag of your FiftyOne config.</p>"},{"location":"faq/deprecation/","title":"FiftyOne Deprecation Notices \u00b6","text":""},{"location":"faq/deprecation/#fiftyone-desktop","title":"FiftyOne Desktop \u00b6","text":"<p>Support ended with FiftyOne 0.25.0</p> <p>A compatible fiftyone-desktop package is no longer available as of <code>fiftyone==0.25.0</code>.</p> <p>Chromium-based browsers, Firefox, or a notebook environment are recommended for the best FiftyOne experience.</p>"},{"location":"faq/deprecation/#python-38","title":"Python 3.8 \u00b6","text":"<p>Support ended October 1, 2024</p> <p>Python 3.8 transitions to <code>end-of-life</code> effective October of 2024. FiftyOne releases after September 30, 2024 will no longer support Python 3.8.</p>"},{"location":"faq/deprecation/#kubernetes-127","title":"Kubernetes 1.27 \u00b6","text":"<p>Support ended November 1, 2024</p> <p>Kubernetes 1.27 transitioned to <code>end-of-life</code> effective July of 2024. FiftyOne Teams releases after October 31, 2024 might no longer be compatible with Kubernetes 1.27 and older.</p>"},{"location":"fiftyone_concepts/","title":"FiftyOne User Guide \u00b6","text":"<p>Each section in this guide provides an example-centric deep dive into a core feature of FiftyOne, with the goal of getting you up-and-running with FiftyOne on your data quickly and easily.</p>"},{"location":"fiftyone_concepts/#fiftyone-basics","title":"FiftyOne basics","text":"<p>Get up to speed with a quick overview of FiftyOne's basic concepts.</p> <p>Learn the basics</p>"},{"location":"fiftyone_concepts/#loading-data-into-fiftyone","title":"Loading data into FiftyOne","text":"<p>Load data into FiftyOne using standard formats, custom formats, or the Dataset Zoo.</p> <p>Learn more about loading data</p>"},{"location":"fiftyone_concepts/#using-datasets","title":"Using datasets","text":"<p>Take a deep dive into FiftyOne datasets and how to use them to manage your data.</p> <p>Learn more about using datasets</p>"},{"location":"fiftyone_concepts/#dataset-views","title":"Dataset views","text":"<p>Use FiftyOne's powerful dataset view interface to search, sort, and filter your data.</p> <p>Learn more about using dataset views</p>"},{"location":"fiftyone_concepts/#grouped-datasets","title":"Grouped datasets","text":"<p>Use grouped datasets to represent your multiview image, video, and point cloud data.</p> <p>Learn more about grouped datasets</p>"},{"location":"fiftyone_concepts/#using-the-app","title":"Using the App","text":"<p>Visualize your datasets in the FiftyOne App and interactively search, sort, and filter them.</p> <p>Learn more about the App</p>"},{"location":"fiftyone_concepts/#annotating-datasets","title":"Annotating datasets","text":"<p>Use builtin or custom integrations to add or edit labels on your FiftyOne datasets.</p> <p>Learn more about annotations</p>"},{"location":"fiftyone_concepts/#evaluating-models-new","title":"Evaluating models NEW","text":"<p>Use FiftyOne's builtin methods to evaluate your models and analyze their strengths and weaknesses.</p> <p>Learn more about evaluating models</p>"},{"location":"fiftyone_concepts/#using-aggregations","title":"Using aggregations","text":"<p>Efficiently compute aggregate statistics about your FiftyOne datasets and views.</p> <p>Learn more about using aggregations</p>"},{"location":"fiftyone_concepts/#interactive-plots","title":"Interactive plots","text":"<p>Use FiftyOne's powerful interactive plotting features to uncover patterns and improve your data.</p> <p>Dive into interactive plotting</p>"},{"location":"fiftyone_concepts/#exporting-datasets","title":"Exporting datasets","text":"<p>Export datasets to disk in any number of common formats, or in your own custom format.</p> <p>Learn more about exporting datasets</p>"},{"location":"fiftyone_concepts/#drawing-labels-on-samples","title":"Drawing labels on samples","text":"<p>Render labels on the samples in your FiftyOne dataset with a single line of code.</p> <p>Learn more about drawing labels</p>"},{"location":"fiftyone_concepts/#configuring-fiftyone","title":"Configuring FiftyOne","text":"<p>Customize the default behavior of the FiftyOne library to suit your needs.</p> <p>Learn how to configure FiftyOne</p>"},{"location":"fiftyone_concepts/annotation/","title":"Annotating Datasets \u00b6","text":"<p>FiftyOne provides a powerful annotation API that makes it easy to add or edit labels on your datasets or specific views into them.</p> <p>Note</p> <p>Did you know? You can request, manage, and import annotations from within the FiftyOne App by installing the @voxel51/annotation plugin!</p> <p>Note</p> <p>Check out this tutorial to see an example workflow that uses the annotation API to create, delete, and fix annotations on a FiftyOne dataset.</p>"},{"location":"fiftyone_concepts/annotation/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The basic workflow to use the annotation API to add or edit labels on your FiftyOne datasets is as follows:</p> <ol> <li> <p>Load a labeled or unlabeled dataset into FiftyOne</p> </li> <li> <p>Explore the dataset using the App or dataset views to locate either unlabeled samples that you wish to annotate or labeled samples whose annotations you want to edit</p> </li> <li> <p>Use the <code>annotate()</code> method on your dataset or view to upload the samples and optionally their existing labels to the annotation backend</p> </li> <li> <p>In the annotation tool, perform the necessary annotation work</p> </li> <li> <p>Back in FiftyOne, load your dataset and use the <code>load_annotations()</code> method to merge the annotations back into your FiftyOne dataset</p> </li> <li> <p>If desired, delete the annotation tasks and the record of the annotation run from your FiftyOne dataset</p> </li> </ol> <p>The example below demonstrates this workflow using the default CVAT backend.</p> <p>Note</p> <p>You must create an account at app.cvat.ai in order to run this example.</p> <p>Note that you can store your credentials as described in this section to avoid entering them manually each time you interact with CVAT.</p> <p>First, we create the annotation tasks:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\n# Step 1: Load your data into FiftyOne\n\ndataset = foz.load_zoo_dataset(\n    \"quickstart\", dataset_name=\"cvat-annotation-example\"\n)\ndataset.persistent = True\n\ndataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\"\n)\n\n# Step 2: Locate a subset of your data requiring annotation\n\n# Create a view that contains only high confidence false positive model\n# predictions, with samples containing the most false positives first\nmost_fp_view = (\n    dataset\n    .filter_labels(\"predictions\", (F(\"confidence\") &gt; 0.8) &amp; (F(\"eval\") == \"fp\"))\n    .sort_by(F(\"predictions.detections\").length(), reverse=True)\n)\n\n# Let's edit the ground truth annotations for the sample with the most\n# high confidence false positives\nsample_id = most_fp_view.first().id\nview = dataset.select(sample_id)\n\n# Step 3: Send samples to CVAT\n\n# A unique identifier for this run\nanno_key = \"cvat_basic_recipe\"\n\nview.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    attributes=[\"iscrowd\"],\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Step 4: Perform annotation in CVAT and save the tasks\n</code></pre> <p>Then, once the annotation work is complete, we merge the annotations back into FiftyOne:</p> <pre><code>import fiftyone as fo\n\nanno_key = \"cvat_basic_recipe\"\n\n# Step 5: Merge annotations back into FiftyOne dataset\n\ndataset = fo.load_dataset(\"cvat-annotation-example\")\ndataset.load_annotations(anno_key)\n\n# Load the view that was annotated in the App\nview = dataset.load_annotation_view(anno_key)\nsession = fo.launch_app(view=view)\n\n# Step 6: Cleanup\n\n# Delete tasks from CVAT\nresults = dataset.load_annotation_results(anno_key)\nresults.cleanup()\n\n# Delete run record (not the labels) from FiftyOne\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>Note</p> <p>Check out this page to see a variety of common annotation patterns using the CVAT backend to illustrate the full process.</p>"},{"location":"fiftyone_concepts/annotation/#setup","title":"Setup \u00b6","text":"<p>By default, all annotation is performed via app.cvat.ai, which simply requires that you create an account and then configure your username and password credentials.</p> <p>However, you can configure FiftyOne to use a self-hosted CVAT server, or you can even use a completely custom backend.</p> <p>Note</p> <p>See this page for CVAT-specific setup instructions.</p>"},{"location":"fiftyone_concepts/annotation/#changing-your-annotation-backend","title":"Changing your annotation backend \u00b6","text":"<p>You can use a specific backend for a particular annotation run by passing the <code>backend</code> parameter to <code>annotate()</code>:</p> <pre><code>view.annotate(..., backend=\"&lt;backend&gt;\", ...)\n</code></pre> <p>Alternatively, you can change your default annotation backend for an entire session by setting the <code>FIFTYONE_ANNOTATION_DEFAULT_BACKEND</code> environment variable.</p> <pre><code>export FIFTYONE_ANNOTATION_DEFAULT_BACKEND=&lt;backend&gt;\n</code></pre> <p>Finally, you can permanently change your default annotation backend by updating the <code>default_backend</code> key of your annotation config at <code>~/.fiftyone/annotation_config.json</code>:</p> <pre><code>{\n    \"default_backend\": \"&lt;backend&gt;\",\n    \"backends\": {\n        \"&lt;backend&gt;\": {...},\n        ...\n    }\n}\n</code></pre>"},{"location":"fiftyone_concepts/annotation/#configuring-your-backend","title":"Configuring your backend \u00b6","text":"<p>Annotation backends may be configured in a variety of backend-specific ways, which you can see by inspecting the parameters of a backend\u2019s associated <code>AnnotationBackendConfig</code> class.</p> <p>The relevant classes for the builtin annotation backends are:</p> <ul> <li> <p><code>\"cvat\"</code>: <code>fiftyone.utils.cvat.CVATBackendConfig</code></p> </li> <li> <p><code>\"labelstudio\"</code>: <code>fiftyone.utils.labelstudio.LabelStudioBackendConfig</code></p> </li> <li> <p><code>\"labelbox\"</code>: <code>fiftyone.utils.labelbox.LabelboxBackendConfig</code></p> </li> </ul> <p>You can configure an annotation backend\u2019s parameters for a specific run by simply passing supported config parameters as keyword arguments each time you call <code>annotate()</code>:</p> <pre><code>view.annotate(\n    ...\n    backend=\"cvat\",\n    url=\"http://localhost:8080\",\n    username=...,\n    password=...,\n)\n</code></pre> <p>Alternatively, you can more permanently configure your backend(s) via your annotation config.</p>"},{"location":"fiftyone_concepts/annotation/#annotation-config","title":"Annotation config \u00b6","text":"<p>FiftyOne provides an annotation config that you can use to either temporarily or permanently configure the behavior of the annotation API.</p>"},{"location":"fiftyone_concepts/annotation/#viewing-your-config","title":"Viewing your config \u00b6","text":"<p>You can print your current annotation config at any time via the Python library and the CLI:</p> <p>Note</p> <p>If you have customized your annotation config via any of the methods described below, printing your config is a convenient way to ensure that the changes you made have taken effect as you expected.</p>"},{"location":"fiftyone_concepts/annotation/#modifying-your-config","title":"Modifying your config \u00b6","text":"<p>You can modify your annotation config in a variety of ways. The following sections describe these options in detail.</p>"},{"location":"fiftyone_concepts/annotation/#order-of-precedence","title":"Order of precedence \u00b6","text":"<p>The following order of precedence is used to assign values to your annotation config settings as runtime:</p> <ol> <li> <p>Config settings applied at runtime by directly editing <code>fiftyone.annotation_config</code></p> </li> <li> <p><code>FIFTYONE_XXX</code> environment variables</p> </li> <li> <p>Settings in your JSON config ( <code>~/.fiftyone/annotation_config.json</code>)</p> </li> <li> <p>The default config values</p> </li> </ol>"},{"location":"fiftyone_concepts/annotation/#editing-your-json-config","title":"Editing your JSON config \u00b6","text":"<p>You can permanently customize your annotation config by creating a <code>~/.fiftyone/annotation_config.json</code> file on your machine. The JSON file may contain any desired subset of config fields that you wish to customize.</p> <p>For example, the following config JSON file customizes the URL of your CVAT server without changing any other default config settings:</p> <pre><code>{\n    \"backends\": {\n        \"cvat\": {\n            \"url\": \"http://localhost:8080\"\n        }\n    }\n}\n</code></pre> <p>When <code>fiftyone</code> is imported, any options from your JSON config are merged into the default config, as per the order of precedence described above.</p> <p>Note</p> <p>You can customize the location from which your JSON config is read by setting the <code>FIFTYONE_ANNOTATION_CONFIG_PATH</code> environment variable.</p>"},{"location":"fiftyone_concepts/annotation/#setting-environment-variables","title":"Setting environment variables \u00b6","text":"<p>Annotation config settings may be customized on a per-session basis by setting the <code>FIFTYONE_XXX</code> environment variable(s) for the desired config settings.</p> <p>The <code>FIFTYONE_ANNOTATION_DEFAULT_BACKEND</code> environment variable allows you to configure your default backend:</p> <pre><code>export FIFTYONE_ANNOTATION_DEFAULT_BACKEND=labelbox\n</code></pre> <p>You can declare parameters for specific annotation backends by setting environment variables of the form <code>FIFTYONE_&lt;BACKEND&gt;_&lt;PARAMETER&gt;</code>. Any settings that you declare in this way will be passed as keyword arguments to methods like <code>annotate()</code> whenever the corresponding backend is in use. For example, you can configure the URL, username, password, and email (if applicable) of your CVAT server as follows:</p> <pre><code>export FIFTYONE_CVAT_URL=http://localhost:8080\nexport FIFTYONE_CVAT_USERNAME=...\nexport FIFTYONE_CVAT_PASSWORD=...\nexport FIFTYONE_CVAT_EMAIL=...  # if applicable\n</code></pre> <p>The <code>FIFTYONE_ANNOTATION_BACKENDS</code> environment variable can be set to a <code>list,of,backends</code> that you want to expose in your session, which may exclude native backends and/or declare additional custom backends whose parameters are defined via additional config modifications of any kind:</p> <pre><code>export FIFTYONE_ANNOTATION_BACKENDS=custom,cvat,labelbox\n</code></pre> <p>When declaring new backends, you can include <code>*</code> to append new backend(s) without omitting or explicitly enumerating the builtin backends. For example, you can add a <code>custom</code> annotation backend as follows:</p> <pre><code>export FIFTYONE_ANNOTATION_BACKENDS=*,custom\nexport FIFTYONE_CUSTOM_CONFIG_CLS=your.custom.AnnotationConfig\n</code></pre>"},{"location":"fiftyone_concepts/annotation/#modifying-your-config-in-code","title":"Modifying your config in code \u00b6","text":"<p>You can dynamically modify your annotation config at runtime by directly editing the <code>fiftyone.annotation_config</code> object.</p> <p>Any changes to your annotation config applied via this manner will immediately take effect in all subsequent calls to <code>fiftyone.annotation_config</code> during your current session.</p> <pre><code>import fiftyone as fo\n\nfo.annotation_config.default_backend = \"&lt;backend&gt;\"\n</code></pre>"},{"location":"fiftyone_concepts/annotation/#requesting-annotations","title":"Requesting annotations \u00b6","text":"<p>Use the <code>annotate()</code> method to send the samples and optionally existing labels in a <code>Dataset</code> or <code>DatasetView</code> to your annotation backend for processing.</p> <p>The basic syntax is:</p> <pre><code>anno_key = \"...\"\nview.annotate(anno_key, ...)\n</code></pre> <p>The <code>anno_key</code> argument defines a unique identifier for the annotation run, and you will provide it to methods like <code>load_annotations()</code>, <code>get_annotation_info()</code>, <code>load_annotation_results()</code>, <code>rename_annotation_run()</code>, and <code>delete_annotation_run()</code> to manage the run in the future.</p> <p>Warning</p> <p>FiftyOne assumes that all labels in an annotation run can fit in memory.</p> <p>If you are annotating very large scale video datasets with dense frame labels, you may violate this assumption. Instead, consider breaking the work into multiple smaller annotation runs that each contain limited subsets of the samples you wish to annotate.</p> <p>You can use <code>Dataset.stats()</code> to get a sense for the total size of the labels in a dataset as a rule of thumb to estimate the size of a candidate annotation run.</p> <p>In addition, <code>annotate()</code> provides various parameters that you can use to customize the annotation tasks that you wish to be performed.</p> <p>The following parameters are supported by all annotation backends:</p> <ul> <li> <p>backend ( None): the annotation backend to use. The supported values are <code>fiftyone.annotation_config.backends.keys()</code> and the default is <code>fiftyone.annotation_config.default_backend</code></p> </li> <li> <p>media_field ( \u201cfilepath\u201d): the sample field containing the path to the source media to upload</p> </li> <li> <p>launch_editor ( False): whether to launch the annotation backend\u2019s editor after uploading the samples</p> </li> </ul> <p>The following parameters allow you to configure the labeling schema to use for your annotation tasks. See this section for more details:</p> <ul> <li> <p>label_schema ( None): a dictionary defining the label schema to use. If this argument is provided, it takes precedence over the remaining fields</p> </li> <li> <p>label_field ( None): a string indicating a new or existing label field to annotate</p> </li> <li> <p>label_type ( None): a string indicating the type of labels to annotate. The possible label types are:</p> </li> <li> <p><code>\"classification\"</code>: a single classification stored in     <code>Classification</code> fields</p> </li> <li> <p><code>\"classifications\"</code>: multilabel classifications stored in     <code>Classifications</code> fields</p> </li> <li> <p><code>\"detections\"</code>: object detections stored in <code>Detections</code> fields</p> </li> <li> <p><code>\"instances\"</code>: instance segmentations stored in <code>Detections</code> fields     with their <code>mask</code>     attributes populated</p> </li> <li> <p><code>\"polylines\"</code>: polylines stored in <code>Polylines</code> fields with their     <code>filled</code> attributes set to     <code>False</code></p> </li> <li> <p><code>\"polygons\"</code>: polygons stored in <code>Polylines</code> fields with their     <code>filled</code> attributes set to     <code>True</code></p> </li> <li> <p><code>\"keypoints\"</code>: keypoints stored in <code>Keypoints</code> fields</p> </li> <li> <p><code>\"segmentation\"</code>: semantic segmentations stored in <code>Segmentation</code>     fields</p> </li> <li> <p><code>\"scalar\"</code>: scalar labels stored in <code>IntField</code>, <code>FloatField</code>,     <code>StringField</code>, or <code>BooleanField</code> fields</p> </li> </ul> <p>All new label fields must have their type specified via this argument or in <code>label_schema</code></p> <ul> <li> <p>classes ( None): a list of strings indicating the class options for <code>label_field</code> or all fields in <code>label_schema</code> without classes specified. All new label fields must have a class list provided via one of the supported methods. For existing label fields, if classes are not provided by this argument nor <code>label_schema</code>, the observed labels on your dataset are used</p> </li> <li> <p>attributes ( True): specifies the label attributes of each label field to include (other than their <code>label</code>, which is always included) in the annotation export. Can be any of the following:</p> </li> <li> <p><code>True</code>: export all label attributes</p> </li> <li> <p><code>False</code>: don\u2019t export any custom label attributes</p> </li> <li> <p>a list of label attributes to export</p> </li> <li> <p>a dict mapping attribute names to dicts specifying the <code>type</code>,     <code>values</code>, and <code>default</code> for each attribute</p> </li> </ul> <p>If a <code>label_schema</code> is also provided, this parameter determines which attributes are included for all fields that do not explicitly define their per-field attributes (in addition to any per-class attributes)</p> <ul> <li> <p>mask_targets ( None): a dict mapping pixel values to semantic label strings. Only applicable when annotating semantic segmentations</p> </li> <li> <p>allow_additions ( True): whether to allow new labels to be added. Only applicable when editing existing label fields</p> </li> <li> <p>allow_deletions ( True): whether to allow labels to be deleted. Only applicable when editing existing label fields</p> </li> <li> <p>allow_label_edits ( True): whether to allow the <code>label</code> attribute of existing labels to be modified. Only applicable when editing existing fields with <code>label</code> attributes</p> </li> <li> <p>allow_index_edits ( True): whether to allow the <code>index</code> attribute of existing video tracks to be modified. Only applicable when editing existing frame fields with <code>index</code> attributes</p> </li> <li> <p>allow_spatial_edits ( True): whether to allow edits to the spatial properties (bounding boxes, vertices, keypoints, masks, etc) of labels. Only applicable when editing existing spatial label fields</p> </li> </ul> <p>In addition, each annotation backend can typically be configured in a variety of backend-specific ways. See this section for more details.</p> <p>Note</p> <p>Specific annotation backends may not support all <code>label_type</code> options.</p>"},{"location":"fiftyone_concepts/annotation/#label-schema","title":"Label schema \u00b6","text":"<p>The <code>label_schema</code>, <code>label_field</code>, <code>label_type</code>, <code>classes</code>, <code>attributes</code>, and <code>mask_targets</code> parameters to <code>annotate()</code> allow you to define the annotation schema that you wish to be used.</p> <p>The label schema may define new label field(s) that you wish to populate, and it may also include existing label field(s), in which case you can add, delete, or edit the existing labels on your FiftyOne dataset.</p> <p>The <code>label_schema</code> argument is the most flexible way to define how to construct tasks in CVAT. In its most verbose form, it is a dictionary that defines the label type, annotation type, possible classes, and possible attributes for each label field:</p> <pre><code>anno_key = \"...\"\n\nlabel_schema = {\n    \"new_field\": {\n        \"type\": \"classifications\",\n        \"classes\": [\"class1\", \"class2\"],\n        \"attributes\": {\n            \"attr1\": {\n                \"type\": \"select\",\n                \"values\": [\"val1\", \"val2\"],\n                \"default\": \"val1\",\n            },\n            \"attr2\": {\n                \"type\": \"radio\",\n                \"values\": [True, False],\n                \"default\": False,\n            }\n        },\n    },\n    \"existing_field\": {\n        \"classes\": [\"class3\", \"class4\"],\n        \"attributes\": {\n            \"attr3\": {\n                \"type\": \"text\",\n            }\n        }\n    },\n}\n\ndataset.annotate(anno_key, label_schema=label_schema)\n</code></pre> <p>You can also define class-specific attributes by setting elements of the <code>classes</code> list to dicts that specify groups of <code>classes</code> and their corresponding <code>attributes</code>. For example, in the configuration below, <code>attr1</code> only applies to <code>class1</code> and <code>class2</code> while <code>attr2</code> applies to all classes:</p> <pre><code>anno_key = \"...\"\n\nlabel_schema = {\n    \"new_field\": {\n        \"type\": \"detections\",\n        \"classes\": [\\\n            {\\\n                \"classes\": [\"class1\", \"class2\"],\\\n                \"attributes\": {\\\n                    \"attr1\": {\\\n                        \"type\": \"select\",\\\n                        \"values\": [\"val1\", \"val2\"],\\\n                        \"default\": \"val1\",\\\n                    }\\\n                 }\\\n            },\\\n            \"class3\",\\\n            \"class4\",\\\n        ],\n        \"attributes\": {\n            \"attr2\": {\n                \"type\": \"radio\",\n                \"values\": [True, False],\n                \"default\": False,\n            }\n        },\n    },\n}\n\ndataset.annotate(anno_key, label_schema=label_schema)\n</code></pre> <p>Alternatively, if you are only editing or creating a single label field, you can use the <code>label_field</code>, <code>label_type</code>, <code>classes</code>, <code>attributes</code>, and <code>mask_targets</code> parameters to specify the components of the label schema individually:</p> <pre><code>anno_key = \"...\"\n\nlabel_field = \"new_field\",\nlabel_type = \"classifications\"\nclasses = [\"class1\", \"class2\"]\n\n# These are optional\nattributes = {\n    \"attr1\": {\n        \"type\": \"select\",\n        \"values\": [\"val1\", \"val2\"],\n        \"default\": \"val1\",\n    },\n    \"attr2\": {\n        \"type\": \"radio\",\n        \"values\": [True, False],\n        \"default\": False,\n    }\n}\n\ndataset.annotate(\n    anno_key,\n    label_field=label_field,\n    label_type=label_type,\n    classes=classes,\n    attributes=attributes,\n)\n</code></pre> <p>When you are annotating existing label fields, you can omit some of these parameters from <code>annotate()</code>, as FiftyOne can infer the appropriate values to use:</p> <ul> <li> <p>label_type: if omitted, the <code>Label</code> type of the field will be used to infer the appropriate value for this parameter</p> </li> <li> <p>classes: if omitted for a non-semantic segmentation field, the observed labels on your dataset will be used to construct a classes list</p> </li> </ul>"},{"location":"fiftyone_concepts/annotation/#label-attributes","title":"Label attributes \u00b6","text":"<p>The <code>attributes</code> parameter allows you to configure whether custom attributes beyond the default <code>label</code> attribute are included in the annotation tasks.</p> <p>When adding new label fields for which you want to include attributes, you must use the dictionary syntax demonstrated below to define the schema of each attribute that you wish to label:</p> <pre><code>anno_key = \"...\"\n\nattributes = {\n    \"occluded\": {\n        \"type\": \"radio\",\n        \"values\": [True, False],\n        \"default\": False,\n    },\n    \"gender\": {\n        \"type\": \"select\",\n        \"values\": [\"male\", \"female\"],\n    },\n    \"caption\": {\n        \"type\": \"text\",\n    }\n}\n\nview.annotate(\n    anno_key,\n    label_field=\"new_field\",\n    label_type=\"detections\",\n    classes=[\"dog\", \"cat\", \"person\"],\n    attributes=attributes,\n)\n</code></pre> <p>You can always omit this parameter if you do not require attributes beyond the default <code>label</code>.</p> <p>Each annotation backend may support different <code>type</code> values, as declared by the <code>supported_attr_types()</code> method of its <code>AnnotationBackend</code> class. For example, CVAT supports the following choices for <code>type</code>:</p> <ul> <li> <p><code>text</code>: a free-form text box. In this case, <code>default</code> is optional and <code>values</code> is unused</p> </li> <li> <p><code>select</code>: a selection dropdown. In this case, <code>values</code> is required and <code>default</code> is optional</p> </li> <li> <p><code>radio</code>: a radio button list UI. In this case, <code>values</code> is required and <code>default</code> is optional</p> </li> <li> <p><code>checkbox</code>: a boolean checkbox UI. In this case, <code>default</code> is optional and <code>values</code> is unused</p> </li> </ul> <p>When you are annotating existing label fields, the <code>attributes</code> parameter can take additional values:</p> <ul> <li> <p><code>True</code> (default): export all custom attributes observed on the existing labels, using their observed values to determine the appropriate UI type and possible values, if applicable</p> </li> <li> <p><code>False</code>: do not include any custom attributes in the export</p> </li> <li> <p>a list of custom attributes to include in the export</p> </li> <li> <p>a full dictionary syntax described above</p> </li> </ul> <p>Note that only scalar-valued label attributes are supported. Other attribute types like lists, dictionaries, and arrays will be omitted.</p>"},{"location":"fiftyone_concepts/annotation/#restricting-additions-deletions-and-edits","title":"Restricting additions, deletions, and edits \u00b6","text":"<p>When you create annotation runs that involve editing existing label fields, you can optionally specify that certain changes are not allowed by passing the following flags to <code>annotate()</code>:</p> <ul> <li> <p>allow_additions ( True): whether to allow new labels to be added</p> </li> <li> <p>allow_deletions ( True): whether to allow labels to be deleted</p> </li> <li> <p>allow_label_edits ( True): whether to allow the <code>label</code> attribute to be modified</p> </li> <li> <p>allow_index_edits ( True): whether to allow the <code>index</code> attribute of video tracks to be modified</p> </li> <li> <p>allow_spatial_edits ( True): whether to allow edits to the spatial properties (bounding boxes, vertices, keypoints, etc) of labels</p> </li> </ul> <p>If you are using the <code>label_schema</code> parameter to provide a full annotation schema to <code>annotate()</code>, you can also directly include the above flags in the configuration dicts for any existing label field(s) you wish.</p> <p>For example, suppose you have an existing <code>ground_truth</code> field that contains objects of various types and you would like to add new <code>sex</code> and <code>age</code> attributes to all people in this field while also strictly enforcing that no objects can be added, deleted, or have their labels or bounding boxes modified. You can configure an annotation run for this as follows:</p> <pre><code>anno_key = \"...\"\n\nattributes = {\n    \"sex\": {\n        \"type\": \"select\",\n        \"values\": [\"male\", \"female\"],\n    },\n    \"age\": {\n        \"type\": \"text\",\n    },\n}\n\nview.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    classes=[\"person\"],\n    attributes=attributes,\n    allow_additions=False,\n    allow_deletions=False,\n    allow_label_edits=False,\n    allow_spatial_edits=False,\n)\n</code></pre> <p>You can also include a <code>read_only=True</code> parameter when uploading existing label attributes to specify that the attribute\u2019s value should be uploaded to the annotation backend for informational purposes, but any edits to the attribute\u2019s value should not be imported back into FiftyOne.</p> <p>For example, if you have vehicles with their <code>make</code> attribute populated and you want to populate a new <code>model</code> attribute based on this information without allowing changes to the vehicle\u2019s <code>make</code>, you can configure an annotation run for this as follows:</p> <pre><code>anno_key = \"...\"\n\nattributes = {\n    \"make\": {\n        \"type\": \"text\",\n        \"read_only\": True,\n    },\n    \"model\": {\n        \"type\": \"text\",\n    },\n}\n\nview.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    classes=[\"vehicle\"],\n    attributes=attributes,\n)\n</code></pre> <p>Note</p> <p>Some annotation backends may not support restrictions to additions, deletions, spatial edits, and read-only attributes in their editing interface.</p> <p>However, any restrictions that you specify via the above parameters will still be enforced when you call <code>load_annotations()</code> to merge the annotations back into FiftyOne.</p>"},{"location":"fiftyone_concepts/annotation/#labeling-videos","title":"Labeling videos \u00b6","text":"<p>When annotating spatiotemporal objects in videos, you have a few additional options at your fingertips.</p> <p>First, each object attribute specification can include a <code>mutable</code> property that controls whether the attribute\u2019s value can change between frames for each object:</p> <pre><code>anno_key = \"...\"\n\nattributes = {\n    \"type\": {\n        \"type\": \"select\",\n        \"values\": [\"sedan\", \"suv\", \"truck\"],\n        \"mutable\": False,\n    },\n    \"occluded\": {\n        \"type\": \"radio\",\n        \"values\": [True, False],\n        \"default\": False,\n        \"mutable\": True,\n    },\n}\n\nview.annotate(\n    anno_key,\n    label_field=\"frames.new_field\",\n    label_type=\"detections\",\n    classes=[\"vehicle\"],\n    attributes=attributes,\n)\n</code></pre> <p>The meaning of the <code>mutable</code> attribute is defined as follows:</p> <ul> <li> <p><code>True</code> (default): the attribute is dynamic and can have a different value for every frame in which the object track appears</p> </li> <li> <p><code>False</code>: the attribute is static and is the same for every frame in which the object track appears</p> </li> </ul> <p>In addition, if you are using an annotation backend like CVAT that supports keyframes, then when you download annotation runs that include track annotations, the downloaded label corresponding to each keyframe of an object track will have its <code>keyframe=True</code> attribute set to denote that it was a keyframe.</p> <p>Similarly, when you create an annotation run on a video dataset that involves editing existing video tracks, if at least one existing label has a <code>keyframe=True</code> attribute set, then the available keyframe information will be uploaded to the annotation backend.</p>"},{"location":"fiftyone_concepts/annotation/#loading-annotations","title":"Loading annotations \u00b6","text":"<p>After your annotations tasks in the annotation backend are complete, you can use the <code>load_annotations()</code> method to download them and merge them back into your FiftyOne dataset.</p> <pre><code>view.load_annotations(anno_key)\n</code></pre> <p>The <code>anno_key</code> parameter is the unique identifier for the annotation run that you provided when calling <code>annotate()</code>. You can use <code>list_annotation_runs()</code> to see the available keys on a dataset.</p> <p>Note</p> <p>By default, calling <code>load_annotations()</code> will not delete any information for the run from the annotation backend.</p> <p>However, you can pass <code>cleanup=True</code> to delete all information associated with the run from the backend after the annotations are downloaded.</p> <p>You can use the optional <code>dest_field</code> parameter to override the task\u2019s label schema and instead load annotations into different field name(s) of your dataset. This can be useful, for example, when editing existing annotations, if you would like to do a before/after comparison of the edits that you import. If the annotation run involves multiple fields, <code>dest_field</code> should be a dictionary mapping label schema field names to destination field names.</p> <p>Some annotation backends like CVAT cannot explicitly prevent annotators from creating labels that don\u2019t obey the run\u2019s label schema. You can pass the optional <code>unexpected</code> parameter to <code>load_annotations()</code> to configure how to deal with any such unexpected labels that are found. The supported values are:</p> <ul> <li> <p><code>\"prompt\"</code> ( default): present an interactive prompt to direct/discard unexpected labels</p> </li> <li> <p><code>\"keep\"</code>: automatically keep all unexpected labels in a field whose name matches the the label type</p> </li> <li> <p><code>\"ignore\"</code>: automatically ignore any unexpected labels</p> </li> <li> <p><code>\"return\"</code>: return a dict containing all unexpected labels, if any</p> </li> </ul>"},{"location":"fiftyone_concepts/annotation/#managing-annotation-runs","title":"Managing annotation runs \u00b6","text":"<p>FiftyOne provides a variety of methods that you can use to manage in-progress or completed annotation runs.</p> <p>For example, you can call <code>list_annotation_runs()</code> to see the available annotation keys on a dataset:</p> <pre><code>dataset.list_annotation_runs()\n</code></pre> <p>Or, you can use <code>get_annotation_info()</code> to retrieve information about the configuration of an annotation run:</p> <pre><code>info = dataset.get_annotation_info(anno_key)\nprint(info)\n</code></pre> <p>Use <code>load_annotation_results()</code> to load the <code>AnnotationResults</code> instance for an annotation run.</p> <p>All results objects provide a <code>cleanup()</code> method that you can use to delete all information associated with a run from the annotation backend.</p> <pre><code>results = dataset.load_annotation_results(anno_key)\nresults.cleanup()\n</code></pre> <p>In addition, the <code>AnnotationResults</code> subclasses for each backend may provide additional utilities such as support for programmatically monitoring the status of the annotation tasks in the run.</p> <p>You can use <code>rename_annotation_run()</code> to rename the annotation key associated with an existing annotation run:</p> <pre><code>dataset.rename_annotation_run(anno_key, new_anno_key)\n</code></pre> <p>Finally, you can use <code>delete_annotation_run()</code> to delete the record of an annotation run from your FiftyOne dataset:</p> <pre><code>dataset.delete_annotation_run(anno_key)\n</code></pre> <p>Note</p> <p>Calling <code>delete_annotation_run()</code> only deletes the record of the annotation run from your FiftyOne dataset; it will not delete any annotations loaded onto your dataset via <code>load_annotations()</code>, nor will it delete any associated information from the annotation backend.</p>"},{"location":"fiftyone_concepts/annotation/#custom-annotation-backends","title":"Custom annotation backends \u00b6","text":"<p>If you would like to use an annotation tool that is not natively supported by FiftyOne, you can follow the instructions below to implement an interface for your tool and then configure your environment so that the <code>annotate()</code> and <code>load_annotations()</code> methods will use your custom backend.</p> <p>Annotation backends are defined by writing subclasses of the following three classes with the appropriate abstract methods implemented:</p> <ul> <li> <p><code>AnnotationBackend</code>: this class implements the logic required for your annotation backend to declare the types of labeling tasks that it supports, as well as the core <code>upload_annotations()</code> and <code>download_annotations()</code> methods, which handle uploading and downloading data and labels to your annotation tool</p> </li> <li> <p><code>AnnotationBackendConfig</code>: this class defines the available parameters that users can pass as keyword arguments to <code>annotate()</code> to customize the behavior of the annotation run</p> </li> <li> <p><code>AnnotationResults</code>: this class stores any intermediate information necessary to track the progress of an annotation run that has been created and is now waiting for its results to be merged back into the FiftyOne dataset</p> </li> </ul> <p>Note</p> <p>Refer to the fiftyone.utils.cvat module for an example of how the above subclasses are implemented for the CVAT backend.</p> <p>The recommended way to expose a custom backend is to add it to your annotation config at <code>~/.fiftyone/annotation_config.json</code> as follows:</p> <pre><code>{\n    \"default_backend\": \"&lt;backend&gt;\",\n    \"backends\": {\n        \"&lt;backend&gt;\": {\n            \"config_cls\": \"your.custom.AnnotationConfig\",\n            # custom parameters here\n        }\n    }\n}\n</code></pre> <p>In the above, <code>&lt;backend&gt;</code> defines the name of your custom backend, which you can henceforward pass as the <code>backend</code> parameter to <code>annotate()</code>, and the <code>config_cls</code> parameter specifies the fully-qualified name of the <code>AnnotationBackendConfig</code> subclass for your annotation backend.</p> <p>With the <code>default_backend</code> parameter set to your custom backend as shown above, calling <code>annotate()</code> will automatically use your backend.</p> <p>Alternatively, you can manually opt to use your custom backend on a per-run basis by passing the <code>backend</code> parameter:</p> <pre><code>view.annotate(..., backend=\"&lt;backend&gt;\", ...)\n</code></pre>"},{"location":"fiftyone_concepts/app/","title":"Using the FiftyOne App \u00b6","text":"<p>The FiftyOne App is a powerful graphical user interface that enables you to visualize, browse, and interact directly with your datasets.</p> <p></p> <p>Note</p> <p>Did you know? You can use FiftyOne\u2019s plugin framework to customize and extend the behavior of the App!</p>"},{"location":"fiftyone_concepts/app/#app-environments","title":"App environments \u00b6","text":"<p>The FiftyOne App can be used in any environment that you\u2019re working in, from a local IPython shell, to a remote machine or cloud instance, to a Jupyter or Colab notebook. Check out the environments guide for best practices when working in each environment.</p>"},{"location":"fiftyone_concepts/app/#sessions","title":"Sessions \u00b6","text":"<p>The basic FiftyOne workflow is to open a Python shell and load a <code>Dataset</code>. From there you can launch the FiftyOne App and interact with it programmatically via a session.</p>"},{"location":"fiftyone_concepts/app/#creating-a-session","title":"Creating a session \u00b6","text":"<p>You can launch an instance of the App by calling <code>launch_app()</code>. This method returns a <code>Session</code> instance, which you can subsequently use to interact programmatically with the App!</p> <pre><code>import fiftyone as fo\n\nsession = fo.launch_app()\n</code></pre> <p></p> <p>App sessions are highly flexible. For example, you can launch launch multiple App instances and connect multiple App instances to the same dataset.</p> <p>By default, when you\u2019re working in a non-notebook context, the App will be opened in a new tab of your web browser. See this FAQ for supported browsers.</p> <p>Note</p> <p><code>fo.launch_app()</code> will launch the App asynchronously and return control to your Python process. The App will then remain connected until the process exits.</p> <p>Therefore, if you are using the App in a script, you should use <code>session.wait()</code> to block execution until you close it manually:</p> <pre><code># Launch the App\nsession = fo.launch_app(...)\n\n# (Perform any additional operations here)\n\n# Blocks execution until the App is closed\nsession.wait()\n\n# Or block execution indefinitely with a negative wait value\n# session.wait(-1)\n</code></pre> <p>Note</p> <p>When working inside a Docker container, FiftyOne should automatically detect and appropriately configure networking. However, if you are unable to load the App in your browser, you many need to manually set the App address to <code>0.0.0.0</code>:</p> <pre><code>session = fo.launch_app(..., address=\"0.0.0.0\")\n</code></pre> <p>See this page for more information about working with FiftyOne inside Docker.</p> <p>Note</p> <p>If you are a Windows user launching the App from a script, you should use the pattern below to avoid multiprocessing issues, since the App is served via a separate process:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.load_dataset(...)\n\nif __name__ == \"__main__\":\n    # Ensures that the App processes are safely launched on Windows\n    session = fo.launch_app(dataset)\n    session.wait()\n</code></pre>"},{"location":"fiftyone_concepts/app/#updating-a-sessions-dataset","title":"Updating a session\u2019s dataset \u00b6","text":"<p>Sessions can be updated to show a new <code>Dataset</code> by updating the <code>Session.dataset</code> property of the session object:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"cifar10\")\n\n# View the dataset in the App\nsession.dataset = dataset\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/app/#updating-a-sessions-view","title":"Updating a session\u2019s view \u00b6","text":"<p>You can also show a specific view into the current dataset in the App by setting the <code>Session.view</code> property of the session.</p> <p>For example, the command below loads a <code>DatasetView</code> in the App that shows the first 10 samples in the dataset sorted by their <code>uniqueness</code> field:</p> <pre><code>session.view = dataset.sort_by(\"uniqueness\").limit(10)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/app/#loading-a-sample-or-group","title":"Loading a sample or group \u00b6","text":"<p>You can immediately load a specific sample in the modal when launching a new <code>Session</code> by providing its ID via the <code>sample_id</code> parameter:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nsample_id = dataset.last().id\n\nsession = fo.launch_app(dataset, sample_id=sample_id)\n</code></pre> <p>You can also programmatically load a sample in the modal on an existing session by setting its <code>session.sample_id</code> property:</p> <pre><code>sample_id = dataset.take(1).first().id\n\nsession.sample_id = sample_id\n</code></pre> <p>Note</p> <p>Did you know? You can link directly to a sample by copy + pasting the App\u2019s URL into your browser search bar!</p> <p>Similarly, for group datasets, you can immediately load a specific group in the modal when launching a new <code>Session</code> by providing its ID via the <code>group_id</code> parameter:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-groups\")\ngroup_id = dataset.last().group.id\n\nsession = fo.launch_app(dataset, group_id=group_id)\n</code></pre> <p>You can also programmatically load a group in the modal on an existing session by setting its <code>session.group_id</code> property:</p> <pre><code>group_id = dataset.take(1).first().group.id\n\nsession.group_id = group_id\n</code></pre> <p>Note</p> <p>Did you know? You can link directly to a group by copy + pasting the App\u2019s URL into your browser search bar!</p>"},{"location":"fiftyone_concepts/app/#remote-sessions","title":"Remote sessions \u00b6","text":"<p>If your data is stored on a remote machine, you can forward a session from the remote machine to your local machine and seamlessly browse your remote dataset from you web browser.</p> <p>Check out the environments page for more information on possible configurations of local/remote/cloud data and App access.</p>"},{"location":"fiftyone_concepts/app/#remote-machine","title":"Remote machine \u00b6","text":"<p>On the remote machine, you can load a <code>Dataset</code> and launch a remote session using either the Python library or the CLI.</p>"},{"location":"fiftyone_concepts/app/#local-machine","title":"Local machine \u00b6","text":"<p>On the local machine, you can access an App instance connected to the remote session by either manually configuring port forwarding or via the FiftyOne CLI:</p> <p>Note</p> <p>Remote sessions are highly flexible. For example, you can connect to multiple remote sessions and run multiple remote sessions from one machine.</p>"},{"location":"fiftyone_concepts/app/#using-the-sidebar","title":"Using the sidebar \u00b6","text":"<p>Any labels, tags, and scalar fields can be overlaid on the samples in the App by toggling the corresponding display options in the App\u2019s sidebar:</p> <p></p> <p>If you have stored metadata on your fields, then you can view this information in the App by hovering over field or attribute names in the App\u2019s sidebar:</p> <p></p>"},{"location":"fiftyone_concepts/app/#filtering-sample-fields","title":"Filtering sample fields \u00b6","text":"<p>The App provides UI elements in both grid view and expanded sample view that you can use to filter your dataset. To view the available filter options for a field, click the caret icon to the right of the field\u2019s name.</p> <p>Whenever you modify a filter element, the App will automatically update to show only those samples and/or labels that match the filter.</p> <p>Note</p> <p>Did you know? When you declare custom attributes on your dataset\u2019s schema, they will automatically become filterable in the App!</p> <p>Note</p> <p>Did you know? When you have applied filter(s) in the App, a bookmark icon appears in the top-left corner of the sample grid. Click this button to convert your filters to an equivalent set of stage(s) in the view bar!</p> <p></p>"},{"location":"fiftyone_concepts/app/#optimizing-query-performance","title":"Optimizing Query Performance \u00b6","text":"<p>The App\u2019s sidebar is optimized to leverage database indexes whenever possible.</p> <p>Fields that are indexed are indicated by lightning bolt icons next to their field/attribute names:</p> <p></p> <p>The above GIF shows query performance in action on the train split of the BDD100K dataset with an index on the <code>detections.detections.label</code> field:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n# The path to the source files that you manually downloaded\nsource_dir = \"/path/to/dir-with-bdd100k-files\"\n\ndataset = foz.load_zoo_dataset(\n    \"bdd100k\",\n    split=\"train\",\n    source_dir=source_dir,\n)\n\ndataset.create_index(\"detections.detections.label\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>When filtering by multiple fields, queries will be more efficient when your first filter is on an indexed field.</p> <p>The SDK provides a number of useful utilities for managing indexes on your datasets:</p> <ul> <li> <p><code>list_indexes()</code> - list all existing indexes</p> </li> <li> <p><code>create_index()</code> - create a new index</p> </li> <li> <p><code>drop_index()</code> - drop an existing index</p> </li> <li> <p><code>get_index_information()</code> - get information about the existing indexes</p> </li> </ul> <p>Note</p> <p>Did you know? With FiftyOne Teams you can manage indexes natively in the App via the Query Performance panel.</p> <p>In general, we recommend indexing only the specific fields that you wish to perform initial filters on:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\n# Index specific top-level fields\ndataset.create_index(\"camera_id\")\ndataset.create_index(\"recorded_at\")\ndataset.create_index(\"annotated_at\")\ndataset.create_index(\"annotated_by\")\n\n# Index specific embedded document fields\ndataset.create_index(\"ground_truth.detections.label\")\ndataset.create_index(\"ground_truth.detections.confidence\")\n\n# Note: it is faster to declare indexes before adding samples\ndataset.add_samples(...)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>Filtering by frame fields of video datasets is not directly optimizable by creating indexes. Instead, use summary fields to efficiently query frame-level information on large video datasets.</p> <p>Frame filtering in the App\u2019s grid view can be disabled by setting <code>disable_frame_filtering=True</code> in your App config.</p> <p>For grouped datasets, you should create two indexes for each field you wish to filter by: the field itself and a compound index that includes the group slice name:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-groups\")\n\n# Index a specific field\ndataset.create_index(\"ground_truth.detections.label\")\ndataset.create_index([(\"group.name\", 1), (\"ground_truth.detections.label\", 1)])\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>For datasets with a small number of fields, you can index all fields by adding a single global wildcard index:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\ndataset.create_index(\"$**\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Warning</p> <p>For large datasets with many fields, global wildcard indexes may require a substantial amount of RAM and query performance may be degraded compared to selectively indexing a smaller number of fields.</p> <p>You can also wildcard index all attributes of a specific embedded document field:</p> <pre><code># Wildcard index for all attributes of ground truth detections\ndataset.create_index(\"ground_truth.detections.$**\")\n</code></pre> <p>Note</p> <p>Numeric field filters are not supported by wildcard indexes.</p>"},{"location":"fiftyone_concepts/app/#disabling-query-performance","title":"Disabling Query Performance \u00b6","text":"<p>Query Performance is enabled by default for all datasets. This is generally the recommended setting for all large datasets to ensure that queries are performant.</p> <p>However, in certain circumstances you may prefer to disable Query Performance, which enables the App\u2019s sidebar to show additional information such as label/value counts that are useful but more expensive to compute.</p> <p>You can disable Query Performance for a particular dataset for its lifetime (in your current browser) via the gear icon in the Samples panel\u2019s actions row:</p> <p></p> <p>You can also disable Query Performance by default for all datasets by setting <code>default_query_performance=False</code> in your App config.</p>"},{"location":"fiftyone_concepts/app/#sidebar-groups","title":"Sidebar groups \u00b6","text":"<p>You can customize the layout of the App\u2019s sidebar by creating/renaming/deleting groups and dragging fields between groups directly in the App:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nsession = fo.launch_app(dataset)\n</code></pre> <p></p> <p>Note</p> <p>Any changes you make to a dataset\u2019s sidebar groups in the App are saved on the dataset and will persist between sessions.</p> <p>You can also programmatically modify a dataset\u2019s sidebar groups by editing the <code>sidebar_groups</code> property of the dataset\u2019s App config:</p> <pre><code># Get the default sidebar groups for the dataset\nsidebar_groups = fo.DatasetAppConfig.default_sidebar_groups(dataset)\n\n# Collapse the `metadata` section by default\nprint(sidebar_groups[2].name)  # metadata\nsidebar_groups[2].expanded = False\n\n# Add a new group\nsidebar_groups.append(fo.SidebarGroupDocument(name=\"new\"))\n\n# Modify the dataset's App config\ndataset.app_config.sidebar_groups = sidebar_groups\ndataset.save()  # must save after edits\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>You can conveniently reset the sidebar groups to their default state by setting <code>sidebar_groups</code> to <code>None</code>:</p> <pre><code># Reset sidebar groups\ndataset.app_config.sidebar_groups = None\ndataset.save()  # must save after edits\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>If a dataset has fields that do not appear in the dataset\u2019s <code>sidebar_groups</code> property, these fields will be dynamically assigned to default groups in the App at runtime.</p>"},{"location":"fiftyone_concepts/app/#using-the-view-bar","title":"Using the view bar \u00b6","text":"<p>The view bar makes all of the powerful searching, sorting, and filtering operations provided by dataset views available directly in the App.</p> <p>Note</p> <p>Any changes to the current view that you make in the view bar are automatically reflected in the <code>DatasetView</code> exposed by the <code>Session.view</code> property of the App\u2019s session object.</p> <p></p>"},{"location":"fiftyone_concepts/app/#grouping-samples","title":"Grouping samples \u00b6","text":"<p>You can use the group action in the App\u2019s menu to dynamically group your samples by a field of your choice:</p> <p></p> <p>In this mode, the App\u2019s grid shows the first sample from each group, and you can click on a sample to view all elements of the group in the modal.</p> <p>You may navigate through the elements of the group either sequentially using the carousel, or randomly using the pagination UI at the bottom of the modal.</p> <p></p> <p>When viewing ordered groups, you have an additional option to render the elements of the group as a video.</p> <p></p>"},{"location":"fiftyone_concepts/app/#field-visibility","title":"Field visibility \u00b6","text":"<p>You can configure which fields of your dataset appear in the App\u2019s sidebar by clicking the settings icon in the upper right of the sidebar to open the Field visibility modal.</p> <p>Consider the following example:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom datetime import datetime\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\ndataset.add_dynamic_sample_fields()\n\nfield = dataset.get_field(\"ground_truth\")\nfield.description = \"Ground truth annotations\"\nfield.info = {\"creator\": \"alice\", \"created_at\": datetime.utcnow()}\nfield.save()\n\nfield = dataset.get_field(\"predictions\")\nfield.description = \"YOLOv8 predictions\"\nfield.info = {\"owner\": \"bob\", \"created_at\": datetime.utcnow()}\nfield.save()\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"fiftyone_concepts/app/#manual-selection","title":"Manual selection \u00b6","text":"<p>You can use the <code>Selection</code> tab to manually select which fields to display. By default, only top-level fields are available for selection, but if you want fine-grained control you can opt to include nested fields (eg dynamic attributes of your label fields) in the selection list as well.</p> <p>Note</p> <p>You cannot exclude default fields/attributes from your dataset\u2019s schema, so these rows are always disabled in the Field visibility UI.</p> <p>Click <code>Apply</code> to reload the App with only the specified fields in the sidebar. When you do so, a filter icon will appear to the left of the settings icon in the sidebar indicating how many fields are currently excluded. You can reset your selection by clicking this icon or reopening the modal and pressing the <code>Reset</code> button at the bottom.</p> <p></p> <p>Note</p> <p>If your dataset has many fields and you frequently work with different subsets of them, you can persist/reload field selections by saving views.</p>"},{"location":"fiftyone_concepts/app/#filter-rules","title":"Filter rules \u00b6","text":"<p>Alternatively, you can use the <code>Filter rule</code> tab to define a rule that is dynamically applied to the dataset\u2019s field metadata each time the App loads to determine which fields to include in the sidebar.</p> <p>Note</p> <p>Filter rules are dynamic. If you save a view that contains a filter rule, the matching fields may increase or decrease over time as you modify the dataset\u2019s schema.</p> <p>Filter rules provide a simple syntax with different options for matching fields:</p> <p></p> <p>Note</p> <p>All filter rules are implemented as substring matches against the stringified contents of the relevant field metadata.</p>"},{"location":"fiftyone_concepts/app/#color-schemes","title":"Color schemes \u00b6","text":"<p>You can configure the color scheme used by the App to render content by clicking on the color palette icon above the sample grid.</p> <p>Consider the following example:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\ndataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\"\n)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"fiftyone_concepts/app/#color-schemes-in-the-app","title":"Color schemes in the App \u00b6","text":"<p>The GIF below demonstrates how to:</p> <ul> <li> <p>Configure a custom color pool from which to draw colors for otherwise unspecified fields/values</p> </li> <li> <p>Configure the colors assigned to specific fields in color by <code>field</code> mode</p> </li> <li> <p>Configure the colors used to render specific annotations based on their attributes in color by <code>value</code> mode</p> </li> <li> <p>Save the customized color scheme as the default for the dataset</p> </li> </ul> <p></p> <p>Note</p> <p>Any customizations you make only apply to the current dataset. Each time you load a new dataset, the color scheme will revert to that dataset\u2019s default color scheme (if any) or else the global default color scheme.</p> <p>To persist a color scheme, you can press <code>Save as default</code> to save the color scheme as the dataset\u2019s default scheme, copy it via the modal\u2019s JSON viewer, or access it programmatically via <code>session.color_scheme</code> as described below.</p> <p>The following table describes the available color scheme customization options in detail:</p> Tab Element Description Global settings Color annotations by Whether to color the annotations in the grid/modal based onthe <code>field</code> that they are in, the <code>value</code> that eachannotation takes, or per <code>instance</code> of the annotation Global settings Color pool A pool of colors from which colors are randomly assignedfor otherwise unspecified fields/values Global settings Label Opacity Color opacity of annotations Global settings Multicolor keypoints Whether to independently coloy keypoint points by their index Global settings Show keypoints skeletons Whether to show keypoint skeletons, if available Global settings Default mask targets colors If the MaskTargetsField is defined with integer keys, thedataset can assign a default color based on the integer keys Global settings Default colorscale The default colorscale to use when rendering heatmaps JSON editor A JSON representation of the current color scheme that youcan directly edit or copy + paste All <code>Reset</code> button Reset the current color scheme to the dataset\u2019s default(if any) or else the global default scheme All <code>Save as default</code> button Save the current color scheme as the default for thecurrent dataset. Note that this scheme can be viewed and/ormodified in Python All <code>Clear default</code> button Deletes the current dataset\u2019s default color scheme <code>FIELD</code> Use custom colors for <code>FIELD</code> Allows you to specify a custom color to use wheneverrendering any content from that field in the grid/modalwhen the App is in color by <code>field</code> mode <code>FIELD</code> Use custom colors forspecific field values Allows you to specify custom colors to use to renderannotations in this field based on the individual valuesthat it takes. In the case of embedded document fields,youmust also specify an attribute of each object. For example,color all<code>Classification</code>instances whose <code>label</code> is <code>\"car\"</code> in <code>#FF0000</code>;<code>Segmentation</code>instances whose <code>mask target integer</code> is <code>12</code> in <code>#FF0000</code>;<code>Heatmap</code>instances using <code>hsv</code> colorscale."},{"location":"fiftyone_concepts/app/#color-schemes-in-python","title":"Color schemes in Python \u00b6","text":"<p>You can also programmatically configure a session\u2019s color scheme by creating <code>ColorScheme</code> instances in Python:</p> <pre><code># Create a custom color scheme\nfo.ColorScheme(\n    color_pool=[\"#ff0000\", \"#00ff00\", \"#0000ff\", \"pink\", \"yellowgreen\"],\n    fields=[\\\n        {\\\n            \"path\": \"ground_truth\",\\\n            \"colorByAttribute\": \"eval\",\\\n            \"valueColors\": [\\\n                 # false negatives: blue\\\n                {\"value\": \"fn\", \"color\": \"#0000ff\"},\\\n                # true positives: green\\\n                {\"value\": \"tp\", \"color\": \"#00ff00\"},\\\n            ]\\\n        },\\\n        {\\\n            \"path\": \"predictions\",\\\n            \"colorByAttribute\": \"eval\",\\\n            \"valueColors\": [\\\n                # false positives: red\\\n                {\"value\": \"fp\", \"color\": \"#ff0000\"},\\\n                 # true positives: green\\\n                {\"value\": \"tp\", \"color\": \"#00ff00\"},\\\n            ]\\\n        },\\\n        {\\\n            \"path\": \"segmentations\",\\\n            \"maskTargetsColors\": [\\\n                 # 12: red\\\n                {\"intTarget\": 12, \"color\": \"#ff0000\"},\\\n                 # 15: green\\\n                {\"intTarget\": 15, \"color\": \"#00ff00\"},\\\n            ]\\\n        }\\\n    ],\n    color_by=\"value\",\n    opacity=0.5,\n    default_colorscale= {\"name\": \"rdbu\", \"list\": None},\n    colorscales=[\\\n        {\\\n             # field definition overrides the default_colorscale\\\n            \"path\": \"heatmap_2\",\\\n             # if name is defined, it will override the list\\\n            \"name\": None,\\\n            \"list\": [\\\n                {\"value\": 0.0, \"color\": \"rgb(0,255,255)\"},\\\n                {\"value\": 0.5, \"color\": \"rgb(255,0,0)\"},\\\n                {\"value\": 1.0, \"color\": \"rgb(0,0,255)\"},\\\n            ],\\\n        }\\\n    ],\n)\n</code></pre> <p>Note</p> <p>Refer to the <code>ColorScheme</code> class for documentation of the available customization options.</p> <p>You can launch the App with a custom color scheme by passing the optional <code>color_scheme</code> parameter to <code>launch_app()</code>:</p> <pre><code># Launch App with a custom color scheme\nsession = fo.launch_app(dataset, color_scheme=color_scheme)\n</code></pre> <p>Once the App is launched, you can retrieve your current color scheme at any time via the <code>session.color_scheme</code> property:</p> <pre><code>print(session.color_scheme)\n</code></pre> <p>You can also dynamically edit your current color scheme by modifying it:</p> <pre><code># Change the session's current color scheme\nsession.color_scheme = fo.ColorScheme(...)\n\n# Edit the existing color scheme in-place\nsession.color_scheme.color_pool = [...]\nsession.refresh()\n</code></pre> <p>Note</p> <p>Did you know? You can also configure default color schemes for individual datasets via Python!</p>"},{"location":"fiftyone_concepts/app/#saving-views","title":"Saving views \u00b6","text":"<p>You can use the menu in the upper-left of the App to record the current state of the App\u2019s view bar and filters sidebar as a saved view into your dataset:</p> <p></p> <p>Saved views are persisted on your dataset under a name of your choice so that you can quickly load them in a future session via this UI.</p> <p>Saved views are a convenient way to record semantically relevant subsets of a dataset, such as:</p> <ul> <li> <p>Samples in a particular state, eg with certain tag(s)</p> </li> <li> <p>A subset of a dataset that was used for a task, eg training a model</p> </li> <li> <p>Samples that contain content of interest, eg object types or image characteristics</p> </li> </ul> <p>Note</p> <p>Saved views only store the rule(s) used to extract content from the underlying dataset, not the actual content itself. Saving views is cheap. Don\u2019t worry about storage space!</p> <p>Keep in mind, though, that the contents of a saved view may change as the underlying dataset is modified. For example, if a save view contains samples with a certain tag, the view\u2019s contents will change as you add/remove this tag from samples.</p> <p>You can load a saved view at any time by selecting it from the saved view menu:</p> <p></p> <p>You can also edit or delete saved views by clicking on their pencil icon:</p> <p></p> <p>Note</p> <p>Did you know? You can also programmatically create, modify, and delete saved views via Python!</p>"},{"location":"fiftyone_concepts/app/#viewing-a-sample","title":"Viewing a sample \u00b6","text":"<p>Click a sample to open an expanded view of the sample. This modal also contains information about the fields of the <code>Sample</code> and allows you to access the raw JSON description of the sample.</p> <p></p> <p>If your labels contain many dynamic attributes, you may find it helpful to configure which attributes are shown in the tooltip. To do so, press <code>ctrl</code> while hovering over a label to lock the tooltip in-place and then use the show/hide buttons to customize the display.</p> <p>Note</p> <p>Tooltip customizations are persisted in your browser\u2019s local storage on a per-dataset and per-field basis.</p> <p></p>"},{"location":"fiftyone_concepts/app/#using-the-image-visualizer","title":"Using the image visualizer \u00b6","text":"<p>The image visualizer allows you to interactively visualize images along with their associated labels. When you hover over an image in the visualizer, a head-up display (HUD) appears with a control bar providing various options.</p> <p>For example, you can zoom in/out and pan around an image by scrolling and click-dragging with your mouse or trackpad. You can also zoom tightly into the currently visible (or selected) labels by clicking on the <code>Crop</code> icon in the controls HUD or using the <code>z</code> keyboard shortcut. Press <code>ESC</code> to reset your view.</p> <p>When multiple labels are overlaid on top of each other, the up and down arrows offer a convenient way to rotate the z-order of the labels that your cursor is hovering over, so every label and it\u2019s tooltip can be viewed.</p> <p>The settings icon in the controls HUD contains a variety of options for customizing the rendering of your labels, including whether to show object labels, confidences, or the tooltip. The default settings for these parameters can be configured via the App config.</p> <p>Keyboard shortcuts are available for almost every action. Click the <code>?</code> icon in the controls HUD or use the <code>?</code> keyboard shortcut to display the list of available actions and their associated hotkeys.</p> <p></p> <p>Note</p> <p>When working in Jupyter/Colab notebooks, you can hold down the <code>SHIFT</code> key when zoom-scrolling or using the arrow keys to navigate between samples/labels to restrict your inputs to the App and thus prevent them from also affecting your browser window.</p>"},{"location":"fiftyone_concepts/app/#using-the-video-visualizer","title":"Using the video visualizer \u00b6","text":"<p>The video visualizer offers all of the same functionality as the image visualizer, as well as some convenient actions and shortcuts for navigating through a video and its labels.</p> <p>There are a variety of additional video-specific keyboard shortcuts. For example, you can press the spacebar to play/pause the video, and you can press <code>0</code>, <code>1</code>, \u2026, <code>9</code> to seek to the 0%, 10%, \u2026, 90% timestamp in the video. When the video is paused, you can use <code>&lt;</code> and <code>&gt;</code> to navigate frame-by-frame through the video.</p> <p>Click the <code>?</code> icon in the controls HUD or use the <code>?</code> keyboard shortcut to display the list of available actions and their associated hotkeys.</p> <p>All of the same options in the image settings are available in the video settings menu in the controls HUD, as well as additional options like whether to show frame numbers rather than timestamp in the HUD. The default settings for all such parameters can be configured via the App config.</p> <p>Playback rate and volume are also available in the video controls HUD. Clicking on one of the icons resets the setting to the default. And when hovering, a slider appears to adjust the setting manually.</p> <p>Note</p> <p>Did you know? The video visualizer streams frame data on-demand, which means that playback begins as soon as possible and even heavyweight label types like segmentations are supported!</p> <p></p> <p>Note</p> <p>When working in Jupyter/Colab notebooks, you can hold down the <code>SHIFT</code> key when zoom-scrolling or using the arrow keys to navigate between samples/labels to restrict your inputs to the App and thus prevent them from also affecting your browser window.</p>"},{"location":"fiftyone_concepts/app/#using-the-3d-visualizer","title":"Using the 3D visualizer \u00b6","text":"<p>The 3D visualizer allows you to interactively visualize 3D samples or point cloud samples along with any associated 3D detections and 3D polylines:</p> <p></p> <p>The table below summarizes the mouse/keyboard controls that the 3D visualizer supports:</p> Input Action Description Wheel Zoom Zoom in and out Drag Rotate Rotate the camera Shift + drag Translate Translate the camera B Background Toggle background on/off F Fullscreen Toggle fullscreen G Grid Toggle the grid on/off T Top-down Reset camera to top-down view E Ego-view Reset the camera to ego view ESC Escape context Escape the current context <p>A variety of context-specific options are available in a draggable panel in the 3D visualizer that let you configure lights, as well as material and visibility of the 3D objects in the scene.</p> <p>In addition, the HUD at the bottom of the 3D visualizer provides the following controls:</p> <ul> <li> <p>Click the grid icon to toggle the grid on/off</p> </li> <li> <p>Click the <code>T</code> to reset the camera to top-down view</p> </li> <li> <p>Click the <code>E</code> to reset the camera to ego-view</p> </li> </ul> <p>For point clouds, when coloring by intensity, the color of each point is computed by mapping the <code>r</code> channel of the <code>rgb</code> field of the PCD file onto a fixed colormap, which is scaled so that the full colormap is matched to the observed dynamic range of <code>r</code> values for each sample.</p> <p>Similarly, when coloring by height, the <code>z</code> value of each point is mapped to the full colormap using the same strategy.</p>"},{"location":"fiftyone_concepts/app/#viewing-3d-samples-in-the-grid","title":"Viewing 3D samples in the grid \u00b6","text":"<p>When you load 3D collections in the App, any 3D detections and 3D polylines fields will be visualized in the grid using an orthographic projection (onto the xy plane by default).</p> <p>In addition, if you have populated orthographic projection images on your dataset, the projection images will be rendered for each sample in the grid:</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.utils3d as fou3d\nimport fiftyone.zoo as foz\n\n# Load an example 3D dataset\ndataset = (\n    foz.load_zoo_dataset(\"quickstart-groups\")\n    .select_group_slices(\"pcd\")\n    .clone()\n)\n\n# Populate orthographic projections\nfou3d.compute_orthographic_projection_images(dataset, (-1, 512), \"/tmp/proj\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/app/#configuring-the-3d-visualizer","title":"Configuring the 3D visualizer \u00b6","text":"<p>The 3D visualizer can be configured by including any subset of the settings shown below under the <code>plugins.3d</code> key of your App config:</p> <pre><code>// The default values are shown below\n{\n    \"plugins\": {\n        \"3d\": {\n            // Whether to show the 3D visualizer\n            \"enabled\": true,\n\n            // The initial camera position in the 3D scene\n            \"defaultCameraPosition\": {\"x\": 0, \"y\": 0, \"z\": 0},\n\n            // The default up direction for the scene\n            \"defaultUp\": [0, 0, 1],\n\n            \"pointCloud\": {\n                // Don't render points below this z value\n                \"minZ\": null\n            }\n        }\n    }\n}\n</code></pre> <p>You can also store dataset-specific plugin settings by storing any subset of the above values on a dataset\u2019s App config:</p> <pre><code># Configure the 3D visualizer for a dataset's PCD/Label data\ndataset.app_config.plugins[\"3d\"] = {\n    \"defaultCameraPosition\": {\"x\": 0, \"y\": 0, \"z\": 100},\n}\ndataset.save()\n</code></pre> <p>Note</p> <p>Dataset-specific plugin settings will override any settings from your global App config.</p>"},{"location":"fiftyone_concepts/app/#spaces","title":"Spaces \u00b6","text":"<p>Spaces provide a customizable framework for organizing interactive Panels of information within the App.</p> <p>FiftyOne natively includes the following Panels:</p> <ul> <li> <p>Samples panel: the media grid that loads by default when you launch the App</p> </li> <li> <p>Embeddings panel: a canvas for working with embeddings visualizations</p> </li> <li> <p>Model Evaluation panel: interactively analyze and visualize your model\u2019s performance</p> </li> <li> <p>Map panel: visualizes the geolocation data of datasets that have a <code>GeoLocation</code> field</p> </li> <li> <p>Histograms panel: a dashboard of histograms for the fields of your dataset</p> </li> </ul> <p>Note</p> <p>You can also configure custom Panels via plugins!</p> <p></p>"},{"location":"fiftyone_concepts/app/#configuring-spaces-in-the-app","title":"Configuring spaces in the App \u00b6","text":"<p>Consider the following example dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nfob.compute_visualization(dataset, brain_key=\"img_viz\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>You can configure spaces visually in the App in a variety of ways described below.</p> <p>Click the <code>+</code> icon in any Space to add a new Panel:</p> <p></p> <p>When you have multiple Panels open in a Space, you can use the divider buttons to split the Space either horizontally or vertically:</p> <p></p> <p>You can rearrange Panels at any time by dragging their tabs between Spaces, or close Panels by clicking their <code>x</code> icon:</p> <p></p>"},{"location":"fiftyone_concepts/app/#configuring-spaces-in-python","title":"Configuring spaces in Python \u00b6","text":"<p>You can also programmatically configure your Space layout and the states of the individual Panels via the <code>Space</code> and <code>Panel</code> classes in Python, as shown below:</p> <pre><code>samples_panel = fo.Panel(type=\"Samples\", pinned=True)\n\nhistograms_panel = fo.Panel(\n    type=\"Histograms\",\n    state=dict(plot=\"Labels\"),\n)\n\nembeddings_panel = fo.Panel(\n    type=\"Embeddings\",\n    state=dict(brainResult=\"img_viz\", colorByField=\"metadata.size_bytes\"),\n)\n\nspaces = fo.Space(\n    children=[\\\n        fo.Space(\\\n            children=[\\\n                fo.Space(children=[samples_panel]),\\\n                fo.Space(children=[histograms_panel]),\\\n            ],\\\n            orientation=\"horizontal\",\\\n        ),\\\n        fo.Space(children=[embeddings_panel]),\\\n    ],\n    orientation=\"vertical\",\n)\n</code></pre> <p>The <code>children</code> property of each <code>Space</code> describes what the Space contains, which can be either:</p> <ul> <li> <p>A list of <code>Space</code> instances. In this case, the Space contains a nested list of Spaces, arranged either horizontally or vertically, as per the <code>orientation</code> property of the parent Space</p> </li> <li> <p>A list of <code>Panel</code> instances describing the Panels that should be available as tabs within the Space</p> </li> </ul> <p>Set a Panel\u2019s <code>pinned</code> property to <code>True</code> if you do not want a Panel\u2019s tab to have a close icon <code>x</code> in the App. Each <code>Panel</code> also has a <code>state</code> dict that can be used to configure the specific state of the Panel to load. Refer to the sections below for each Panel\u2019s available state.</p> <p>You can launch the App with an initial spaces layout by passing the optional <code>spaces</code> parameter to <code>launch_app()</code>:</p> <pre><code># Launch the App with an initial Spaces layout\nsession = fo.launch_app(dataset, spaces=spaces)\n</code></pre> <p>Once the App is launched, you can retrieve your current layout at any time via the <code>session.spaces</code> property:</p> <pre><code>print(session.spaces)\n</code></pre> <p>You can also programmatically configure the App\u2019s current layout by setting <code>session.spaces</code> to any valid <code>Space</code> instance:</p> <pre><code># Change the session's current Spaces layout\nsession.spaces = spaces\n</code></pre> <p>Note</p> <p>Inspecting <code>session.spaces</code> of a session whose Spaces layout you\u2019ve configured in the App is a convenient way to discover the available state options for each Panel type!</p> <p>You can reset your spaces to their default state by setting <code>session.spaces</code> to None:</p> <pre><code># Reset spaces layout in the App\nsession.spaces = None\n</code></pre>"},{"location":"fiftyone_concepts/app/#saving-workspaces","title":"Saving workspaces \u00b6","text":"<p>If you find yourself frequently using/recreating a certain spaces layout, you can save it as a workspace with a name of your choice and then load it later via the App or programmatically!</p>"},{"location":"fiftyone_concepts/app/#saving-workspaces-in-the-app","title":"Saving workspaces in the App \u00b6","text":"<p>Continuing from the example above, once you\u2019ve configured a spaces layout of interest, click the \u201cUnsaved workspace\u201d icon in the upper right corner to open the workspaces menu and save your current workspace with a name and optional description/color of your choice:</p> <p></p> <p>Note</p> <p>Saved workspaces include all aspects of your current spaces layout, including panel types, layouts, sizes, and even the current state of each panel!</p> <p>You can load saved workspaces at any time later via this same menu:</p> <p></p> <p>You can also edit the details of an existing saved workspace at any time by clicking on its pencil icon in the workspace menu:</p> <p></p> <p>Note</p> <p>If you want to modify the layout of an existing saved workspace, you must delete the existing workspace and then re-save it under the same name after modifying the layout in the App.</p>"},{"location":"fiftyone_concepts/app/#saving-workspaces-in-python","title":"Saving workspaces in Python \u00b6","text":"<p>You can also programmatically create and manage saved workspaces!</p> <p>Use <code>save_workspace()</code> to create a new saved workspace with a name of your choice:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nsamples_panel = fo.Panel(type=\"Samples\", pinned=True)\n\nhistograms_panel = fo.Panel(\n    type=\"Histograms\",\n    state=dict(plot=\"Labels\"),\n)\n\nembeddings_panel = fo.Panel(\n    type=\"Embeddings\",\n    state=dict(brainResult=\"img_viz\", colorByField=\"metadata.size_bytes\"),\n)\n\nworkspace = fo.Space(\n    children=[\\\n        fo.Space(\\\n            children=[\\\n                fo.Space(children=[samples_panel]),\\\n                fo.Space(children=[histograms_panel]),\\\n            ],\\\n            orientation=\"horizontal\",\\\n        ),\\\n        fo.Space(children=[embeddings_panel]),\\\n    ],\n    orientation=\"vertical\",\n)\n\ndataset.save_workspace(\n    \"my-workspace\",\n    workspace,\n    description=\"Samples, embeddings, histograms, oh my!\",\n    color=\"#FF6D04\",\n)\n</code></pre> <p>Note</p> <p>Pro tip! You can save your current spaces layout in the App via <code>session.spaces</code>:</p> <pre><code>workspace = session.spaces\ndataset.save_workspace(\"my-workspace\", workspace, ...)\n</code></pre> <p>Then in a future session you can load the workspace by name with <code>load_workspace()</code>:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.load_dataset(\"quickstart\")\n\n# Retrieve a saved workspace and launch app with it\nworkspace = dataset.load_workspace(\"my-workspace\")\nsession = fo.launch_app(dataset, spaces=workspace)\n\n# Or, load a workspace on an existing session\nsession.spaces = workspace\n</code></pre> <p>Saved workspaces have certain editable metadata such as a name, description, and color that you can view via <code>get_workspace_info()</code> and update via <code>update_workspace_info()</code>:</p> <pre><code># Get a saved workspace's editable info\nprint(dataset.get_workspace_info(\"my-workspace\"))\n\n# Update the workspace's name and add a description\ninfo = dict(\n    name=\"still-my-workspace\",\n    description=\"Samples, embeddings, histograms, oh my oh my!!\",\n)\ndataset.update_workspace_info(\"my-workspace\", info)\n\n# Verify that the info has been updated\nprint(dataset.get_workspace_info(\"still-my-workspace\"))\n# {\n#   'name': 'still-my-workspace',\n#   'description': 'Samples, embeddings, histograms, oh my oh my!!',\n#   'color': None\n# }\n</code></pre> <p>You can also use <code>list_workspaces()</code>, <code>has_workspace()</code>, and <code>delete_workspace()</code> to manage your saved workspaces.</p>"},{"location":"fiftyone_concepts/app/#samples-panel","title":"Samples panel \u00b6","text":"<p>By default, when you launch the App, your spaces layout will contain a single space with the Samples panel active:</p> <p></p> <p>When configuring spaces in Python, you can create a Samples panel as follows:</p> <pre><code>samples_panel = fo.Panel(type=\"Samples\")\n</code></pre>"},{"location":"fiftyone_concepts/app/#embeddings-panel","title":"Embeddings panel \u00b6","text":"<p>When you load a dataset in the App that contains an embeddings visualization, you can open the Embeddings panel to visualize and interactively explore a scatterplot of the embeddings in the App:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Image embeddings\nfob.compute_visualization(dataset, brain_key=\"img_viz\")\n\n# Object patch embeddings\nfob.compute_visualization(\n    dataset, patches_field=\"ground_truth\", brain_key=\"gt_viz\"\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Use the two menus in the upper-left corner of the Panel to configure your plot:</p> <ul> <li> <p>Brain key: the brain key associated with the <code>compute_visualization()</code> run to display</p> </li> <li> <p>Color by: an optional sample field (or label attribute, for patches embeddings) to color the points by</p> </li> </ul> <p>From there you can lasso points in the plot to show only the corresponding samples/patches in the Samples panel:</p> <p></p> <p>Note</p> <p>Did you know? With FiftyOne Teams you can generate embeddings visualizations natively from the App in the background while you work.</p> <p>The embeddings UI also provides a number of additional controls:</p> <ul> <li> <p>Press the <code>pan</code> icon in the menu (or type <code>g</code>) to switch to pan mode, in which you can click and drag to change your current field of view</p> </li> <li> <p>Press the <code>lasso</code> icon (or type <code>s</code>) to switch back to lasso mode</p> </li> <li> <p>Press the <code>locate</code> icon to reset the plot\u2019s viewport to a tight crop of the current view\u2019s embeddings</p> </li> <li> <p>Press the <code>x</code> icon (or double click anywhere in the plot) to clear the current selection</p> </li> </ul> <p>When coloring points by categorical fields (strings and integers) with fewer than 100 unique classes, you can also use the legend to toggle the visibility of each class of points:</p> <ul> <li> <p>Single click on a legend trace to show/hide that class in the plot</p> </li> <li> <p>Double click on a legend trace to show/hide all other classes in the plot</p> </li> </ul> <p></p> <p>When configuring spaces in Python, you can define an Embeddings panel as follows:</p> <pre><code>embeddings_panel = fo.Panel(\n    type=\"Embeddings\",\n    state=dict(brainResult=\"img_viz\", colorByField=\"uniqueness\"),\n)\n</code></pre> <p>The Embeddings panel supports the following <code>state</code> parameters:</p> <ul> <li> <p>brainResult: the brain key associated with the <code>compute_visualization()</code> run to display</p> </li> <li> <p>colorByField: an optional sample field (or label attribute, for patches embeddings) to color the points by</p> </li> </ul>"},{"location":"fiftyone_concepts/app/#model-evaluation-panel-new","title":"Model Evaluation panel NEW \u00b6","text":"<p>When you load a dataset in the App that contains one or more evaluations, you can open the Model Evaluation panel to visualize and interactively explore the evaluation results in the App:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Evaluate the objects in the `predictions` field with respect to the\n# objects in the `ground_truth` field\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval\",\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>The panel\u2019s home page shows a list of evaluation on the dataset, their current review status, and any evaluation notes that you\u2019ve added. Click on an evaluation to open its expanded view, which provides a set of expandable cards that dives into various aspects of the model\u2019s performance:</p> <p></p> <p>Note</p> <p>Did you know? With FiftyOne Teams you can execute model evaluations natively from the App in the background while you work.</p>"},{"location":"fiftyone_concepts/app/#review-status","title":"Review status \u00b6","text":"<p>You can use the status pill in the upper right-hand corner of the panel to toggle an evaluation between <code>Needs Review</code>, <code>In Review</code>, and <code>Reviewed</code>:</p> <p></p>"},{"location":"fiftyone_concepts/app/#evaluation-notes","title":"Evaluation notes \u00b6","text":"<p>The Evaluation Notes card provides a place to add your own Markdown-formatted notes about the model\u2019s performance:</p> <p></p>"},{"location":"fiftyone_concepts/app/#summary","title":"Summary \u00b6","text":"<p>The Summary card provides a table of common model performance metrics. You can click on the grid icons next to TP/FP/FN to load the corresponding labels in the Samples panel:</p> <p></p>"},{"location":"fiftyone_concepts/app/#metric-performance","title":"Metric performance \u00b6","text":"<p>The Metric Performance card provides a graphical summary of key model performance metrics:</p> <p></p>"},{"location":"fiftyone_concepts/app/#class-performance","title":"Class performance \u00b6","text":"<p>The Class Performance card provides a per-class breakdown of each model performance metric. If an evaluation contains many classes, you can use the settings menu to control which classes are shown. The histograms are also interactive: you can click on bars to show the corresponding labels in the Samples panel:</p> <p></p>"},{"location":"fiftyone_concepts/app/#confusion-matrices","title":"Confusion matrices \u00b6","text":"<p>The Confusion Matrices card provides an interactive confusion matrix for the evaluation. If an evaluation contains many classes, you can use the settings menu to control which classes are shown. You can also click on cells to show the corresponding labels in the Samples panel:</p> <p></p>"},{"location":"fiftyone_concepts/app/#comparing-models","title":"Comparing models \u00b6","text":"<p>When a dataset contains multiple evaluations, you can compare two model\u2019s performance by selecting a \u201cCompare against\u201d key:</p> <pre><code>model = foz.load_zoo_model(\"yolo11s-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions_yolo11\")\n\ndataset.evaluate_detections(\n    \"predictions_yolo11\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval_yolo11\",\n)\n\nsession.refresh()\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/app/#map-panel","title":"Map panel \u00b6","text":"<p>When you load a dataset in the App that contains a <code>GeoLocation</code> field with <code>point</code> data populated, you can open the Map panel to visualize and interactively explore a scatterplot of the location data:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-geo\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>You must configure a Mapbox access token in order to use the Map UI. See below for instructions.</p> <p>FiftyOne uses the Mapbox GL JS API, which is free up to 50,000 map loads each month.</p> <p></p> <p>You can lasso points in the map to show only the corresponding data in the Samples panel. Confirm the selection by either double-clicking the last vertex or typing <code>enter</code>:</p> <p></p> <p>The map UI also provides a number of additional controls:</p> <ul> <li> <p>Use the menu in the upper-left corner to choose between the available map types</p> </li> <li> <p>Press the <code>locate</code> icon to reset the map\u2019s viewport to a tight crop of the current view\u2019s location data</p> </li> <li> <p>Press the <code>x</code> icon to clear the current selection</p> </li> </ul> <p></p> <p>When configuring spaces in Python, you can define a Map panel as follows:</p> <pre><code>map_panel = fo.Panel(type=\"Map\")\n</code></pre> <p>Additionally, the map UI can be configured by including any subset of the settings shown below under the <code>plugins.map</code> key of your App config:</p> <pre><code>// The default values are shown below\n{\n    \"plugins\": {\n        \"map\": {\n            // Your mapbox token. This is required\n            \"mapboxAccessToken\": \"XXXXXXXX\",\n\n            // Whether to enable clustering\n            \"clustering\": true,\n\n            // Never use clustering beyond this zoom level\n            // https://docs.mapbox.com/help/glossary/zoom-level\n            \"clusterMaxZoom\": 11,\n\n            // Controls the look and feel of clusters\n            \"clusters\": {\n                \"paint\": {\n                    \"circle-color\": \"rgb(244, 113, 6)\",\n                    \"circle-opacity\": 0.7,\n\n                    // Step expressions can be used\n                    // https://docs.mapbox.com/mapbox-gl-js/style-spec/#expressions-step\n                    // 20px circles when point count is less than 10\n                    // 30px circles when point count is between 10 and 25\n                    // 40px circles when point count is greater than or equal to 25\n                    \"circle-radius\": [\"step\", [\"get\", \"point_count\"], 20, 10, 30, 25, 40]\n                }\n            },\n\n            // Controls the look and feel of individual scatter points\n            \"pointPaint\": {\n                \"circle-color\": \"rgb(244, 113, 6)\",\n                \"circle-opacity\": 0.7,\n                \"circle-radius\": 4\n            }\n        }\n    }\n}\n</code></pre> <p>If you prefer, you can provide your Mapbox token by setting the <code>MAPBOX_TOKEN</code> environment variable:</p> <pre><code>export MAPBOX_TOKEN=XXXXXXXX\n</code></pre> <p>You can also store dataset-specific plugin settings by storing any subset of the above values on a dataset\u2019s App config:</p> <pre><code># Disable clustering for this dataset\ndataset.app_config.plugins[\"map\"] = {\"clustering\": False}\ndataset.save()\n</code></pre> <p>Note</p> <p>Dataset-specific plugin settings will override any settings from your global App config.</p>"},{"location":"fiftyone_concepts/app/#histograms-panel","title":"Histograms panel \u00b6","text":"<p>The Histograms panel in the App lets you visualize different statistics about the fields of your dataset.</p> <ul> <li> <p>The <code>Sample tags</code> and <code>Label tags</code> modes show the distribution of any tags that you\u2019ve added to your dataset</p> </li> <li> <p>The <code>Labels</code> mode shows the class distributions for each labels field that you\u2019ve added to your dataset. For example, you may have histograms of ground truth labels and one more sets of model predictions</p> </li> <li> <p>The <code>Other fields</code> mode shows distributions for numeric (integer or float) or categorical (e.g., string) primitive fields that you\u2019ve added to your dataset. For example, if you computed uniqueness on your dataset, a histogram of uniqueness values will be available under this mode.</p> </li> </ul> <p>Note</p> <p>The statistics in the plots automatically update to reflect the current view that you have loaded in the App!</p> <p></p> <p>When configuring spaces in Python, you can define a Histograms panel as follows:</p> <pre><code>histograms_panel = fo.Panel(type=\"Histograms\", state=dict(plot=\"Labels\"))\n</code></pre> <p>The Histograms panel supports the following <code>state</code> parameters:</p> <ul> <li>plot: the histograms to plot. Supported values are <code>\"Sample tags\"</code>, <code>\"Label tags\"</code>, <code>\"Labels\"</code>, and <code>\"Other fields\"</code></li> </ul>"},{"location":"fiftyone_concepts/app/#selecting-samples","title":"Selecting samples \u00b6","text":"<p>As previously explained, the <code>Session</code> object created when you launch the App lets you interact with the App from your Python process.</p> <p>One common workflow is to select samples visually in the App and then access the data for the selected samples in Python. To perform this workflow, first select some samples in the App:</p> <p></p> <p>The selected samples checkmark in the options row in the upper-left corner of the sample grid records the number of samples that you have currently selected. You can also take actions such as updating the view to only show (or exclude) the currently selected samples.</p> <p>Tagging also automatically applies to selected samples or their labels when any samples are selected. See tagging for more details.</p> <p>You can also access the <code>Session.selected</code> property of your session to retrieve the IDs of the currently selected samples in the App:</p> <pre><code># Print the IDs of the currently selected samples\nprint(session.selected)\n\n# Create a view containing only the selected samples\nselected_view = dataset.select(session.selected)\n</code></pre> <pre><code>['5ef0eef405059ebb0ddfa6cc',\\\n '5ef0eef405059ebb0ddfa7c4',\\\n '5ef0eef405059ebb0ddfa86e',\\\n '5ef0eef405059ebb0ddfa93c']\n</code></pre>"},{"location":"fiftyone_concepts/app/#selecting-labels","title":"Selecting labels \u00b6","text":"<p>You can also use the App to select individual labels within samples. You can use this functionality to visually show/hide labels of interest in the App; or you can access the data for the selected labels from Python, for example by creating a <code>DatasetView</code> that includes/excludes the selected labels.</p> <p>To perform this workflow, open the expanded sample view by clicking on a sample in the App. Then click on individual labels to select them:</p> <p></p> <p>Selected labels will appear with dotted lines around them. The example above shows selecting an object detection, but classifications, polygons, polylines, segmentations, and keypoints can be selected as well.</p> <p>When you have selected labels in the App, you can use the selected labels options in the upper-right (the orange checkmark button) to hide these labels from view or exclude all other labels.</p> <p>You can also access the <code>Session.selected_labels</code> property of your session to retrieve information about the currently selected labels in the App:</p> <pre><code># Print information about the currently selected samples in the App\nfo.pprint(session.selected_labels)\n\n# Create a view containing only the selected labels\nselected_view = dataset.select_labels(session.selected_labels)\n\n# Create a view containing everything except the selected labels\nexcluded_view = dataset.exclude_labels(session.selected_labels)\n</code></pre> <pre><code>[\\\n    {\\\n        'object_id': '5f99d2eb36208058abbfc02a',\\\n        'sample_id': '5f99d2eb36208058abbfc030',\\\n        'field': 'ground_truth',\\\n    },\\\n    {\\\n        'object_id': '5f99d2eb36208058abbfc02b',\\\n        'sample_id': '5f99d2eb36208058abbfc030',\\\n        'field': 'ground_truth',\\\n    },\\\n    ...\\\n]\n</code></pre>"},{"location":"fiftyone_concepts/app/#tags-and-tagging","title":"Tags and tagging \u00b6","text":"<p>Tagging is a first-class citizen in FiftyOne, as both <code>Sample</code> and <code>Label</code> instances have a <code>tags</code> attribute that you can use to store arbitrary string tags for your data.</p> <p>The FiftyOne API provides methods like <code>tag_samples()</code> and <code>tag_labels()</code> that you can use to programmatically manage the tags on your dataset. However, the App also provides a convenient UI for interactively adding, removing, and filtering by <code>Sample</code> and <code>Label</code> tags.</p> <p>You can tag or untag batches of samples/labels in the App by clicking on the tag icon above the sample grid.</p> <p>For example, take the following steps to tag all labels in the <code>predictions</code> field of a dataset:</p> <ul> <li> <p>Make sure that <code>predictions</code> is the only <code>Label</code> field checked in the filters sidebar</p> </li> <li> <p>Click the tag icon in the top-left corner of the grid</p> </li> <li> <p>Select <code>Labels</code>, type in the tag, and then click <code>Apply</code></p> </li> </ul> <p>You can also use the tag menu to remove existing tags.</p> <p>Note</p> <p>Any tagging operations that you perform using the tagging UI above the sample grid will be applied to your current view, respecting any filters or show/hide checkboxes you have applied in the filters sidebar, unless you have selected individual samples, in which case the operation will only apply to the selected samples.</p> <p></p> <p>The App also supports tagging data in individual samples when you have opened the expanded sample view by clicking on a sample. The tag icon is located in the top-right corner of the modal.</p> <p>Note</p> <p>Any tagging operations that you perform using the tagging UI in expanded sample mode will be applied to the current sample, respecting any filters or show/hide checkboxes you have applied, unless you have selected individual labels, in which case the operation will only apply to the selected labels. The latter may span multiple samples.</p> <p></p> <p>If your dataset has sample or label tags, you can use the <code>SAMPLE TAGS</code> and <code>LABEL TAGS</code> sections of the filters sidebar to filter by your tags.</p> <p>When you click the eye icon next to a sample tag, your view will update to only include samples with the tag(s) you have selected. When you click the eye icon next to a label tag, your view will update to only include labels with tag(s) you have selected, and any samples with no matches will be automatically excluded.</p> <p>Note</p> <p>Did you know? When you have applied filter(s) in the App, a save icon appears in the top-left corner of the sample grid. Clicking this button will convert your filters to an equivalent set of stage(s) in the view bar!</p>"},{"location":"fiftyone_concepts/app/#viewing-object-patches","title":"Viewing object patches \u00b6","text":"<p>Whenever you load a dataset in the App that contains label list fields in <code>Detections</code> or <code>Polylines</code> format, you can use the patches menu to create a view into your data that contains one sample per object patch in a specified label field of your dataset.</p> <p>To switch to patches view, simply click the patches icon above the sample grid in the App, toggle to the <code>Labels</code> submenu, and then choose the field whose object patches you want to view. After you make a selection, a new <code>ToPatches</code> view stage will be appended to the view bar and your view will be updated to show the patches.</p> <p>By default, patches are cropped so only the label patch is visible, but you can zoom in/out and pan as desired in the image visualizer. If you would like to see the entire image for each patch by default, click on the settings icon and uncheck the <code>Crop to patch</code> setting. The setting is available in both the grid and expanded sample view.</p> <p>Note</p> <p>Switching to patches view will create patches for only the contents of your current view, so you can use the view bar and the filters sidebar to select only the content of interest prior to extracting patches.</p> <p></p> <p>You can interact with object patches views in the App just like you would with any other view, including:</p> <ul> <li> <p>You can filter and transform objects patches views using the filter sidebar or the view bar</p> </li> <li> <p>Any modifications to patch label tags that you make via the tagging menu will be reflected on the source dataset</p> </li> </ul> <p>One notable exception is that tagging or untagging patches themselves (as opposed to their labels) will not affect the sample tags of the underlying <code>Sample</code>.</p> <p>Note</p> <p>Did you know? You can construct object patches views programmatically via dataset views!</p>"},{"location":"fiftyone_concepts/app/#viewing-evaluation-patches","title":"Viewing evaluation patches \u00b6","text":"<p>Whenever you load a dataset in the App that contains object detections on which you have run evaluation, you can use the patches menu to create a view into your data that contains one sample for each true positive, false positive, and false negative example.</p> <p>To switch to evaluation patches view, simply click the patches icon above the sample grid in the App, toggle to the <code>Evaluations</code> submenu, and then choose the <code>eval_key</code> under which you saved the evaluation results that you want view. After you make a selection, a new <code>ToEvaluationPatches</code> view stage will be appended to the view bar and your view will be updated to show the evaluation patches!</p> <p>By default, evaluation patches are cropped so only the label(s) that make up the patch are visible, but you can zoom in/out and pan as desired in the image visualizer. If you would like to see the entire image for each patch by default, click on the settings icon and uncheck the <code>Crop to patch</code> setting. The setting is available in both the grid and expanded sample view.</p> <p>Note</p> <p>Refer to the evaluation guide guide for more information about running evaluations and using evaluation patches views to analyze object detection models.</p> <p></p> <p>You can interact with evaluation patches views in the App just like you would with any other view, including:</p> <ul> <li> <p>You can filter and transform evaluation patches views using the filter sidebar or the view bar</p> </li> <li> <p>Any modifications to the tags of the ground truth or predicted labels that you make via the tagging menu will be reflected on the source dataset</p> </li> </ul> <p>One notable exception is that tagging or untagging patches themselves (as opposed to their labels) will not affect the sample tags of the underlying <code>Sample</code>.</p> <p>Note</p> <p>Switching to evaluation patches view will generate patches for only the contents of the current view, which may differ from the view on which the <code>eval_key</code> evaluation was performed. This may exclude some labels that were evaluated and/or include labels that were not evaluated.</p> <p>If you would like to see patches for the exact view on which an evaluation was performed, first call <code>load_evaluation_view()</code> to load the view and then convert to patches.</p>"},{"location":"fiftyone_concepts/app/#viewing-video-clips","title":"Viewing video clips \u00b6","text":"<p>Whenever you load a video dataset in the App that contains <code>TemporalDetection</code> labels or frame-level label lists such as <code>Detections</code>, you can use the patches menu to create a view into your data that contains one sample per clip defined by a specified label field of your dataset.</p> <p>To switch to clips view, simply click the patches icon above the sample grid in the App, toggle to the <code>Labels</code> submenu, and then choose the field whose clips you want to view. After you make a selection, a new <code>ToClips</code> view stage will be appended to the view bar and your view will be updated to show the clips.</p> <p>Creating a clips view for a <code>TemporalDetection</code> or <code>TemporalDetections</code> field will create one sample per temporal detection defined by its <code>[first, last]</code> frame support:</p> <p></p> <p>Creating a clips view for a frame-level label list field such as <code>Detections</code> will contain one sample per contiguous range of frames that contains at least one label in the specified field:</p> <p></p> <p>Note</p> <p>Switching to clips view will create clips for only the contents of your current view, so you can use the view bar and the filters sidebar to select only the content of interest prior to extracting clips.</p> <p>See this section for more information about defining clip views.</p> <p>When you hover over a clip in the grid view, the clip and its labels will play on loop. Similarly, when you open a clip in the video visualizer, you will see only the clip when you play the video. If you would like to see other segments of the video from which a clip was extracted, simply drag the video scrubber outside the range of the clip.</p> <p>You can interact with clip views in the App just like you would with any other view, including:</p> <ul> <li> <p>You can filter and transform clip views using the filter sidebar or the view bar</p> </li> <li> <p>Any modifications to label tags that you make via the tagging menu will be reflected on the source dataset</p> </li> </ul> <p>One notable exception is that tagging or untagging clips themselves (as opposed to their labels) will not affect the sample tags of the underlying <code>Sample</code>.</p> <p>Note</p> <p>Did you know? You can construct clip views programmatically via dataset views!</p>"},{"location":"fiftyone_concepts/app/#sorting-by-similarity","title":"Sorting by similarity \u00b6","text":"<p>Whenever you select samples, patches, or labels in the App in a <code>Dataset</code> that has been indexed by similarity, you can use the similarity menu in the App to sort or filter your current view based on similarity to the chosen image or object.</p> <p>Note</p> <p>Refer to the Brain guide for more information about indexing datasets by image/object similarity for use with this feature.</p>"},{"location":"fiftyone_concepts/app/#image-similarity","title":"Image similarity \u00b6","text":"<p>Whenever one or more images are selected in the App, the similarity menu icon appears above the grid. If you have indexed the dataset by image similarity, then you will be able to sort by similarity to your current selection.</p> <p>You can use the advanced settings menu to choose between multiple brain keys and optionally specify a maximum number of matches to return ( <code>k</code>) and whether to query by greatest or least similarity (if supported).</p> <p></p> <p>Note</p> <p>For large datasets, you may notice longer load times the first time you use a similarity index in a session. Subsequent similarity searches will use cached results and will be faster!</p>"},{"location":"fiftyone_concepts/app/#object-similarity","title":"Object similarity \u00b6","text":"<p>Whenever one or more labels or patches are selected in the App, the similarity menu icon appears above the sample grid. If you have indexed the dataset by object similarity, then you will be able to sort by similarity to your current selection.</p> <p>The typical workflow for object similarity is to first switch to object patches view for the label field of interest. In this view, the similarity menu icon will appear whenever you have selected one or more patches from the grid, and the resulting view will sort the patches according to the similarity of their objects with respect to the objects in the query patches.</p> <p>You can use the advanced settings menu to choose between multiple brain keys and optionally specify a maximum number of matches to return ( <code>k</code>) and whether to query by greatest or least similarity (if supported).</p> <p></p> <p>You can also sort by similarity to an object from the expanded sample view in the App by selecting an object and then using the similarity menu that appears in the upper-right corner of the modal:</p> <p></p> <p>Note</p> <p>For large datasets, you may notice longer load times the first time you use a similarity index in a session. Subsequent similarity searches will use cached results and will be faster!</p>"},{"location":"fiftyone_concepts/app/#text-similarity","title":"Text similarity \u00b6","text":"<p>If you have indexed your dataset with a model that supports text queries, you can use the text similarity menu in the App to search for images (or object patches) of interest via arbitrary text queries!</p> <p>You can use the advanced settings menu to choose between multiple brain keys and optionally specify a maximum number of matches to return ( <code>k</code>) and whether to query by greatest or least similarity (if supported).</p> <p></p> <p>Note</p> <p>Did you know? You can also perform text queries via the SDK by passing a prompt directly to <code>sort_by_similarity()</code>!</p>"},{"location":"fiftyone_concepts/app/#multiple-media-fields","title":"Multiple media fields \u00b6","text":"<p>There are use cases where you may want to associate multiple media versions with each sample in your dataset, such as:</p> <ul> <li> <p>Thumbnail images</p> </li> <li> <p>Anonymized (e.g., blurred) versions of the images</p> </li> </ul> <p>You can work with multiple media sources in FiftyOne by simply adding extra field(s) to your dataset containing the paths to each media source and then configuring your dataset to expose these multiple media fields in the App.</p> <p>For example, let\u2019s create thumbnail images for use in the App\u2019s grid view and store their paths in a <code>thumbnail_path</code> field:</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.image as foui\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Generate some thumbnail images\nfoui.transform_images(\n    dataset,\n    size=(-1, 32),\n    output_field=\"thumbnail_path\",\n    output_dir=\"/tmp/thumbnails\",\n)\n\nprint(dataset)\n</code></pre> <pre><code>Name:        quickstart\nMedia type:  image\nNum samples: 200\nPersistent:  False\nTags:        []\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:       fiftyone.core.fields.FloatField\n    predictions:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    thumbnail_path:   fiftyone.core.fields.StringField\n</code></pre> <p>We can expose the thumbnail images to the App by modifying the dataset\u2019s App config:</p> <pre><code># Modify the dataset's App config\ndataset.app_config.media_fields = [\"filepath\", \"thumbnail_path\"]\ndataset.app_config.grid_media_field = \"thumbnail_path\"\ndataset.save()  # must save after edits\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Adding <code>thumbnail_path</code> to the <code>media_fields</code> property adds it to the <code>Media Field</code> selector under the App\u2019s settings menu, and setting the <code>grid_media_field</code> property to <code>thumbnail_path</code> instructs the App to use the thumbnail images by default in the grid view:</p> <p></p> <p>Warning</p> <p>When populating multiple media fields on samples, keep in mind that all media sources must have the same type (e.g., image) and aspect ratio as the sample\u2019s primary <code>filepath</code>, since the media must be compatible with the dataset\u2019s spatial labels (e.g., object detections).</p>"},{"location":"fiftyone_concepts/app/#configuring-the-app","title":"Configuring the App \u00b6","text":"<p>The App\u2019s behavior can be configured on a per-session, per-dataset, or global basis.</p> <p>The order of precedence is:</p> <ol> <li> <p>Any changes that you make to the <code>session.config</code> of a live session</p> </li> <li> <p>Any settings stored in a dataset\u2019s <code>app_config</code></p> </li> <li> <p>Settings from your global App config</p> </li> </ol> <p>Any settings or changes made at higher levels of precedence will override any lower priority settings the next time you load/refresh the App.</p>"},{"location":"fiftyone_concepts/app/#global-app-config","title":"Global App config \u00b6","text":"<p>FiftyOne provides a global App config that you can use to customize the default App behavior for all sessions and datasets on your machine.</p> <p>You can also customize the global App config on a per-session basis:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Your default App config\nprint(fo.app_config)\n\n# Create a custom App config\napp_config = fo.app_config.copy()\napp_config.show_confidence = False\napp_config.show_label = True\nprint(app_config)\n\n# Launch App with custom config\nsession = fo.launch_app(dataset, config=app_config)\nprint(session.config)\n</code></pre>"},{"location":"fiftyone_concepts/app/#modifying-your-session","title":"Modifying your session \u00b6","text":"<p>You can configure a live <code>Session</code> by editing its <code>session.config</code> property and calling <code>session.refresh()</code> to apply the changes:</p> <pre><code>print(session.config)\n\n# Customize the config of a live session\nsession.config.show_confidence = True\nsession.config.show_label = True\nsession.refresh()  # must refresh after edits\n</code></pre>"},{"location":"fiftyone_concepts/app/#dataset-app-config","title":"Dataset App config \u00b6","text":"<p>Datasets also provide an app_config property that you can use to customize the behavior of the App for that particular dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.image as foui\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# View the dataset's current App config\nprint(dataset.app_config)\n\n# Generate some thumbnail images\nfoui.transform_images(\n    dataset,\n    size=(-1, 32),\n    output_field=\"thumbnail_path\",\n    output_dir=\"/tmp/thumbnails\",\n)\n\n# Modify the dataset's App config\ndataset.app_config.media_fields = [\"filepath\", \"thumbnail_path\"]\ndataset.app_config.grid_media_field = \"thumbnail_path\"\ndataset.save()  # must save after edits\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>Any settings stored in a dataset\u2019s <code>app_config</code> will override the corresponding settings from your global App config.</p>"},{"location":"fiftyone_concepts/basics/","title":"FiftyOne Basics \u00b6","text":"<p>This page provides a brief overview of FiftyOne\u2019s basic concepts.</p>"},{"location":"fiftyone_concepts/basics/#datasets","title":"Datasets \u00b6","text":"<p>The <code>Dataset</code> class is the core data structure in FiftyOne, allowing you to represent your data and manipulate it through the Python library and the FiftyOne App.</p> <p>FiftyOne Datasets allow you to easily load, modify, visualize, and evaluate your data along with any related labels (classifications, detections, etc). They provide a consistent interface for loading images, videos, annotations, and model predictions into a format that can be visualized in the FiftyOne App, synced with your annotation source, and shared with others.</p> <p>If you have your own collection of data, loading it as a <code>Dataset</code> will allow you to easily search and sort your samples. You can use FiftyOne to identify unique samples as well as possible mistakes in your labels.</p> <p>If you are training a model, its predictions and associated data such as embeddings and logits can be loaded into your <code>Dataset</code>. The FiftyOne App makes it easy to visually debug what your model has learned, even for complex label types like polygons and segmentation masks. With this knowledge, you can update your <code>Dataset</code> to include more representative samples and samples that your model found difficult into your training set.</p> <p>Note</p> <p>Check out creating FiftyOne datasets for more information about loading your data into FiftyOne.</p> <p>A <code>Dataset</code> is composed of multiple <code>Sample</code> objects which contain <code>Field</code> attributes, all of which can be dynamically created, modified and deleted. FiftyOne uses a lightweight non-relational database to store datasets, so you can easily scale to datasets of any size without worrying about RAM constraints on your machine.</p> <p>Datasets are ordered collections of samples. When a <code>Sample</code> is added to a <code>Dataset</code>, it is assigned a unique ID that can be used to retrieve the sample from the dataset.</p> <p>Slicing and other batch operations on datasets are done through the use of dataset views. A <code>DatasetView</code> provides a view into the <code>Dataset</code>, which can be filtered, sorted, sampled, etc. along various axes to obtain a desired subset of the samples.</p> <p>Learn more about using datasets</p> <pre><code>import fiftyone as fo\n\n# Create an empty dataset\ndataset = fo.Dataset(\"test-dataset\")\n\nprint(dataset)\n</code></pre> <pre><code>Name:           test-dataset\nMedia type:     None\nNum samples:    0\nPersistent:     False\nTags:           []\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n</code></pre>"},{"location":"fiftyone_concepts/basics/#samples","title":"Samples \u00b6","text":"<p>Samples are the atomic elements of a <code>Dataset</code> that store all the information related to a given piece of data (e.g., an image or video).</p> <p>All <code>Sample</code> instances store the path to their source data on disk in their <code>filepath</code> field. Any number of fields can be dynamically added to samples to store additional custom information about the sample.</p> <p>Learn more about using samples</p> <pre><code>import fiftyone as fo\n\n# An image sample\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\n# A video sample\nsample = fo.Sample(filepath=\"/path/to/video.mp4\")\n</code></pre>"},{"location":"fiftyone_concepts/basics/#fields","title":"Fields \u00b6","text":"<p>Fields are attributes of <code>Sample</code> instances that store customizable information about the samples. Thinking of a <code>Dataset</code> as a table where each row is a <code>Sample</code>, each column of the table is a <code>Field</code>.</p> <p>All samples must have their <code>filepath</code> field populated, which points to the source data for the sample on disk. By default, samples are also given <code>id</code>, <code>media_type</code>, <code>tags</code>, <code>metadata</code>, <code>created_at</code>, and <code>last_modified_at</code> fields that store common information:</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': 'path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n}&gt;\n</code></pre> <p>Custom fields can contain any Python primitive data type:</p> <ul> <li> <p><code>BooleanField</code>: contains Python <code>bool</code> instances</p> </li> <li> <p><code>IntField</code>: contains Python <code>int</code> instances</p> </li> <li> <p><code>FloatField</code>: contains Python <code>float</code> instances</p> </li> <li> <p><code>StringField</code>: contains Python <code>str</code> instances</p> </li> <li> <p><code>DateField</code>: contains Python <code>date</code> instances</p> </li> <li> <p><code>DateTimeField</code>: contains Python <code>datetime</code> instances</p> </li> <li> <p><code>ListField</code>: contains Python <code>list</code> instances</p> </li> <li> <p><code>DictField</code>: contains Python <code>dict</code> instances</p> </li> </ul> <p>The elements of list and dict fields may be homogeneous or heterogeneous, and may even contain nested lists and dicts. Fields can also contain more complex data types like labels.</p> <p>Fields can be dynamically created, modified, and deleted. When a new <code>Field</code> is assigned to a <code>Sample</code> in a <code>Dataset</code>, or a <code>Sample</code> with new fields is added to a <code>Dataset</code>, the appropriate fields are automatically added to the dataset\u2019s schema and thus accessible on all other samples in the dataset.</p> <p>Note</p> <p>If a <code>Field</code> has not been set on a particular <code>Sample</code> in a <code>Dataset</code>, its value will be <code>None</code>.</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\nsample[\"quality\"] = 89.7\nsample[\"keypoints\"] = [[31, 27], [63, 72]]\nsample[\"geo_json\"] = {\n    \"type\": \"Feature\",\n    \"geometry\": {\"type\": \"Point\", \"coordinates\": [125.6, 10.1]},\n    \"properties\": {\"name\": \"camera\"},\n}\n\ndataset = fo.Dataset(\"fields-test\")\ndataset.add_sample(sample)\n\nprint(dataset)\n</code></pre> <pre><code>Name:           fields-test\nMedia type:     image\nNum samples:    1\nPersistent:     False\nTags:           []\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    quality:          fiftyone.core.fields.FloatField\n    keypoints:        fiftyone.core.fields.ListField\n    geo_json:         fiftyone.core.fields.DictField\n</code></pre> <p>Learn more about sample fields</p>"},{"location":"fiftyone_concepts/basics/#media-type","title":"Media type \u00b6","text":"<p>When a <code>Sample</code> is created, its media type is inferred from the <code>filepath</code> to the source media and exposed via the <code>media_type</code> attribute of the sample.</p> <p>Learn more about media types</p>"},{"location":"fiftyone_concepts/basics/#tags","title":"Tags \u00b6","text":"<p>All <code>Sample</code> instances have a <code>tags</code> attribute, which stores a list of strings that can be used flexibly to store information about a sample.</p> <p>A typical use case is to tag the dataset split ( <code>test</code>, <code>train</code>, <code>validation</code>) to which the <code>Sample</code> belongs. However, you are free to use tags however you like.</p> <p>See more information about using tags</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\", tags=[\"train\"])\nsample.tags.append(\"my_favorite_samples\")\n\nprint(sample.tags)\n# [\"train\", \"my_favorite_samples\"]\n</code></pre>"},{"location":"fiftyone_concepts/basics/#metadata","title":"Metadata \u00b6","text":"<p>All <code>Sample</code> instances have a <code>metadata</code> attribute that stores type-specific metadata about the source media of the sample.</p> <p>Learn more about adding metadata to your samples</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\ndataset = fo.Dataset()\ndataset.add_sample(sample)\n\n# Populate the `metadata` field of all samples in the dataset\ndataset.compute_metadata()\n\nprint(dataset.first())\n</code></pre> <pre><code>&lt;Sample: {\n    'id': '60302b9dca4a8b5f74e84f16',\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': &lt;ImageMetadata: {\n        'size_bytes': 544559,\n        'mime_type': 'image/png',\n        'width': 698,\n        'height': 664,\n        'num_channels': 3,\n    }&gt;,\n    'created_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n    'last_modified_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n}&gt;\n</code></pre>"},{"location":"fiftyone_concepts/basics/#labels","title":"Labels \u00b6","text":"<p>Labels store semantic information about the sample, such as ground annotations or model predictions.</p> <p>FiftyOne provides label classes for many common tasks:</p> <ul> <li> <p>Regression: a regression value</p> </li> <li> <p>Classification: a classification label</p> </li> <li> <p>Classifications: a list of classifications (typically for multilabel tasks)</p> </li> <li> <p>Detections: a list of object detections (with optional instance masks)</p> </li> <li> <p>Polylines: a list of polylines or polygons in an image</p> </li> <li> <p>Cuboids: a list of 2D cuboids in an image</p> </li> <li> <p>Rotated bounding boxes: a list of rotated boxes in an image</p> </li> <li> <p>Keypoints: a list of keypoints in an image</p> </li> <li> <p>Segmentation: a semantic segmentation mask for an image</p> </li> <li> <p>Heatmap: an intensity heatmap for an image</p> </li> <li> <p>Temporal detection: events with a temporal frame support in a video</p> </li> <li> <p>3D detections: a list of 3D detections in a scene</p> </li> <li> <p>3D polylines: a list of 3D polylines or polygons in a scene</p> </li> <li> <p>GeoLocation: geolocation point(s), line(s), or polygon(s)</p> </li> </ul> <p>Using FiftyOne\u2019s <code>Label</code> types enables you to visualize your labels in the the App.</p> <p>Learn more about storing labels in your samples</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\nsample[\"weather\"] = fo.Classification(label=\"sunny\")\nsample[\"animals\"] = fo.Detections(\n    detections=[\\\n        fo.Detection(label=\"cat\", bounding_box=[0.5, 0.5, 0.4, 0.3]),\\\n        fo.Detection(label=\"dog\", bounding_box=[0.2, 0.2, 0.2, 0.4]),\\\n    ]\n)\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': 'path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'weather': &lt;Classification: {'label': 'sunny', 'confidence': None, 'logits': None}&gt;,\n    'animals': &lt;Detections: {\n        'detections': [\\\n            &lt;Detection: {\\\n                'label': 'cat',\\\n                'bounding_box': [0.5, 0.5, 0.4, 0.3],\\\n                'confidence': None,\\\n                'attributes': {},\\\n            }&gt;,\\\n            &lt;Detection: {\\\n                'label': 'dog',\\\n                'bounding_box': [0.2, 0.2, 0.2, 0.4],\\\n                'confidence': None,\\\n                'attributes': {},\\\n            }&gt;,\\\n        ],\n    }&gt;,\n}&gt;\n</code></pre>"},{"location":"fiftyone_concepts/basics/#datasetviews","title":"DatasetViews \u00b6","text":"<p>Dataset views are a powerful tool for exploring your datasets. You can use <code>DatasetView</code> instances to search, filter, sort, and manipulate subsets of your datasets to perform the analysis that you need.</p> <p>Get a full walkthrough of dataset views</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.brain as fob\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\n\ncats = dataset.match(F(\"ground_truth.label\") == \"cat\")\nfob.compute_uniqueness(cats)\n\nsimilar_cats = cats.sort_by(\"uniqueness\", reverse=False)\n\nsession = fo.launch_app(view=similar_cats)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/basics/#aggregations","title":"Aggregations \u00b6","text":"<p>Dataset views allow you to search for samples in your datasets and filter their contents. Complementary to this, one is often interested in computing aggregate statistics about a dataset or view, such as label counts, distributions, and ranges.</p> <p>FiftyOne provides a powerful aggregations framework that provides a highly-efficient approach to computing statistics about your data.</p> <p>Learn more about using aggregations</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Compute a histogram of the predicted labels in the `predictions` field\nprint(dataset.count_values(\"predictions.detections.label\"))\n# {'bicycle': 13, 'hot dog': 8, ..., 'skis': 52}\n\n# Compute the range of confidences of `cat` predictions in the dataset\nprint(\n    dataset\n    .filter_labels(\"predictions\", F(\"label\") == \"cat\")\n    .bounds(\"predictions.detections.confidence\")\n)\n# (0.05223553627729416, 0.9965479969978333)\n</code></pre>"},{"location":"fiftyone_concepts/brain/","title":"FiftyOne Brain \u00b6","text":"<p>The FiftyOne Brain provides powerful machine learning techniques that are designed to transform how you curate your data from an art into a measurable science.</p> <p>Note</p> <p>Did you know? You can execute Brain methods from the FiftyOne App by installing the @voxel51/brain plugin!</p> <p>The FiftyOne Brain methods are useful across the stages of the machine learning workflow:</p> <ul> <li> <p>Visualizing embeddings: Tired of combing through individual images/videos and staring at aggregate performance metrics trying to figure out how to improve the performance of your model? Using FiftyOne to visualize your dataset in a low-dimensional embedding space can reveal patterns and clusters in your data that can help you answer many important questions about your data, from identifying the most critical failure modes of your model, to isolating examples of critical scenarios, to recommending new samples to add to your training dataset, and more!</p> </li> <li> <p>Similarity: When constructing a dataset or training a model, have you ever wanted to find similar examples to an image or object of interest? For example, you may have found a failure case of your model and now want to search for similar scenarios in your evaluation set to diagnose the issue, or you want to mine your data lake to augment your training set to fix the issue. Use the FiftyOne Brain to index your data by similarity and you can easily query and sort your datasets to find similar examples, both programmatically and via point-and-click in the App.</p> </li> <li> <p>Leaky splits: Often when sourcing data en masse, duplicates and near duplicates can slip through the cracks. The FiftyOne Brain offers a leaky splits analysis that can be used to find potential leaks between dataset splits. Such leaks can be misleading when evaluating a model, giving an overly optimistic measure for the quality of training.</p> </li> <li> <p>Near duplicates: When curating massive datasets, you may inadvertently add near duplicate data to your datasets, which can bias or otherwise confuse your models. The FiftyOne Brain offers a near duplicate detection algorithm that automatically surfaces such data quality issues and prompts you to take action to resolve them.</p> </li> <li> <p>Exact duplicates: Despite your best efforts, you may accidentally add duplicate data to a dataset. The FiftyOne Brain provides an exact duplicate detection method that scans your data and alerts you if a dataset contains duplicate samples, either under the same or different filenames.</p> </li> <li> <p>Uniqueness: During the training loop for a model, the best results will be seen when training on unique data. The FiftyOne Brain provides a uniqueness measure for images that compare the content of every image in a dataset with all other images. Uniqueness operates on raw images and does not require any prior annotation on the data. It is hence very useful in the early stages of the machine learning workflow when you are likely asking \u201cWhat data should I select to annotate?\u201d</p> </li> <li> <p>Mistakenness: Annotations mistakes create an artificial ceiling on the performance of your models. However, finding these mistakes by hand is at least as arduous as the original annotation was, especially in cases of larger datasets. The FiftyOne Brain provides a quantitative mistakenness measure to identify possible label mistakes. Mistakenness operates on labeled images and requires the logit-output of your model predictions in order to provide maximum efficacy. It also works on detection datasets to find missed objects, incorrect annotations, and localization issues.</p> </li> <li> <p>Hardness: While a model is training, it will learn to understand attributes of certain samples faster than others. The FiftyOne Brain provides a hardness measure that calculates how easy or difficult it is for your model to understand any given sample. Mining hard samples is a tried and true measure of mature machine learning processes. Use your current model instance to compute predictions on unlabeled samples to determine which are the most valuable to have annotated and fed back into the system as training samples, for example.</p> </li> <li> <p>Representativeness: When working with large datasets, it can be hard to determine what samples within it are outliers and which are more typical. The FiftyOne Brain offers a representativeness measure that can be used to find the most common types of images in your dataset. This is especially helpful to find easy examples to train on in your data and for visualizing common modes of the data.</p> </li> </ul> <p>Note</p> <p>Check out the tutorials page for detailed examples demonstrating the use of many Brain capabilities.</p>"},{"location":"fiftyone_concepts/brain/#visualizing-embeddings","title":"Visualizing embeddings \u00b6","text":"<p>The FiftyOne Brain provides a powerful <code>compute_visualization()</code> method that you can use to generate low-dimensional representations of the samples and/or individual objects in your datasets.</p> <p>These representations can be visualized natively in the App\u2019s Embeddings panel, where you can interactively select points of interest and view the corresponding samples/labels of interest in the Samples panel, and vice versa.</p> <p></p> <p>There are two primary components to an embedding visualization: the method used to generate the embeddings, and the dimensionality reduction method used to compute a low-dimensional representation of the embeddings.</p>"},{"location":"fiftyone_concepts/brain/#embedding-methods","title":"Embedding methods \u00b6","text":"<p>The <code>embeddings</code> and <code>model</code> parameters of <code>compute_visualization()</code> support a variety of ways to generate embeddings for your data:</p> <ul> <li> <p>Provide nothing, in which case a default general purpose model is used to embed your data</p> </li> <li> <p>Provide a <code>Model</code> instance or the name of any model from the Model Zoo that supports embeddings</p> </li> <li> <p>Provide your own precomputed embeddings in array form</p> </li> <li> <p>Provide the name of a <code>VectorField</code> or <code>ArrayField</code> of your dataset in which precomputed embeddings are stored</p> </li> </ul>"},{"location":"fiftyone_concepts/brain/#dimensionality-reduction-methods","title":"Dimensionality reduction methods \u00b6","text":"<p>The <code>method</code> parameter of <code>compute_visualization()</code> allows you to specify the dimensionality reduction method to use. The supported methods are:</p> <ul> <li> <p>umap ( default): Uniform Manifold Approximation and Projection ( UMAP)</p> </li> <li> <p>tsne: t-distributed Stochastic Neighbor Embedding ( t-SNE)</p> </li> <li> <p>pca: Principal Component Analysis ( PCA)</p> </li> <li> <p>manual: provide a manually computed low-dimensional representation</p> </li> </ul> <pre><code>import fiftyone.brain as fob\n\nresults = fob.compute_visualization(\n    dataset,\n    method=\"umap\",  # \"umap\", \"tsne\", \"pca\", etc\n    brain_key=\"...\",\n    ...\n)\n</code></pre> <p>Note</p> <p>When you use the default UMAP method for the first time, you will be prompted to install the umap-learn package.</p> <p>Note</p> <p>Refer to this section for more information about creating visualization runs.</p>"},{"location":"fiftyone_concepts/brain/#applications","title":"Applications \u00b6","text":"<p>How can embedding-based visualization of your data be used in practice? These visualizations often uncover hidden structure in you data that has important semantic meaning depending on the data you use to color/size the points.</p> <p>Here are a few of the many possible applications:</p> <ul> <li> <p>Identifying anomalous and/or visually similar examples</p> </li> <li> <p>Uncovering patterns in incorrect/spurious predictions</p> </li> <li> <p>Finding examples of target scenarios in your data lake</p> </li> <li> <p>Mining hard examples for your evaluation pipeline</p> </li> <li> <p>Recommending samples from your data lake for classes that need additional training data</p> </li> <li> <p>Unsupervised pre-annotation of training data</p> </li> </ul> <p>The best part about embedding visualizations is that you will likely discover more applications specific to your use case when you try it out on your data!</p> <p>Note</p> <p>Check out the image embeddings tutorial to see example uses of the Brain\u2019s embeddings-powered visualization methods to uncover hidden structure in datasets.</p>"},{"location":"fiftyone_concepts/brain/#image-embeddings-example","title":"Image embeddings example \u00b6","text":"<p>The following example gives a taste of the powers of visual embeddings in FiftyOne using the BDD100K dataset from the dataset zoo, embeddings generated by a mobilenet model from the model zoo, and the default UMAP dimensionality reduction method.</p> <p>In this setup, the scatterpoints in the Embeddings panel correspond to images in the validation split colored by the <code>time of day</code> labels provided by the BDD100K dataset. When points are lasso-ed in the plot, the corresponding samples are automatically selected in the Samples panel:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\n# The BDD dataset must be manually downloaded. See the zoo docs for details\nsource_dir = \"/path/to/dir-with-bdd100k-files\"\n\ndataset = foz.load_zoo_dataset(\n    \"bdd100k\", split=\"validation\", source_dir=source_dir,\n)\n\n# Compute embeddings\n# You will likely want to run this on a machine with GPU, as this requires\n# running inference on 10,000 images\nmodel = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\nembeddings = dataset.compute_embeddings(model)\n\n# Compute visualization\nresults = fob.compute_visualization(\n    dataset, embeddings=embeddings, seed=51, brain_key=\"img_viz\"\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>Did you know? You can programmatically configure your Spaces layout!</p> <p></p> <p>The GIF shows the variety of insights that are revealed by running this simple protocol:</p> <ul> <li> <p>The first cluster of points selected reveals a set of samples whose field of view is corrupted by hardware gradients at the top and bottom of the image</p> </li> <li> <p>The second cluster of points reveals a set of images in rainy conditions with water droplets on the windshield</p> </li> <li> <p>Hiding the primary cluster of <code>daytime</code> points and selecting the remaining <code>night</code> points reveals that the <code>night</code> points have incorrect labels</p> </li> </ul>"},{"location":"fiftyone_concepts/brain/#object-embeddings-example","title":"Object embeddings example \u00b6","text":"<p>The following example demonstrates how embeddings can be used to visualize the ground truth objects in the quickstart dataset using the <code>compute_visualization()</code> method\u2019s default embeddings model and dimensionality method.</p> <p>In this setup, we generate a visualization for all ground truth objects, but then we create a view that restricts the visualization to only objects in a subset of the classes. The scatterpoints in the Embeddings panel correspond to objects, colored by their <code>label</code>. When points are lasso-ed in the plot, the corresponding object patches are automatically selected in the Samples panel:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Generate visualization for `ground_truth` objects\nresults = fob.compute_visualization(\n    dataset, patches_field=\"ground_truth\", brain_key=\"gt_viz\"\n)\n\n# Restrict to the 10 most common classes\ncounts = dataset.count_values(\"ground_truth.detections.label\")\nclasses = sorted(counts, key=counts.get, reverse=True)[:10]\nview = dataset.filter_labels(\"ground_truth\", F(\"label\").is_in(classes))\n\nsession = fo.launch_app(view)\n</code></pre> <p>Note</p> <p>Did you know? You can programmatically configure your Spaces layout!</p> <p></p> <p>As you can see, the coloring of the scatterpoints allows you to discover natural clusters of objects, such as visually similar carrots or kites in the air.</p>"},{"location":"fiftyone_concepts/brain/#visualization-api","title":"Visualization API \u00b6","text":"<p>This section describes how to setup, create, and manage visualizations in detail.</p>"},{"location":"fiftyone_concepts/brain/#changing-your-visualization-method","title":"Changing your visualization method \u00b6","text":"<p>You can use a specific dimensionality reduction method for a particular visualization run by passing the <code>method</code> parameter to <code>compute_visualization()</code>:</p> <pre><code>index = fob.compute_visualization(..., method=\"&lt;method&gt;\", ...)\n</code></pre> <p>Alternatively, you can change your default dimensionality reduction method for an entire session by setting the <code>FIFTYONE_BRAIN_DEFAULT_VISUALIZATION_METHOD</code> environment variable:</p> <pre><code>export FIFTYONE_BRAIN_DEFAULT_VISUALIZATION_METHOD=&lt;method&gt;\n</code></pre> <p>Finally, you can permanently change your default dimensionality reduction method by updating the <code>default_visualization_method</code> key of your brain config at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"default_visualization_method\": \"&lt;method&gt;\",\n    \"visualization_methods\": {\n        \"&lt;method&gt;\": {...},\n        ...\n    }\n}\n</code></pre>"},{"location":"fiftyone_concepts/brain/#configuring-your-visualization-method","title":"Configuring your visualization method \u00b6","text":"<p>Dimensionality reduction methods may be configured in a variety of method-specific ways, which you can see by inspecting the parameters of a method\u2019s associated <code>VisualizationConfig</code> class.</p> <p>The relevant classes for the builtin dimensionality reduction methods are:</p> <ul> <li> <p>umap: <code>fiftyone.brain.visualization.UMAPVisualizationConfig</code></p> </li> <li> <p>tsne: <code>fiftyone.brain.visualization.TSNEVisualizationConfig</code></p> </li> <li> <p>pca: <code>fiftyone.brain.visualization.PCAVisualizationConfig</code></p> </li> <li> <p>manual: <code>fiftyone.brain.visualization.ManualVisualizationConfig</code></p> </li> </ul> <p>You can configure a dimensionality reduction method\u2019s parameters for a specific run by simply passing supported config parameters as keyword arguments each time you call <code>compute_visualization()</code>:</p> <pre><code>index = fob.compute_visualization(\n    ...\n    method=\"umap\",\n    min_dist=0.2,\n)\n</code></pre> <p>Alternatively, you can more permanently configure your dimensionality reduction method(s) via your brain config.</p>"},{"location":"fiftyone_concepts/brain/#similarity","title":"Similarity \u00b6","text":"<p>The FiftyOne Brain provides a <code>compute_similarity()</code> method that you can use to index the images or object patches in a dataset by similarity.</p> <p>Once you\u2019ve indexed a dataset by similarity, you can use the <code>sort_by_similarity()</code> view stage to programmatically sort your dataset by similarity to any image(s) or object patch(es) of your choice in your dataset. In addition, the App provides a convenient point-and-click interface for sorting by similarity with respect to an index on a dataset.</p> <p>Note</p> <p>Did you know? You can search by natural language using similarity indexes!</p>"},{"location":"fiftyone_concepts/brain/#embedding-methods_1","title":"Embedding methods \u00b6","text":"<p>Like embeddings visualization, similarity leverages deep embeddings to generate an index for a dataset.</p> <p>The <code>embeddings</code> and <code>model</code> parameters of <code>compute_similarity()</code> support a variety of ways to generate embeddings for your data:</p> <ul> <li> <p>Provide nothing, in which case a default general purpose model is used to index your data</p> </li> <li> <p>Provide a <code>Model</code> instance or the name of any model from the Model Zoo that supports embeddings</p> </li> <li> <p>Provide your own precomputed embeddings in array form</p> </li> <li> <p>Provide the name of a <code>VectorField</code> or <code>ArrayField</code> of your dataset in which precomputed embeddings are stored</p> </li> </ul>"},{"location":"fiftyone_concepts/brain/#similarity-backends","title":"Similarity backends \u00b6","text":"<p>By default, all similarity indexes are served using a builtin scikit-learn backend, but you can pass the optional <code>backend</code> parameter to <code>compute_similarity()</code> to switch to another supported backend:</p> <ul> <li> <p>sklearn ( default): a scikit-learn backend</p> </li> <li> <p>qdrant: a Qdrant backend</p> </li> <li> <p>redis: a Redis backend</p> </li> <li> <p>pinecone: a Pinecone backend</p> </li> <li> <p>mongodb: a MongoDB backend</p> </li> <li> <p>elasticsearch: a Elasticsearch backend</p> </li> <li> <p>milvus: a Milvus backend</p> </li> <li> <p>lancedb: a LanceDB backend</p> </li> </ul> <pre><code>import fiftyone.brain as fob\n\nresults = fob.compute_similarity(\n    dataset,\n    backend=\"sklearn\",  # \"sklearn\", \"qdrant\", \"redis\", etc\n    brain_key=\"...\",\n    ...\n)\n</code></pre> <p>Note</p> <p>Refer to this section for more information about creating, managing and deleting similarity indexes.</p>"},{"location":"fiftyone_concepts/brain/#image-similarity","title":"Image similarity \u00b6","text":"<p>This section demonstrates the basic workflow of:</p> <ul> <li> <p>Indexing an image dataset by similarity</p> </li> <li> <p>Using the App\u2019s image similarity UI to query by visual similarity</p> </li> <li> <p>Using the SDK\u2019s <code>sort_by_similarity()</code> view stage to programmatically query the index</p> </li> </ul> <p>To index a dataset by image similarity, pass the <code>Dataset</code> or <code>DatasetView</code> of interest to <code>compute_similarity()</code> along with a name for the index via the <code>brain_key</code> argument.</p> <p>Next load the dataset in the App and select some image(s). Whenever there is an active selection in the App, a similarity icon will appear above the grid, enabling you to sort by similarity to your current selection.</p> <p>You can use the advanced settings menu to choose between multiple brain keys and optionally specify a maximum number of matches to return ( <code>k</code>) and whether to query by greatest or least similarity (if supported).</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Index images by similarity\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"img_sim\",\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>In the example above, we specify a zoo model with which to generate embeddings, but you can also provide precomputed embeddings.</p> <p></p> <p>Alternatively, you can use the <code>sort_by_similarity()</code> view stage to programmatically construct a view that contains the sorted results:</p> <pre><code># Choose a random image from the dataset\nquery_id = dataset.take(1).first().id\n\n# Programmatically construct a view containing the 15 most similar images\nview = dataset.sort_by_similarity(query_id, k=15, brain_key=\"img_sim\")\n\nsession.view = view\n</code></pre> <p>Note</p> <p>Performing a similarity search on a <code>DatasetView</code> will only return results from the view; if the view contains samples that were not included in the index, they will never be included in the result.</p> <p>This means that you can index an entire <code>Dataset</code> once and then perform searches on subsets of the dataset by constructing views that contain the images of interest.</p> <p>Note</p> <p>For large datasets, you may notice longer load times the first time you use a similarity index in a session. Subsequent similarity searches will use cached results and will be faster!</p>"},{"location":"fiftyone_concepts/brain/#object-similarity","title":"Object similarity \u00b6","text":"<p>This section demonstrates the basic workflow of:</p> <ul> <li> <p>Indexing a dataset of objects by similarity</p> </li> <li> <p>Using the App\u2019s object similarity UI to query by visual similarity</p> </li> <li> <p>Using the SDK\u2019s <code>sort_by_similarity()</code> view stage to programmatically query the index</p> </li> </ul> <p>You can index any objects stored on datasets in <code>Detection</code>, <code>Detections</code>, <code>Polyline</code>, or <code>Polylines</code> format. See this section for more information about adding labels to your datasets.</p> <p>To index by object patches, simply pass the <code>Dataset</code> or <code>DatasetView</code> of interest to <code>compute_similarity()</code> along with the name of the patches field and a name for the index via the <code>brain_key</code> argument.</p> <p>Next load the dataset in the App and switch to object patches view by clicking the patches icon above the grid and choosing the label field of interest from the dropdown.</p> <p>Now whenever you have selected one or more patches in the App, a similarity icon will appear above the grid, enabling you to sort by similarity to your current selection.</p> <p>You can use the advanced settings menu to choose between multiple brain keys and optionally specify a maximum number of matches to return ( <code>k</code>) and whether to query by greatest or least similarity (if supported).</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Index ground truth objects by similarity\nfob.compute_similarity(\n    dataset,\n    patches_field=\"ground_truth\",\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"gt_sim\",\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>In the example above, we specify a zoo model with which to generate embeddings, but you can also provide precomputed embeddings.</p> <p></p> <p>Alternatively, you can directly use the <code>sort_by_similarity()</code> view stage to programmatically construct a view that contains the sorted results:</p> <pre><code># Convert to patches view\npatches = dataset.to_patches(\"ground_truth\")\n\n# Choose a random patch object from the dataset\nquery_id = patches.take(1).first().id\n\n# Programmatically construct a view containing the 15 most similar objects\nview = patches.sort_by_similarity(query_id, k=15, brain_key=\"gt_sim\")\n\nsession.view = view\n</code></pre> <p>Note</p> <p>Performing a similarity search on a <code>DatasetView</code> will only return results from the view; if the view contains objects that were not included in the index, they will never be included in the result.</p> <p>This means that you can index an entire <code>Dataset</code> once and then perform searches on subsets of the dataset by constructing views that contain the objects of interest.</p> <p>Note</p> <p>For large datasets, you may notice longer load times the first time you use a similarity index in a session. Subsequent similarity searches will use cached results and will be faster!</p>"},{"location":"fiftyone_concepts/brain/#text-similarity","title":"Text similarity \u00b6","text":"<p>When you create a similarity index powered by the CLIP model, you can also search by arbitrary natural language queries natively in the App!</p> <p></p> <p>You can also perform text queries via the SDK by passing a prompt directly to <code>sort_by_similarity()</code> along with the <code>brain_key</code> of a compatible similarity index:</p> <p>Note</p> <p>In general, any custom model that is made available via the model zoo interface that implements the <code>PromptMixin</code> interface can support text similarity queries!</p>"},{"location":"fiftyone_concepts/brain/#similarity-api","title":"Similarity API \u00b6","text":"<p>This section describes how to setup, create, and manage similarity indexes in detail.</p>"},{"location":"fiftyone_concepts/brain/#changing-your-similarity-backend","title":"Changing your similarity backend \u00b6","text":"<p>You can use a specific backend for a particular similarity index by passing the <code>backend</code> parameter to <code>compute_similarity()</code>:</p> <pre><code>index = fob.compute_similarity(..., backend=\"&lt;backend&gt;\", ...)\n</code></pre> <p>Alternatively, you can change your default similarity backend for an entire session by setting the <code>FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND</code> environment variable.</p> <pre><code>export FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND=&lt;backend&gt;\n</code></pre> <p>Finally, you can permanently change your default similarity backend by updating the <code>default_similarity_backend</code> key of your brain config at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"default_similarity_backend\": \"&lt;backend&gt;\",\n    \"similarity_backends\": {\n        \"&lt;backend&gt;\": {...},\n        ...\n    }\n}\n</code></pre>"},{"location":"fiftyone_concepts/brain/#configuring-your-backend","title":"Configuring your backend \u00b6","text":"<p>Similarity backends may be configured in a variety of backend-specific ways, which you can see by inspecting the parameters of a backend\u2019s associated <code>SimilarityConfig</code> class.</p> <p>The relevant classes for the builtin similarity backends are:</p> <ul> <li> <p>sklearn: <code>fiftyone.brain.internal.core.sklearn.SklearnSimilarityConfig</code></p> </li> <li> <p>qdrant: <code>fiftyone.brain.internal.core.qdrant.QdrantSimilarityConfig</code></p> </li> <li> <p>redis: <code>fiftyone.brain.internal.core.redis.RedisSimilarityConfig</code></p> </li> <li> <p>pinecone: <code>fiftyone.brain.internal.core.pinecone.PineconeSimilarityConfig</code></p> </li> <li> <p>mongodb: <code>fiftyone.brain.internal.core.mongodb.MongoDBSimilarityConfig</code></p> </li> <li> <p>elasticsearch: a fiftyone.brain.internal.core.elasticsearch.ElasticsearchSimilarityConfig</p> </li> <li> <p>milvus: <code>fiftyone.brain.internal.core.milvus.MilvusSimilarityConfig</code></p> </li> <li> <p>lancedb: <code>fiftyone.brain.internal.core.lancedb.LanceDBSimilarityConfig</code></p> </li> </ul> <p>You can configure a similarity backend\u2019s parameters for a specific index by simply passing supported config parameters as keyword arguments each time you call <code>compute_similarity()</code>:</p> <pre><code>index = fob.compute_similarity(\n    ...\n    backend=\"qdrant\",\n    url=\"http://localhost:6333\",\n)\n</code></pre> <p>Alternatively, you can more permanently configure your backend(s) via your brain config.</p>"},{"location":"fiftyone_concepts/brain/#creating-an-index","title":"Creating an index \u00b6","text":"<p>The <code>compute_similarity()</code> method provides a number of different syntaxes for initializing a similarity index. Let\u2019s see some common patterns on the quickstart dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n</code></pre>"},{"location":"fiftyone_concepts/brain/#default-behavior","title":"Default behavior \u00b6","text":"<p>With no arguments, embeddings will be automatically computed for all images or patches in the dataset using a default model and added to a new index in your default backend:</p>"},{"location":"fiftyone_concepts/brain/#custom-model-custom-backend-add-embeddings-later","title":"Custom model, custom backend, add embeddings later \u00b6","text":"<p>With the syntax below, we\u2019re specifying a similarity backend of our choice, specifying a custom model from the Model Zoo to use to generate embeddings, and using the <code>embeddings=False</code> syntax to create the index without initially adding any embeddings to it:</p>"},{"location":"fiftyone_concepts/brain/#precomputed-embeddings","title":"Precomputed embeddings \u00b6","text":"<p>You can pass precomputed image or object embeddings to <code>compute_similarity()</code> via the <code>embeddings</code> argument:</p>"},{"location":"fiftyone_concepts/brain/#adding-embeddings-to-an-index","title":"Adding embeddings to an index \u00b6","text":"<p>You can use <code>add_to_index()</code> to add new embeddings or overwrite existing embeddings in an index at any time:</p> <p>Note</p> <p>When using the default <code>sklearn</code> backend, you must manually call <code>save()</code> after adding or removing embeddings from an index in order to save the index to the database. This is not required when using external vector databases like Qdrant.</p> <p>Note</p> <p>Did you know? If you provided the name of a zoo model when creating the similarity index, you can use <code>get_model()</code> to load the model later. Or, you can use <code>compute_embeddings()</code> to conveniently generate embeddings for new samples/objects using the index\u2019s model.</p>"},{"location":"fiftyone_concepts/brain/#retrieving-embeddings-in-an-index","title":"Retrieving embeddings in an index \u00b6","text":"<p>You can use <code>get_embeddings()</code> to retrieve the embeddings for any or all IDs of interest from an existing index:</p>"},{"location":"fiftyone_concepts/brain/#removing-embeddings-from-an-index","title":"Removing embeddings from an index \u00b6","text":"<p>You can use <code>remove_from_index()</code> to delete embeddings from an index by their ID:</p> <p>Note</p> <p>When using the default <code>sklearn</code> backend, you must manually call <code>save()</code> after adding or removing embeddings from an index in order to save the index to the database.</p> <p>This is not required when using external vector databases like Qdrant.</p>"},{"location":"fiftyone_concepts/brain/#deleting-an-index","title":"Deleting an index \u00b6","text":"<p>When working with backends like Qdrant that leverage external vector databases, you can call <code>cleanup()</code> to delete the external index/collection:</p> <p>Note</p> <p>Calling <code>cleanup()</code> has no effect when working with the default sklearn backend. The index is deleted only when you call <code>delete_brain_run()</code>.</p>"},{"location":"fiftyone_concepts/brain/#applications_1","title":"Applications \u00b6","text":"<p>How can similarity be used in practice? A common pattern is to mine your dataset for similar examples to certain images or object patches of interest, e.g., those that represent failure modes of a model that need to be studied in more detail or underrepresented classes that need more training examples.</p> <p>Here are a few of the many possible applications:</p> <ul> <li> <p>Pruning near-duplicate images from your training dataset</p> </li> <li> <p>Identifying failure patterns of a model</p> </li> <li> <p>Finding examples of target scenarios in your data lake</p> </li> <li> <p>Mining hard examples for your evaluation pipeline</p> </li> <li> <p>Recommending samples from your data lake for classes that need additional training data</p> </li> </ul>"},{"location":"fiftyone_concepts/brain/#leaky-splits","title":"Leaky splits \u00b6","text":"<p>Despite our best efforts, duplicates and other forms of non-IID samples show up in our data. When these samples end up in different splits, this can have consequences when evaluating a model. It can often be easy to overestimate model capability due to this issue. The FiftyOne Brain offers a way to identify such cases in dataset splits.</p> <p>The leaks of a dataset can be computed directly without the need for the predictions of a pre-trained model via the <code>compute_leaky_splits()</code> method:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\n\ndataset = fo.load_dataset(...)\n\n# Splits defined via tags\nsplit_tags = [\"train\", \"test\"]\nindex = fob.compute_leaky_splits(dataset, splits=split_tags)\nleaks = index.leaks_view()\n\n# Splits defined via field\nsplit_field = \"split\"  # holds split values e.g. 'train' or 'test'\nindex = fob.compute_leaky_splits(dataset, splits=split_field)\nleaks = index.leaks_view()\n\n# Splits defined via views\nsplit_views = {\"train\": train_view, \"test\": test_view}\nindex = fob.compute_leaky_splits(dataset, splits=split_views)\nleaks = index.leaks_view()\n</code></pre> <p>Notice how the splits of the dataset can be defined in three ways: through sample tags, through a string field that assigns each split a unique value in the field, or by directly providing views that define the splits.</p> <p>Input: A <code>Dataset</code> or <code>DatasetView</code>, and a definition of splits through one of tags, a field, or views.</p> <p>Output: An index that will allow you to look through your leaks with <code>leaks_view()</code> and also provides some useful actions once they are discovered such as automatically cleaning the dataset with <code>no_leaks_view()</code> or tagging the leaks for the future action with <code>tag_leaks()</code>.</p> <p>What to expect: Leaky splits works by embedding samples with a powerful model and finding very close samples in different splits in this space. Large, powerful models that were not trained on a dataset can provide insight into visual and semantic similarity between images, without creating further leaks in the process.</p> <p>Similarity index: Under the hood, leaky splits leverages the brain\u2019s <code>SimilarityIndex</code> to detect leaks. Any similarity backend that implements the <code>DuplicatesMixin</code> can be used to compute leaky splits. You can either pass an existing similarity index by passing its brain key to the argument <code>similarity_index</code>, or have the method create one on the fly for you.</p> <p>Embeddings: You can customize the model used to compute embeddings via the <code>model</code> argument of <code>compute_leaky_splits()</code>. You can also precompute embeddings and tell leaky splits to use them by passing them via the <code>embeddings</code> argument.</p> <p>Thresholds: Leaky splits uses a threshold to decide what samples are too close and thus mark them as potential leaks. This threshold can be customized either by passing a value to the <code>threshold</code> argument of <code>compute_leaky_splits()</code>. The best value for your use case may vary depending on your dataset, as well as the embeddings used. A threshold that\u2019s too big may have a lot of false positives, while a threshold that\u2019s too small may have a lot of false negatives.</p> <p>The example code below runs leaky splits analysis on the COCO dataset. Try it for yourself and see what you find!</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\nimport fiftyone.utils.random as four\n\n# Load some COCO data\ndataset = foz.load_zoo_dataset(\"coco-2017\", split=\"test\")\n\n# Set up splits via tags\ndataset.untag_samples(dataset.distinct(\"tags\"))\nfour.random_split(dataset, {\"train\": 0.7, \"test\": 0.3})\n\n# Find leaks\nindex = fob.compute_leaky_splits(dataset, splits=[\"train\", \"test\"])\nleaks = index.leaks_view()\n</code></pre> <p>The <code>leaks_view()</code> method returns a view that contains only the leaks in the input splits. Once you have these leaks, it is wise to look through them. You may gain some insight into the source of the leaks:</p> <pre><code>session = fo.launch_app(leaks)\n</code></pre> <p>Before evaluating your model on your test set, consider getting a version of it with the leaks removed. This can be easily done via <code>no_leaks_view()</code>:</p> <pre><code># The original test split\ntest_set = index.split_views[\"test\"]\n\n# The test set with leaks removed\ntest_set_no_leaks = index.no_leaks_view(test_set)\n\nsession.view = test_set_no_leaks\n</code></pre> <p>Performance on the clean test set will can be closer to the performance of the model in the wild. If you found some leaks in your dataset, consider comparing performance on the base test set against the clean test set.</p> <p></p>"},{"location":"fiftyone_concepts/brain/#near-duplicates","title":"Near duplicates \u00b6","text":"<p>When curating massive datasets, you may inadvertently add near duplicate data to your datasets, which can bias or otherwise confuse your models.</p> <p>The <code>compute_near_duplicates()</code> method leverages embeddings to automatically surface near-duplicate samples in your dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\n\ndataset = fo.load_dataset(...)\n\nindex = fob.compute_near_duplicates(dataset)\nprint(index.duplicate_ids)\n\ndups_view = index.duplicates_view()\nsession = fo.launch_app(dups_view)\n</code></pre> <p>Input: An unlabeled (or labeled) dataset. There are recipes for building datasets from a wide variety of image formats, ranging from a simple directory of images to complicated dataset structures like COCO.</p> <p>Output: A <code>SimilarityIndex</code> object that provides powerful methods such as <code>duplicate_ids</code>, <code>neighbors_map</code> and <code>duplicates_view()</code> to analyze potential near duplicates as demonstrated below</p> <p>What to expect: Near duplicates analysis leverages embeddings to identify samples that are too close to their nearest neighbors. You can provide pre-computed embeddings, specify a zoo model of your choice to use to compute embeddings, or provide nothing and rely on the method\u2019s default model to generate embeddings.</p> <p>Thresholds: When using custom embeddings/models, you may need to adjust the distance threshold used to detect potential duplicates. You can do this by passing a value to the <code>threshold</code> argument of <code>compute_near_duplicates()</code>. The best value for your use case may vary depending on your dataset, as well as the embeddings used. A threshold that\u2019s too big may have a lot of false positives, while a threshold that\u2019s too small may have a lot of false negatives.</p> <p>The following example demonstrates how to use <code>compute_near_duplicates()</code> to detect near duplicate images on the CIFAR-10 dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\n</code></pre> <p>To proceed, we first need some suitable image embeddings for the dataset. Although the <code>compute_near_duplicates()</code> method is equipped with a default general-purpose model to generate embeddings if none are provided, you\u2019ll typically find higher-quality insights when a domain-specific model is used to generate embeddings.</p> <p>In this case, we\u2019ll use a classifier that has been fine-tuned on CIFAR-10 to pre-compute embeddings and them feed them to <code>compute_near_duplicates()</code>:</p> <pre><code>import fiftyone.brain as fob\nimport fiftyone.brain.internal.models as fbm\n\n# Compute embeddings via a pre-trained CIFAR-10 classifier\nmodel = fbm.load_model(\"simple-resnet-cifar10\")\nembeddings = dataset.compute_embeddings(model, batch_size=16)\n\n# Scan for near-duplicates\nindex = fob.compute_near_duplicates(\n    dataset,\n    embeddings=embeddings,\n    thresh=0.02,\n)\n</code></pre>"},{"location":"fiftyone_concepts/brain/#finding-near-duplicate-samples","title":"Finding near-duplicate samples \u00b6","text":"<p>The <code>neighbors_map</code> property of the index provides a data structure that summarizes the findings. The keys of the dictionary are the sample IDs of each non-duplicate sample, and the values are lists of <code>(id, distance)</code> tuples listing the sample IDs of the duplicate samples for each reference sample together with the embedding distance between the two samples:</p> <pre><code>print(index.neighbors_map)\n</code></pre> <pre><code>{\n    '61143408db40df926c571a6b': [\\\n        ('61143409db40df926c573075', 5.667297674385298),\\\n        ('61143408db40df926c572ab6', 6.231051661334058)\\\n    ],\n    '6114340cdb40df926c577f2a': [\\\n        ('61143408db40df926c572b54', 6.042934361555487)\\\n    ],\n    '61143408db40df926c572aa3': [\\\n        ('6114340bdb40df926c5772e9', 5.88984758067434),\\\n        ('61143408db40df926c572b64', 6.063986454046798),\\\n        ('61143409db40df926c574571', 6.10303338363576),\\\n        ('6114340adb40df926c5749a2', 6.161749290179865)\\\n    ],\n    ...\n}\n</code></pre> <p>We can conveniently visualize this information in the App via the <code>duplicates_view()</code> method of the index, which constructs a view with the duplicate samples arranged directly after their corresponding reference sample, with optional additional fields recording the type and nearest reference sample ID/distance:</p> <pre><code>duplicates_view = index.duplicates_view(\n    type_field=\"dup_type\",\n    id_field=\"dup_id\",\n    dist_field=\"dup_dist\",\n)\n\nsession = fo.launch_app(duplicates_view)\n</code></pre> <p></p> <p>Note</p> <p>You can also use the <code>find_duplicates()</code> method of the index to rerun the duplicate detection with a different <code>threshold</code> without calling <code>compute_near_duplicates()</code> again.</p>"},{"location":"fiftyone_concepts/brain/#finding-maximally-unique-samples","title":"Finding maximally unique samples \u00b6","text":"<p>You can also use the <code>find_unique()</code> method of the index to identify a set of samples of any desired size that are maximally unique with respect to each other:</p> <pre><code># Use the similarity index to identify 500 maximally unique samples\nindex.find_unique(500)\nprint(index.unique_ids[:5])\n</code></pre> <p>We can also conveniently visualize the results of this operation via the <code>visualize_unique()</code> method of the index, which generates a scatterplot with the unique samples colored separately:</p> <pre><code># Generate a 2D visualization\nviz_results = fob.compute_visualization(dataset, embeddings=embeddings)\n\n# Visualize the unique samples in embeddings space\nplot = index.visualize_unique(viz_results)\nplot.show(height=800, yaxis_scaleanchor=\"x\")\n</code></pre> <p></p> <p>And of course we can load a view containing the unique samples in the App to explore the results in detail:</p> <pre><code># Visualize the unique images in the App\nunique_view = dataset.select(index.unique_ids)\nsession = fo.launch_app(view=unique_view)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/brain/#exact-duplicates","title":"Exact duplicates \u00b6","text":"<p>Despite your best efforts, you may accidentally add duplicate data to a dataset. Left unmitigated, such quality issues can bias your models and confound your analysis.</p> <p>The <code>compute_exact_duplicates()</code> method scans your dataset and determines if you have duplicate data either under the same or different filenames:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\n\ndataset = fo.load_dataset(...)\n\nduplicates_map = fob.compute_exact_duplicates(dataset)\nprint(duplicates_map)\n</code></pre> <p>Input: An unlabeled (or labeled) dataset. There are recipes for building datasets from a wide variety of image formats, ranging from a simple directory of images to complicated dataset structures like COCO.</p> <p>Output: A dictionary mapping IDs of samples with exact duplicates to lists of IDs of the duplicates for the corresponding sample</p> <p>What to expect: Exact duplicates analysis uses filehases to identify duplicate data, regardless of whether they are stored under the same or different filepaths in your dataset.</p>"},{"location":"fiftyone_concepts/brain/#image-uniqueness","title":"Image uniqueness \u00b6","text":"<p>The FiftyOne Brain allows for the computation of the uniqueness of an image, in comparison with other images in a dataset; it does so without requiring any model from you. One good use of uniqueness is in the early stages of the machine learning workflow when you are deciding what subset of data with which to bootstrap your models. Unique samples are vital in creating training batches that help your model learn as efficiently and effectively as possible.</p> <p>The uniqueness of a <code>Dataset</code> can be computed directly without need the predictions of a pre-trained model via the <code>compute_uniqueness()</code> method:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\n\ndataset = fo.load_dataset(...)\n\nfob.compute_uniqueness(dataset)\n</code></pre> <p>Input: An unlabeled (or labeled) image dataset. There are recipes for building datasets from a wide variety of image formats, ranging from a simple directory of images to complicated dataset structures like COCO.</p> <p>Note</p> <p>Did you know? Instead of using FiftyOne\u2019s default model to generate embeddings, you can provide your own embeddings or specify a model from the Model Zoo to use to generate embeddings via the optional <code>embeddings</code> and <code>model</code> argument to <code>compute_uniqueness()</code>.</p> <p>Output: A scalar-valued <code>uniqueness</code> field is populated on each sample that ranks the uniqueness of that sample (higher value means more unique). The uniqueness values for a dataset are normalized to <code>[0, 1]</code>, with the most unique sample in the collection having a uniqueness value of <code>1</code>.</p> <p>You can customize the name of this field by passing the optional <code>uniqueness_field</code> argument to <code>compute_uniqueness()</code>.</p> <p>What to expect: Uniqueness uses a tuned algorithm that measures the distribution of each <code>Sample</code> in the <code>Dataset</code>. Using this distribution, it ranks each sample based on its relative similarity to other samples. Those that are close to other samples are not unique whereas those that are far from most other samples are more unique.</p> <p>Note</p> <p>Did you know? You can specify a region of interest within each image to use to compute uniqueness by providing the optional <code>roi_field</code> argument to <code>compute_uniqueness()</code>, which contains <code>Detections</code> or <code>Polylines</code> that define the ROI for each sample.</p> <p>Note</p> <p>Check out the uniqueness tutorial to see an example use case of the Brain\u2019s uniqueness method to detect near-duplicate images in a dataset.</p> <p></p>"},{"location":"fiftyone_concepts/brain/#label-mistakes","title":"Label mistakes \u00b6","text":"<p>Label mistakes can be calculated for both classification and detection datasets.</p> <p></p>"},{"location":"fiftyone_concepts/brain/#sample-hardness","title":"Sample hardness \u00b6","text":"<p>During training, it is useful to identify samples that are more difficult for a model to learn so that training can be more focused around these hard samples. These hard samples are also useful as seeds when considering what other new samples to add to a training dataset.</p> <p>In order to compute hardness, all you need to do is add your model predictions and their logits to your FiftyOne <code>Dataset</code> and then run the <code>compute_hardness()</code> method:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\n\ndataset = fo.load_dataset(...)\n\nfob.compute_hardness(dataset, \"predictions\")\n</code></pre> <p>Input: A <code>Dataset</code> or <code>DatasetView</code> on which predictions have been computed and are stored in the <code>\"predictions\"</code> argument. Ground truth annotations are not required for hardness.</p> <p>Output: A scalar-valued <code>hardness</code> field is populated on each sample that ranks the hardness of the sample. You can customize the name of this field via the <code>hardness_field</code> argument of <code>compute_hardness()</code>.</p> <p>What to expect: Hardness is computed in the context of a prediction model. The FiftyOne Brain hardness measure defines hard samples as those for which the prediction model is unsure about what label to assign. This measure incorporates prediction confidence and logits in a tuned model that has demonstrated empirical value in many model training exercises.</p> <p>Note</p> <p>Check out the classification evaluation tutorial to see example uses of the Brain\u2019s hardness method to uncover annotation mistakes in a dataset.</p> <p></p>"},{"location":"fiftyone_concepts/brain/#image-representativeness","title":"Image representativeness \u00b6","text":"<p>During the early stages of the ML workflow it can be useful to find prototypical samples in your data that accurately describe all the different aspects of your data. FiftyOne Brain provides a representativeness method that finds samples which are very similar to large clusters of your data. Highly representative samples are great for finding modes or easy examples in your dataset.</p> <p>The representativeness of a <code>Dataset</code> can be computed directly without the need for the predictions of a pre-trained model via the <code>compute_representativeness()</code> method:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\n\ndataset = fo.load_dataset(...)\n\nfob.compute_representativeness(dataset)\n</code></pre> <p>Input: An unlabeled (or labeled) image dataset. There are recipes for building datasets from a wide variety of image formats, ranging from a simple directory of images to complicated dataset structures like COCO.</p> <p>Output: A scalar-valued <code>representativeness</code> field is populated for each sample that ranks the representativeness of that sample (higher value means more representative). The representativeness values for a dataset are normalized to <code>[0, 1]</code>, with the most representative samples in the collection having a representativeness value of <code>1</code>.</p> <p>You can customize the name of this field by passing the optional <code>representativeness_field</code> argument to <code>compute_representativeness()</code> .</p> <p>What to expect: Representativeness uses a clustering algorithm to find similar looking groups of samples. The representativeness is then computed based on each sample\u2019s proximity to the computed cluster centers, farther samples being less representative and closer samples being more representative.</p> <p>Note</p> <p>Did you know? You can specify a region of interest within each image to use to compute representativeness by providing the optional <code>roi_field</code> argument to <code>compute_representativeness()</code>, which contains <code>Detections</code> or <code>Polylines</code> that define the ROI for each sample.</p> <p></p>"},{"location":"fiftyone_concepts/brain/#managing-brain-runs","title":"Managing brain runs \u00b6","text":"<p>When you run a brain method with a <code>brain_key</code> argument, the run is recorded on the dataset and you can retrieve information about it later, rename it, delete it (along with any modifications to your dataset that were performed by it), and even retrieve the view that you computed on using the following methods on your dataset:</p> <ul> <li> <p><code>list_brain_runs()</code></p> </li> <li> <p><code>get_brain_info()</code></p> </li> <li> <p><code>load_brain_results()</code></p> </li> <li> <p><code>load_brain_view()</code></p> </li> <li> <p><code>rename_brain_run()</code></p> </li> <li> <p><code>delete_brain_run()</code></p> </li> </ul> <p>The example below demonstrates the basic interface:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nview = dataset.take(100)\n\n# Run a brain method that returns results\nresults = fob.compute_visualization(view, brain_key=\"visualization\")\n\n# Run a brain method that populates a new sample field on the dataset\nfob.compute_uniqueness(view)\n\n# List the brain methods that have been run\nprint(dataset.list_brain_runs())\n# ['visualization', 'uniqueness']\n\n# Print information about a brain run\nprint(dataset.get_brain_info(\"visualization\"))\n\n# Load the results of a previous brain run\nalso_results = dataset.load_brain_results(\"visualization\")\n\n# Load the view on which a brain run was performed\nsame_view = dataset.load_brain_view(\"visualization\")\n\n# Rename a brain run\ndataset.rename_brain_run(\"visualization\", \"still_visualization\")\n\n# Delete brain runs\n# This will delete any stored results and fields that were populated\ndataset.delete_brain_run(\"still_visualization\")\ndataset.delete_brain_run(\"uniqueness\")\n</code></pre>"},{"location":"fiftyone_concepts/brain/#brain-config","title":"Brain config \u00b6","text":"<p>FiftyOne provides a brain config that you can use to either temporarily or permanently configure the behavior of brain methods.</p>"},{"location":"fiftyone_concepts/brain/#viewing-your-config","title":"Viewing your config \u00b6","text":"<p>You can print your current brain config at any time via the Python library and the CLI:</p> <p>Note</p> <p>If you have customized your brain config via any of the methods described below, printing your config is a convenient way to ensure that the changes you made have taken effect as you expected.</p>"},{"location":"fiftyone_concepts/brain/#modifying-your-config","title":"Modifying your config \u00b6","text":"<p>You can modify your brain config in a variety of ways. The following sections describe these options in detail.</p>"},{"location":"fiftyone_concepts/brain/#order-of-precedence","title":"Order of precedence \u00b6","text":"<p>The following order of precedence is used to assign values to your brain config settings as runtime:</p> <ol> <li> <p>Config settings applied at runtime by directly editing <code>fiftyone.brain.brain_config</code></p> </li> <li> <p><code>FIFTYONE_BRAIN_XXX</code> environment variables</p> </li> <li> <p>Settings in your JSON config ( <code>~/.fiftyone/brain_config.json</code>)</p> </li> <li> <p>The default config values</p> </li> </ol>"},{"location":"fiftyone_concepts/brain/#editing-your-json-config","title":"Editing your JSON config \u00b6","text":"<p>You can permanently customize your brain config by creating a <code>~/.fiftyone/brain_config.json</code> file on your machine. The JSON file may contain any desired subset of config fields that you wish to customize.</p> <p>For example, the following config JSON file customizes the URL of your Qdrant server without changing any other default config settings:</p> <pre><code>{\n    \"similarity_backends\": {\n        \"qdrant\": {\n            \"url\": \"http://localhost:8080\"\n        }\n    }\n}\n</code></pre> <p>When <code>fiftyone.brain</code> is imported, any options from your JSON config are merged into the default config, as per the order of precedence described above.</p> <p>Note</p> <p>You can customize the location from which your JSON config is read by setting the <code>FIFTYONE_BRAIN_CONFIG_PATH</code> environment variable.</p>"},{"location":"fiftyone_concepts/brain/#setting-environment-variables","title":"Setting environment variables \u00b6","text":"<p>Brain config settings may be customized on a per-session basis by setting the <code>FIFTYONE_BRAIN_XXX</code> environment variable(s) for the desired config settings.</p> <p>The <code>FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND</code> environment variable allows you to configure your default similarity backend:</p> <pre><code>export FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND=qdrant\n</code></pre> <p>Similarity backends</p> <p>You can declare parameters for specific similarity backends by setting environment variables of the form <code>FIFTYONE_BRAIN_SIMILARITY_&lt;BACKEND&gt;_&lt;PARAMETER&gt;</code>. Any settings that you declare in this way will be passed as keyword arguments to methods like <code>compute_similarity()</code> whenever the corresponding backend is in use. For example, you can configure the URL of your Qdrant server as follows:</p> <pre><code>export FIFTYONE_BRAIN_SIMILARITY_QDRANT_URL=http://localhost:8080\n</code></pre> <p>The <code>FIFTYONE_BRAIN_SIMILARITY_BACKENDS</code> environment variable can be set to a <code>list,of,backends</code> that you want to expose in your session, which may exclude native backends and/or declare additional custom backends whose parameters are defined via additional config modifications of any kind:</p> <pre><code>export FIFTYONE_BRAIN_SIMILARITY_BACKENDS=custom,sklearn,qdrant\n</code></pre> <p>When declaring new backends, you can include <code>*</code> to append new backend(s) without omitting or explicitly enumerating the builtin backends. For example, you can add a <code>custom</code> similarity backend as follows:</p> <pre><code>export FIFTYONE_BRAIN_SIMILARITY_BACKENDS=*,custom\nexport FIFTYONE_BRAIN_SIMILARITY_CUSTOM_CONFIG_CLS=your.custom.SimilarityConfig\n</code></pre> <p>Visualization methods</p> <p>You can declare parameters for specific visualization methods by setting environment variables of the form <code>FIFTYONE_BRAIN_VISUALIZATION_&lt;METHOD&gt;_&lt;PARAMETER&gt;</code>. Any settings that you declare in this way will be passed as keyword arguments to methods like <code>compute_visualization()</code> whenever the corresponding method is in use. For example, you can suppress logging messages for the UMAP method as follows:</p> <pre><code>export FIFTYONE_BRAIN_VISUALIZATION_UMAP_VERBOSE=false\n</code></pre> <p>The <code>FIFTYONE_BRAIN_VISUALIZATION_METHODS</code> environment variable can be set to a <code>list,of,methods</code> that you want to expose in your session, which may exclude native methods and/or declare additional custom methods whose parameters are defined via additional config modifications of any kind:</p> <pre><code>export FIFTYONE_BRAIN_VISUALIZATION_METHODS=custom,umap,tsne\n</code></pre> <p>When declaring new methods, you can include <code>*</code> to append new method(s) without omitting or explicitly enumerating the builtin methods. For example, you can add a <code>custom</code> visualization method as follows:</p> <pre><code>export FIFTYONE_BRAIN_VISUALIZATION_METHODS=*,custom\nexport FIFTYONE_BRAIN_VISUALIZATION_CUSTOM_CONFIG_CLS=your.custom.VisualzationConfig\n</code></pre>"},{"location":"fiftyone_concepts/brain/#modifying-your-config-in-code","title":"Modifying your config in code \u00b6","text":"<p>You can dynamically modify your brain config at runtime by directly editing the <code>fiftyone.brain.brain_config</code> object.</p> <p>Any changes to your brain config applied via this manner will immediately take effect in all subsequent calls to <code>fiftyone.brain.brain_config</code> during your current session.</p> <pre><code>import fiftyone.brain as fob\n\nfob.brain_config.default_similarity_backend = \"qdrant\"\nfob.brain_config.default_visualization_method = \"tsne\"\n</code></pre>"},{"location":"fiftyone_concepts/config/","title":"Configuring FiftyOne \u00b6","text":"<p>FiftyOne can be configured in various ways. This guide covers the various options that exist, how to view your current config, and how to customize your config as desired.</p>"},{"location":"fiftyone_concepts/config/#configuration-options","title":"Configuration options \u00b6","text":"<p>FiftyOne supports the configuration options described below:</p> Config field Environment variable Default value Description <code>database_admin</code> <code>FIFTYONE_DATABASE_ADMIN</code> <code>True</code> Whether the client is allowed to trigger database migrations. Seethis section for more information. <code>database_dir</code> <code>FIFTYONE_DATABASE_DIR</code> <code>~/.fiftyone/var/lib/mongo</code> The directory in which to store FiftyOne\u2019s backing database. Only applicable if<code>database_uri</code> is not defined. <code>database_name</code> <code>FIFTYONE_DATABASE_NAME</code> <code>fiftyone</code> A name to use for FiftyOne\u2019s backing database in your MongoDB instance. The databaseis automatically created if necessary. <code>database_uri</code> <code>FIFTYONE_DATABASE_URI</code> <code>None</code> A MongoDB URI tospecifying a custom MongoDB database to which to connect. Seethis section for more information. <code>database_validation</code> <code>FIFTYONE_DATABASE_VALIDATION</code> <code>True</code> Whether to validate the compatibility of database before connecting to it. Seethis section for more information. <code>dataset_zoo_dir</code> <code>FIFTYONE_DATASET_ZOO_DIR</code> <code>~/fiftyone</code> The default directory in which to store datasets that are downloaded from theFiftyOne Dataset Zoo. <code>dataset_zoo_manifest_paths</code> <code>FIFTYONE_ZOO_MANIFEST_PATHS</code> <code>None</code> A list of manifest JSON files specifying additional zoo datasets. Seeadding datasets to the zoo for more information. <code>default_dataset_dir</code> <code>FIFTYONE_DEFAULT_DATASET_DIR</code> <code>~/fiftyone</code> The default directory to use when performing FiftyOne operations thatrequire writing dataset contents to disk, such as ingesting datasets via<code>ingest_labeled_images()</code>. <code>default_ml_backend</code> <code>FIFTYONE_DEFAULT_ML_BACKEND</code> <code>torch</code> The default ML backend to use when performing operations such asdownloading datasets from the FiftyOne Dataset Zoo that support multiple MLbackends. Supported values are <code>torch</code> and <code>tensorflow</code>. By default,<code>torch</code> is used if PyTorch is installed in yourPython environment, and <code>tensorflow</code> is used ifTensorFlow is installed. If no supported backendis detected, this defaults to <code>None</code>, and any operation that requires aninstalled ML backend will raise an informative error message if invoked inthis state. <code>default_batch_size</code> <code>FIFTYONE_DEFAULT_BATCH_SIZE</code> <code>None</code> A default batch size to use when applying models to datasets. <code>default_batcher</code> <code>FIFTYONE_DEFAULT_BATCHER</code> <code>latency</code> Batching implementation to use in some batched database operations such as<code>add_samples()</code>,<code>set_values()</code>, and<code>save_context()</code>.Supported values are <code>latency</code>, <code>size</code>, and <code>static</code>.<code>latency</code> is the default, which uses a dynamic batch size to achieve a target latencyof <code>batcher_target_latency</code> between calls. The default changes to <code>size</code> for theFiftyOne Teams SDK in API connection mode, which targetsa size of <code>batcher_target_size_bytes</code> for each call. <code>static</code> uses a fixed batch sizeof <code>batcher_static_size</code>. <code>batcher_static_size</code> <code>FIFTYONE_BATCHER_STATIC_SIZE</code> <code>100</code> Fixed size of batches. Only used when <code>default_batcher</code> is <code>static</code>. <code>batcher_target_size_bytes</code> <code>FIFTYONE_BATCHER_TARGET_SIZE_BYTES</code> <code>2 ** 20</code> Target content size of batches, in bytes. Only used when <code>default_batcher</code> is <code>size</code>. <code>batcher_target_latency</code> <code>FIFTYONE_BATCHER_TARGET_LATENCY</code> <code>0.2</code> Target latency between batches, in seconds. Only used when <code>default_batcher</code> is<code>latency</code>. <code>default_sequence_idx</code> <code>FIFTYONE_DEFAULT_SEQUENCE_IDX</code> <code>%06d</code> The default numeric string pattern to use when writing sequential lists offiles. <code>default_image_ext</code> <code>FIFTYONE_DEFAULT_IMAGE_EXT</code> <code>.jpg</code> The default image format to use when writing images to disk. <code>default_video_ext</code> <code>FIFTYONE_DEFAULT_VIDEO_EXT</code> <code>.mp4</code> The default video format to use when writing videos to disk. <code>default_app_port</code> <code>FIFTYONE_DEFAULT_APP_PORT</code> <code>5151</code> The default port to use to serve the FiftyOne App. <code>default_app_address</code> <code>FIFTYONE_DEFAULT_APP_ADDRESS</code> <code>localhost</code> The default address to use to serve the FiftyOne App. This maybe either an IP address or hostname. If it\u2019s a hostname, the App will listen to allIP addresses associated with the name. The default is <code>localhost</code>, which means the Appwill only listen on the local interface. See this pagefor more information. <code>do_not_track</code> <code>FIFTYONE_DO_NOT_TRACK</code> <code>False</code> Controls whether UUID based import and App usage events are tracked. <code>logging_level</code> <code>FIFTYONE_LOGGING_LEVEL</code> <code>INFO</code> Controls FiftyOne\u2019s package-wide logging level. Can be any valid <code>logging</code> level asa string: <code>DEBUG, INFO, WARNING, ERROR, CRITICAL</code>. <code>max_thread_pool_workers</code> <code>FIFTYONE_MAX_THREAD_POOL_WORKERS</code> <code>None</code> An optional maximum number of workers to use when creating thread pools <code>max_process_pool_workers</code> <code>FIFTYONE_MAX_PROCESS_POOL_WORKERS</code> <code>None</code> An optional maximum number of workers to use when creating process pools <code>model_zoo_dir</code> <code>FIFTYONE_MODEL_ZOO_DIR</code> <code>~/fiftyone/__models__</code> The default directory in which to store models that are downloaded from theFiftyOne Model Zoo. <code>model_zoo_manifest_paths</code> <code>FIFTYONE_MODEL_ZOO_MANIFEST_PATHS</code> <code>None</code> A list of manifest JSON files specifying additional zoo models. Seeadding models to the zoo for more information. <code>module_path</code> <code>FIFTYONE_MODULE_PATH</code> <code>None</code> A list of modules that should be automatically imported whenever FiftyOne is imported.See this page for an example usage. <code>operator_timeout</code> <code>FIFTYONE_OPERATOR_TIMEOUT</code> <code>600</code> The timeout for execution of an operator. See this page formore information. <code>allow_legacy_orchestrators</code> <code>FIFTYONE_ALLOW_LEGACY_ORCHESTRATORS</code> <code>False</code> Whether to allow delegated operations to be scheduled locally.See this page for more information. <code>plugins_dir</code> <code>FIFTYONE_PLUGINS_DIR</code> <code>None</code> A directory containing custom App plugins. See this page formore information. <code>plugins_cache_enabled</code> <code>FIFTYONE_PLUGINS_CACHE_ENABLED</code> <code>False</code> When set to <code>True</code> plugins will be cached until their directory\u2019s <code>mtime</code> changes.This is intended to be used in production. <code>do_not_track</code> <code>FIFTYONE_DO_NOT_TRACK</code> <code>False</code> Controls whether UUID based import and App usage events are tracked. <code>show_progress_bars</code> <code>FIFTYONE_SHOW_PROGRESS_BARS</code> <code>True</code> Controls whether progress bars are printed to the terminal when performingoperations such reading/writing large datasets or activating FiftyOneBrain methods on datasets. <code>timezone</code> <code>FIFTYONE_TIMEZONE</code> <code>None</code> An optional timezone string. If provided, all datetimes read from FiftyOne datasetswill be expressed in this timezone. See this section formore information."},{"location":"fiftyone_concepts/config/#viewing-your-config","title":"Viewing your config \u00b6","text":"<p>You can print your current FiftyOne config at any time via the Python library and the CLI:</p> <p>Note</p> <p>If you have customized your FiftyOne config via any of the methods described below, printing your config is a convenient way to ensure that the changes you made have taken effect as you expected.</p>"},{"location":"fiftyone_concepts/config/#modifying-your-config","title":"Modifying your config \u00b6","text":"<p>You can modify your FiftyOne config in a variety of ways. The following sections describe these options in detail.</p>"},{"location":"fiftyone_concepts/config/#order-of-precedence","title":"Order of precedence \u00b6","text":"<p>The following order of precedence is used to assign values to your FiftyOne config settings at runtime:</p> <ol> <li> <p>Config changes applied at runtime by directly editing <code>fiftyone.config</code></p> </li> <li> <p><code>FIFTYONE_XXX</code> environment variables</p> </li> <li> <p>Settings in your JSON config ( <code>~/.fiftyone/config.json</code>)</p> </li> <li> <p>The default config values</p> </li> </ol>"},{"location":"fiftyone_concepts/config/#editing-your-json-config","title":"Editing your JSON config \u00b6","text":"<p>You can permanently customize your FiftyOne config by creating a <code>~/.fiftyone/config.json</code> file on your machine. The JSON file may contain any desired subset of config fields that you wish to customize.</p> <p>For example, a valid config JSON file is:</p> <pre><code>{\n    \"default_ml_backend\": \"tensorflow\",\n    \"show_progress_bars\": true\n}\n</code></pre> <p>When <code>fiftyone</code> is imported, any options from your JSON config are applied, as per the order of precedence described above.</p> <p>Note</p> <p>You can customize the location from which your JSON config is read by setting the <code>FIFTYONE_CONFIG_PATH</code> environment variable.</p>"},{"location":"fiftyone_concepts/config/#setting-environment-variables","title":"Setting environment variables \u00b6","text":"<p>FiftyOne config settings may be customized on a per-session basis by setting the <code>FIFTYONE_XXX</code> environment variable(s) for the desired config settings.</p> <p>When <code>fiftyone</code> is imported, all config environment variables are applied, as per the order of precedence described above.</p> <p>For example, you can customize your FiftyOne config in a Terminal session by issuing the following commands prior to launching your Python interpreter:</p> <pre><code>export FIFTYONE_DEFAULT_ML_BACKEND=tensorflow\nexport FIFTYONE_SHOW_PROGRESS_BARS=true\n</code></pre>"},{"location":"fiftyone_concepts/config/#modifying-your-config-in-code","title":"Modifying your config in code \u00b6","text":"<p>You can dynamically modify your FiftyOne config at runtime by editing the <code>fiftyone.config</code> object.</p> <p>Any changes to your FiftyOne config applied via this manner will immediately take effect for all subsequent calls to <code>fiftyone.config</code> during your current session.</p> <pre><code>import fiftyone as fo\n\nfo.config.default_ml_backend = \"tensorflow\"\nfo.config.show_progress_bars = True\n</code></pre>"},{"location":"fiftyone_concepts/config/#configuring-a-mongodb-connection","title":"Configuring a MongoDB connection \u00b6","text":"<p>By default, FiftyOne is installed with its own MongoDB database distribution. This database is managed by FiftyOne automatically as a service that runs whenever at least one FiftyOne Python client is alive.</p> <p>Alternatively, you can configure FiftyOne to connect to your own self-managed MongoDB instance. To do so, simply set the <code>database_uri</code> property of your FiftyOne config to any valid MongoDB connection string URI.</p> <p>You can achieve this by adding the following entry to your <code>~/.fiftyone/config.json</code> file:</p> <pre><code>{\n    \"database_uri\": \"mongodb://[username:password@]host[:port]\"\n}\n</code></pre> <p>or you can set the following environment variable:</p> <pre><code>export FIFTYONE_DATABASE_URI=mongodb://[username:password@]host[:port]\n</code></pre> <p>If you are running MongoDB with authentication enabled (the <code>--auth</code> flag), FiftyOne must connect as a root user.</p> <p>You can create a root user with the Mongo shell as follows:</p> <pre><code>mongo --shell\n&gt; use admin\n&gt; db.createUser({user: \"username\", pwd: passwordPrompt(), roles: [\"root\"]})\n</code></pre> <p>You must also add <code>?authSource=admin</code> to your database URI:</p> <pre><code>mongodb://[username:password@]host[:port]/?authSource=admin\n</code></pre>"},{"location":"fiftyone_concepts/config/#using-a-different-mongodb-version","title":"Using a different MongoDB version \u00b6","text":"<p>FiftyOne is designed for MongoDB v4.4 or later.</p> <p>If you wish to connect FiftyOne to a MongoDB database whose version is not explicitly supported, you will also need to set the <code>database_validation</code> property of your FiftyOne config to <code>False</code> to suppress a runtime error that will otherwise occur.</p> <p>You can achieve this by adding the following entry to your <code>~/.fiftyone/config.json</code> file:</p> <pre><code>{\n    \"database_validation\": false\n}\n</code></pre> <p>or you can set the following environment variable:</p> <pre><code>export FIFTYONE_DATABASE_VALIDATION=false\n</code></pre>"},{"location":"fiftyone_concepts/config/#controlling-database-migrations","title":"Controlling database migrations \u00b6","text":"<p>If you are working with a shared MongoDB database, you can use database admin privileges to control which clients are allowed to migrate the shared database.</p>"},{"location":"fiftyone_concepts/config/#example-custom-database-usage","title":"Example custom database usage \u00b6","text":"<p>In order to use a custom MongoDB database with FiftyOne, you must manually start the database before importing FiftyOne. MongoDB provides a variety of options for this, including running the database as a daemon automatically.</p> <p>In the simplest case, you can just run <code>mongod</code> in one shell:</p> <pre><code>mkdir -p /path/for/db\nmongod --dbpath /path/for/db\n</code></pre> <p>Then, in another shell, configure the database URI and launch FiftyOne:</p> <pre><code>export FIFTYONE_DATABASE_URI=mongodb://localhost\n</code></pre> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"fiftyone_concepts/config/#database-migrations","title":"Database migrations \u00b6","text":"<p>New FiftyOne versions occasionally introduce data model changes that require database migrations when you upgrade or downgrade.</p> <p>By default, database upgrades happen automatically in two steps:</p> <ul> <li> <p>Database: when you import FiftyOne for the first time using a newer version of the Python package, the database\u2019s version is automatically updated to match your client version</p> </li> <li> <p>Datasets are lazily migrated to the current database version on a per-dataset basis whenever you load the dataset for the first time using a newer version of the FiftyOne package</p> </li> </ul> <p>Database downgrades must be manually performed. See this page for instructions.</p> <p>You can use the fiftyone migrate command to view the current versions of your client, database, and datasets:</p> <pre><code># View your client, database, and dataset versions\nfiftyone migrate --info\n</code></pre> <pre><code>Client version: 0.16.6\nCompatible versions: &gt;=0.16.3,&lt;0.17\n\nDatabase version: 0.16.6\n\ndataset                      version\n---------------------------  ---------\nbdd100k-validation           0.16.5\nquickstart                   0.16.5\n...\n</code></pre>"},{"location":"fiftyone_concepts/config/#restricting-migrations","title":"Restricting migrations \u00b6","text":"<p>You can use the <code>database_admin</code> config setting to control whether a client is allowed to upgrade/downgrade your FiftyOne database. The default is <code>True</code>, which means that upgrades are automatically performed when you connect to your database with newer Python client versions.</p> <p>If you set <code>database_admin</code> to <code>False</code>, your client will never cause the database to be migrated to a new version. Instead, you\u2019ll see the following behavior:</p> <ul> <li> <p>If your client is compatible with the current database version, you will be allowed to connect to the database and use FiftyOne</p> </li> <li> <p>If your client is not compatible with the current database version, you will see an informative error message when you import the library</p> </li> </ul> <p>You can restrict migrations by adding the following entry to your <code>~/.fiftyone/config.json</code> file:</p> <pre><code>{\n    \"database_admin\": false\n}\n</code></pre> <p>or by setting the following environment variable:</p> <pre><code>export FIFTYONE_DATABASE_ADMIN=false\n</code></pre> <p>Note</p> <p>A common pattern when working with custom/shared MongoDB databases is to adopt a convention that all non-administrators set their <code>database_admin</code> config setting to <code>False</code> to ensure that they cannot trigger automatic database upgrades by connecting to the database with newer Python client versions.</p>"},{"location":"fiftyone_concepts/config/#coordinating-a-migration","title":"Coordinating a migration \u00b6","text":"<p>If you are working in an environment where multiple services are connecting to your MongoDB database at any given time, use this strategy to upgrade your deployment:</p> <ol> <li>Ensure that all clients are running without database admin privileges, e.g., by adding this to their <code>~/.fiftyone/config.json</code>:</li> </ol> <pre><code>{\n    \"database_admin\": false\n}\n</code></pre> <ol> <li>Perform a test upgrade of one client and ensure that it is compatible with your current database version:</li> </ol> <pre><code># In a test environment\npip install --upgrade fiftyone\n\n# View client's compatibility info\nfiftyone migrate --info\n</code></pre> <pre><code>import fiftyone as fo\n\n# Convince yourself that the new client can load a dataset\ndataset = fo.load_dataset(...)\n</code></pre> <ol> <li>Now upgrade the client version used by all services:</li> </ol> <pre><code># In all client environments\npip install --upgrade fiftyone\n</code></pre> <ol> <li>Once all services are running the new client version, upgrade the database with admin privileges:</li> </ol> <pre><code>export FIFTYONE_DATABASE_ADMIN=true\n\npip install --upgrade fiftyone\nfiftyone migrate --all\n</code></pre> <p>Note</p> <p>Newly created datasets will always bear the <code>version</code> of the Python client that created them, which may differ from your database\u2019s version if you are undergoing a migration.</p> <p>If the new client\u2019s version is not in the compatibility range for the old clients that are still in use, the old clients will not be able to load the new datasets.</p> <p>Therefore, it is recommended to upgrade all clients as soon as possible!</p>"},{"location":"fiftyone_concepts/config/#configuring-a-timezone","title":"Configuring a timezone \u00b6","text":"<p>By default, FiftyOne loads all datetimes in FiftyOne datasets as naive <code>datetime</code> objects expressed in UTC time.</p> <p>However, you can configure FiftyOne to express datetimes in a specific timezone by setting the <code>timezone</code> property of your FiftyOne config.</p> <p>The <code>timezone</code> property can be set to any timezone string supported by <code>pytz.timezone()</code>, or <code>\"local\"</code> to use your current local timezone.</p> <p>For example, you could set the <code>FIFTYONE_TIMEZONE</code> environment variable:</p> <pre><code># Local timezone\nexport FIFTYONE_TIMEZONE=local\n\n# US Eastern timezone\nexport FIFTYONE_TIMEZONE=US/Eastern\n</code></pre> <p>Or, you can even dynamically change the timezone while you work in Python:</p> <pre><code>from datetime import datetime\nimport fiftyone as fo\n\nsample = fo.Sample(filepath=\"image.png\", created_at=datetime.utcnow())\n\ndataset = fo.Dataset()\ndataset.add_sample(sample)\n\nprint(sample.created_at)\n# 2021-08-24 20:24:09.723021\n\nfo.config.timezone = \"local\"\ndataset.reload()\n\nprint(sample.created_at)\n# 2021-08-24 16:24:09.723000-04:00\n</code></pre> <p>Note</p> <p>The <code>timezone</code> setting does not affect the internal database representation of datetimes, which are always stored as UTC timestamps.</p>"},{"location":"fiftyone_concepts/config/#configuring-the-app","title":"Configuring the App \u00b6","text":"<p>The FiftyOne App can also be configured in various ways. A new copy of your App config is applied to each <code>Session</code> object that is created when you launch the App. A session\u2019s config can be inspected and modified via the <code>session.config</code> property.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nprint(fo.app_config)\n\nsession = fo.launch_app(dataset)\nprint(session.config)\n</code></pre> <p>Note</p> <p>For changes to a live session\u2019s config to take effect in the App, you must call <code>session.refresh()</code> or invoke another state-updating action such as <code>session.view = my_view</code>.</p> <p>The FiftyOne App can be configured in the ways described below:</p> Config field Environment variable Default value Description <code>color_by</code> <code>FIFTYONE_APP_COLOR_BY</code> <code>\"field\"</code> Whether to color labels by their field name ( <code>\"field\"</code>), <code>label</code> value ( <code>\"label\"</code>), orrender each instance ID/trajectory index ( <code>\"instance\"</code>). <code>color_pool</code> <code>FIFTYONE_APP_COLOR_POOL</code> See below A list of browser supported color strings from which the App should draw from whendrawing labels (e.g., object bounding boxes). <code>colorscale</code> <code>FIFTYONE_APP_COLORSCALE</code> <code>\"viridis\"</code> The colorscale to use when rendering heatmaps in the App. Seethis section for more details. <code>default_query_performance</code> <code>FIFTYONE_APP_DEFAULT_QUERY_PERFORMANCE</code> <code>True</code> Default if a user hasn\u2019t selected a query performance mode in their current session. Seethis section for more details. <code>disable_frame_filtering</code> <code>FIFTYONE_APP_DISABLE_FRAME_FILTERING</code> <code>False</code> Whether to disable frame filtering for video datasets in the App\u2019s grid view. Seethis section for more details. <code>enable_query_performance</code> <code>FIFTYONE_APP_ENABLE_QUERY_PERFORMANCE</code> <code>True</code> Whether to show the query performance toggle in the UI for users to select. Seethis section for more details. <code>grid_zoom</code> <code>FIFTYONE_APP_GRID_ZOOM</code> <code>5</code> The zoom level of the App\u2019s sample grid. Larger values result in larger samples (and thusfewer samples in the grid). Supported values are <code>{0, 1, ..., 10}</code>. <code>loop_videos</code> <code>FIFTYONE_APP_LOOP_VIDEOS</code> <code>False</code> Whether to loop videos by default in the expanded sample view. <code>media_fallback</code> <code>FIFTYONE_APP_MEDIA_FALLBACK</code> <code>False</code> Whether to fall back to the default media field ( <code>\"filepath\"</code>) when the configured mediafield\u2019s value for a sample is not defined. <code>multicolor_keypoints</code> <code>FIFTYONE_APP_MULTICOLOR_KEYPOINTS</code> <code>False</code> Whether to independently coloy keypoint points by their index <code>notebook_height</code> <code>FIFTYONE_APP_NOTEBOOK_HEIGHT</code> <code>800</code> The height of App instances displayed in notebook cells. <code>proxy_url</code> <code>FIFTYONE_APP_PROXY_URL</code> <code>None</code> A URL string to override the default server URL. Useful for configuring the sessionthrough a reverse proxy in notebook environments. <code>show_confidence</code> <code>FIFTYONE_APP_SHOW_CONFIDENCE</code> <code>True</code> Whether to show confidences when rendering labels in the App\u2019s expanded sample view. <code>show_index</code> <code>FIFTYONE_APP_SHOW_INDEX</code> <code>True</code> Whether to show indexes when rendering labels in the App\u2019s expanded sample view. <code>show_label</code> <code>FIFTYONE_APP_SHOW_LABEL</code> <code>True</code> Whether to show the label value when rendering detection labels in the App\u2019s expandedsample view. <code>show_skeletons</code> <code>FIFTYONE_APP_SHOW_SKELETONS</code> <code>True</code> Whether to show keypoint skeletons, if available. <code>show_tooltip</code> <code>FIFTYONE_APP_SHOW_TOOLTIP</code> <code>True</code> Whether to show the tooltip when hovering over labels in the App\u2019s expanded sample view. <code>theme</code> <code>FIFTYONE_APP_THEME</code> <code>\"browser\"</code> The default theme to use in the App. Supported values are <code>{\"browser\", \"dark\", \"light\"}</code>.If <code>\"browser\"</code>, your current theme will be persisted in your browser\u2019s storage. <code>use_frame_number</code> <code>FIFTYONE_APP_USE_FRAME_NUMBER</code> <code>False</code> Whether to use the frame number instead of a timestamp in the expanded sample view. Onlyapplicable to video samples. <code>plugins</code> N/A <code>{}</code> A dict of plugin configurations. See this section fordetails."},{"location":"fiftyone_concepts/config/#viewing-your-app-config","title":"Viewing your App config \u00b6","text":"<p>You can print your App config at any time via the Python library and the CLI:</p> <p>Note</p> <p>If you have customized your App config via any of the methods described below, printing your config is a convenient way to ensure that the changes you made have taken effect as you expected.</p>"},{"location":"fiftyone_concepts/config/#modifying-your-app-config","title":"Modifying your App config \u00b6","text":"<p>You can modify your App config in a variety of ways. The following sections describe these options in detail.</p> <p>Note</p> <p>Did you know? You can also configure the behavior of the App on a per-dataset basis by customizing your dataset\u2019s App config.</p>"},{"location":"fiftyone_concepts/config/#order-of-precedence_1","title":"Order of precedence \u00b6","text":"<p>The following order of precedence is used to assign values to your App config settings at runtime:</p> <ol> <li> <p>Config settings of a <code>Session</code> instance in question</p> </li> <li> <p>App config settings applied at runtime by directly editing <code>fiftyone.app_config</code></p> </li> <li> <p><code>FIFTYONE_APP_XXX</code> environment variables</p> </li> <li> <p>Settings in your JSON App config ( <code>~/.fiftyone/app_config.json</code>)</p> </li> <li> <p>The default App config values</p> </li> </ol>"},{"location":"fiftyone_concepts/config/#launching-the-app-with-a-custom-config","title":"Launching the App with a custom config \u00b6","text":"<p>You can launch the FiftyOne App with a customized App config on a one-off basis via the following pattern:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Create a custom App config\napp_config = fo.app_config.copy()\napp_config.show_confidence = False\napp_config.show_label = False\n\nsession = fo.launch_app(dataset, config=app_config)\n</code></pre> <p>You can also configure a live <code>Session</code> by editing its <code>session.config</code> property and calling <code>session.refresh()</code> to apply the changes:</p> <pre><code># Customize the config of a live session\nsession.config.show_confidence = True\nsession.config.show_label = True\nsession.refresh()  # must refresh after edits\n</code></pre>"},{"location":"fiftyone_concepts/config/#editing-your-json-app-config","title":"Editing your JSON App config \u00b6","text":"<p>You can permanently customize your App config by creating a <code>~/.fiftyone/app_config.json</code> file on your machine. The JSON file may contain any desired subset of config fields that you wish to customize.</p> <p>For example, a valid App config JSON file is:</p> <pre><code>{\n    \"show_confidence\": false,\n    \"show_label\": false\n}\n</code></pre> <p>When <code>fiftyone</code> is imported, any options from your JSON App config are applied, as per the order of precedence described above.</p> <p>Note</p> <p>You can customize the location from which your JSON App config is read by setting the <code>FIFTYONE_APP_CONFIG_PATH</code> environment variable.</p>"},{"location":"fiftyone_concepts/config/#setting-app-environment-variables","title":"Setting App environment variables \u00b6","text":"<p>App config settings may be customized on a per-session basis by setting the <code>FIFTYONE_APP_XXX</code> environment variable(s) for the desired App config settings.</p> <p>When <code>fiftyone</code> is imported, all App config environment variables are applied, as per the order of precedence described above.</p> <p>For example, you can customize your App config in a Terminal session by issuing the following commands prior to launching your Python interpreter:</p> <pre><code>export FIFTYONE_APP_SHOW_CONFIDENCE=false\nexport FIFTYONE_APP_SHOW_LABEL=false\n</code></pre>"},{"location":"fiftyone_concepts/config/#modifying-your-app-config-in-code","title":"Modifying your App config in code \u00b6","text":"<p>You can dynamically modify your App config at runtime by editing the <code>fiftyone.app_config</code> object.</p> <p>Any changes to your App config applied via this manner will immediately take effect for all subsequent calls to <code>fiftyone.app_config</code> during your current session.</p> <pre><code>import fiftyone as fo\n\nfo.app_config.show_confidence = False\nfo.app_config.show_label = False\n</code></pre>"},{"location":"fiftyone_concepts/config/#configuring-plugins","title":"Configuring plugins \u00b6","text":"<p>You can store system-wide plugin configurations under the <code>plugins</code> key of your App config.</p> <p>Builtin plugins that you can configure include:</p> <ul> <li> <p>The builtin Map panel</p> </li> <li> <p>The builtin 3D visualizer</p> </li> <li> <p>Any custom plugins that you\u2019ve registered</p> </li> </ul> <p>For example, you may add the following to your JSON App config ( <code>~/.fiftyone/app_config.json</code>) to register a Mapbox token globally on your system:</p> <pre><code>{\n    \"plugins\": {\n        \"map\": {\n            \"mapboxAccessToken\": \"XXXXXXXX\"\n        }\n    }\n}\n</code></pre> <p>Note</p> <p>You can also store dataset-specific plugin settings by storing any subset of the above values on a dataset\u2019s App config.</p>"},{"location":"fiftyone_concepts/config/#configuring-a-proxy-url","title":"Configuring a proxy URL \u00b6","text":"<p>When running FiftyOne in a cloud machine, such as a SageMaker Notebook, a <code>proxy_url</code> should be set in your FiftyOne App config before launching the App in order for browser windows or notebook cells to point to a correct App URL. For SageMaker Notebooks, the below code snippet shows how to configure the proxy based on your instance.</p> <pre><code>import fiftyone as fo\n\n# before launching the App, configure a proxy_url\nfo.app_config.proxy_url = \"https://&lt;myinstance&gt;.notebook.&lt;region&gt;.sagemaker.aws/proxy/&lt;port&gt;/\"\n\nsession = fo.launch_app(port=&lt;port&gt;)\n</code></pre>"},{"location":"fiftyone_concepts/draw_labels/","title":"Drawing Labels on Samples \u00b6","text":"<p>FiftyOne provides native support for rendering annotated versions of image and video samples with label fields overlaid on the source media.</p>"},{"location":"fiftyone_concepts/draw_labels/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The interface for drawing labels on samples is exposed via the Python library and the CLI. You can easily annotate one or more label fields on entire datasets or arbitrary subsets of your datasets that you have identified by constructing a <code>DatasetView</code>.</p>"},{"location":"fiftyone_concepts/draw_labels/#examples","title":"Examples \u00b6","text":""},{"location":"fiftyone_concepts/draw_labels/#drawing-labels-on-images","title":"Drawing labels on images \u00b6","text":"<p>The following snippet renders the ground truth and predicted labels on a few samples from the quickstart dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\", max_samples=10)\n\nanno_image_paths = dataset.draw_labels(\n    \"/tmp/quickstart/draw-labels\",\n    label_fields=None,                  # all label fields\n    # label_fields=[\"predictions\"],     # only predictions\n)\nprint(anno_image_paths)\n</code></pre>"},{"location":"fiftyone_concepts/draw_labels/#drawing-labels-on-videos","title":"Drawing labels on videos \u00b6","text":"<p>The following snippet renders both sample-level and frame-level labels on a few videos from the quickstart-video dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=2).clone()\n\n# Add some temporal detections\nsample1 = dataset.first()\nsample1[\"events\"] = fo.TemporalDetections(\n    detections=[\\\n        fo.TemporalDetection(label=\"first\", support=[31, 60]),\\\n        fo.TemporalDetection(label=\"second\", support=[90, 120]),\\\n    ]\n)\nsample1.save()\n\nsample2 = dataset.last()\nsample2[\"events\"] = fo.TemporalDetections(\n    detections=[\\\n        fo.TemporalDetection(label=\"first\", support=[16, 45]),\\\n        fo.TemporalDetection(label=\"second\", support=[75, 104]),\\\n    ]\n)\nsample2.save()\n\nanno_video_paths = dataset.draw_labels(\n    \"/tmp/quickstart-video/draw-labels\",\n    label_fields=None,                      # all sample and frame labels\n    # label_fields=[\"events\"],              # only sample-level labels\n    # label_fields=[\"frames.detections\"],   # only frame-level labels\n)\nprint(anno_video_paths)\n</code></pre>"},{"location":"fiftyone_concepts/draw_labels/#individual-samples","title":"Individual samples \u00b6","text":"<p>You can also render annotated versions of individual samples directly by using the various methods exposed in the <code>fiftyone.utils.annotations</code> module.</p> <p>For example, you can render an annotated version of an image sample with <code>Classification</code> and <code>Detections</code> labels overlaid via <code>draw_labeled_image()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.annotations as foua\n\n# Example data\nsample = fo.Sample(\n    filepath=\"~/fiftyone/coco-2017/validation/data/000000000632.jpg\",\n    gt_label=fo.Classification(label=\"bedroom\"),\n    pred_label=fo.Classification(label=\"house\", confidence=0.95),\n    gt_objects=fo.Detections(\n        detections=[\\\n            fo.Detection(\\\n                label=\"bed\",\\\n                bounding_box=[0.00510938, 0.55248447, 0.62692188, 0.43115942],\\\n            ),\\\n            fo.Detection(\\\n                label=\"chair\",\\\n                bounding_box=[0.38253125, 0.47712215, 0.16362500, 0.18155280],\\\n            ),\\\n        ]\n    ),\n    pred_objects=fo.Detections(\n        detections=[\\\n            fo.Detection(\\\n                label=\"bed\",\\\n                bounding_box=[0.10, 0.63, 0.50, 0.35],\\\n                confidence=0.74,\\\n            ),\\\n            fo.Detection(\\\n                label=\"chair\",\\\n                bounding_box=[0.39, 0.53, 0.15, 0.13],\\\n                confidence=0.92,\\\n            ),\\\n        ]\n    ),\n)\n\n# The path to write the annotated image\noutpath = \"/path/for/image-annotated.jpg\"\n\n# Render the annotated image\nfoua.draw_labeled_image(sample, outpath)\n</code></pre> <p></p> <p>Similarly, you can draw an annotated version of a video sample with its frame labels overlaid via <code>draw_labeled_video()</code>.</p>"},{"location":"fiftyone_concepts/draw_labels/#customizing-label-rendering","title":"Customizing label rendering \u00b6","text":"<p>You can customize the look-and-feel of the labels rendered by FiftyOne by providing a custom <code>DrawConfig</code> to the relevant drawing method, such as <code>SampleCollection.draw_labels()</code> or the underlying methods in the <code>fiftyone.utils.annotations</code> module.</p> <p>Consult the <code>DrawConfig</code> docs for a complete description of the available parameters.</p> <p>For example, the snippet below increases the font size and line thickness of the labels in the example above and includes the confidence of the predictions:</p> <pre><code># Continuing from example above...\n\n# Customize annotation rendering\nconfig = foua.DrawConfig(\n    {\n        \"font_size\": 24,\n        \"bbox_linewidth\": 5,\n        \"show_all_confidences\": True,\n        \"per_object_label_colors\": False,\n    }\n)\n\n# Render the annotated image\nfoua.draw_labeled_image(sample, outpath, config=config)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/evaluation/","title":"Evaluating Models \u00b6","text":"<p>FiftyOne provides a variety of builtin methods for evaluating your model predictions, including regressions, classifications, detections, polygons, instance and semantic segmentations, on both image and video datasets.</p> <p>When you evaluate a model in FiftyOne, you get access to the standard aggregate metrics such as classification reports, confusion matrices, and PR curves for your model. In addition, FiftyOne can also record fine-grained statistics like accuracy and false positive counts at the sample-level, which you can interactively explore in the App to diagnose the strengths and weaknesses of your models on individual data samples.</p> <p>Sample-level analysis often leads to critical insights that will help you improve your datasets and models. For example, viewing the samples with the most false positive predictions can reveal errors in your annotation schema. Or, viewing the cluster of samples with the lowest accuracy can reveal gaps in your training dataset that you need to address in order to improve your model\u2019s performance. A key goal of FiftyOne is to help you uncover these insights on your data!</p> <p>Note</p> <p>Check out the tutorials page for in-depth walkthroughs of evaluating various types of models with FiftyOne.</p>"},{"location":"fiftyone_concepts/evaluation/#overview","title":"Overview \u00b6","text":"<p>FiftyOne\u2019s evaluation methods are conveniently exposed as methods on all <code>Dataset</code> and <code>DatasetView</code> objects, which means that you can evaluate entire datasets or specific views into them via the same syntax.</p> <p>Let\u2019s illustrate the basic workflow by loading the quickstart dataset and analyzing the object detections in its <code>predictions</code> field using the <code>evaluate_detections()</code> method:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Evaluate the objects in the `predictions` field with respect to the\n# objects in the `ground_truth` field\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval\",\n)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"fiftyone_concepts/evaluation/#model-evaluation-panel-new","title":"Model Evaluation panel NEW \u00b6","text":"<p>When you load a dataset in the App that contains one or more evaluations, you can open the Model Evaluation panel to visualize and interactively explore the evaluation results in the App:</p> <p></p>"},{"location":"fiftyone_concepts/evaluation/#per-class-metrics","title":"Per-class metrics \u00b6","text":"<p>You can also retrieve and interact with evaluation results via the SDK.</p> <p>Running an evaluation returns an instance of a task-specific subclass of <code>EvaluationResults</code> that provides a handful of methods for generating aggregate statistics about your dataset.</p> <pre><code># Get the 10 most common classes in the dataset\ncounts = dataset.count_values(\"ground_truth.detections.label\")\nclasses = sorted(counts, key=counts.get, reverse=True)[:10]\n\n# Print a classification report for the top-10 classes\nresults.print_report(classes=classes)\n</code></pre> <pre><code>               precision    recall  f1-score   support\n\n       person       0.45      0.74      0.56       783\n         kite       0.55      0.72      0.62       156\n          car       0.12      0.54      0.20        61\n         bird       0.63      0.67      0.65       126\n       carrot       0.06      0.49      0.11        47\n         boat       0.05      0.24      0.08        37\n    surfboard       0.10      0.43      0.17        30\ntraffic light       0.22      0.54      0.31        24\n     airplane       0.29      0.67      0.40        24\n      giraffe       0.26      0.65      0.37        23\n\n    micro avg       0.32      0.68      0.44      1311\n    macro avg       0.27      0.57      0.35      1311\n weighted avg       0.42      0.68      0.51      1311\n</code></pre> <p>Note</p> <p>For details on micro, macro, and weighted averaging, see the sklearn.metrics documentation.</p>"},{"location":"fiftyone_concepts/evaluation/#per-sample-metrics","title":"Per-sample metrics \u00b6","text":"<p>In addition to standard aggregate metrics, when you pass an <code>eval_key</code> parameter to the evaluation routine, FiftyOne will populate helpful task-specific information about your model\u2019s predictions on each sample, such as false negative/positive counts and per-sample accuracies.</p> <p>Continuing with our example, let\u2019s use dataset views and the FiftyOne App to leverage these sample metrics to investigate the samples with the most false positive predictions in the dataset:</p> <pre><code>import fiftyone as fo\nfrom fiftyone import ViewField as F\n\n# Create a view that has samples with the most false positives first, and\n# only includes false positive boxes in the `predictions` field\nview = (\n    dataset\n    .sort_by(\"eval_fp\", reverse=True)\n    .filter_labels(\"predictions\", F(\"eval\") == \"fp\")\n)\n\n# Visualize results in the App\nsession = fo.launch_app(view=view)\n</code></pre> <p></p> <p>Notice anything wrong? The sample with the most false positives is a plate of carrots where the entire plate has been boxed as a single example in the ground truth while the model is generating predictions for individual carrots!</p> <p>If you\u2019re familiar with COCO format (which is recognized by <code>evaluate_detections()</code> by default), you\u2019ll notice that the issue here is that the <code>iscrowd</code> attribute of this ground truth annotation has been incorrectly set to <code>0</code>. Resolving mistakes like these will provide a much more accurate picture of the real performance of a model.</p>"},{"location":"fiftyone_concepts/evaluation/#confusion-matrices","title":"Confusion matrices \u00b6","text":"<p>Note</p> <p>The easiest way to work with confusion matrices in FiftyOne is via the Model Evaluation panel!</p> <p>When you use evaluation methods such as <code>evaluate_detections()</code> that support confusion matrices, you can use the <code>plot_confusion_matrix()</code> method to render responsive plots that can be attached to App instances to interactively explore specific cases of your model\u2019s performance:</p> <pre><code># Plot confusion matrix\nplot = results.plot_confusion_matrix(classes=classes)\nplot.show()\n\n# Connect to session\nsession.plots.attach(plot)\n</code></pre> <p></p> <p>In this setup, you can click on individual cells of the confusion matrix to select the corresponding ground truth and/or predicted objects in the App. For example, if you click on a diagonal cell of the confusion matrix, you will see the true positive examples of that class in the App.</p> <p>Likewise, whenever you modify the Session\u2019s view, either in the App or by programmatically setting <code>session.view</code>, the confusion matrix is automatically updated to show the cell counts for only those objects that are included in the current view.</p>"},{"location":"fiftyone_concepts/evaluation/#managing-evaluations","title":"Managing evaluations \u00b6","text":"<p>When you run an evaluation with an <code>eval_key</code> argument, the evaluation is recorded on the dataset and you can retrieve information about it later, rename it, delete it (along with any modifications to your dataset that were performed by it), and retrieve the view that you evaluated on using the following methods on your dataset:</p> <ul> <li> <p><code>list_evaluations()</code></p> </li> <li> <p><code>get_evaluation_info()</code></p> </li> <li> <p><code>load_evaluation_results()</code></p> </li> <li> <p><code>load_evaluation_view()</code></p> </li> <li> <p><code>rename_evaluation()</code></p> </li> <li> <p><code>delete_evaluation()</code></p> </li> </ul> <p>The example below demonstrates the basic interface:</p> <pre><code># List evaluations you've run on a dataset\ndataset.list_evaluations()\n# ['eval']\n\n# Print information about an evaluation\nprint(dataset.get_evaluation_info(\"eval\"))\n\n# Load existing evaluation results and use them\nresults = dataset.load_evaluation_results(\"eval\")\nresults.print_report()\n\n# Rename the evaluation\n# This will automatically rename any evaluation fields on your dataset\ndataset.rename_evaluation(\"eval\", \"still_eval\")\n\n# Delete the evaluation\n# This will remove any evaluation data that was populated on your dataset\ndataset.delete_evaluation(\"still_eval\")\n</code></pre> <p>The sections below discuss evaluating various types of predictions in more detail.</p>"},{"location":"fiftyone_concepts/evaluation/#regressions","title":"Regressions \u00b6","text":"<p>You can use the <code>evaluate_regressions()</code> method to evaluate the predictions of a regression model stored in a <code>Regression</code> field of your dataset.</p> <p>Invoking <code>evaluate_regressions()</code> returns a <code>RegressionResults</code> instance that provides a variety of methods for evaluating your model.</p> <p>In addition, when you specify an <code>eval_key</code> parameter, helpful fields will be populated on each sample that you can leverage via the FiftyOne App to interactively explore the strengths and weaknesses of your model on individual samples.</p>"},{"location":"fiftyone_concepts/evaluation/#simple-evaluation-default","title":"Simple evaluation (default) \u00b6","text":"<p>By default, <code>evaluate_regressions()</code> will evaluate each prediction by directly comparing its <code>value</code> to the associated ground truth value.</p> <p>You can explicitly request that simple evaluation be used by setting the <code>method</code> parameter to <code>\"simple\"</code>.</p> <p>When you specify an <code>eval_key</code> parameter, a float <code>eval_key</code> field will be populated on each sample that records the error of that sample\u2019s prediction with respect to its ground truth value. By default, the squared error will be computed, but you can customize this via the optional <code>metric</code> argument to <code>evaluate_regressions()</code>, which can take any value supported by <code>SimpleEvaluationConfig</code>.</p> <p>The example below demonstrates simple evaluation on the quickstart dataset with some fake regression data added to it to demonstrate the workflow:</p> <pre><code>import random\nimport numpy as np\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\").select_fields().clone()\n\n# Populate some fake regression + weather data\nfor idx, sample in enumerate(dataset, 1):\n    ytrue = random.random() * idx\n    ypred = ytrue + np.random.randn() * np.sqrt(ytrue)\n    confidence = random.random()\n    sample[\"ground_truth\"] = fo.Regression(value=ytrue)\n    sample[\"predictions\"] = fo.Regression(value=ypred, confidence=confidence)\n    sample[\"weather\"] = random.choice([\"sunny\", \"cloudy\", \"rainy\"])\n    sample.save()\n\nprint(dataset)\n\n# Evaluate the predictions in the `predictions` field with respect to the\n# values in the `ground_truth` field\nresults = dataset.evaluate_regressions(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval\",\n)\n\n# Print some standard regression evaluation metrics\nresults.print_metrics()\n\n# Plot a scatterplot of the results colored by `weather` and scaled by\n# `confidence`\nplot = results.plot_results(labels=\"weather\", sizes=\"predictions.confidence\")\nplot.show()\n\n# Launch the App to explore\nsession = fo.launch_app(dataset)\n\n# Show the samples with the smallest regression error\nsession.view = dataset.sort_by(\"eval\")\n\n# Show the samples with the largest regression error\nsession.view = dataset.sort_by(\"eval\", reverse=True)\n</code></pre> <pre><code>mean squared error        59.69\nroot mean squared error   7.73\nmean absolute error       5.48\nmedian absolute error     3.57\nr2 score                  0.97\nexplained variance score  0.97\nmax error                 31.77\nsupport                   200\n</code></pre> <p></p> <p>Note</p> <p>Did you know? You can attach regression plots to the App and interactively explore them by selecting scatter points and/or modifying your view in the App.</p>"},{"location":"fiftyone_concepts/evaluation/#classifications","title":"Classifications \u00b6","text":"<p>You can use the <code>evaluate_classifications()</code> method to evaluate the predictions of a classifier stored in a <code>Classification</code> field of your dataset.</p> <p>By default, the classifications will be treated as a generic multiclass classification task, but you can specify other evaluation strategies such as top-k accuracy or binary evaluation via the <code>method</code> parameter.</p> <p>Invoking <code>evaluate_classifications()</code> returns a <code>ClassificationResults</code> instance that provides a variety of methods for generating various aggregate evaluation reports about your model.</p> <p>In addition, when you specify an <code>eval_key</code> parameter, a number of helpful fields will be populated on each sample that you can leverage via the FiftyOne App to interactively explore the strengths and weaknesses of your model on individual samples.</p>"},{"location":"fiftyone_concepts/evaluation/#simple-evaluation-default_1","title":"Simple evaluation (default) \u00b6","text":"<p>By default, <code>evaluate_classifications()</code> will treat your classifications as generic multiclass predictions, and it will evaluate each prediction by directly comparing its <code>label</code> to the associated ground truth prediction.</p> <p>You can explicitly request that simple evaluation be used by setting the <code>method</code> parameter to <code>\"simple\"</code>.</p> <p>When you specify an <code>eval_key</code> parameter, a boolean <code>eval_key</code> field will be populated on each sample that records whether that sample\u2019s prediction is correct.</p> <p>The example below demonstrates simple evaluation on the CIFAR-10 dataset with some fake predictions added to it to demonstrate the workflow:</p> <pre><code>import random\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\n    \"cifar10\",\n    split=\"test\",\n    max_samples=1000,\n    shuffle=True,\n)\n\n#\n# Create some test predictions by copying the ground truth labels into a\n# new `predictions` field with 10% of the labels perturbed at random\n#\n\nclasses = dataset.distinct(\"ground_truth.label\")\n\ndef jitter(val):\n    if random.random() &lt; 0.10:\n        return random.choice(classes)\n\n    return val\n\npredictions = [\\\n    fo.Classification(label=jitter(gt.label), confidence=random.random())\\\n    for gt in dataset.values(\"ground_truth\")\\\n]\n\ndataset.set_values(\"predictions\", predictions)\n\nprint(dataset)\n\n# Evaluate the predictions in the `predictions` field with respect to the\n# labels in the `ground_truth` field\nresults = dataset.evaluate_classifications(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval_simple\",\n)\n\n# Print a classification report\nresults.print_report()\n\n# Plot a confusion matrix\nplot = results.plot_confusion_matrix()\nplot.show()\n\n# Launch the App to explore\nsession = fo.launch_app(dataset)\n\n# View only the incorrect predictions in the App\nsession.view = dataset.match(F(\"eval_simple\") == False)\n</code></pre> <pre><code>              precision    recall  f1-score   support\n\n    airplane       0.91      0.90      0.91       118\n  automobile       0.93      0.90      0.91       101\n        bird       0.93      0.87      0.90       103\n         cat       0.92      0.91      0.92        94\n        deer       0.88      0.92      0.90       116\n         dog       0.85      0.84      0.84        86\n        frog       0.85      0.92      0.88        84\n       horse       0.88      0.91      0.89        96\n        ship       0.93      0.95      0.94        97\n       truck       0.92      0.89      0.90       105\n\n    accuracy                           0.90      1000\n   macro avg       0.90      0.90      0.90      1000\nweighted avg       0.90      0.90      0.90      1000\n</code></pre> <p></p> <p>Note</p> <p>The easiest way to analyze models in FiftyOne is via the Model Evaluation panel!</p>"},{"location":"fiftyone_concepts/evaluation/#top-k-evaluation","title":"Top-k evaluation \u00b6","text":"<p>Set the <code>method</code> parameter of <code>evaluate_classifications()</code> to <code>top-k</code> in order to use top-k matching to evaluate your classifications.</p> <p>Under this strategy, predictions are deemed to be correct if the corresponding ground truth label is within the top <code>k</code> predictions.</p> <p>When you specify an <code>eval_key</code> parameter, a boolean <code>eval_key</code> field will be populated on each sample that records whether that sample\u2019s prediction is correct.</p> <p>Note</p> <p>In order to use top-k evaluation, you must populate the <code>logits</code> field of your predictions, and you must provide the list of corresponding class labels via the <code>classes</code> parameter of <code>evaluate_classifications()</code>.</p> <p>Did you know? Many models from the Model Zoo provide support for storing logits for their predictions!</p> <p>The example below demonstrates top-k evaluation on a small ImageNet sample with predictions from a pre-trained model from the Model Zoo:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\", dataset_name=\"top-k-eval-demo\"\n)\n\n# We need the list of class labels corresponding to the logits\nlogits_classes = dataset.default_classes\n\n# Add predictions (with logits) to 25 random samples\npredictions_view = dataset.take(25, seed=51)\nmodel = foz.load_zoo_model(\"resnet50-imagenet-torch\")\npredictions_view.apply_model(model, \"predictions\", store_logits=True)\n\nprint(predictions_view)\n\n# Evaluate the predictions in the `predictions` field with respect to the\n# labels in the `ground_truth` field using top-5 accuracy\nresults = predictions_view.evaluate_classifications(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval_top_k\",\n    method=\"top-k\",\n    classes=logits_classes,\n    k=5,\n)\n\n# Get the 10 most common classes in the view\ncounts = predictions_view.count_values(\"ground_truth.label\")\nclasses = sorted(counts, key=counts.get, reverse=True)[:10]\n\n# Print a classification report for the top-10 classes\nresults.print_report(classes=classes)\n\n# Launch the App to explore\nsession = fo.launch_app(dataset)\n\n# View only the incorrect predictions for the 10 most common classes\nsession.view = (\n    predictions_view\n    .match(F(\"ground_truth.label\").is_in(classes))\n    .match(F(\"eval_top_k\") == False)\n)\n</code></pre> <p></p> <p>Note</p> <p>The easiest way to analyze models in FiftyOne is via the Model Evaluation panel!</p>"},{"location":"fiftyone_concepts/evaluation/#binary-evaluation","title":"Binary evaluation \u00b6","text":"<p>If your classifier is binary, set the <code>method</code> parameter of <code>evaluate_classifications()</code> to <code>\"binary\"</code> in order to access binary-specific evaluation information such as precision-recall curves for your model.</p> <p>When you specify an <code>eval_key</code> parameter, a string <code>eval_key</code> field will be populated on each sample that records whether the sample is a true positive, false positive, true negative, or false negative.</p> <p>Note</p> <p>In order to use binary evaluation, you must provide the <code>(neg_label, pos_label)</code> for your model via the <code>classes</code> parameter of <code>evaluate_classifications()</code>.</p> <p>The example below demonstrates binary evaluation on the CIFAR-10 dataset with some fake binary predictions added to it to demonstrate the workflow:</p> <pre><code>import random\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Load a small sample from the ImageNet dataset\ndataset = foz.load_zoo_dataset(\n    \"cifar10\",\n    split=\"test\",\n    max_samples=1000,\n    shuffle=True,\n)\n\n#\n# Binarize the ground truth labels to `cat` and `other`, and add\n# predictions that are correct proportionally to their confidence\n#\n\nclasses = [\"other\", \"cat\"]\n\nfor sample in dataset:\n    gt_label = \"cat\" if sample.ground_truth.label == \"cat\" else \"other\"\n\n    confidence = random.random()\n    if random.random() &gt; confidence:\n        pred_label = \"cat\" if gt_label == \"other\" else \"other\"\n    else:\n        pred_label = gt_label\n\n    sample.ground_truth.label = gt_label\n    sample[\"predictions\"] = fo.Classification(\n        label=pred_label, confidence=confidence\n    )\n\n    sample.save()\n\nprint(dataset)\n\n# Evaluate the predictions in the `predictions` field with respect to the\n# labels in the `ground_truth` field\nresults = dataset.evaluate_classifications(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval_binary\",\n    method=\"binary\",\n    classes=classes,\n)\n\n# Print a classification report\nresults.print_report()\n\n# Plot a PR curve\nplot = results.plot_pr_curve()\nplot.show()\n</code></pre> <pre><code>              precision    recall  f1-score   support\n\n       other       0.90      0.48      0.63       906\n         cat       0.09      0.50      0.15        94\n\n    accuracy                           0.48      1000\n   macro avg       0.50      0.49      0.39      1000\nweighted avg       0.83      0.48      0.59      1000\n</code></pre> <p></p> <p>Note</p> <p>The easiest way to analyze models in FiftyOne is via the Model Evaluation panel!</p>"},{"location":"fiftyone_concepts/evaluation/#detections","title":"Detections \u00b6","text":"<p>You can use the <code>evaluate_detections()</code> method to evaluate the predictions of an object detection model stored in a <code>Detections</code>, <code>Polylines</code>, or <code>Keypoints</code> field of your dataset or of a temporal detection model stored in a <code>TemporalDetections</code> field of your dataset.</p> <p>Invoking <code>evaluate_detections()</code> returns a <code>DetectionResults</code> instance that provides a variety of methods for generating various aggregate evaluation reports about your model.</p> <p>In addition, when you specify an <code>eval_key</code> parameter, a number of helpful fields will be populated on each sample and its predicted/ground truth objects that you can leverage via the FiftyOne App to interactively explore the strengths and weaknesses of your model on individual samples.</p> <p>Note</p> <p>FiftyOne uses the COCO-style evaluation by default, but Open Images-style evaluation is also natively supported.</p>"},{"location":"fiftyone_concepts/evaluation/#supported-types","title":"Supported types \u00b6","text":"<p>The <code>evaluate_detections()</code> method supports all of the following task types:</p> <ul> <li> <p>Object detection</p> </li> <li> <p>Instance segmentations</p> </li> <li> <p>Polygon detection</p> </li> <li> <p>Keypoints</p> </li> <li> <p>Temporal detections</p> </li> <li> <p>3D detections</p> </li> </ul> <p>The only difference between each task type is in how the IoU between objects is calculated:</p> <ul> <li> <p>For object detections, IoUs are computed between each pair of bounding boxes</p> </li> <li> <p>For instance segmentations and polygons, IoUs are computed between the polygonal shapes rather than their rectangular bounding boxes</p> </li> <li> <p>For keypoint tasks, object keypoint similarity is computed for each pair of objects, using the extent of the ground truth keypoints as a proxy for the area of the object\u2019s bounding box, and assuming uniform falloff (\u03ba\u03ba)</p> </li> <li> <p>For temporal detections, IoU is computed between the 1D support of two temporal segments</p> </li> </ul> <p>For object detection tasks, the ground truth and predicted objects should be stored in <code>Detections</code> format.</p> <p>For instance segmentation tasks, the ground truth and predicted objects should be stored in <code>Detections</code> format, and each <code>Detection</code> instance should have its <code>mask</code> attribute populated to define the extent of the object within its bounding box.</p> <p>Note</p> <p>In order to use instance masks for IoU calculations, pass <code>use_masks=True</code> to <code>evaluate_detections()</code>.</p> <p>For polygon detection tasks, the ground truth and predicted objects should be stored in <code>Polylines</code> format with their <code>filled</code> attribute set to <code>True</code> to indicate that they represent closed polygons (as opposed to polylines).</p> <p>Note</p> <p>If you are evaluating polygons but would rather use bounding boxes rather than the actual polygonal geometries for IoU calculations, you can pass <code>use_boxes=True</code> to <code>evaluate_detections()</code>.</p> <p>For keypoint tasks, each <code>Keypoint</code> instance must contain point arrays of equal length and semantic ordering.</p> <p>Note</p> <p>If a particular point is missing or not visible for a <code>Keypoint</code> instance, use nan values for its coordinates. See here for more information about structuring keypoints.</p> <p>For temporal detection tasks, the ground truth and predicted objects should be stored in <code>TemporalDetections</code> format.</p>"},{"location":"fiftyone_concepts/evaluation/#evaluation-patches-views","title":"Evaluation patches views \u00b6","text":"<p>Once you have run <code>evaluate_detections()</code> on a dataset, you can use <code>to_evaluation_patches()</code> to transform the dataset (or a view into it) into a new view that contains one sample for each true positive, false positive, and false negative example.</p> <p>True positive examples will result in samples with both their ground truth and predicted fields populated, while false positive/negative examples will only have one of their corresponding predicted/ground truth fields populated, respectively.</p> <p>If multiple predictions are matched to a ground truth object (e.g., if the evaluation protocol includes a crowd attribute), then all matched predictions will be stored in the single sample along with the ground truth object.</p> <p>Evaluation patches views also have top-level <code>type</code> and <code>iou</code> fields populated based on the evaluation results for that example, as well as a <code>sample_id</code> field recording the sample ID of the example, and a <code>crowd</code> field if the evaluation protocol defines a crowd attribute.</p> <p>Note</p> <p>Evaluation patches views generate patches for only the contents of the current view, which may differ from the view on which the <code>eval_key</code> evaluation was performed. This may exclude some labels that were evaluated and/or include labels that were not evaluated.</p> <p>If you would like to see patches for the exact view on which an evaluation was performed, first call <code>load_evaluation_view()</code> to load the view and then convert to patches.</p> <p>The example below demonstrates loading an evaluation patches view for the results of an evaluation on the quickstart dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Evaluate `predictions` w.r.t. labels in `ground_truth` field\ndataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\"\n)\n\nsession = fo.launch_app(dataset)\n\n# Convert to evaluation patches\neval_patches = dataset.to_evaluation_patches(\"eval\")\nprint(eval_patches)\n\nprint(eval_patches.count_values(\"type\"))\n# {'fn': 246, 'fp': 4131, 'tp': 986}\n\n# View patches in the App\nsession.view = eval_patches\n</code></pre> <pre><code>Dataset:     quickstart\nMedia type:  image\nNum patches: 5363\nPatch fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    predictions:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    sample_id:    fiftyone.core.fields.StringField\n    type:         fiftyone.core.fields.StringField\n    iou:          fiftyone.core.fields.FloatField\n    crowd:        fiftyone.core.fields.BooleanField\nView stages:\n    1. ToEvaluationPatches(eval_key='eval', config=None)\n</code></pre> <p>Note</p> <p>Did you know? You can convert to evaluation patches view directly from the App!</p> <p></p> <p>Evaluation patches views are just like any other dataset view in the sense that:</p> <ul> <li> <p>You can append view stages via the App view bar or views API</p> </li> <li> <p>Any modifications to ground truth or predicted label tags that you make via the App\u2019s tagging menu or via API methods like <code>tag_labels()</code> and <code>untag_labels()</code> will be reflected on the source dataset</p> </li> <li> <p>Any modifications to the predicted or ground truth <code>Label</code> elements in the patches view that you make by iterating over the contents of the view or calling <code>set_values()</code> will be reflected on the source dataset</p> </li> <li> <p>Calling <code>save()</code> on an evaluation patches view (typically one that contains additional view stages that filter or modify its contents) will sync any <code>Label</code> edits or deletions with the source dataset</p> </li> </ul> <p>However, because evaluation patches views only contain a subset of the contents of a <code>Sample</code> from the source dataset, there are some differences in behavior compared to non-patch views:</p> <ul> <li> <p>Tagging or untagging patches themselves (as opposed to their labels) will not affect the tags of the underlying <code>Sample</code></p> </li> <li> <p>Any new fields that you add to an evaluation patches view will not be added to the source dataset</p> </li> </ul>"},{"location":"fiftyone_concepts/evaluation/#coco-style-evaluation-default-spatial","title":"COCO-style evaluation (default spatial) \u00b6","text":"<p>By default, <code>evaluate_detections()</code> will use COCO-style evaluation to analyze predictions when the specified label fields are <code>Detections</code> or <code>Polylines</code>.</p> <p>You can also explicitly request that COCO-style evaluation be used by setting the <code>method</code> parameter to <code>\"coco\"</code>.</p> <p>Note</p> <p>FiftyOne\u2019s implementation of COCO-style evaluation matches the reference implementation available via pycocotools.</p>"},{"location":"fiftyone_concepts/evaluation/#overview_1","title":"Overview \u00b6","text":"<p>When running COCO-style evaluation using <code>evaluate_detections()</code>:</p> <ul> <li> <p>Predicted and ground truth objects are matched using a specified IoU threshold (default = 0.50). This threshold can be customized via the <code>iou</code> parameter</p> </li> <li> <p>By default, only objects with the same <code>label</code> will be matched. Classwise matching can be disabled via the <code>classwise</code> parameter</p> </li> <li> <p>Ground truth objects can have an <code>iscrowd</code> attribute that indicates whether the annotation contains a crowd of objects. Multiple predictions can be matched to crowd ground truth objects. The name of this attribute can be customized by passing the optional <code>iscrowd</code> attribute of <code>COCOEvaluationConfig</code> to <code>evaluate_detections()</code></p> </li> </ul> <p>When you specify an <code>eval_key</code> parameter, a number of helpful fields will be populated on each sample and its predicted/ground truth objects:</p> <ul> <li>True positive (TP), false positive (FP), and false negative (FN) counts for the each sample are saved in top-level fields of each sample:</li> </ul> <pre><code>TP: sample.&lt;eval_key&gt;_tp\nFP: sample.&lt;eval_key&gt;_fp\nFN: sample.&lt;eval_key&gt;_fn\n</code></pre> <ul> <li>The fields listed below are populated on each individual object instance; these fields tabulate the TP/FP/FN status of the object, the ID of the matching object (if any), and the matching IoU:</li> </ul> <pre><code>TP/FP/FN: object.&lt;eval_key&gt;\n        ID: object.&lt;eval_key&gt;_id\n       IoU: object.&lt;eval_key&gt;_iou\n</code></pre> <p>Note</p> <p>See <code>COCOEvaluationConfig</code> for complete descriptions of the optional keyword arguments that you can pass to <code>evaluate_detections()</code> when running COCO-style evaluation.</p>"},{"location":"fiftyone_concepts/evaluation/#example-evaluation","title":"Example evaluation \u00b6","text":"<p>The example below demonstrates COCO-style detection evaluation on the quickstart dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nprint(dataset)\n\n# Evaluate the objects in the `predictions` field with respect to the\n# objects in the `ground_truth` field\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval\",\n)\n\n# Get the 10 most common classes in the dataset\ncounts = dataset.count_values(\"ground_truth.detections.label\")\nclasses = sorted(counts, key=counts.get, reverse=True)[:10]\n\n# Print a classification report for the top-10 classes\nresults.print_report(classes=classes)\n\n# Print some statistics about the total TP/FP/FN counts\nprint(\"TP: %d\" % dataset.sum(\"eval_tp\"))\nprint(\"FP: %d\" % dataset.sum(\"eval_fp\"))\nprint(\"FN: %d\" % dataset.sum(\"eval_fn\"))\n\n# Create a view that has samples with the most false positives first, and\n# only includes false positive boxes in the `predictions` field\nview = (\n    dataset\n    .sort_by(\"eval_fp\", reverse=True)\n    .filter_labels(\"predictions\", F(\"eval\") == \"fp\")\n)\n\n# Visualize results in the App\nsession = fo.launch_app(view=view)\n</code></pre> <pre><code>               precision    recall  f1-score   support\n\n       person       0.45      0.74      0.56       783\n         kite       0.55      0.72      0.62       156\n          car       0.12      0.54      0.20        61\n         bird       0.63      0.67      0.65       126\n       carrot       0.06      0.49      0.11        47\n         boat       0.05      0.24      0.08        37\n    surfboard       0.10      0.43      0.17        30\n     airplane       0.29      0.67      0.40        24\ntraffic light       0.22      0.54      0.31        24\n        bench       0.10      0.30      0.15        23\n\n    micro avg       0.32      0.68      0.43      1311\n    macro avg       0.26      0.54      0.32      1311\n weighted avg       0.42      0.68      0.50      1311\n</code></pre> <p></p> <p>Note</p> <p>The easiest way to analyze models in FiftyOne is via the Model Evaluation panel!</p>"},{"location":"fiftyone_concepts/evaluation/#map-mar-and-pr-curves","title":"mAP, mAR and PR curves \u00b6","text":"<p>You can compute mean average precision (mAP), mean average recall (mAR), and precision-recall (PR) curves for your predictions by passing the <code>compute_mAP=True</code> flag to <code>evaluate_detections()</code>:</p> <p>Note</p> <p>All mAP and mAR calculations are performed according to the COCO evaluation protocol.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nprint(dataset)\n\n# Performs an IoU sweep so that mAP, mAR, and PR curves can be computed\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    compute_mAP=True,\n)\n\nprint(results.mAP())\n# 0.3957\n\nprint(results.mAR())\n# 0.5210\n\nplot = results.plot_pr_curves(classes=[\"person\", \"kite\", \"car\"])\nplot.show()\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/evaluation/#confusion-matrices_1","title":"Confusion matrices \u00b6","text":"<p>You can also easily generate confusion matrices for the results of COCO-style evaluations.</p> <p>In order for the confusion matrix to capture anything other than false positive/negative counts, you will likely want to set the <code>classwise</code> parameter to <code>False</code> during evaluation so that predicted objects can be matched with ground truth objects of different classes.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Perform evaluation, allowing objects to be matched between classes\nresults = dataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", classwise=False\n)\n\n# Generate a confusion matrix for the specified classes\nplot = results.plot_confusion_matrix(classes=[\"car\", \"truck\", \"motorcycle\"])\nplot.show()\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/evaluation/#open-images-style-evaluation","title":"Open Images-style evaluation \u00b6","text":"<p>The <code>evaluate_detections()</code> method also supports Open Images-style evaluation.</p> <p>In order to run Open Images-style evaluation, simply set the <code>method</code> parameter to <code>\"open-images\"</code>.</p> <p>Note</p> <p>FiftyOne\u2019s implementation of Open Images-style evaluation matches the reference implementation available via the TF Object Detection API.</p>"},{"location":"fiftyone_concepts/evaluation/#overview_2","title":"Overview \u00b6","text":"<p>Open Images-style evaluation provides additional features not found in COCO-style evaluation that you may find useful when evaluating your custom datasets.</p> <p>The two primary differences are:</p> <ul> <li> <p>Non-exhaustive image labeling: positive and negative sample-level <code>Classifications</code> fields can be provided to indicate which object classes were considered when annotating the image. Predicted objects whose classes are not included in the sample-level labels for a sample are ignored. The names of these fields can be specified via the <code>pos_label_field</code> and <code>neg_label_field</code> parameters</p> </li> <li> <p>Class hierarchies: If your dataset includes a class hierarchy, you can configure this evaluation protocol to automatically expand ground truth and/or predicted leaf classes so that all levels of the hierarchy can be correctly evaluated. You can provide a label hierarchy via the <code>hierarchy</code> parameter. By default, if you provide a hierarchy, then image-level label fields and ground truth detections will be expanded to incorporate parent classes (child classes for negative image-level labels). You can disable this feature by setting the <code>expand_gt_hierarchy</code> parameter to <code>False</code>. Alternatively, you can expand predictions by setting the <code>expand_pred_hierarchy</code> parameter to <code>True</code></p> </li> </ul> <p>In addition, note that:</p> <ul> <li> <p>Like VOC-style evaluation, only one IoU (default = 0.5) is used to calculate mAP. You can customize this value via the <code>iou</code> parameter</p> </li> <li> <p>When dealing with crowd objects, Open Images-style evaluation dictates that if a crowd is matched with multiple predictions, each counts as one true positive when computing mAP</p> </li> </ul> <p>When you specify an <code>eval_key</code> parameter, a number of helpful fields will be populated on each sample and its predicted/ground truth objects:</p> <ul> <li>True positive (TP), false positive (FP), and false negative (FN) counts for the each sample are saved in top-level fields of each sample:</li> </ul> <pre><code>TP: sample.&lt;eval_key&gt;_tp\nFP: sample.&lt;eval_key&gt;_fp\nFN: sample.&lt;eval_key&gt;_fn\n</code></pre> <ul> <li>The fields listed below are populated on each individual <code>Detection</code> instance; these fields tabulate the TP/FP/FN status of the object, the ID of the matching object (if any), and the matching IoU:</li> </ul> <pre><code>TP/FP/FN: object.&lt;eval_key&gt;\n        ID: object.&lt;eval_key&gt;_id\n       IoU: object.&lt;eval_key&gt;_iou\n</code></pre> <p>Note</p> <p>See <code>OpenImagesEvaluationConfig</code> for complete descriptions of the optional keyword arguments that you can pass to <code>evaluate_detections()</code> when running Open Images-style evaluation.</p>"},{"location":"fiftyone_concepts/evaluation/#example-evaluation_1","title":"Example evaluation \u00b6","text":"<p>The example below demonstrates Open Images-style detection evaluation on the quickstart dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nprint(dataset)\n\n# Evaluate the objects in the `predictions` field with respect to the\n# objects in the `ground_truth` field\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    method=\"open-images\",\n    eval_key=\"eval\",\n)\n\n# Get the 10 most common classes in the dataset\ncounts = dataset.count_values(\"ground_truth.detections.label\")\nclasses = sorted(counts, key=counts.get, reverse=True)[:10]\n\n# Print a classification report for the top-10 classes\nresults.print_report(classes=classes)\n\n# Print some statistics about the total TP/FP/FN counts\nprint(\"TP: %d\" % dataset.sum(\"eval_tp\"))\nprint(\"FP: %d\" % dataset.sum(\"eval_fp\"))\nprint(\"FN: %d\" % dataset.sum(\"eval_fn\"))\n\n# Create a view that has samples with the most false positives first, and\n# only includes false positive boxes in the `predictions` field\nview = (\n    dataset\n    .sort_by(\"eval_fp\", reverse=True)\n    .filter_labels(\"predictions\", F(\"eval\") == \"fp\")\n)\n\n# Visualize results in the App\nsession = fo.launch_app(view=view)\n</code></pre> <pre><code>               precision    recall  f1-score   support\n\n       person       0.25      0.86      0.39       378\n         kite       0.27      0.75      0.40        75\n          car       0.18      0.80      0.29        61\n         bird       0.20      0.51      0.28        51\n       carrot       0.09      0.74      0.16        47\n         boat       0.09      0.46      0.16        37\n    surfboard       0.17      0.73      0.28        30\n     airplane       0.36      0.83      0.50        24\ntraffic light       0.32      0.79      0.45        24\n      giraffe       0.36      0.91      0.52        23\n\n    micro avg       0.21      0.79      0.34       750\n    macro avg       0.23      0.74      0.34       750\n weighted avg       0.23      0.79      0.36       750\n</code></pre> <p></p> <p>Note</p> <p>The easiest way to analyze models in FiftyOne is via the Model Evaluation panel!</p>"},{"location":"fiftyone_concepts/evaluation/#map-and-pr-curves","title":"mAP and PR curves \u00b6","text":"<p>You can easily compute mean average precision (mAP) and precision-recall (PR) curves using the results object returned by <code>evaluate_detections()</code>:</p> <p>Note</p> <p>FiftyOne\u2019s implementation of Open Images-style evaluation matches the reference implementation available via the TF Object Detection API.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nprint(dataset)\n\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    method=\"open-images\",\n)\n\nprint(results.mAP())\n# 0.599\n\nplot = results.plot_pr_curves(classes=[\"person\", \"dog\", \"car\"])\nplot.show()\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/evaluation/#confusion-matrices_2","title":"Confusion matrices \u00b6","text":"<p>You can also easily generate confusion matrices for the results of Open Images-style evaluations.</p> <p>In order for the confusion matrix to capture anything other than false positive/negative counts, you will likely want to set the <code>classwise</code> parameter to <code>False</code> during evaluation so that predicted objects can be matched with ground truth objects of different classes.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Perform evaluation, allowing objects to be matched between classes\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    method=\"open-images\",\n    classwise=False,\n)\n\n# Generate a confusion matrix for the specified classes\nplot = results.plot_confusion_matrix(classes=[\"car\", \"truck\", \"motorcycle\"])\nplot.show()\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/evaluation/#activitynet-style-evaluation-default-temporal","title":"ActivityNet-style evaluation (default temporal) \u00b6","text":"<p>By default, <code>evaluate_detections()</code> will use ActivityNet-style temporal detection evaluation. to analyze predictions when the specified label fields are <code>TemporalDetections</code>.</p> <p>You can also explicitly request that ActivityNet-style evaluation be used by setting the <code>method</code> parameter to <code>\"activitynet\"</code>.</p> <p>Note</p> <p>FiftyOne\u2019s implementation of ActivityNet-style evaluation matches the reference implementation available via the ActivityNet API.</p>"},{"location":"fiftyone_concepts/evaluation/#overview_3","title":"Overview \u00b6","text":"<p>When running ActivityNet-style evaluation using <code>evaluate_detections()</code>:</p> <ul> <li> <p>Predicted and ground truth segments are matched using a specified IoU threshold (default = 0.50). This threshold can be customized via the <code>iou</code> parameter</p> </li> <li> <p>By default, only segments with the same <code>label</code> will be matched. Classwise matching can be disabled by passing <code>classwise=False</code></p> </li> <li> <p>mAP is computed by averaging over the same range of IoU values used by COCO</p> </li> </ul> <p>When you specify an <code>eval_key</code> parameter, a number of helpful fields will be populated on each sample and its predicted/ground truth segments:</p> <ul> <li>True positive (TP), false positive (FP), and false negative (FN) counts for the each sample are saved in top-level fields of each sample:</li> </ul> <pre><code>TP: sample.&lt;eval_key&gt;_tp\nFP: sample.&lt;eval_key&gt;_fp\nFN: sample.&lt;eval_key&gt;_fn\n</code></pre> <ul> <li>The fields listed below are populated on each individual temporal detection segment; these fields tabulate the TP/FP/FN status of the segment, the ID of the matching segment (if any), and the matching IoU:</li> </ul> <pre><code>TP/FP/FN: segment.&lt;eval_key&gt;\n        ID: segment.&lt;eval_key&gt;_id\n       IoU: segment.&lt;eval_key&gt;_iou\n</code></pre> <p>Note</p> <p>See <code>ActivityNetEvaluationConfig</code> for complete descriptions of the optional keyword arguments that you can pass to <code>evaluate_detections()</code> when running ActivityNet-style evaluation.</p>"},{"location":"fiftyone_concepts/evaluation/#example-evaluation_2","title":"Example evaluation \u00b6","text":"<p>The example below demonstrates ActivityNet-style temporal detection evaluation on the ActivityNet 200 dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\nimport random\n\n# Load subset of ActivityNet 200\nclasses = [\"Bathing dog\", \"Walking the dog\"]\ndataset = foz.load_zoo_dataset(\n    \"activitynet-200\",\n    split=\"validation\",\n    classes=classes,\n    max_samples=10,\n)\nprint(dataset)\n\n# Generate some fake predictions for this example\nrandom.seed(51)\ndataset.clone_sample_field(\"ground_truth\", \"predictions\")\nfor sample in dataset:\n    for det in sample.predictions.detections:\n        det.support[0] += random.randint(-10,10)\n        det.support[1] += random.randint(-10,10)\n        det.support[0] = max(det.support[0], 1)\n        det.support[1] = max(det.support[1], det.support[0] + 1)\n        det.confidence = random.random()\n        det.label = random.choice(classes)\n\n    sample.save()\n\n# Evaluate the segments in the `predictions` field with respect to the\n# segments in the `ground_truth` field\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval\",\n)\n\n# Print a classification report for the classes\nresults.print_report(classes=classes)\n\n# Print some statistics about the total TP/FP/FN counts\nprint(\"TP: %d\" % dataset.sum(\"eval_tp\"))\nprint(\"FP: %d\" % dataset.sum(\"eval_fp\"))\nprint(\"FN: %d\" % dataset.sum(\"eval_fn\"))\n\n# Create a view that has samples with the most false positives first, and\n# only includes false positive segments in the `predictions` field\nview = (\n    dataset\n    .sort_by(\"eval_fp\", reverse=True)\n    .filter_labels(\"predictions\", F(\"eval\") == \"fp\")\n)\n\n# Visualize results in the App\nsession = fo.launch_app(view=view)\n</code></pre> <pre><code>                 precision    recall  f1-score   support\n\n    Bathing dog       0.50      0.40      0.44         5\nWalking the dog       0.50      0.60      0.55         5\n\n      micro avg       0.50      0.50      0.50        10\n      macro avg       0.50      0.50      0.49        10\n   weighted avg       0.50      0.50      0.49        10\n</code></pre> <p></p> <p>Note</p> <p>The easiest way to analyze models in FiftyOne is via the Model Evaluation panel!</p>"},{"location":"fiftyone_concepts/evaluation/#map-and-pr-curves_1","title":"mAP and PR curves \u00b6","text":"<p>You can compute mean average precision (mAP) and precision-recall (PR) curves for your segments by passing the <code>compute_mAP=True</code> flag to <code>evaluate_detections()</code>:</p> <p>Note</p> <p>All mAP calculations are performed according to the ActivityNet evaluation protocol.</p> <pre><code>import random\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Load subset of ActivityNet 200\nclasses = [\"Bathing dog\", \"Walking the dog\"]\ndataset = foz.load_zoo_dataset(\n    \"activitynet-200\",\n    split=\"validation\",\n    classes=classes,\n    max_samples=10,\n)\nprint(dataset)\n\n# Generate some fake predictions for this example\nrandom.seed(51)\ndataset.clone_sample_field(\"ground_truth\", \"predictions\")\nfor sample in dataset:\n    for det in sample.predictions.detections:\n        det.support[0] += random.randint(-10,10)\n        det.support[1] += random.randint(-10,10)\n        det.support[0] = max(det.support[0], 1)\n        det.support[1] = max(det.support[1], det.support[0] + 1)\n        det.confidence = random.random()\n        det.label = random.choice(classes)\n\n    sample.save()\n\n# Performs an IoU sweep so that mAP and PR curves can be computed\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval\",\n    compute_mAP=True,\n)\n\nprint(results.mAP())\n# 0.367\n\nplot = results.plot_pr_curves(classes=classes)\nplot.show()\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/evaluation/#confusion-matrices_3","title":"Confusion matrices \u00b6","text":"<p>You can also easily generate confusion matrices for the results of ActivityNet-style evaluations.</p> <p>In order for the confusion matrix to capture anything other than false positive/negative counts, you will likely want to set the <code>classwise</code> parameter to <code>False</code> during evaluation so that predicted segments can be matched with ground truth segments of different classes.</p> <pre><code>import random\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Load subset of ActivityNet 200\nclasses = [\"Bathing dog\", \"Grooming dog\", \"Grooming horse\", \"Walking the dog\"]\ndataset = foz.load_zoo_dataset(\n    \"activitynet-200\",\n    split=\"validation\",\n    classes=classes,\n    max_samples=20,\n)\nprint(dataset)\n\n# Generate some fake predictions for this example\nrandom.seed(51)\ndataset.clone_sample_field(\"ground_truth\", \"predictions\")\nfor sample in dataset:\n    for det in sample.predictions.detections:\n        det.support[0] += random.randint(-10,10)\n        det.support[1] += random.randint(-10,10)\n        det.support[0] = max(det.support[0], 1)\n        det.support[1] = max(det.support[1], det.support[0] + 1)\n        det.confidence = random.random()\n        det.label = random.choice(classes)\n\n    sample.save()\n\n# Perform evaluation, allowing objects to be matched between classes\nresults = dataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", classwise=False\n)\n\n# Generate a confusion matrix for the specified classes\nplot = results.plot_confusion_matrix(classes=classes)\nplot.show()\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/evaluation/#semantic-segmentations","title":"Semantic segmentations \u00b6","text":"<p>You can use the <code>evaluate_segmentations()</code> method to evaluate the predictions of a semantic segmentation model stored in a <code>Segmentation</code> field of your dataset.</p> <p>By default, the full segmentation masks will be evaluated at a pixel level, but you can specify other evaluation strategies such as evaluating only boundary pixels (see below for details).</p> <p>Invoking <code>evaluate_segmentations()</code> returns a <code>SegmentationResults</code> instance that provides a variety of methods for generating various aggregate evaluation reports about your model.</p> <p>In addition, when you specify an <code>eval_key</code> parameter, a number of helpful fields will be populated on each sample that you can leverage via the FiftyOne App to interactively explore the strengths and weaknesses of your model on individual samples.</p> <p>Note</p> <p>You can store mask targets for your <code>Segmentation</code> fields on your dataset so that you can view semantic labels in the App and avoid having to manually specify the set of mask targets each time you run <code>evaluate_segmentations()</code> on a dataset.</p>"},{"location":"fiftyone_concepts/evaluation/#simple-evaluation-default_2","title":"Simple evaluation (default) \u00b6","text":"<p>By default, <code>evaluate_segmentations()</code> will perform pixelwise evaluation of the segmentation masks, treating each pixel as a multiclass classification.</p> <p>Here are some things to keep in mind:</p> <ul> <li> <p>If the size of a predicted mask does not match the ground truth mask, it is resized to match the ground truth.</p> </li> <li> <p>You can specify the optional <code>bandwidth</code> parameter to evaluate only along the contours of the ground truth masks. By default, the entire masks are evaluated.</p> </li> </ul> <p>You can explicitly request that this strategy be used by setting the <code>method</code> parameter to <code>\"simple\"</code>.</p> <p>When you specify an <code>eval_key</code> parameter, the accuracy, precision, and recall of each sample is recorded in top-level fields of each sample:</p> <pre><code> Accuracy: sample.&lt;eval_key&gt;_accuracy\nPrecision: sample.&lt;eval_key&gt;_precision\n   Recall: sample.&lt;eval_key&gt;_recall\n</code></pre> <p>Note</p> <p>The mask values <code>0</code> and <code>#000000</code> are treated as a background class for the purposes of computing evaluation metrics like precision and recall.</p> <p>The example below demonstrates segmentation evaluation by comparing the masks generated by two DeepLabv3 models (with ResNet50 and ResNet101 backbones):</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Load a few samples from COCO-2017\ndataset = foz.load_zoo_dataset(\n    \"quickstart\",\n    dataset_name=\"segmentation-eval-demo\",\n    max_samples=10,\n    shuffle=True,\n)\n\n# The models are trained on the VOC classes\nCLASSES = (\n    \"background,aeroplane,bicycle,bird,boat,bottle,bus,car,cat,chair,cow,\" +\n    \"diningtable,dog,horse,motorbike,person,pottedplant,sheep,sofa,train,\" +\n    \"tvmonitor\"\n)\ndataset.default_mask_targets = {\n    idx: label for idx, label in enumerate(CLASSES.split(\",\"))\n}\n\n# Add DeepLabv3-ResNet101 predictions to dataset\nmodel = foz.load_zoo_model(\"deeplabv3-resnet101-coco-torch\")\ndataset.apply_model(model, \"resnet101\")\n\n# Add DeepLabv3-ResNet50 predictions to dataset\nmodel = foz.load_zoo_model(\"deeplabv3-resnet50-coco-torch\")\ndataset.apply_model(model, \"resnet50\")\n\nprint(dataset)\n\n# Evaluate the masks w/ ResNet50 backbone, treating the masks w/ ResNet101\n# backbone as \"ground truth\"\nresults = dataset.evaluate_segmentations(\n    \"resnet50\",\n    gt_field=\"resnet101\",\n    eval_key=\"eval_simple\",\n)\n\n# Get a sense for the per-sample variation in likeness\nprint(\"Accuracy range: (%f, %f)\" % dataset.bounds(\"eval_simple_accuracy\"))\nprint(\"Precision range: (%f, %f)\" % dataset.bounds(\"eval_simple_precision\"))\nprint(\"Recall range: (%f, %f)\" % dataset.bounds(\"eval_simple_recall\"))\n\n# Print a classification report\nresults.print_report()\n\n# Visualize results in the App\nsession = fo.launch_app(dataset)\n</code></pre> <p></p> <p>Note</p> <p>The easiest way to analyze models in FiftyOne is via the Model Evaluation panel!</p>"},{"location":"fiftyone_concepts/evaluation/#advanced-usage","title":"Advanced usage \u00b6","text":""},{"location":"fiftyone_concepts/evaluation/#evaluating-views-into-your-dataset","title":"Evaluating views into your dataset \u00b6","text":"<p>All evaluation methods are exposed on <code>DatasetView</code> objects, which means that you can define arbitrarily complex views into your datasets and run evaluation on those.</p> <p>For example, the snippet below evaluates only the medium-sized objects in a dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\", dataset_name=\"eval-demo\")\ndataset.compute_metadata()\n\n# Create an expression that will match objects whose bounding boxes have\n# areas between 32^2 and 96^2 pixels\nbbox_area = (\n    F(\"$metadata.width\") * F(\"bounding_box\")[2] *\n    F(\"$metadata.height\") * F(\"bounding_box\")[3]\n)\nmedium_boxes = (32 ** 2 &lt; bbox_area) &amp; (bbox_area &lt; 96 ** 2)\n\n# Create a view that contains only medium-sized objects\nmedium_view = (\n    dataset\n    .filter_labels(\"ground_truth\", medium_boxes)\n    .filter_labels(\"predictions\", medium_boxes)\n)\n\nprint(medium_view)\n\n# Evaluate the medium-sized objects\nresults = medium_view.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval_medium\",\n)\n\n# Print some aggregate metrics\nprint(results.metrics())\n\n# View results in the App\nsession = fo.launch_app(view=medium_view)\n</code></pre> <p>Note</p> <p>If you run evaluation on a complex view, don\u2019t worry, you can always load the view later!</p>"},{"location":"fiftyone_concepts/evaluation/#loading-a-previous-evaluation-result","title":"Loading a previous evaluation result \u00b6","text":"<p>You can view a list of evaluation keys for evaluations that you have previously run on a dataset via <code>list_evaluations()</code>.</p> <p>Evaluation keys are stored at the dataset-level, but if a particular evaluation was run on a view into your dataset, you can use <code>load_evaluation_view()</code> to retrieve the exact view on which you evaluated:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.load_dataset(...)\n\n# List available evaluations\ndataset.list_evaluations()\n# [\"my_eval1\", \"my_eval2\", ...]\n\n# Load the view into the dataset on which `my_eval1` was run\neval1_view = dataset.load_evaluation_view(\"my_eval1\")\n</code></pre> <p>Note</p> <p>If you have run multiple evaluations on a dataset, you can use the <code>select_fields</code> parameter of the <code>load_evaluation_view()</code> method to hide any fields that were populated by other evaluation runs, allowing you to, for example, focus on a specific set of evaluation results in the App:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.load_dataset(...)\n\n# Load a view that contains the results of evaluation `my_eval1` and\n# hides all other evaluation data\neval1_view = dataset.load_evaluation_view(\"my_eval1\", select_fields=True)\n\nsession = fo.launch_app(view=eval1_view)\n</code></pre>"},{"location":"fiftyone_concepts/evaluation/#evaluating-videos","title":"Evaluating videos \u00b6","text":"<p>All evaluation methods can be applied to frame-level labels in addition to sample-level labels.</p> <p>You can evaluate frame-level labels of a video dataset by adding the <code>frames</code> prefix to the relevant prediction and ground truth frame fields.</p> <p>Note</p> <p>When evaluating frame-level labels, helpful statistics are tabulated at both the sample- and frame-levels of your dataset. Refer to the documentation of the relevant evaluation method for more details.</p> <p>The example below demonstrates evaluating (mocked) frame-level detections on the quickstart-video dataset from the Dataset Zoo:</p> <pre><code>import random\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"quickstart-video\", dataset_name=\"video-eval-demo\"\n)\n\n#\n# Create some test predictions by copying the ground truth objects into a\n# new `predictions` field of the frames with 10% of the labels perturbed at\n# random\n#\n\nclasses = dataset.distinct(\"frames.detections.detections.label\")\n\ndef jitter(val):\n    if random.random() &lt; 0.10:\n        return random.choice(classes)\n\n    return val\n\npredictions = []\nfor sample_gts in dataset.values(\"frames.detections\"):\n    sample_predictions = []\n    for frame_gts in sample_gts:\n        sample_predictions.append(\n            fo.Detections(\n                detections=[\\\n                    fo.Detection(\\\n                        label=jitter(gt.label),\\\n                        bounding_box=gt.bounding_box,\\\n                        confidence=random.random(),\\\n                    )\\\n                    for gt in frame_gts.detections\\\n                ]\n            )\n        )\n\n    predictions.append(sample_predictions)\n\ndataset.set_values(\"frames.predictions\", predictions)\n\nprint(dataset)\n\n# Evaluate the frame-level `predictions` against the frame-level\n# `detections` objects\nresults = dataset.evaluate_detections(\n    \"frames.predictions\",\n    gt_field=\"frames.detections\",\n    eval_key=\"eval\",\n)\n\n# Print a classification report\nresults.print_report()\n</code></pre> <pre><code>              precision    recall  f1-score   support\n\n      person       0.76      0.93      0.84      1108\n   road sign       0.90      0.94      0.92      2726\n     vehicle       0.98      0.94      0.96      7511\n\n   micro avg       0.94      0.94      0.94     11345\n   macro avg       0.88      0.94      0.91     11345\nweighted avg       0.94      0.94      0.94     11345\n</code></pre> <p>You can also view frame-level evaluation results as evaluation patches by first converting to frames and then to patches!</p> <pre><code># Convert to frame evaluation patches\nframes = dataset.to_frames(sample_frames=True)\nframe_eval_patches = frames.to_evaluation_patches(\"eval\")\nprint(frame_eval_patches)\n\nprint(frame_eval_patches.count_values(\"type\"))\n# {'tp': 10578, 'fn': 767, 'fp': 767}\n\nsession = fo.launch_app(view=frame_eval_patches)\n</code></pre> <pre><code>Dataset:     video-eval-demo\nMedia type:  image\nNum patches: 12112\nPatch fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    sample_id:        fiftyone.core.fields.ObjectIdField\n    frame_id:         fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    frame_number:     fiftyone.core.fields.FrameNumberField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    predictions:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    type:             fiftyone.core.fields.StringField\n    iou:              fiftyone.core.fields.FloatField\n    crowd:            fiftyone.core.fields.BooleanField\nView stages:\n    1. ToFrames(config=None)\n    2. ToEvaluationPatches(eval_key='eval', config=None)\n</code></pre>"},{"location":"fiftyone_concepts/evaluation/#custom-evaluation-backends","title":"Custom evaluation backends \u00b6","text":"<p>If you would like to use an evaluation protocol that is not natively supported by FiftyOne, you can follow the instructions below to implement an interface for your protocol and then configure your environment so that FiftyOne\u2019s evaluation methods will use it.</p>"},{"location":"fiftyone_concepts/evaluation/#evaluation-config","title":"Evaluation config \u00b6","text":"<p>FiftyOne provides an evaluation config that you can use to either temporarily or permanently configure the behavior of the evaluation API.</p>"},{"location":"fiftyone_concepts/evaluation/#viewing-your-config","title":"Viewing your config \u00b6","text":"<p>You can print your current evaluation config at any time via the Python library and the CLI:</p> <p>Note</p> <p>If you have customized your evaluation config via any of the methods described below, printing your config is a convenient way to ensure that the changes you made have taken effect as you expected.</p>"},{"location":"fiftyone_concepts/evaluation/#modifying-your-config","title":"Modifying your config \u00b6","text":"<p>You can modify your evaluation config in a variety of ways. The following sections describe these options in detail.</p>"},{"location":"fiftyone_concepts/evaluation/#order-of-precedence","title":"Order of precedence \u00b6","text":"<p>The following order of precedence is used to assign values to your evaluation config settings as runtime:</p> <ol> <li> <p>Config settings applied at runtime by directly editing <code>fiftyone.evaluation_config</code></p> </li> <li> <p><code>FIFTYONE_XXX</code> environment variables</p> </li> <li> <p>Settings in your JSON config ( <code>~/.fiftyone/evaluation_config.json</code>)</p> </li> <li> <p>The default config values</p> </li> </ol>"},{"location":"fiftyone_concepts/evaluation/#editing-your-json-config","title":"Editing your JSON config \u00b6","text":"<p>You can permanently customize your evaluation config by creating a <code>~/.fiftyone/evaluation_config.json</code> file on your machine. The JSON file may contain any desired subset of config fields that you wish to customize.</p> <p>For example, the following config JSON file declares a new <code>custom</code> detection evaluation backend without changing any other default config settings:</p> <pre><code>{\n    \"default_detection_backend\": \"custom\",\n    \"detection_backends\": {\n        \"custom\": {\n            \"config_cls\": \"path.to.your.CustomDetectionEvaluationConfig\"\n        }\n    }\n}\n</code></pre> <p>When <code>fiftyone</code> is imported, any options from your JSON config are merged into the default config, as per the order of precedence described above.</p> <p>Note</p> <p>You can customize the location from which your JSON config is read by setting the <code>FIFTYONE_EVALUATION_CONFIG_PATH</code> environment variable.</p>"},{"location":"fiftyone_concepts/evaluation/#setting-environment-variables","title":"Setting environment variables \u00b6","text":"<p>Evaluation config settings may be customized on a per-session basis by setting the <code>FIFTYONE_&lt;TYPE&gt;_XXX</code> environment variable(s) for the desired config settings, where <code>&lt;TYPE&gt;</code> can be <code>REGRESSION</code>, <code>CLASSIFICATION</code>, <code>DETECTION</code>, or <code>SEGMENTATION</code>.</p> <p>The <code>FIFTYONE_DEFAULT_&lt;TYPE&gt;_BACKEND</code> environment variables allows you to configure your default backend:</p> <pre><code>export FIFTYONE_DEFAULT_DETECTION_BACKEND=coco\n</code></pre> <p>You can declare parameters for specific evaluation backends by setting environment variables of the form <code>FIFTYONE_&lt;TYPE&gt;_&lt;BACKEND&gt;_&lt;PARAMETER&gt;</code>. Any settings that you declare in this way will be passed as keyword arguments to methods like <code>evaluate_detections()</code> whenever the corresponding backend is in use:</p> <pre><code>export FIFTYONE_DETECTION_COCO_ISCROWD=is_crowd\n</code></pre> <p>The <code>FIFTYONE_&lt;TYPE&gt;_BACKENDS</code> environment variables can be set to a <code>list,of,backends</code> that you want to expose in your session, which may exclude native backends and/or declare additional custom backends whose parameters are defined via additional config modifications of any kind:</p> <pre><code>export FIFTYONE_DETECTION_BACKENDS=custom,coco,open-images\n</code></pre> <p>When declaring new backends, you can include <code>*</code> to append new backend(s) without omitting or explicitly enumerating the builtin backends. For example, you can add a <code>custom</code> detection evaluation backend as follows:</p> <pre><code>export FIFTYONE_DETECTION_BACKENDS=*,custom\nexport FIFTYONE_DETECTION_CUSTOM_CONFIG_CLS=your.custom.DetectionEvaluationConfig\n</code></pre>"},{"location":"fiftyone_concepts/evaluation/#modifying-your-config-in-code","title":"Modifying your config in code \u00b6","text":"<p>You can dynamically modify your evaluation config at runtime by directly editing the <code>fiftyone.evaluation_config</code> object.</p> <p>Any changes to your evaluation config applied via this manner will immediately take effect in all subsequent calls to <code>fiftyone.evaluation_config</code> during your current session.</p> <pre><code>import fiftyone as fo\n\nfo.evaluation_config.default_detection_backend = \"custom\"\n</code></pre>"},{"location":"fiftyone_concepts/export_datasets/","title":"Exporting FiftyOne Datasets \u00b6","text":"<p>FiftyOne provides native support for exporting datasets to disk in a variety of common formats, and it can be easily extended to export datasets in custom formats.</p> <p>Note</p> <p>Did you know? You can export media and/or labels from within the FiftyOne App by installing the @voxel51/io plugin!</p>"},{"location":"fiftyone_concepts/export_datasets/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The interface for exporting a FiftyOne <code>Dataset</code> is conveniently exposed via the Python library and the CLI. You can easily export entire datasets as well as arbitrary subsets of your datasets that you have identified by constructing a <code>DatasetView</code> into any format of your choice via the basic recipe below.</p>"},{"location":"fiftyone_concepts/export_datasets/#label-type-coercion","title":"Label type coercion \u00b6","text":"<p>For your convenience, the <code>export()</code> method will automatically coerce the data to match the requested export types in a variety of common cases listed below.</p>"},{"location":"fiftyone_concepts/export_datasets/#single-labels-to-lists","title":"Single labels to lists \u00b6","text":"<p>Many export formats expect label list types ( <code>Classifications</code>, <code>Detections</code>, <code>Polylines</code>, or <code>Keypoints</code>). If you provide a label field to <code>export()</code> that refers to a single label type ( <code>Classification</code>, <code>Detection</code>, <code>Polyline</code>, or <code>Keypoint</code>), then the labels will be automatically upgraded to single-label lists to match the export type\u2019s expectations.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\npatches = dataset.to_patches(\"ground_truth\")\n\n# The `ground_truth` field has type `Detection`, but COCO format expects\n# `Detections`, so the labels are automatically coerced to single-label lists\npatches.export(\n    export_dir=\"/tmp/quickstart/detections\",\n    dataset_type=fo.types.COCODetectionDataset,\n    label_field=\"ground_truth\",\n)\n</code></pre>"},{"location":"fiftyone_concepts/export_datasets/#classifications-as-detections","title":"Classifications as detections \u00b6","text":"<p>When exporting in labeled image dataset formats that expect <code>Detections</code> labels, if you provide a label field to <code>export()</code> that has type <code>Classification</code>, the classification labels will be automatically upgraded to detections that span the entire images.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\").limit(5).clone()\n\nfor idx, sample in enumerate(dataset):\n    sample[\"attribute\"] = fo.Classification(label=str(idx))\n    sample.save()\n\n# Exports the `attribute` classifications as detections that span entire images\ndataset.export(\n    export_dir=\"/tmp/quickstart/attributes\",\n    dataset_type=fo.types.COCODetectionDataset,\n    label_field=\"attribute\",\n)\n</code></pre>"},{"location":"fiftyone_concepts/export_datasets/#object-patches","title":"Object patches \u00b6","text":"<p>When exporting in either an unlabeled image or image classification format, if a spatial label field ( <code>Detection</code>, <code>Detections</code>, <code>Polyline</code>, or <code>Polylines</code>) is provided to <code>export()</code>, the object patches of the provided samples will be exported.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# No label field is provided; only images are exported\ndataset.export(\n    export_dir=\"/tmp/quickstart/images\",\n    dataset_type=fo.types.ImageDirectory,\n)\n\n# A detections field is provided, so the object patches are exported as a\n# directory of images\ndataset.export(\n    export_dir=\"/tmp/quickstart/patches\",\n    dataset_type=fo.types.ImageDirectory,\n    label_field=\"ground_truth\",\n)\n\n# A detections field is provided, so the object patches are exported as an\n# image classification directory tree\ndataset.export(\n    export_dir=\"/tmp/quickstart/objects\",\n    dataset_type=fo.types.ImageClassificationDirectoryTree,\n    label_field=\"ground_truth\",\n)\n</code></pre> <p>You can also directly call <code>export()</code> on patches views to export the specified object patches along with their appropriately typed labels.</p> <pre><code># Continuing from above...\n\npatches = dataset.to_patches(\"ground_truth\")\n\n# Export the object patches as a directory of images\npatches.export(\n    export_dir=\"/tmp/quickstart/also-patches\",\n    dataset_type=fo.types.ImageDirectory,\n)\n\n# Export the object patches as an image classification directory tree\npatches.export(\n    export_dir=\"/tmp/quickstart/also-objects\",\n    dataset_type=fo.types.ImageClassificationDirectoryTree,\n)\n</code></pre>"},{"location":"fiftyone_concepts/export_datasets/#video-clips","title":"Video clips \u00b6","text":"<p>When exporting in either an unlabeled video or video classification format, if a <code>TemporalDetection</code> or <code>TemporalDetections</code> field is provided to <code>export()</code>, the specified video clips will be exported.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=2)\n\n# Add some temporal detections to the dataset\nsample1 = dataset.first()\nsample1[\"events\"] = fo.TemporalDetections(\n    detections=[\\\n        fo.TemporalDetection(label=\"first\", support=[31, 60]),\\\n        fo.TemporalDetection(label=\"second\", support=[90, 120]),\\\n    ]\n)\nsample1.save()\n\nsample2 = dataset.last()\nsample2[\"events\"] = fo.TemporalDetections(\n    detections=[\\\n        fo.TemporalDetection(label=\"first\", support=[16, 45]),\\\n        fo.TemporalDetection(label=\"second\", support=[75, 104]),\\\n    ]\n)\nsample2.save()\n\n# A temporal detection field is provided, so the clips are exported as a\n# directory of videos\ndataset.export(\n    export_dir=\"/tmp/quickstart-video/clips\",\n    dataset_type=fo.types.VideoDirectory,\n    label_field=\"events\",\n)\n\n# A temporal detection field is provided, so the clips are exported as a\n# video classification directory tree\ndataset.export(\n    export_dir=\"/tmp/quickstart-video/video-classifications\",\n    dataset_type=fo.types.VideoClassificationDirectoryTree,\n    label_field=\"events\",\n)\n</code></pre> <p>You can also directly call <code>export()</code> on clip views to export the specified video clips along with their appropriately typed labels.</p> <pre><code># Continuing from above...\n\nclips = dataset.to_clips(\"events\")\n\n# Export the clips as a directory of videos\nclips.export(\n    export_dir=\"/tmp/quickstart-video/also-clips\",\n    dataset_type=fo.types.VideoDirectory,\n)\n\n# Export the clips as a video classification directory tree\nclips.export(\n    export_dir=\"/tmp/quickstart-video/clip-classifications\",\n    dataset_type=fo.types.VideoClassificationDirectoryTree,\n)\n\n# Export the clips along with their associated frame labels\nclips.export(\n    export_dir=\"/tmp/quickstart-video/clip-frame-labels\",\n    dataset_type=fo.types.FiftyOneVideoLabelsDataset,\n    frame_labels_field=\"detections\",\n)\n</code></pre>"},{"location":"fiftyone_concepts/export_datasets/#class-lists","title":"Class lists \u00b6","text":"<p>Certain labeled image/video export formats such as COCO and YOLO store an explicit list of classes for the label field being exported.</p> <p>By convention, all exporters provided by FiftyOne should provide a <code>classes</code> parameter that allows for manually specifying the classes list to use.</p> <p>If no explicit class list is provided, the observed classes in the collection being exported are used, which may be a subset of the classes in the parent dataset when exporting a view.</p> <p>Note</p> <p>See this section for more information about storing class lists on FiftyOne datasets.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\n# Load 10 samples containing cats and dogs (among other objects)\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    classes=[\"cat\", \"dog\"],\n    shuffle=True,\n    max_samples=10,\n)\n\n# Loading zoo datasets generally populates the `default_classes` attribute\nprint(len(dataset.default_classes))  # 91\n\n# Create a view that only contains cats and dogs\nview = dataset.filter_labels(\"ground_truth\", F(\"label\").is_in([\"cat\", \"dog\"]))\n\n# By default, only the observed classes will be stored as COCO categories\nview.export(\n    labels_path=\"/tmp/coco1.json\",\n    dataset_type=fo.types.COCODetectionDataset,\n)\n\n# However, if desired, we can explicitly provide a classes list\nview.export(\n    labels_path=\"/tmp/coco2.json\",\n    dataset_type=fo.types.COCODetectionDataset,\n    classes=dataset.default_classes,\n)\n</code></pre>"},{"location":"fiftyone_concepts/export_datasets/#supported-formats","title":"Supported formats \u00b6","text":"<p>Each supported dataset type is represented by a subclass of <code>fiftyone.types.Dataset</code>, which is used by the Python library and CLI to refer to the corresponding dataset format when writing the dataset to disk.</p> Dataset Type Description ImageDirectory A directory of images. VideoDirectory A directory of videos. MediaDirectory A directory of media files. FiftyOneImageClassificationDataset A labeled dataset consisting of images and their associated classification labelsin a simple JSON format. ImageClassificationDirectoryTree A directory tree whose subfolders define an image classification dataset. VideoClassificationDirectoryTree A directory tree whose subfolders define a video classification dataset. TFImageClassificationDataset A labeled dataset consisting of images and their associated classification labelsstored as TFRecords. FiftyOneImageDetectionDataset A labeled dataset consisting of images and their associated object detectionsstored in a simple JSON format. FiftyOneTemporalDetectionDataset A labeled dataset consisting of videos and their associated temporal detections ina simple JSON format. COCODetectionDataset A labeled dataset consisting of images and their associated object detectionssaved in COCO Object Detection Format. VOCDetectionDataset A labeled dataset consisting of images and their associated object detectionssaved in VOC format. KITTIDetectionDataset A labeled dataset consisting of images and their associated object detectionssaved in KITTI format. YOLOv4Dataset A labeled dataset consisting of images and their associated object detectionssaved in YOLOv4 format. YOLOv5Dataset A labeled dataset consisting of images and their associated object detectionssaved in YOLOv5 format. TFObjectDetectionDataset A labeled dataset consisting of images and their associated object detectionsstored as TFRecords in TF Object Detection API format. ImageSegmentationDirectory A labeled dataset consisting of images and their associated semantic segmentationsstored as images on disk. CVATImageDataset A labeled dataset consisting of images and their associated object detectionsstored in CVAT image format. CVATVideoDataset A labeled dataset consisting of videos and their associated object detectionsstored in CVAT video format. FiftyOneImageLabelsDataset A labeled dataset consisting of images and their associated multitask predictionsstored in ETA ImageLabels format. FiftyOneVideoLabelsDataset A labeled dataset consisting of videos and their associated multitask predictionsstored in ETA VideoLabels format. BDDDataset A labeled dataset consisting of images and their associated multitask predictionssaved in Berkeley DeepDrive (BDD) format. CSVDataset A flexible CSV format that represents slice(s) of a dataset\u2019s values as columns ofa CSV file. GeoJSONDataset An image or video dataset whose location data and labels are stored inGeoJSON format. FiftyOneDataset A dataset consisting of an entire serialized <code>Dataset</code> and its associated sourcemedia. Custom formats Export datasets in custom formats by defining your own <code>Dataset</code> or<code>DatasetExporter</code> class."},{"location":"fiftyone_concepts/export_datasets/#imagedirectory","title":"ImageDirectory \u00b6","text":"<p>The <code>fiftyone.types.ImageDirectory</code> type represents a directory of images.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    &lt;filename1&gt;.&lt;ext&gt;\n    &lt;filename2&gt;.&lt;ext&gt;\n    ...\n</code></pre> <p>Note</p> <p>See <code>ImageDirectoryExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export the images in a FiftyOne dataset as a directory of images on disk as follows:</p>"},{"location":"fiftyone_concepts/export_datasets/#videodirectory","title":"VideoDirectory \u00b6","text":"<p>The <code>fiftyone.types.VideoDirectory</code> type represents a directory of videos.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    &lt;filename1&gt;.&lt;ext&gt;\n    &lt;filename2&gt;.&lt;ext&gt;\n    ...\n</code></pre> <p>Note</p> <p>See <code>VideoDirectoryExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export the videos in a FiftyOne dataset as a directory of videos on disk as follows:</p>"},{"location":"fiftyone_concepts/export_datasets/#mediadirectory","title":"MediaDirectory \u00b6","text":"<p>The <code>fiftyone.types.MediaDirectory</code> type represents a directory of media files.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    &lt;filename1&gt;.&lt;ext&gt;\n    &lt;filename2&gt;.&lt;ext&gt;\n    ...\n</code></pre> <p>Note</p> <p>See <code>MediaDirectoryExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export the media in a FiftyOne dataset as a directory of media files on disk as follows:</p>"},{"location":"fiftyone_concepts/export_datasets/#fiftyoneimageclassificationdataset","title":"FiftyOneImageClassificationDataset \u00b6","text":"<p>Supported label types</p> <p><code>Classification</code>, <code>Classifications</code></p> <p>The <code>fiftyone.types.FiftyOneImageClassificationDataset</code> type represents a labeled dataset consisting of images and their associated classification label(s) stored in a simple JSON format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels.json\n</code></pre> <p>In the simplest case, <code>labels.json</code> will be a JSON file in the following format:</p> <pre><code>{\n    \"classes\": [\\\n        \"&lt;labelA&gt;\",\\\n        \"&lt;labelB&gt;\",\\\n        ...\\\n    ],\n    \"labels\": {\n        \"&lt;uuid1&gt;\": &lt;target&gt;,\n        \"&lt;uuid2&gt;\": &lt;target&gt;,\n        ...\n    }\n}\n</code></pre> <p>If the <code>classes</code> field is included in the JSON, the <code>target</code> values are class IDs that are mapped to class label strings via <code>classes[target]</code>. If no <code>classes</code> are included, then the <code>target</code> values directly store the label strings.</p> <p>The target value in <code>labels</code> for unlabeled images is <code>None</code>.</p> <p>If you wish to export classifications with associated confidences and/or additional attributes, you can use the <code>include_confidence</code> and <code>include_attributes</code> parameters to include this information in the export. In this case, <code>labels.json</code> will have the following format:</p> <pre><code>{\n    \"classes\": [\\\n        \"&lt;labelA&gt;\",\\\n        \"&lt;labelB&gt;\",\\\n        ...\\\n    ],\n    \"labels\": {\n        \"&lt;uuid1&gt;\": {\n            \"label\": &lt;target&gt;,\n            \"confidence\": &lt;optional-confidence&gt;,\n            \"attributes\": {\n                &lt;optional-name&gt;: &lt;optional-value&gt;,\n                ...\n            }\n        },\n        \"&lt;uuid2&gt;\": {\n            \"label\": &lt;target&gt;,\n            \"confidence\": &lt;optional-confidence&gt;,\n            \"attributes\": {\n                &lt;optional-name&gt;: &lt;optional-value&gt;,\n                ...\n            }\n        },\n        ...\n    }\n}\n</code></pre> <p>You can also export multilabel classification fields, in which case <code>labels.json</code> will have the following format:</p> <pre><code>{\n    \"classes\": [\\\n        \"&lt;labelA&gt;\",\\\n        \"&lt;labelB&gt;\",\\\n        ...\\\n    ],\n    \"labels\": {\n        \"&lt;uuid1&gt;\": [&lt;target1&gt;, &lt;target2&gt;, ...],\n        \"&lt;uuid2&gt;\": [&lt;target1&gt;, &lt;target2&gt;, ...],\n        ...\n    }\n}\n</code></pre> <p>where the target values in <code>labels</code> may be class strings, class IDs, or dicts in the format described above defining class labels, confidences, and optional attributes, depending on how you configured the export.</p> <p>Note</p> <p>See <code>FiftyOneImageClassificationDatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as an image classification dataset stored on disk in the above format as follows:</p> <p>Note</p> <p>You can pass the optional <code>classes</code> parameter to <code>export()</code> to explicitly define the class list to use in the exported labels. Otherwise, the strategy outlined in this section will be used to populate the class list.</p> <p>You can also perform labels-only exports in this format by providing the <code>labels_path</code> parameter instead of <code>export_dir</code> to <code>export()</code> to specify a location to write (only) the labels.</p> <p>Note</p> <p>You can optionally include the <code>export_media=False</code> option to <code>export()</code> to make it explicit that you only wish to export labels, although this will be inferred if you do not provide an <code>export_dir</code> or <code>data_path</code>.</p> <p>By default, the filenames of your images will be used as keys in the exported labels. However, you can also provide the optional <code>rel_dir</code> parameter to <code>export()</code> to specify a prefix to strip from each image path to generate a key for the image. This argument allows for populating nested subdirectories that match the shape of the input paths.</p>"},{"location":"fiftyone_concepts/export_datasets/#imageclassificationdirectorytree","title":"ImageClassificationDirectoryTree \u00b6","text":"<p>Supported label types</p> <p><code>Classification</code></p> <p>The <code>fiftyone.types.ImageClassificationDirectoryTree</code> type represents a directory tree whose subfolders define an image classification dataset.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    &lt;classA&gt;/\n        &lt;image1&gt;.&lt;ext&gt;\n        &lt;image2&gt;.&lt;ext&gt;\n        ...\n    &lt;classB&gt;/\n        &lt;image1&gt;.&lt;ext&gt;\n        &lt;image2&gt;.&lt;ext&gt;\n        ...\n    ...\n</code></pre> <p>Unlabeled images are stored in a subdirectory named <code>_unlabeled</code>.</p> <p>Note</p> <p>See <code>ImageClassificationDirectoryTreeExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as an image classification directory tree stored on disk in the above format as follows:</p>"},{"location":"fiftyone_concepts/export_datasets/#videoclassificationdirectorytree","title":"VideoClassificationDirectoryTree \u00b6","text":"<p>Supported label types</p> <p><code>Classification</code></p> <p>The <code>fiftyone.types.VideoClassificationDirectoryTree</code> type represents a directory tree whose subfolders define a video classification dataset.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    &lt;classA&gt;/\n        &lt;video1&gt;.&lt;ext&gt;\n        &lt;video2&gt;.&lt;ext&gt;\n        ...\n    &lt;classB&gt;/\n        &lt;video1&gt;.&lt;ext&gt;\n        &lt;video2&gt;.&lt;ext&gt;\n        ...\n    ...\n</code></pre> <p>Unlabeled videos are stored in a subdirectory named <code>_unlabeled</code>.</p> <p>Note</p> <p>See <code>VideoClassificationDirectoryTreeExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as a video classification directory tree stored on disk in the above format as follows:</p>"},{"location":"fiftyone_concepts/export_datasets/#tfimageclassificationdataset","title":"TFImageClassificationDataset \u00b6","text":"<p>Supported label types</p> <p><code>Classification</code></p> <p>The <code>fiftyone.types.TFImageClassificationDataset</code> type represents a labeled dataset consisting of images and their associated classification labels stored as TFRecords.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    tf.records-?????-of-?????\n</code></pre> <p>where the features of the (possibly sharded) TFRecords are stored in the following format:</p> <pre><code>{\n    # Image dimensions\n    \"height\": tf.io.FixedLenFeature([], tf.int64),\n    \"width\": tf.io.FixedLenFeature([], tf.int64),\n    \"depth\": tf.io.FixedLenFeature([], tf.int64),\n    # Image filename\n    \"filename\": tf.io.FixedLenFeature([], tf.int64),\n    # The image extension\n    \"format\": tf.io.FixedLenFeature([], tf.string),\n    # Encoded image bytes\n    \"image_bytes\": tf.io.FixedLenFeature([], tf.string),\n    # Class label string\n    \"label\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n}\n</code></pre> <p>For unlabeled samples, the TFRecords do not contain <code>label</code> features.</p> <p>Note</p> <p>See <code>TFImageClassificationDatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as a directory of TFRecords in the above format as follows:</p> <p>Note</p> <p>You can provide the <code>tf_records_path</code> argument instead of <code>export_dir</code> in the examples above to directly specify the path to the TFRecord(s) to write. See <code>TFImageClassificationDatasetExporter</code> for details.</p>"},{"location":"fiftyone_concepts/export_datasets/#fiftyoneimagedetectiondataset","title":"FiftyOneImageDetectionDataset \u00b6","text":"<p>Supported label types</p> <p><code>Detections</code></p> <p>The <code>fiftyone.types.FiftyOneImageDetectionDataset</code> type represents a labeled dataset consisting of images and their associated object detections stored in a simple JSON format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels.json\n</code></pre> <p>where <code>labels.json</code> is a JSON file in the following format:</p> <pre><code>{\n    \"classes\": [\\\n        &lt;labelA&gt;,\\\n        &lt;labelB&gt;,\\\n        ...\\\n    ],\n    \"labels\": {\n        &lt;uuid1&gt;: [\\\n            {\\\n                \"label\": &lt;target&gt;,\\\n                \"bounding_box\": [\\\n                    &lt;top-left-x&gt;, &lt;top-left-y&gt;, &lt;width&gt;, &lt;height&gt;\\\n                ],\\\n                \"confidence\": &lt;optional-confidence&gt;,\\\n                \"attributes\": {\\\n                    &lt;optional-name&gt;: &lt;optional-value&gt;,\\\n                    ...\\\n                }\\\n            },\\\n            ...\\\n        ],\n        &lt;uuid2&gt;: [\\\n            ...\\\n        ],\n        ...\n    }\n}\n</code></pre> <p>and where the bounding box coordinates are expressed as relative values in <code>[0, 1] x [0, 1]</code>.</p> <p>If the <code>classes</code> field is included in the JSON, the <code>target</code> values are class IDs that are mapped to class label strings via <code>classes[target]</code>. If no <code>classes</code> are included, then the <code>target</code> values directly store the label strings.</p> <p>The target value in <code>labels</code> for unlabeled images is <code>None</code>.</p> <p>By default, confidences and any additional dynamic attributes of your detections will be automatically included in the export. However, you can provide the optional <code>include_confidence</code> and <code>include_attributes</code> parameters to customize this behavior.</p> <p>Note</p> <p>See <code>FiftyOneImageDetectionDatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as an image detection dataset in the above format as follows:</p> <p>Note</p> <p>You can pass the optional <code>classes</code> parameter to <code>export()</code> to explicitly define the class list to use in the exported labels. Otherwise, the strategy outlined in this section will be used to populate the class list.</p> <p>You can also perform labels-only exports in this format by providing the <code>labels_path</code> parameter instead of <code>export_dir</code> to <code>export()</code> to specify a location to write (only) the labels.</p> <p>Note</p> <p>You can optionally include the <code>export_media=False</code> option to <code>export()</code> to make it explicit that you only wish to export labels, although this will be inferred if you do not provide an <code>export_dir</code> or <code>data_path</code>.</p> <p>By default, the filenames of your images will be used as keys in the exported labels. However, you can also provide the optional <code>rel_dir</code> parameter to <code>export()</code> to specify a prefix to strip from each image path to generate a key for the image. This argument allows for populating nested subdirectories that match the shape of the input paths.</p>"},{"location":"fiftyone_concepts/export_datasets/#fiftyonetemporaldetectiondataset","title":"FiftyOneTemporalDetectionDataset \u00b6","text":"<p>Supported label types</p> <p><code>TemporalDetections</code></p> <p>The <code>fiftyone.types.FiftyOneTemporalDetectionDataset</code> type represents a labeled dataset consisting of videos and their associated temporal detections stored in a simple JSON format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels.json\n</code></pre> <p>where <code>labels.json</code> is a JSON file in the following format:</p> <pre><code>{\n    \"classes\": [\\\n        \"&lt;labelA&gt;\",\\\n        \"&lt;labelB&gt;\",\\\n        ...\\\n    ],\n    \"labels\": {\n        \"&lt;uuid1&gt;\": [\\\n            {\\\n                \"label\": &lt;target&gt;,\\\n                \"support\": [&lt;first-frame&gt;, &lt;last-frame&gt;],\\\n                \"confidence\": &lt;optional-confidence&gt;,\\\n                \"attributes\": {\\\n                    &lt;optional-name&gt;: &lt;optional-value&gt;,\\\n                    ...\\\n                }\\\n            },\\\n            {\\\n                \"label\": &lt;target&gt;,\\\n                \"support\": [&lt;first-frame&gt;, &lt;last-frame&gt;],\\\n                \"confidence\": &lt;optional-confidence&gt;,\\\n                \"attributes\": {\\\n                    &lt;optional-name&gt;: &lt;optional-value&gt;,\\\n                    ...\\\n                }\\\n            },\\\n            ...\\\n        ],\n        \"&lt;uuid2&gt;\": [\\\n            {\\\n                \"label\": &lt;target&gt;,\\\n                \"timestamps\": [&lt;start-timestamp&gt;, &lt;stop-timestamp&gt;],\\\n                \"confidence\": &lt;optional-confidence&gt;,\\\n                \"attributes\": {\\\n                    &lt;optional-name&gt;: &lt;optional-value&gt;,\\\n                    ...\\\n                }\\\n            },\\\n            {\\\n                \"label\": &lt;target&gt;,\\\n                \"timestamps\": [&lt;start-timestamp&gt;, &lt;stop-timestamp&gt;],\\\n                \"confidence\": &lt;optional-confidence&gt;,\\\n                \"attributes\": {\\\n                    &lt;optional-name&gt;: &lt;optional-value&gt;,\\\n                    ...\\\n                }\\\n            },\\\n        ],\n        ...\n    }\n}\n</code></pre> <p>By default, the <code>support</code> keys will be populated with the <code>[first, last]</code> frame numbers of the detections, but you can pass the <code>use_timestamps=True</code> key during export to instead populate the <code>timestamps</code> keys with the <code>[start, stop]</code> timestamps of the detections, in seconds.</p> <p>If the <code>classes</code> field is included in the JSON, the <code>target</code> values are class IDs that are mapped to class label strings via <code>classes[target]</code>. If no <code>classes</code> are included, then the <code>target</code> values directly store the label strings.</p> <p>The target value in <code>labels</code> for unlabeled videos is <code>None</code>.</p> <p>By default, confidences and any additional dynamic attributes of your detections will be automatically included in the export. However, you can provide the optional <code>include_confidence</code> and <code>include_attributes</code> parameters to customize this behavior.</p> <p>Note</p> <p>See <code>FiftyOneTemporalDetectionDatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as a temporal detection dataset stored on disk in the above format as follows:</p> <p>Note</p> <p>You can pass the optional <code>classes</code> parameter to <code>export()</code> to explicitly define the class list to use in the exported labels. Otherwise, the strategy outlined in this section will be used to populate the class list.</p> <p>You can also perform labels-only exports in this format by providing the <code>labels_path</code> parameter instead of <code>export_dir</code> to <code>export()</code> to specify a location to write (only) the labels.</p> <p>Note</p> <p>You can optionally include the <code>export_media=False</code> option to <code>export()</code> to make it explicit that you only wish to export labels, although this will be inferred if you do not provide an <code>export_dir</code> or <code>data_path</code>.</p> <p>By default, the filenames of your images will be used as keys in the exported labels. However, you can also provide the optional <code>rel_dir</code> parameter to <code>export()</code> to specify a prefix to strip from each image path to generate a key for the image. This argument allows for populating nested subdirectories that match the shape of the input paths.</p>"},{"location":"fiftyone_concepts/export_datasets/#cocodetectiondataset","title":"COCODetectionDataset \u00b6","text":"<p>Supported label types</p> <p><code>Detections</code>, <code>Polylines</code>, <code>Keypoints</code></p> <p>The <code>fiftyone.types.COCODetectionDataset</code> type represents a labeled dataset consisting of images and their associated object detections saved in COCO Object Detection Format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;filename0&gt;.&lt;ext&gt;\n        &lt;filename1&gt;.&lt;ext&gt;\n        ...\n    labels.json\n</code></pre> <p>where <code>labels.json</code> is a JSON file in the following format:</p> <pre><code>{\n    \"info\": {\n        \"year\": \"\",\n        \"version\": \"\",\n        \"description\": \"Exported from FiftyOne\",\n        \"contributor\": \"\",\n        \"url\": \"https://voxel51.com/fiftyone\",\n        \"date_created\": \"2020-06-19T09:48:27\"\n    },\n    \"licenses\": [],\n    \"categories\": [\\\n        {\\\n            \"id\": 1,\\\n            \"name\": \"cat\",\\\n            \"supercategory\": \"animal\"\\\n        },\\\n        ...\\\n    ],\n    \"images\": [\\\n        {\\\n            \"id\": 1,\\\n            \"license\": null,\\\n            \"file_name\": \"&lt;filename0&gt;.&lt;ext&gt;\",\\\n            \"height\": 480,\\\n            \"width\": 640,\\\n            \"date_captured\": null\\\n        },\\\n        ...\\\n    ],\n    \"annotations\": [\\\n        {\\\n            \"id\": 1,\\\n            \"image_id\": 1,\\\n            \"category_id\": 1,\\\n            \"bbox\": [260, 177, 231, 199],\\\n            \"segmentation\": [...],\\\n            \"score\": 0.95,\\\n            \"area\": 45969,\\\n            \"iscrowd\": 0\\\n        },\\\n        ...\\\n    ]\n}\n</code></pre> <p>See this page for a full specification of the <code>segmentation</code> field, which will only be included if you export <code>Detections</code> with instance masks populated or <code>Polylines</code>.</p> <p>For unlabeled datasets, <code>labels.json</code> does not contain an <code>annotations</code> field.</p> <p>The <code>file_name</code> attribute of the labels file encodes the location of the corresponding images, which can be any of the following:</p> <ul> <li> <p>The filename of an image in the <code>data/</code> folder</p> </li> <li> <p>A relative path like <code>path/to/filename.ext</code> specifying the relative path to the image in a nested subfolder of <code>data/</code></p> </li> <li> <p>An absolute path to an image, which may or may not be in the <code>data/</code> folder</p> </li> </ul> <p>Note</p> <p>See <code>COCODetectionDatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as a COCO detection dataset in the above format as follows:</p> <p>Note</p> <p>You can pass the optional <code>classes</code> or <code>categories</code> parameters to <code>export()</code> to explicitly define the class list/category IDs to use in the exported labels. Otherwise, the strategy outlined in this section will be used to populate the class list.</p> <p>You can also perform labels-only exports of COCO-formatted labels by providing the <code>labels_path</code> parameter instead of <code>export_dir</code>:</p>"},{"location":"fiftyone_concepts/export_datasets/#vocdetectiondataset","title":"VOCDetectionDataset \u00b6","text":"<p>Supported label types</p> <p><code>Detections</code></p> <p>The <code>fiftyone.types.VOCDetectionDataset</code> type represents a labeled dataset consisting of images and their associated object detections saved in VOC format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;uuid1&gt;.xml\n        &lt;uuid2&gt;.xml\n        ...\n</code></pre> <p>where the labels XML files are in the following format:</p> <pre><code>&lt;annotation&gt;\n    &lt;folder&gt;&lt;/folder&gt;\n    &lt;filename&gt;image.ext&lt;/filename&gt;\n    &lt;path&gt;/path/to/dataset-dir/data/image.ext&lt;/path&gt;\n    &lt;source&gt;\n        &lt;database&gt;&lt;/database&gt;\n    &lt;/source&gt;\n    &lt;size&gt;\n        &lt;width&gt;640&lt;/width&gt;\n        &lt;height&gt;480&lt;/height&gt;\n        &lt;depth&gt;3&lt;/depth&gt;\n    &lt;/size&gt;\n    &lt;segmented&gt;&lt;/segmented&gt;\n    &lt;object&gt;\n        &lt;name&gt;cat&lt;/name&gt;\n        &lt;pose&gt;&lt;/pose&gt;\n        &lt;truncated&gt;0&lt;/truncated&gt;\n        &lt;difficult&gt;0&lt;/difficult&gt;\n        &lt;occluded&gt;0&lt;/occluded&gt;\n        &lt;bndbox&gt;\n            &lt;xmin&gt;256&lt;/xmin&gt;\n            &lt;ymin&gt;200&lt;/ymin&gt;\n            &lt;xmax&gt;450&lt;/xmax&gt;\n            &lt;ymax&gt;400&lt;/ymax&gt;\n        &lt;/bndbox&gt;\n    &lt;/object&gt;\n    &lt;object&gt;\n        &lt;name&gt;dog&lt;/name&gt;\n        &lt;pose&gt;&lt;/pose&gt;\n        &lt;truncated&gt;1&lt;/truncated&gt;\n        &lt;difficult&gt;1&lt;/difficult&gt;\n        &lt;occluded&gt;1&lt;/occluded&gt;\n        &lt;bndbox&gt;\n            &lt;xmin&gt;128&lt;/xmin&gt;\n            &lt;ymin&gt;100&lt;/ymin&gt;\n            &lt;xmax&gt;350&lt;/xmax&gt;\n            &lt;ymax&gt;300&lt;/ymax&gt;\n        &lt;/bndbox&gt;\n    &lt;/object&gt;\n    ...\n&lt;/annotation&gt;\n</code></pre> <p>Samples with no values for certain attributes (like <code>pose</code> in the above example) are left empty.</p> <p>Unlabeled images have no corresponding file in <code>labels/</code>.</p> <p>Note</p> <p>See <code>VOCDetectionDatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as a VOC detection dataset in the above format as follows:</p> <p>You can also perform labels-only exports of VOC-formatted labels by providing the <code>labels_path</code> parameter instead of <code>export_dir</code>:</p>"},{"location":"fiftyone_concepts/export_datasets/#kittidetectiondataset","title":"KITTIDetectionDataset \u00b6","text":"<p>Supported label types</p> <p><code>Detections</code></p> <p>The <code>fiftyone.types.KITTIDetectionDataset</code> type represents a labeled dataset consisting of images and their associated object detections saved in KITTI format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;uuid1&gt;.txt\n        &lt;uuid2&gt;.txt\n        ...\n</code></pre> <p>where the labels TXT files are space-delimited files where each row corresponds to an object and the 15 (and optional 16th score) columns have the following meanings:</p> # ofcolumns Name Description Default 1 type The object label 1 truncated A float in <code>[0, 1]</code>, where 0 is non-truncated and1 is fully truncated. Here, truncation refers to the objectleaving image boundaries 0 1 occluded An int in <code>(0, 1, 2, 3)</code> indicating occlusion state,where:- 0 = fully visible- 1 = partly occluded- 2 =largely occluded- 3 = unknown 0 1 alpha Observation angle of the object, in <code>[-pi, pi]</code> 0 4 bbox 2D bounding box of object in the image in pixels, in theformat <code>[xtl, ytl, xbr, ybr]</code> 1 dimensions 3D object dimensions, in meters, in the format<code>[height, width, length]</code> 0 1 location 3D object location <code>(x, y, z)</code> in camera coordinates(in meters) 0 1 rotation_y Rotation around the y-axis in camera coordinates, in<code>[-pi, pi]</code> 0 1 score <code>(optional)</code> A float confidence for the detection <p>The <code>default</code> column above indicates the default value that will be used when writing datasets in this type whose samples do not contain the necessary field(s).</p> <p>Unlabeled images have no corresponding file in <code>labels/</code>.</p> <p>Note</p> <p>See <code>KITTIDetectionDatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as a KITTI detection dataset in the above format as follows:</p> <p>You can also perform labels-only exports of KITTI-formatted labels by providing the <code>labels_path</code> parameter instead of <code>export_dir</code>:</p>"},{"location":"fiftyone_concepts/export_datasets/#yolov4dataset","title":"YOLOv4Dataset \u00b6","text":"<p>Supported label types</p> <p><code>Detections</code> <code>Polylines</code></p> <p>The <code>fiftyone.types.YOLOv4Dataset</code> type represents a labeled dataset consisting of images and their associated object detections saved in YOLOv4 format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    obj.names\n    images.txt\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid1&gt;.txt\n        &lt;uuid2&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.txt\n        ...\n</code></pre> <p>where <code>obj.names</code> contains the object class labels:</p> <pre><code>&lt;label-0&gt;\n&lt;label-1&gt;\n...\n</code></pre> <p>and <code>images.txt</code> contains the list of images in <code>data/</code>:</p> <pre><code>data/&lt;uuid1&gt;.&lt;ext&gt;\ndata/&lt;uuid2&gt;.&lt;ext&gt;\n...\n</code></pre> <p>and the TXT files in <code>data/</code> are space-delimited files where each row corresponds to an object in the image of the same name, in one of the following formats:</p> <pre><code># Detections\n&lt;target&gt; &lt;x-center&gt; &lt;y-center&gt; &lt;width&gt; &lt;height&gt;\n&lt;target&gt; &lt;x-center&gt; &lt;y-center&gt; &lt;width&gt; &lt;height&gt; &lt;confidence&gt;\n\n# Polygons\n&lt;target&gt; &lt;x1&gt; &lt;y1&gt; &lt;x2&gt; &lt;y2&gt; &lt;x3&gt; &lt;y3&gt; ...\n</code></pre> <p>where <code>&lt;target&gt;</code> is the zero-based integer index of the object class label from <code>obj.names</code>, all coordinates are expressed as relative values in <code>[0, 1] x [0, 1]</code>, and <code>&lt;confidence&gt;</code> is an optional confidence in <code>[0, 1]</code>, which will be included only if you pass the optional <code>include_confidence=True</code> flag to the export.</p> <p>Unlabeled images have no corresponding TXT file in <code>data/</code>.</p> <p>Note</p> <p>See <code>YOLOv4DatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as a YOLOv4 dataset in the above format as follows:</p> <p>Note</p> <p>You can pass the optional <code>classes</code> parameter to <code>export()</code> to explicitly define the class list to use in the exported labels. Otherwise, the strategy outlined in this section will be used to populate the class list.</p> <p>You can also perform labels-only exports of YOLO-formatted labels by providing the <code>labels_path</code> parameter instead of <code>export_dir</code>:</p>"},{"location":"fiftyone_concepts/export_datasets/#yolov5dataset","title":"YOLOv5Dataset \u00b6","text":"<p>Supported label types</p> <p><code>Detections</code> <code>Polylines</code></p> <p>The <code>fiftyone.types.YOLOv5Dataset</code> type represents a labeled dataset consisting of images and their associated object detections saved in YOLOv5 format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    dataset.yaml\n    images/\n        train/\n            &lt;uuid1&gt;.&lt;ext&gt;\n            &lt;uuid2&gt;.&lt;ext&gt;\n            ...\n        val/\n            &lt;uuid3&gt;.&lt;ext&gt;\n            &lt;uuid4&gt;.&lt;ext&gt;\n            ...\n    labels/\n        train/\n            &lt;uuid1&gt;.txt\n            &lt;uuid2&gt;.txt\n            ...\n        val/\n            &lt;uuid3&gt;.txt\n            &lt;uuid4&gt;.txt\n            ...\n</code></pre> <p>where <code>dataset.yaml</code> contains the following information:</p> <pre><code>path: &lt;dataset_dir&gt;  # optional\ntrain: ./images/train/\nval: ./images/val/\n\nnames:\n  0: list\n  1: of\n  2: classes\n  ...\n</code></pre> <p>See this page for a full description of the possible format of <code>dataset.yaml</code>. In particular, the dataset may contain one or more splits with arbitrary names, as the specific split being imported or exported is specified by the <code>split</code> argument to <code>fiftyone.utils.yolo.YOLOv5DatasetExporter</code>. Also, <code>dataset.yaml</code> can be located outside of <code>&lt;dataset_dir&gt;</code> as long as the optional <code>path</code> is provided.</p> <p>The TXT files in <code>labels/</code> are space-delimited files where each row corresponds to an object in the image of the same name, in one of the following formats:</p> <pre><code># Detections\n&lt;target&gt; &lt;x-center&gt; &lt;y-center&gt; &lt;width&gt; &lt;height&gt;\n&lt;target&gt; &lt;x-center&gt; &lt;y-center&gt; &lt;width&gt; &lt;height&gt; &lt;confidence&gt;\n\n# Polygons\n&lt;target&gt; &lt;x1&gt; &lt;y1&gt; &lt;x2&gt; &lt;y2&gt; &lt;x3&gt; &lt;y3&gt; ...\n</code></pre> <p>where <code>&lt;target&gt;</code> is the zero-based integer index of the object class label from <code>names</code>, all coordinates are expressed as relative values in <code>[0, 1] x [0, 1]</code>, and <code>&lt;confidence&gt;</code> is an optional confidence in <code>[0, 1]</code>, which will be included only if you pass the optional <code>include_confidence=True</code> flag to the export.</p> <p>Unlabeled images have no corresponding TXT file in <code>labels/</code>. The label file path for each image is obtained by replacing <code>images/</code> with <code>labels/</code> in the respective image path.</p> <p>Note</p> <p>See <code>YOLOv5DatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as a YOLOv5 dataset in the above format as follows:</p> <pre><code>import fiftyone as fo\n\nexport_dir = \"/path/for/yolov5-dataset\"\nlabel_field = \"ground_truth\"  # for example\n\n# The splits to export\nsplits = [\"train\", \"val\"]\n\n# All splits must use the same classes list\nclasses = [\"list\", \"of\", \"classes\"]\n\n# The dataset or view to export\n# We assume the dataset uses sample tags to encode the splits to export\ndataset_or_view = fo.load_dataset(...)\n\n# Export the splits\nfor split in splits:\n    split_view = dataset_or_view.match_tags(split)\n    split_view.export(\n        export_dir=export_dir,\n        dataset_type=fo.types.YOLOv5Dataset,\n        label_field=label_field,\n        split=split,\n        classes=classes,\n    )\n</code></pre> <p>Note</p> <p>You can pass the optional <code>classes</code> parameter to <code>export()</code> to explicitly define the class list to use in the exported labels. Otherwise, the strategy outlined in this section will be used to populate the class list.</p> <p>You can also perform labels-only exports of YOLO-formatted labels by providing the <code>labels_path</code> parameter instead of <code>export_dir</code>:</p> <pre><code>import fiftyone as fo\n\nlabels_path = \"/path/for/yolo-labels\"\nlabel_field = \"ground_truth\"  # for example\n\n# The dataset or view to export\ndataset_or_view = fo.load_dataset(...)\n\n# Export labels\ndataset_or_view.export(\n    dataset_type=fo.types.YOLOv5Dataset,\n    labels_path=labels_path,\n    label_field=label_field,\n)\n</code></pre>"},{"location":"fiftyone_concepts/export_datasets/#tfobjectdetectiondataset","title":"TFObjectDetectionDataset \u00b6","text":"<p>Supported label types</p> <p><code>Detections</code></p> <p>The <code>fiftyone.types.TFObjectDetectionDataset</code> type represents a labeled dataset consisting of images and their associated object detections stored as TFRecords in TF Object Detection API format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    tf.records-?????-of-?????\n</code></pre> <p>where the features of the (possibly sharded) TFRecords are stored in the following format:</p> <pre><code>{\n    # Image dimensions\n    \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n    \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n\n    # Image filename is used for both of these when writing\n    \"image/filename\": tf.io.FixedLenFeature([], tf.string),\n    \"image/source_id\": tf.io.FixedLenFeature([], tf.string),\n\n    # Encoded image bytes\n    \"image/encoded\": tf.io.FixedLenFeature([], tf.string),\n\n    # Image format, either `jpeg` or `png`\n    \"image/format\": tf.io.FixedLenFeature([], tf.string),\n\n    # Normalized bounding box coordinates in `[0, 1]`\n    \"image/object/bbox/xmin\": tf.io.FixedLenSequenceFeature(\n        [], tf.float32, allow_missing=True\n    ),\n    \"image/object/bbox/xmax\": tf.io.FixedLenSequenceFeature(\n        [], tf.float32, allow_missing=True\n    ),\n    \"image/object/bbox/ymin\": tf.io.FixedLenSequenceFeature(\n        [], tf.float32, allow_missing=True\n    ),\n    \"image/object/bbox/ymax\": tf.io.FixedLenSequenceFeature(\n        [], tf.float32, allow_missing=True\n    ),\n\n    # Class label string\n    \"image/object/class/text\": tf.io.FixedLenSequenceFeature(\n        [], tf.string, allow_missing=True\n    ),\n\n    # Integer class ID\n    \"image/object/class/label\": tf.io.FixedLenSequenceFeature(\n        [], tf.int64, allow_missing=True\n    ),\n}\n</code></pre> <p>The TFRecords for unlabeled samples do not contain <code>image/object/*</code> features.</p> <p>Note</p> <p>See <code>TFObjectDetectionDatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as a directory of TFRecords in the above format as follows:</p> <p>Note</p> <p>You can provide the <code>tf_records_path</code> argument instead of <code>export_dir</code> in the examples above to directly specify the path to the TFRecord(s) to write. See <code>TFObjectDetectionDatasetExporter</code> for details.</p> <p>Note</p> <p>You can pass the optional <code>classes</code> parameter to <code>export()</code> to explicitly define the class list to use in the exported labels. Otherwise, the strategy outlined in this section will be used to populate the class list.</p>"},{"location":"fiftyone_concepts/export_datasets/#imagesegmentationdirectory","title":"ImageSegmentationDirectory \u00b6","text":"<p>Supported label types</p> <p><code>Segmentation</code>, <code>Detections</code>, <code>Polylines</code></p> <p>The <code>fiftyone.types.ImageSegmentationDirectory</code> type represents a labeled dataset consisting of images and their associated semantic segmentations stored as images on disk.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;filename1&gt;.&lt;ext&gt;\n        &lt;filename2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;filename1&gt;.&lt;ext&gt;\n        &lt;filename2&gt;.&lt;ext&gt;\n        ...\n</code></pre> <p>where <code>labels/</code> contains the semantic segmentations stored as images.</p> <p>By default, the masks will be stored as PNG images, but you can customize this by passing the optional <code>mask_format</code> parameter. The masks will be stored as 8 bit images if they contain at most 256 classes, otherwise 16 bits will be used.</p> <p>Unlabeled images have no corresponding file in <code>labels/</code>.</p> <p>Note</p> <p>See <code>ImageSegmentationDirectoryExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as an image segmentation dataset in the above format as follows:</p> <p>You can also export only the segmentation masks by providing the <code>labels_path</code> parameter instead of <code>export_dir</code>:</p>"},{"location":"fiftyone_concepts/export_datasets/#cvatimagedataset","title":"CVATImageDataset \u00b6","text":"<p>Supported label types</p> <p><code>Classifications</code>, <code>Detections</code>, <code>Polylines</code>, <code>Keypoints</code></p> <p>The <code>fiftyone.types.CVATImageDataset</code> type represents a labeled dataset consisting of images and their associated tags and object detections stored in CVAT image format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels.xml\n</code></pre> <p>where <code>labels.xml</code> is an XML file in the following format:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;annotations&gt;\n    &lt;version&gt;1.1&lt;/version&gt;\n    &lt;meta&gt;\n        &lt;task&gt;\n            &lt;id&gt;0&lt;/id&gt;\n            &lt;name&gt;task-name&lt;/name&gt;\n            &lt;size&gt;51&lt;/size&gt;\n            &lt;mode&gt;annotation&lt;/mode&gt;\n            &lt;overlap&gt;&lt;/overlap&gt;\n            &lt;bugtracker&gt;&lt;/bugtracker&gt;\n            &lt;flipped&gt;False&lt;/flipped&gt;\n            &lt;created&gt;2017-11-20 11:51:51.000000+00:00&lt;/created&gt;\n            &lt;updated&gt;2017-11-20 11:51:51.000000+00:00&lt;/updated&gt;\n            &lt;labels&gt;\n                &lt;label&gt;\n                    &lt;name&gt;car&lt;/name&gt;\n                    &lt;attributes&gt;\n                        &lt;attribute&gt;\n                            &lt;name&gt;type&lt;/name&gt;\n                            &lt;values&gt;coupe\\\\nsedan\\\\ntruck&lt;/values&gt;\n                        &lt;/attribute&gt;\n                        ...\n                    &lt;/attributes&gt;\n                &lt;/label&gt;\n                &lt;label&gt;\n                    &lt;name&gt;traffic_line&lt;/name&gt;\n                    &lt;attributes&gt;\n                        &lt;attribute&gt;\n                            &lt;name&gt;color&lt;/name&gt;\n                            &lt;values&gt;white\\\\nyellow&lt;/values&gt;\n                        &lt;/attribute&gt;\n                        ...\n                    &lt;/attributes&gt;\n                &lt;/label&gt;\n                ...\n            &lt;/labels&gt;\n        &lt;/task&gt;\n        &lt;segments&gt;\n            &lt;segment&gt;\n                &lt;id&gt;0&lt;/id&gt;\n                &lt;start&gt;0&lt;/start&gt;\n                &lt;stop&gt;50&lt;/stop&gt;\n                &lt;url&gt;&lt;/url&gt;\n            &lt;/segment&gt;\n        &lt;/segments&gt;\n        &lt;owner&gt;\n            &lt;username&gt;&lt;/username&gt;\n            &lt;email&gt;&lt;/email&gt;\n        &lt;/owner&gt;\n        &lt;dumped&gt;2017-11-20 11:51:51.000000+00:00&lt;/dumped&gt;\n    &lt;/meta&gt;\n    &lt;image id=\"0\" name=\"&lt;uuid1&gt;.&lt;ext&gt;\" width=\"640\" height=\"480\"&gt;\n        &lt;tag label=\"urban\"&gt;&lt;/tag&gt;\n        ...\n        &lt;box label=\"car\" xtl=\"100\" ytl=\"50\" xbr=\"325\" ybr=\"190\" occluded=\"0\"&gt;\n            &lt;attribute name=\"type\"&gt;sedan&lt;/attribute&gt;\n            ...\n        &lt;/box&gt;\n        ...\n        &lt;polygon label=\"car\" points=\"561.30,916.23;561.30,842.77;...;560.20,966.67\" occluded=\"0\"&gt;\n            &lt;attribute name=\"make\"&gt;Honda&lt;/attribute&gt;\n            ...\n        &lt;/polygon&gt;\n        ...\n        &lt;polyline label=\"traffic_line\" points=\"462.10,0.00;126.80,1200.00\" occluded=\"0\"&gt;\n            &lt;attribute name=\"color\"&gt;yellow&lt;/attribute&gt;\n            ...\n        &lt;/polyline&gt;\n        ...\n        &lt;points label=\"wheel\" points=\"574.90,939.48;1170.16,907.90;...;600.16,459.48\" occluded=\"0\"&gt;\n            &lt;attribute name=\"location\"&gt;front_driver_side&lt;/attribute&gt;\n            ...\n        &lt;/points&gt;\n        ...\n    &lt;/image&gt;\n    ...\n    &lt;image id=\"50\" name=\"&lt;uuid51&gt;.&lt;ext&gt;\" width=\"640\" height=\"480\"&gt;\n        ...\n    &lt;/image&gt;\n&lt;/annotations&gt;\n</code></pre> <p>Unlabeled images have no corresponding <code>image</code> tag in <code>labels.xml</code>.</p> <p>The <code>name</code> field of the <code>&lt;image&gt;</code> tags in the labels file encodes the location of the corresponding images, which can be any of the following:</p> <ul> <li> <p>The filename of an image in the <code>data/</code> folder</p> </li> <li> <p>A relative path like <code>path/to/filename.ext</code> specifying the relative path to the image in a nested subfolder of <code>data/</code></p> </li> <li> <p>An absolute path to an image, which may or may not be in the <code>data/</code> folder</p> </li> </ul> <p>Note</p> <p>See <code>CVATImageDatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as a CVAT image dataset in the above format as follows:</p> <p>You can also perform labels-only exports of CVAT-formatted labels by providing the <code>labels_path</code> parameter instead of <code>export_dir</code>:</p>"},{"location":"fiftyone_concepts/export_datasets/#cvatvideodataset","title":"CVATVideoDataset \u00b6","text":"<p>Supported label types</p> <p><code>Detections</code>, <code>Polylines</code>, <code>Keypoints</code></p> <p>The <code>fiftyone.types.CVATVideoDataset</code> type represents a labeled dataset consisting of videos and their associated object detections stored in CVAT video format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;uuid1&gt;.xml\n        &lt;uuid2&gt;.xml\n        ...\n</code></pre> <p>where the labels XML files are stored in the following format:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;annotations&gt;\n    &lt;version&gt;1.1&lt;/version&gt;\n    &lt;meta&gt;\n        &lt;task&gt;\n            &lt;id&gt;task-id&lt;/id&gt;\n            &lt;name&gt;task-name&lt;/name&gt;\n            &lt;size&gt;51&lt;/size&gt;\n            &lt;mode&gt;interpolation&lt;/mode&gt;\n            &lt;overlap&gt;&lt;/overlap&gt;\n            &lt;bugtracker&gt;&lt;/bugtracker&gt;\n            &lt;flipped&gt;False&lt;/flipped&gt;\n            &lt;created&gt;2017-11-20 11:51:51.000000+00:00&lt;/created&gt;\n            &lt;updated&gt;2017-11-20 11:51:51.000000+00:00&lt;/updated&gt;\n            &lt;labels&gt;\n                &lt;label&gt;\n                    &lt;name&gt;car&lt;/name&gt;\n                    &lt;attributes&gt;\n                        &lt;attribute&gt;\n                            &lt;name&gt;type&lt;/name&gt;\n                            &lt;values&gt;coupe\\\\nsedan\\\\ntruck&lt;/values&gt;\n                        &lt;/attribute&gt;\n                        ...\n                    &lt;/attributes&gt;\n                &lt;/label&gt;\n                &lt;label&gt;\n                    &lt;name&gt;traffic_line&lt;/name&gt;\n                    &lt;attributes&gt;\n                        &lt;attribute&gt;\n                            &lt;name&gt;color&lt;/name&gt;\n                            &lt;values&gt;white\\\\nyellow&lt;/values&gt;\n                        &lt;/attribute&gt;\n                        ...\n                    &lt;/attributes&gt;\n                &lt;/label&gt;\n                ...\n            &lt;/labels&gt;\n        &lt;/task&gt;\n        &lt;segments&gt;\n            &lt;segment&gt;\n                &lt;id&gt;0&lt;/id&gt;\n                &lt;start&gt;0&lt;/start&gt;\n                &lt;stop&gt;50&lt;/stop&gt;\n                &lt;url&gt;&lt;/url&gt;\n            &lt;/segment&gt;\n        &lt;/segments&gt;\n        &lt;owner&gt;\n            &lt;username&gt;&lt;/username&gt;\n            &lt;email&gt;&lt;/email&gt;\n        &lt;/owner&gt;\n        &lt;original_size&gt;\n            &lt;width&gt;640&lt;/width&gt;\n            &lt;height&gt;480&lt;/height&gt;\n        &lt;/original_size&gt;\n        &lt;dumped&gt;2017-11-20 11:51:51.000000+00:00&lt;/dumped&gt;\n    &lt;/meta&gt;\n    &lt;track id=\"0\" label=\"car\"&gt;\n        &lt;box frame=\"0\" xtl=\"100\" ytl=\"50\" xbr=\"325\" ybr=\"190\" outside=\"0\" occluded=\"0\" keyframe=\"1\"&gt;\n            &lt;attribute name=\"type\"&gt;sedan&lt;/attribute&gt;\n            ...\n        &lt;/box&gt;\n        ...\n    &lt;/track&gt;\n    &lt;track id=\"1\" label=\"car\"&gt;\n        &lt;polygon frame=\"0\" points=\"561.30,916.23;561.30,842.77;...;560.20,966.67\" outside=\"0\" occluded=\"0\" keyframe=\"1\"&gt;\n            &lt;attribute name=\"make\"&gt;Honda&lt;/attribute&gt;\n            ...\n        &lt;/polygon&gt;\n        ...\n    &lt;/track&gt;\n    ...\n    &lt;track id=\"10\" label=\"traffic_line\"&gt;\n        &lt;polyline frame=\"10\" points=\"462.10,0.00;126.80,1200.00\" outside=\"0\" occluded=\"0\" keyframe=\"1\"&gt;\n            &lt;attribute name=\"color\"&gt;yellow&lt;/attribute&gt;\n            ...\n        &lt;/polyline&gt;\n        ...\n    &lt;/track&gt;\n    ...\n    &lt;track id=\"88\" label=\"wheel\"&gt;\n        &lt;points frame=\"176\" points=\"574.90,939.48;1170.16,907.90;...;600.16,459.48\" outside=\"0\" occluded=\"0\" keyframe=\"1\"&gt;\n            &lt;attribute name=\"location\"&gt;front_driver_side&lt;/attribute&gt;\n            ...\n        &lt;/points&gt;\n        ...\n    &lt;/track&gt;\n&lt;/annotations&gt;\n</code></pre> <p>Unlabeled videos have no corresponding file in <code>labels/</code>.</p> <p>Note</p> <p>See <code>CVATVideoDatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as a CVAT video dataset in the above format as follows:</p> <p>You can also perform labels-only exports of CVAT-formatted labels by providing the <code>labels_path</code> parameter instead of <code>export_dir</code>:</p>"},{"location":"fiftyone_concepts/export_datasets/#fiftyoneimagelabelsdataset","title":"FiftyOneImageLabelsDataset \u00b6","text":"<p>Supported label types</p> <p><code>Classifications</code>, <code>Detections</code>, <code>Polylines</code>, <code>Keypoints</code></p> <p>The <code>fiftyone.types.FiftyOneImageLabelsDataset</code> type represents a labeled dataset consisting of images and their associated multitask predictions stored in ETA ImageLabels format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;uuid1&gt;.json\n        &lt;uuid2&gt;.json\n        ...\n    manifest.json\n</code></pre> <p>where <code>manifest.json</code> is a JSON file in the following format:</p> <pre><code>{\n    \"type\": \"eta.core.datasets.LabeledImageDataset\",\n    \"description\": \"\",\n    \"index\": [\\\n        {\\\n            \"data\": \"data/&lt;uuid1&gt;.&lt;ext&gt;\",\\\n            \"labels\": \"labels/&lt;uuid1&gt;.json\"\\\n        },\\\n        {\\\n            \"data\": \"data/&lt;uuid2&gt;.&lt;ext&gt;\",\\\n            \"labels\": \"labels/&lt;uuid2&gt;.json\"\\\n        },\\\n        ...\\\n    ]\n}\n</code></pre> <p>and where each labels JSON file is stored in ETA ImageLabels format.</p> <p>For unlabeled images, an empty <code>eta.core.image.ImageLabels</code> file is stored.</p> <p>Note</p> <p>See <code>FiftyOneImageLabelsDatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as an image labels dataset in the above format as follows:</p>"},{"location":"fiftyone_concepts/export_datasets/#fiftyonevideolabelsdataset","title":"FiftyOneVideoLabelsDataset \u00b6","text":"<p>Supported label types</p> <p><code>Classifications</code>, <code>Detections</code>, <code>TemporalDetections</code>, <code>Polylines</code>, <code>Keypoints</code></p> <p>The <code>fiftyone.types.FiftyOneVideoLabelsDataset</code> type represents a labeled dataset consisting of videos and their associated labels stored in ETA VideoLabels format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;uuid1&gt;.json\n        &lt;uuid2&gt;.json\n        ...\n    manifest.json\n</code></pre> <p>where <code>manifest.json</code> is a JSON file in the following format:</p> <pre><code>{\n    \"type\": \"eta.core.datasets.LabeledVideoDataset\",\n    \"description\": \"\",\n    \"index\": [\\\n        {\\\n            \"data\": \"data/&lt;uuid1&gt;.&lt;ext&gt;\",\\\n            \"labels\": \"labels/&lt;uuid1&gt;.json\"\\\n        },\\\n        {\\\n            \"data\": \"data/&lt;uuid2&gt;.&lt;ext&gt;\",\\\n            \"labels\": \"labels/&lt;uuid2&gt;.json\"\\\n        },\\\n        ...\\\n    ]\n}\n</code></pre> <p>and where each labels JSON file is stored in ETA VideoLabels format.</p> <p>For unlabeled videos, an empty <code>eta.core.video.VideoLabels</code> file is stored.</p> <p>Note</p> <p>See <code>FiftyOneVideoLabelsDatasetExporter</code> for parameters that can be passed to methods like <code>export()</code> to customize the export of datasets of this type.</p> <p>You can export a FiftyOne dataset as a video labels dataset in the above format as follows:</p>"},{"location":"fiftyone_concepts/export_datasets/#bdddataset","title":"BDDDataset \u00b6","text":"<p>Supported label types</p> <p><code>Classifications</code>, <code>Detections</code>, <code>Polylines</code></p> <p>The <code>fiftyone.types.BDDDataset</code> type represents a labeled dataset consisting of images and their associated multitask predictions saved in Berkeley DeepDrive (BDD) format.</p> <p>Datasets of this type are exported in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;filename0&gt;.&lt;ext&gt;\n        &lt;filename1&gt;.&lt;ext&gt;\n        ...\n    labels.json\n</code></pre> <p>where <code>labels.json</code> is a JSON file in the following format:</p> <p><code>python [\\     {\\         \"name\": \"&lt;filename0&gt;.&lt;ext&gt;\",\\         \"attributes\": {\\             \"scene\": \"city street\",\\             \"timeofday\": \"daytime\",\\             \"weather\": \"overcast\"\\         },\\         \"labels\": [\\             {\\                 \"id\": 0,\\                 \"category\": \"traffic sign\",\\                 \"manualAttributes\": true,\\                 \"manualShape\": true,\\                 \"attributes\": {\\                     \"occluded\": false,\\                     \"trafficLightColor\": \"none\",\\                     \"truncated\": false\\                 },\\                 \"box2d\": {\\                     \"x1\": 1000.698742,\\                     \"x2\": 1040.626872,\\                     \"y1\": 281.992415,\\                     \"y2\": 326.91156\\                 },\\                 \"score\": 0.95\\             },\\             ...\\             {\\                 \"id\": 34,\\                 \"category\": \"drivable area\",\\                 \"manualAttributes\": true,\\                 \"manualShape\": true,\\                 \"attributes\": {\\                     \"areaType\": \"direct\"\\                 },\\                 \"poly2d\": [\\                     {\\                         \"types\": \"LLLLCCC\",\\                         \"closed\": true,\\                         \"vertices\": [\\                             [241.143645, 697.923453],\\                             [541.525255, 380.564983],\\                             ...\\                         ]\\                     }\\                 ],\\                 \"score\": 0.87\\             },\\             ...\\             {\\                 \"id\": 109356,\\                 \"category\": \"lane\",\\                 \"attributes\": {\\                     \"laneDirection\": \"parallel\",\\                     \"laneStyle\": \"dashed\",\\                     \"laneType\": \"single white\"\\                 },\\                 \"manualShape\": true,\\                 \"manualAttributes\": true,\\                 \"poly2d\": [\\                     {\\                         \"types\": \"LL\",\\                         \"closed\": false,\\                         \"vertices\": [\\                             [492.879546, 331.939543],\\                             [0, 471.076658],\\                             ...\\                         ]\\                     }\\                 ],\\                 \"score\": 0.98\\             },\\             ...\\         }\\     }\\     ...\\ ]\\ \\</code>\\ \\ Unlabeled images have no corresponding entry in <code>labels.json</code>.\\ \\ The <code>name</code> attribute of the labels file encodes the location of the\\ corresponding images, which can be any of the following:\\ \\ - The filename of an image in the <code>data/</code> folder\\ \\ - A relative path like <code>path/to/filename.ext</code> specifying the relative path to\\ the image in a nested subfolder of <code>data/</code>\\ \\ - An absolute path to an image, which may or may not be in the <code>data/</code> folder\\ \\ \\ Note\\ \\ See <code>BDDDatasetExporter</code>\\ for parameters that can be passed to methods like\\ <code>export()</code>\\ to customize the export of datasets of this type.\\ \\ You can export a FiftyOne dataset as a BDD dataset in the above format as\\ follows:\\ \\ You can also perform labels-only exports of BDD-formatted labels by providing\\ the <code>labels_path</code> parameter instead of <code>export_dir</code>:\\ \\ ## CSVDataset \u00b6\\ \\ The <code>fiftyone.types.CSVDataset</code> type is a flexible CSV format that\\ represents slice(s) of field values of a dataset as columns of a CSV file.\\ \\ Datasets of this type are exported in the following format:\\ \\ <code>\\ &lt;dataset_dir&gt;/\\     data/\\         &lt;filename1&gt;.&lt;ext&gt;\\         &lt;filename2&gt;.&lt;ext&gt;\\         ...\\     labels.csv\\ \\</code>\\ \\ where <code>labels.csv</code> is a CSV file in the following format:\\ \\ <code>\\ field1,field2,field3,...\\ value1,value2,value3,...\\ value1,value2,value3,...\\ ...\\ \\</code>\\ \\ where the columns of interest are specified via the <code>fields</code> parameter, and may\\ contain any number of top-level or embedded fields such as strings, ints,\\ floats, booleans, or lists of such values.\\ \\ List values are encoded as <code>\"list,of,values\"</code> with double quotes to escape the\\ commas. Missing field values are encoded as empty cells.\\ \\ Note\\ \\ See <code>CSVDatasetExporter</code> for\\ parameters that can be passed to methods like\\ <code>export()</code>\\ to customize the export of datasets of this type.\\ \\ You can export a FiftyOne dataset as a CSV dataset in the above format as\\ follows:\\ \\ You can also directly export a CSV file of field values and absolute media\\ paths without exporting the actual media files by providing the <code>labels_path</code>\\ parameter instead of <code>export_dir</code>:\\ \\ ## GeoJSONDataset \u00b6\\ \\ The <code>fiftyone.types.GeoJSONDataset</code> type represents a dataset consisting\\ of images or videos and their associated geolocation data and optional\\ properties stored in GeoJSON format.\\ \\ Datasets of this type are exported in the following format:\\ \\ <code>\\ &lt;dataset_dir&gt;/\\     data/\\         &lt;filename1&gt;.&lt;ext&gt;\\         &lt;filename2&gt;.&lt;ext&gt;\\         ...\\     labels.json\\ \\</code>\\ \\ where <code>labels.json</code> is a GeoJSON file containing a <code>FeatureCollection</code> in\\ the following format:\\ \\ <code>\\ {\\     \"type\": \"FeatureCollection\",\\     \"features\": [\\         {\\             \"type\": \"Feature\",\\             \"geometry\": {\\                 \"type\": \"Point\",\\                 \"coordinates\": [\\                     -73.99496451958454,\\                     40.66338032487842\\                 ]\\             },\\             \"properties\": {\\                 \"filename\": &lt;filename1&gt;.&lt;ext&gt;,\\                 ...\\             }\\         },\\         {\\             \"type\": \"Feature\",\\             \"geometry\": {\\                 \"type\": \"Point\",\\                 \"coordinates\": [\\                     -73.80992143421788,\\                     40.65611832778962\\                 ]\\             },\\             \"properties\": {\\                 \"filename\": &lt;filename2&gt;.&lt;ext&gt;,\\                 ...\\             }\\         },\\         ...\\     ]\\ }\\ \\</code>\\ \\ where the <code>geometry</code> field may contain any valid GeoJSON geometry object, and\\ the <code>filename</code> property encodes the name of the corresponding media in the\\ <code>data/</code> folder. The <code>filename</code> property can also be an absolute path, which\\ may or may not be in the <code>data/</code> folder.\\ \\ Samples with no location data will have a null <code>geometry</code> field.\\ \\ The <code>properties</code> field of each feature can contain additional labels for\\ each sample.\\ \\ Note\\ \\ See <code>GeoJSONDatasetExporter</code>\\ for parameters that can be passed to methods like\\ <code>export()</code>\\ to customize the export of datasets of this type.\\ \\ You can export a FiftyOne dataset as a GeoJSON dataset in the above format as\\ follows:\\ \\ You can also perform labels-only exports of GeoJSON-formatted labels by\\ providing the <code>labels_path</code> parameter instead of <code>export_dir</code>:\\ \\ ## FiftyOneDataset \u00b6\\ \\ The <code>fiftyone.types.FiftyOneDataset</code> provides a disk representation of\\ an entire <code>Dataset</code> in a serialized JSON format along with its source media.\\ \\ Datasets of this type are exported in the following format:\\ \\ <code>\\ &lt;dataset_dir&gt;/\\     metadata.json\\     samples.json\\     data/\\         &lt;filename1&gt;.&lt;ext&gt;\\         &lt;filename2&gt;.&lt;ext&gt;\\         ...\\     annotations/\\         &lt;anno_key1&gt;.json\\         &lt;anno_key2&gt;.json\\         ...\\     brain/\\         &lt;brain_key1&gt;.json\\         &lt;brain_key2&gt;.json\\         ...\\     evaluations/\\         &lt;eval_key1&gt;.json\\         &lt;eval_key2&gt;.json\\         ...\\ \\</code>\\ \\ where <code>metadata.json</code> is a JSON file containing metadata associated with the\\ dataset, <code>samples.json</code> is a JSON file containing a serialized representation\\ of the samples in the dataset, <code>annotations/</code> contains any serialized\\ <code>AnnotationResults</code>, <code>brain/</code> contains any serialized <code>BrainResults</code>, and\\ <code>evaluations/</code> contains any serialized <code>EvaluationResults</code>.\\ \\ Video datasets have an additional <code>frames.json</code> file that contains a serialized\\ representation of the frame labels for each video in the dataset.\\ \\ Note\\ \\ See <code>FiftyOneDatasetExporter</code>\\ for parameters that can be passed to methods like\\ <code>export()</code>\\ to customize the export of datasets of this type.\\ \\ You can export a FiftyOne dataset to disk in the above format as follows:\\ \\ You can export datasets in this this format without copying the source media\\ files by including <code>export_media=False</code> in your call to\\ <code>export()</code>.\\ \\ You can also pass <code>use_dirs=True</code> to export per-sample/frame JSON files rather\\ than storing all samples/frames in single JSON files.\\ \\ By default, the absolute filepath of each image will be included in the export.\\ However, if you want to re-import this dataset on a different machine with the\\ source media files stored in a different root directory, you can include the\\ optional <code>rel_dir</code> parameter to specify a common prefix to strip from each\\ image\u2019s filepath, and then provide the new <code>rel_dir</code> when\\ importing the dataset:\\ \\ Note\\ \\ Exporting in <code>fiftyone.types.FiftyOneDataset</code> format as shown above\\ using the <code>export_media=False</code> and <code>rel_dir</code> parameters is a convenient way\\ to transfer datasets between work environments, since this enables you to\\ store the media files wherever you wish in each environment and then simply\\ provide the appropriate <code>rel_dir</code> value when\\ importing the dataset into FiftyOne in a\\ new environment.\\ \\ You can also pass in a <code>chunk_size</code> parameter to create nested directories of\\ media files with a maximum number of files per directory. This can be useful\\ when exporting large datasets to avoid filesystem limits on the number of files\\ in a single directory.\\ \\ As an example, the following code exports a dataset with a maximum of 1000\\ media files per directory:\\ \\ <code>\\ import fiftyone as fo\\ \\ export_dir = \"/path/for/fiftyone-dataset\"\\ \\ # The dataset or view to export\\ dataset_or_view = fo.load_dataset(...)\\ \\ # Export the dataset with a maximum of 1000 media files per directory\\ dataset_or_view.export(\\     export_dir=export_dir,\\     dataset_type=fo.types.FiftyOneDataset,\\     chunk_size=1000,\\ )\\ \\</code>\\ \\ This will create a directory structure like the following:\\ \\ <code>\\ &lt;dataset_dir&gt;/\\     metadata.json\\     samples.json\\     data/\\         data_0/\\             &lt;filename1&gt;.&lt;ext&gt;\\             &lt;filename2&gt;.&lt;ext&gt;\\             ...\\         data_1/\\             &lt;filename1&gt;.&lt;ext&gt;\\             &lt;filename2&gt;.&lt;ext&gt;\\         ...\\ \\</code>\\ \\ ## Custom formats \u00b6\\ \\ The <code>export()</code> method\\ provides an optional <code>dataset_exporter</code> keyword argument that can be used to\\ export a dataset using any <code>DatasetExporter</code> instance.\\ \\ This means that you can define your own <code>DatasetExporter</code> class and then export\\ a <code>Dataset</code> or <code>DatasetView</code> in your custom format using the following recipe:\\ \\ <code>\\ import fiftyone as fo\\ \\ # The dataset or view to export\\ dataset_or_view = fo.load_dataset(...)\\ \\ # Create an instance of your custom dataset exporter\\ exporter = CustomDatasetExporter(...)\\ \\ # Export the dataset\\ dataset_or_view.export(dataset_exporter=exporter, ...)\\ \\</code>\\ \\ You can also define a custom <code>Dataset</code> type, which enables you to export\\ datasets in your custom format using the following recipe:\\ \\ <code>\\ import fiftyone as fo\\ \\ # The `fiftyone.types.Dataset` subclass for your custom dataset\\ dataset_type = CustomDataset\\ \\ # The dataset or view to export\\ dataset_or_view = fo.load_dataset(...)\\ \\ # Export the dataset\\ dataset_or_view.export(dataset_type=dataset_type, ...)\\ \\</code>\\ \\ ### Writing a custom DatasetExporter \u00b6\\ \\ <code>DatasetExporter</code> is an abstract interface; the concrete interface that you\\ should implement is determined by the type of dataset that you are exporting.\\ \\ ### Writing a custom Dataset type \u00b6\\ \\ FiftyOne provides the <code>Dataset</code> type system so that dataset formats can be\\ conveniently referenced by their type when reading/writing datasets on disk.\\ \\ The primary function of the <code>Dataset</code> subclasses is to define the\\ <code>DatasetImporter</code> that should be used to read instances of the dataset from\\ disk and the <code>DatasetExporter</code> that should be used to write instances of the\\ dataset to disk.\\ \\ See this page for more information\\ about defining custom <code>DatasetImporter</code> classes.\\ \\ Custom dataset types can be declared by implementing the <code>Dataset</code> subclass\\ corresponding to the type of dataset that you are working with.\\ \\</p>"},{"location":"fiftyone_concepts/groups/","title":"Grouped datasets \u00b6","text":"<p>FiftyOne supports the creation of grouped datasets, which contain multiple slices of samples of possibly different modalities (e.g., image, video, or 3D scenes) that are organized into groups.</p> <p>Grouped datasets can be used to represent multiview scenes, where data for multiple perspectives of the same scene can be stored, visualized, and queried in ways that respect the relationships between the slices of data.</p> <p></p> <p>Note</p> <p>Did you know? You can also create dynamic group views into your datasets based on a field or expression of interest.</p>"},{"location":"fiftyone_concepts/groups/#overview","title":"Overview \u00b6","text":"<p>In this section, we\u2019ll cover the basics of creating and working with grouped datasets via Python.</p> <p>Let\u2019s start by creating some test data. We\u2019ll use the quickstart dataset to construct some mocked triples of left/center/right images:</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.random as four\nimport fiftyone.zoo as foz\n\ngroups = [\"left\", \"center\", \"right\"]\n\nd = foz.load_zoo_dataset(\"quickstart\")\nfour.random_split(d, {g: 1 / len(groups) for g in groups})\nfilepaths = [d.match_tags(g).values(\"filepath\") for g in groups]\nfilepaths = [dict(zip(groups, fps)) for fps in zip(*filepaths)]\n\nprint(filepaths[:2])\n</code></pre> <pre><code>[\\\n    {\\\n        'left': '~/fiftyone/quickstart/data/000880.jpg',\\\n        'center': '~/fiftyone/quickstart/data/002799.jpg',\\\n        'right': '~/fiftyone/quickstart/data/001599.jpg',\\\n    },\\\n    {\\\n        'left': '~/fiftyone/quickstart/data/003344.jpg',\\\n        'center': '~/fiftyone/quickstart/data/001057.jpg',\\\n        'right': '~/fiftyone/quickstart/data/001430.jpg',\\\n    },\\\n]\n</code></pre>"},{"location":"fiftyone_concepts/groups/#creating-grouped-datasets","title":"Creating grouped datasets \u00b6","text":"<p>To create a grouped dataset, simply use <code>add_group_field()</code> to declare a <code>Group</code> field on your dataset before you add samples to it:</p> <pre><code>dataset = fo.Dataset(\"groups-overview\")\ndataset.add_group_field(\"group\", default=\"center\")\n</code></pre> <p>The optional <code>default</code> parameter specifies the slice of samples that will be returned via the API or visualized in the App\u2019s grid view by default. If you don\u2019t specify a default, one will be inferred from the first sample you add to the dataset.</p> <p>Note</p> <p>Datasets may contain only one <code>Group</code> field.</p>"},{"location":"fiftyone_concepts/groups/#adding-samples","title":"Adding samples \u00b6","text":"<p>To populate a grouped dataset with samples, create a single <code>Group</code> instance for each group of samples and use <code>Group.element()</code> to generate values for the group field of each <code>Sample</code> object in the group based on their slice\u2019s <code>name</code>. The <code>Sample</code> objects can then simply be added to the dataset as usual:</p> <pre><code>samples = []\nfor fps in filepaths:\n    group = fo.Group()\n    for name, filepath in fps.items():\n        sample = fo.Sample(filepath=filepath, group=group.element(name))\n        samples.append(sample)\n\ndataset.add_samples(samples)\n\nprint(dataset)\n</code></pre> <pre><code>Name:        groups-overview\nMedia type:  group\nGroup slice: center\nNum groups:  66\nPersistent:  False\nTags:        []\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    group:            fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.groups.Group)\n</code></pre> <p>Note</p> <p>Every sample in a grouped dataset must have its group field populated with a <code>Group</code> element.</p>"},{"location":"fiftyone_concepts/groups/#dataset-properties","title":"Dataset properties \u00b6","text":"<p>Grouped datasets have a <code>media_type</code> of <code>\"group\"</code>:</p> <pre><code>print(dataset.media_type)\n# group\n</code></pre> <p>The <code>group_field</code> property contains the name of the <code>Group</code> field storing the dataset\u2019s group membership information:</p> <pre><code>print(dataset.group_field)\n# group\n</code></pre> <p>The <code>group_slices</code> property contains the names of all group slices in the dataset:</p> <pre><code>print(dataset.group_slices)\n# ['left', 'center', 'right']\n</code></pre> <p>The <code>group_media_types</code> property is a dict mapping each slice name to its corresponding media type:</p> <pre><code>print(dataset.group_media_types)\n# {'left': 'image', 'center': 'image', 'right': 'image'}\n</code></pre> <p>The list of group slices and their corresponding media types are dynamically expanded as you add samples to a grouped dataset.</p> <p>Note</p> <p>Grouped datasets may contain a mix of different modalities (e.g., images, videos, and 3D scenes), but FiftyOne strictly enforces that each slice of a grouped dataset must have a homogeneous media type.</p> <p>For example, you would see an error if you tried to add a video sample to the <code>left</code> slice of the above dataset, since it contains images.</p> <p>The <code>default_group_slice</code> property stores the name of the default group slice:</p> <pre><code>print(dataset.default_group_slice)\n# center\n</code></pre> <p>The default group slice controls the slice of samples that will be returned via the API\u2014for example when you directly iterate over the dataset\u2014or visualized in the App\u2019s grid view by default:</p> <pre><code>print(dataset.first())\n</code></pre> <pre><code>&lt;Sample: {\n    'id': '62db2ce147e9efc3615cd450',\n    'media_type': 'image',\n    'filepath': '~/fiftyone/quickstart/data/003344.jpg',\n    'tags': [],\n    'metadata': None,\n    'created_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n    'last_modified_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n    'group': &lt;Group: {'id': '62db2ce147e9efc3615cd346', 'name': 'center'}&gt;,\n}&gt;\n</code></pre> <p>You can change the active group slice in your current session by setting the <code>group_slice</code> property:</p> <pre><code>dataset.group_slice = \"left\"\n\nprint(dataset.first())\n</code></pre> <pre><code>&lt;Sample: {\n    'id': '62db2ce147e9efc3615cd44e',\n    'media_type': 'image',\n    'filepath': '~/fiftyone/quickstart/data/001599.jpg',\n    'tags': [],\n    'metadata': None,\n    'created_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n    'last_modified_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n    'group': &lt;Group: {'id': '62db2ce147e9efc3615cd346', 'name': 'left'}&gt;,\n}&gt;\n</code></pre> <p>You can reset the active group slice to the default value by setting <code>group_slice</code> to <code>None</code>:</p> <pre><code># Resets to `default_group_slice`\ndataset.group_slice = None\n</code></pre> <p>You can also change the default group slice at any time by setting the <code>default_group_slice</code> property.</p>"},{"location":"fiftyone_concepts/groups/#adding-fields","title":"Adding fields \u00b6","text":"<p>You are free to add arbitrary sample- and frame-level fields to your grouped datasets just as you would with ungrouped datasets:</p> <pre><code>sample = dataset.first()\n\nsample[\"int_field\"] = 51\nsample[\"ground_truth\"] = fo.Classification(label=\"outdoor\")\n\nsample.save()\n</code></pre> <p>You can also use methods like <code>set_values()</code> and <code>save()</code> to perform bulk edits to the active slice of a grouped dataset.</p> <p>Note that all slices of a grouped dataset share the same schema, and hence any fields you add to samples from a particular slice will be implicitly declared on all samples from that slice and all other slices:</p> <pre><code>print(dataset)\n</code></pre> <pre><code>Name:        groups-overview\nMedia type:  group\nGroup slice: center\nNum groups:  66\nPersistent:  False\nTags:        []\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    group:            fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.groups.Group)\n    int_field:        fiftyone.core.fields.IntField\n    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n</code></pre> <p>Note</p> <p>Like ungrouped datasets, any fields in a grouped dataset\u2019s schema that have not been explicitly set on a <code>Sample</code> in the dataset will be <code>None</code>.</p> <p>You can use methods like <code>clone_sample_field()</code>, <code>rename_sample_field()</code>, <code>delete_sample_field()</code>, <code>clear_sample_field()</code>, and <code>keep_fields()</code> to perform batch edits to the fields across all slices of a grouped dataset.</p>"},{"location":"fiftyone_concepts/groups/#accessing-samples","title":"Accessing samples \u00b6","text":"<p>You can access a sample from any slice of grouped dataset via its ID or filepath:</p> <pre><code># Grab a random sample across all slices\nsample = dataset.select_group_slices().shuffle().first()\n\n# Directly lookup same sample by ID\nalso_sample = dataset[sample.id]\n</code></pre> <p>In addition, you can also use <code>get_group()</code> to retrieve a dict containing all samples in a group with a given ID:</p> <pre><code># Grab a random group ID\nsample = dataset.shuffle().first()\ngroup_id = sample.group.id\n\ngroup = dataset.get_group(group_id)\nprint(group)\n</code></pre> <pre><code>{\n    'left': &lt;Sample: {\n        'id': '62f810ba59e644568f229dac',\n        'media_type': 'image',\n        'filepath': '~/fiftyone/quickstart/data/001227.jpg',\n        'tags': [],\n        'metadata': None,\n        'created_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n        'last_modified_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n        'group': &lt;Group: {'id': '62f810ba59e644568f229c62', 'name': 'left'}&gt;,\n    }&gt;,\n    'center': &lt;Sample: {\n        'id': '62f810ba59e644568f229dad',\n        'media_type': 'image',\n        'filepath': '~/fiftyone/quickstart/data/004172.jpg',\n        'tags': [],\n        'metadata': None,\n        'created_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n        'last_modified_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n        'group': &lt;Group: {'id': '62f810ba59e644568f229c62', 'name': 'center'}&gt;,\n    }&gt;,\n    'right': &lt;Sample: {\n        'id': '62f810ba59e644568f229dae',\n        'media_type': 'image',\n        'filepath': '~/fiftyone/quickstart/data/000594.jpg',\n        'tags': [],\n        'metadata': None,\n        'created_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n        'last_modified_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n        'group': &lt;Group: {'id': '62f810ba59e644568f229c62', 'name': 'right'}&gt;,\n    }&gt;,\n}\n</code></pre>"},{"location":"fiftyone_concepts/groups/#deleting-samples","title":"Deleting samples \u00b6","text":"<p>Like ungrouped datasets, you can use <code>delete_samples()</code> to delete individual sample(s) from a grouped dataset:</p> <pre><code># Grab a random sample across all slices\nsample = dataset.select_group_slices().shuffle().first()\n\ndataset.delete_samples(sample)\n</code></pre> <p>In addition, you can use <code>delete_groups()</code> to delete all samples in a specific group(s):</p> <pre><code># Continuing from above, delete the rest of the group\ngroup_id = sample.group.id\n\ndataset.delete_groups(group_id)\n</code></pre> <p>You can also use methods like <code>clear()</code> and <code>keep()</code> to perform batch edits to the groups in a grouped dataset.</p>"},{"location":"fiftyone_concepts/groups/#iterating-over-grouped-datasets","title":"Iterating over grouped datasets \u00b6","text":"<p>When you directly iterate over a grouped dataset, you will get samples from the dataset\u2019s active slice:</p> <pre><code>print(dataset.group_slice)\n# center\n\nfor sample in dataset:\n    pass\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': '62f10dbb68f4ed13eba7c5e7',\n    'media_type': 'image',\n    'filepath': '~/fiftyone/quickstart/data/001394.jpg',\n    'tags': [],\n    'metadata': None,\n    'created_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n    'last_modified_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n    'group': &lt;Group: {'id': '62f10dbb68f4ed13eba7c4a0', 'name': 'center'}&gt;,\n}&gt;\n</code></pre> <p>Note</p> <p>You can customize the dataset\u2019s active slice by setting the <code>group_slice</code> property to another slice name.</p> <p>You can also use <code>iter_groups()</code> to iterate over dicts containing all samples in each group:</p> <pre><code>for group in dataset.iter_groups():\n    pass\n\nprint(group)\n</code></pre> <pre><code>{\n    'left': &lt;Sample: {\n        'id': '62f10dbb68f4ed13eba7c5e6',\n        'media_type': 'image',\n        'filepath': '~/fiftyone/quickstart/data/002538.jpg',\n        'tags': [],\n        'metadata': None,\n        'created_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n        'last_modified_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n        'group': &lt;Group: {'id': '62f10dbb68f4ed13eba7c4a0', 'name': 'left'}&gt;,\n    }&gt;,\n    'center': &lt;Sample: {\n        'id': '62f10dbb68f4ed13eba7c5e7',\n        'media_type': 'image',\n        'filepath': '~/fiftyone/quickstart/data/001394.jpg',\n        'tags': [],\n        'metadata': None,\n        'created_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n        'last_modified_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n        'group': &lt;Group: {'id': '62f10dbb68f4ed13eba7c4a0', 'name': 'center'}&gt;,\n    }&gt;,\n    'right': &lt;Sample: {\n        'id': '62f10dbb68f4ed13eba7c5e8',\n        'media_type': 'image',\n        'filepath': '~/fiftyone/quickstart/data/000020.jpg',\n        'tags': [],\n        'metadata': None,\n        'created_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n        'last_modified_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n        'group': &lt;Group: {'id': '62f10dbb68f4ed13eba7c4a0', 'name': 'right'}&gt;,\n    }&gt;,\n}\n</code></pre>"},{"location":"fiftyone_concepts/groups/#example-datasets","title":"Example datasets \u00b6","text":"<p>The FiftyOne Dataset Zoo contains grouped datasets that you can use out-of-the-box to test drive FiftyOne\u2019s group-related features.</p>"},{"location":"fiftyone_concepts/groups/#quickstart-groups","title":"Quickstart groups \u00b6","text":"<p>The fastest way to get started is by loading the quickstart-groups dataset, which consists of 200 scenes from the train split of the KITTI dataset, each containing left camera, right camera, point cloud, and 2D/3D object annotation data:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-groups\")\n\nprint(dataset.group_media_types)\n# {'left': 'image', 'right': 'image', 'pcd': '3d'}\n\nprint(dataset)\n</code></pre> <pre><code>Name:        quickstart-groups\nMedia type:  group\nGroup slice: left\nNum groups:  200\nPersistent:  False\nTags:        []\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    group:            fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.groups.Group)\n    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n</code></pre>"},{"location":"fiftyone_concepts/groups/#kitti-multiview","title":"KITTI multiview \u00b6","text":"<p>You can also load the full kitti-multiview dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"kitti-multiview\", split=\"train\")\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/groups/#toy-dataset","title":"Toy dataset \u00b6","text":"<p>The snippet below generates a toy dataset containing 3D cuboids filled with points that demonstrates how 3D detections are represented:</p> <pre><code>import fiftyone as fo\nimport numpy as np\nimport open3d as o3d\n\ndetections = []\npoint_cloud = []\n\nfor _ in range(10):\n    dimensions = np.random.uniform([1, 1, 1], [3, 3, 3])\n    location = np.random.uniform([-10, -10, 0], [10, 10, 10])\n    rotation = np.random.uniform(-np.pi, np.pi, size=3)\n\n    detection = fo.Detection(\n        dimensions=list(dimensions),\n        location=list(location),\n        rotation=list(rotation),\n    )\n    detections.append(detection)\n\n    R = o3d.geometry.get_rotation_matrix_from_xyz(rotation)\n    points = np.random.uniform(-dimensions / 2, dimensions / 2, size=(1000, 3))\n    points = points @ R.T + location[np.newaxis, :]\n    point_cloud.extend(points)\n\npc = o3d.geometry.PointCloud()\npc.points = o3d.utility.Vector3dVector(np.array(point_cloud))\no3d.io.write_point_cloud(\"/tmp/toy.pcd\", pc)\n\nscene = fo.Scene()\nscene.add(fo.PointCloud(\"point cloud\", \"/tmp/toy.pcd\"))\nscene.write(\"/tmp/toy.fo3d\")\n\ngroup = fo.Group()\nsamples = [\\\n    fo.Sample(\\\n        filepath=\"/tmp/toy.png\",  # non-existent\\\n        group=group.element(\"image\"),\\\n    ),\\\n    fo.Sample(\\\n        filepath=\"/tmp/toy.fo3d\",\\\n        group=group.element(\"pcd\"),\\\n        detections=fo.Detections(detections=detections),\\\n    )\\\n]\n\ndataset = fo.Dataset()\ndataset.add_samples(samples)\n\ndataset.app_config.plugins[\"3d\"] = {\n    \"defaultCameraPosition\": {\"x\": 0, \"y\": 0, \"z\": 20}\n}\ndataset.save()\n\nsession = fo.launch_app(dataset)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/groups/#grouped-views","title":"Grouped views \u00b6","text":"<p>You have the entire dataset view language at your disposal to sort, slice, and search your grouped datasets!</p>"},{"location":"fiftyone_concepts/groups/#basics","title":"Basics \u00b6","text":"<p>You can perform simple operations like shuffling and limiting grouped datasets:</p> <pre><code># Select 10 random groups from the dataset\nview = dataset.shuffle().limit(10)\n\nprint(view)\n</code></pre> <pre><code>Dataset:     groups-overview\nMedia type:  group\nGroup slice: center\nNum groups:  10\nGroup fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    group:            fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.groups.Group)\nView stages:\n    1. Shuffle(seed=None)\n    2. Limit(limit=10)\n</code></pre> <p>As you can see, the basic properties of grouped datasets carry over to views into them:</p> <pre><code>print(view.media_type)\n# group\n\nprint(view.group_slice)\n# center\n\nprint(view.group_media_types)\n# {'left': 'image', 'center': 'image', 'right': 'image'}\n</code></pre> <p>You can also perform all the usual operations on grouped views, such as accessing samples, and iterating over them:</p> <pre><code>for group in view.iter_groups():\n    pass\n\nsample = view.last()\nprint(sample)\n\ngroup_id = sample.group.id\ngroup = view.get_group(group_id)\nprint(group)\n</code></pre>"},{"location":"fiftyone_concepts/groups/#filtering","title":"Filtering \u00b6","text":"<p>You can write views that match and filter the contents of grouped datasets:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-groups\")\n\nprint(dataset.group_slice)\n# left\n\n# Filters based on the content in the 'left' slice\nview = (\n    dataset\n    .match_tags(\"train\")\n    .filter_labels(\"ground_truth\", F(\"label\") == \"Pedestrian\")\n)\n</code></pre> <p>Remember that, just as when iterating over grouped datasets, any filtering operations will only be applied to the active slice.</p> <p>However, you can write views that reference specific slice(s) of a grouped collection via the special <code>\"groups.&lt;slice&gt;.field.name\"</code> syntax:</p> <pre><code>from fiftyone import ViewField as F\n\ndataset.compute_metadata()\n\n# Match groups whose `left` image has a height of at least 640 pixels and\n# whose `right` image has a height of at most 480 pixels\nview = dataset.match(\n    (F(\"groups.left.metadata.height\") &gt;= 640)\n    &amp; (F(\"groups.right.metadata.height\") &lt;= 480)\n)\n\nprint(view)\n</code></pre>"},{"location":"fiftyone_concepts/groups/#selecting-groups","title":"Selecting groups \u00b6","text":"<p>You can use <code>select_groups()</code> to create a view that contains certain group(s) of interest by their IDs:</p> <pre><code># Select two groups at random\nview = dataset.take(2)\n\ngroup_ids = view.values(\"group.id\")\n\n# Select the same groups (default: unordered)\nsame_groups = dataset.select_groups(group_ids)\nassert set(view.values(\"id\")) == set(same_groups.values(\"id\"))\n\n# Select the same groups (ordered)\nsame_order = dataset.select_groups(group_ids, ordered=True)\nassert view.values(\"id\") == same_order.values(\"id\")\n</code></pre>"},{"location":"fiftyone_concepts/groups/#excluding-groups","title":"Excluding groups \u00b6","text":"<p>You can use <code>exclude_groups()</code> to create a view that excludes certain group(s) of interest by their IDs:</p> <pre><code># Exclude two groups at random\nview = dataset.take(2)\n\ngroup_ids = view.values(\"group.id\")\nother_groups = dataset.exclude_groups(group_ids)\nassert len(set(group_ids) &amp; set(other_groups.values(\"group.id\"))) == 0\n</code></pre>"},{"location":"fiftyone_concepts/groups/#selecting-slices","title":"Selecting slices \u00b6","text":"<p>You can use <code>select_group_slices()</code> to create non-grouped views that contain one or more slices of data from a grouped dataset.</p> <p>For example, you can create an image view that contains only the left camera images from the grouped dataset:</p> <pre><code>left_view = dataset.select_group_slices(\"left\")\nprint(left_view)\n</code></pre> <pre><code>Dataset:     groups-overview\nMedia type:  image\nNum samples: 108\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    group:            fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.groups.Group)\nView stages:\n    1. SelectGroupSlices(slices='left')\n</code></pre> <p>or you could create an image collection containing the left and right camera images:</p> <pre><code>lr_view = dataset.select_group_slices([\"left\", \"right\"])\nprint(lr_view)\n</code></pre> <pre><code>Dataset:     groups-overview\nMedia type:  image\nNum samples: 216\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    group:            fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.groups.Group)\nView stages:\n    1. SelectGroupSlices(slices=['left', 'right'])\n</code></pre> <p>Note that the <code>media_type</code> of the above collections are <code>image</code>, not <code>group</code>. This means you can perform any valid operation for image collections to these views, without worrying about the fact that their data is sourced from a grouped dataset!</p> <pre><code>image_view = dataset.shuffle().limit(10).select_group_slices(\"left\")\n\nanother_view = image_view.match(F(\"metadata.width\") &gt;= 640)\n\n# Add fields/tags, run evaluation, export, etc\n</code></pre> <p>Also note that any filtering that you apply prior to a <code>select_group_slices()</code> stage in a view is not automatically reflected by the output view, as the stage looks up unfiltered slice data from the source collection:</p> <pre><code># Filter the active slice to locate groups of interest\nmatch_view = dataset.filter_labels(...).match(...)\n\n# Lookup all image slices for the matching groups\n# This view contains *unfiltered* image slices\nimages_view = match_view.select_group_slices(media_type=\"image\")\n</code></pre> <p>Instead, you can apply the same (or different) filtering after the <code>select_group_slices()</code> stage:</p> <pre><code># Now apply filters to the flattened collection\nmatch_images_view = images_view.filter_labels(...).match(...)\n</code></pre>"},{"location":"fiftyone_concepts/groups/#grouped-aggregations","title":"Grouped aggregations \u00b6","text":"<p>You can use the entire aggregations framework to efficiently compute statistics on grouped datasets.</p> <p>Remember that, just as when iterating over or writing views into grouped datasets, aggregations will only include samples from the active slice:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-groups\")\n\n# Expression that computes the area of a bounding box, in pixels\nbbox_width = F(\"bounding_box\")[2] * F(\"$metadata.width\")\nbbox_height = F(\"bounding_box\")[3] * F(\"$metadata.height\")\nbbox_area = bbox_width * bbox_height\n\nprint(dataset.group_slice)\n# left\n\nprint(dataset.count(\"ground_truth.detections\"))\n# 1438\n\nprint(dataset.mean(\"ground_truth.detections[]\", expr=bbox_area))\n# 8878.752327468706\n</code></pre> <p>You can customize the dataset\u2019s active slice by setting the <code>group_slice</code> property to another slice name:</p> <pre><code>dataset.group_slice = \"right\"\n\nprint(dataset.count(\"ground_truth.detections\"))\n# 1438\n\nprint(dataset.bounds(\"ground_truth.detections[]\", expr=bbox_area))\n# 9457.586300995526\n</code></pre> <p>As usual, you can combine views and aggregations to refine your statistics to any subset of the dataset:</p> <pre><code>print(dataset.count_values(\"ground_truth.detections.label\"))\n# {'Pedestrian': 128, 'Car': 793, ...}\n\nview1 = dataset.take(5)\nprint(view1.count_values(\"ground_truth.detections.label\"))\n# {'Pedestrian': 1, 'Car': 23, ...}\n\nview2 = dataset.filter_labels(\"ground_truth\", F(\"label\") == \"Pedestrian\")\nprint(view2.count_values(\"ground_truth.detections.label\"))\n# {'Pedestrian': 128}\n</code></pre> <p>In particular, if you would like to compute statistics across multiple group slices, you can select them!</p> <pre><code>print(dataset.count())  # 200\nprint(dataset.count(\"ground_truth.detections\"))  # 1438\n\nview3 = dataset.select_group_slices([\"left\", \"right\"])\n\nprint(view3.count())  # 400\nprint(view3.count(\"ground_truth.detections\"))  # 2876\n</code></pre>"},{"location":"fiftyone_concepts/groups/#groups-in-the-app","title":"Groups in the App \u00b6","text":"<p>When you load a grouped dataset or view in the App, you\u2019ll see the samples from the collection\u2019s default group slice in the grid view by default.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-groups\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>You can use the selector shown below to change which slice you are viewing:</p> <p></p> <p>Note</p> <p>In order to view 3D scenes in the grid view, you must populate orthographic projection images.</p> <p>When you open the expanded modal with a grouped dataset or view loaded in the App, you\u2019ll have access to all samples in the current group.</p> <p>If the group contains image/video slices, the lefthand side of the modal will contain a scrollable carousel that you can use to choose which sample to load in the maximized image/video visualizer below.</p> <p>If the group contains 3D slices, the righthand side of the modal will contain a 3D visualizer:</p> <p></p> <p>By default, the filters sidebar shows statistics for only the group slice that currently has focus in the grid/modal. In the grid view, the active slice is denoted by the selector in the upper-right corner of the grid, and in the modal, the active sample is denoted by the <code>pin icon</code> in the upper-left corner.</p> <p>However, you can opt to show statistics across all slices of a grouped dataset by selecting <code>group</code> mode under the App\u2019s settings menu:</p> <p></p> <p>You can also use the App\u2019s dynamic groups feature to view groups of groups organized by a field of your choice.</p> <p>For example, if you have a grouped dataset whose group slices contain different sensor modalities for each <code>frame_number</code> of a scene with a given <code>scene_id</code>, you can use the dynamic grouping action to playback scenes in sequential order:</p> <p></p> <p>Note</p> <p>Did you know? You can also create dynamic group views into your grouped datasets via Python.</p>"},{"location":"fiftyone_concepts/groups/#importing-groups","title":"Importing groups \u00b6","text":"<p>The simplest way to import grouped datasets is to write a Python loop:</p> <pre><code>samples = []\nfor fps in filepaths:\n    group = fo.Group()\n    for name, filepath in fps.items():\n        sample = fo.Sample(filepath=filepath, group=group.element(name))\n        samples.append(sample)\n\ndataset.add_samples(samples)\n\nprint(dataset)\n</code></pre> <p>Remember that each group is represented by a <code>Group</code> instance, and each sample in a group is denoted by its slice <code>name</code> using <code>Group.element()</code>. The <code>Sample</code> objects can then simply be added to the dataset as usual.</p> <p>Alternatively, you can write your own importer and then import grouped datasets in your custom format using the syntax below:</p> <pre><code># Create an instance of your custom dataset importer\nimporter = CustomGroupDatasetImporter(...)\n\ndataset = fo.Dataset.from_importer(importer)\n</code></pre>"},{"location":"fiftyone_concepts/groups/#exporting-groups","title":"Exporting groups \u00b6","text":"<p>If you need to export an entire grouped dataset (or a view into it), you can use FiftyOneDataset format:</p> <pre><code>view = dataset.shuffle().limit(10)\n\nview.export(\n    export_dir=\"/tmp/groups\",\n    dataset_type=fo.types.FiftyOneDataset,\n)\n\ndataset2 = fo.Dataset.from_dir(\n    dataset_dir=\"/tmp/groups\",\n    dataset_type=fo.types.FiftyOneDataset,\n)\n</code></pre> <p>You can also select specific slice(s) and then export the resulting ungrouped collection in all the usual ways:</p> <pre><code>left_view = dataset.shuffle().limit(10).select_group_slices(\"left\")\n\nleft_view.export(\n    export_dir=\"/tmp/groups-left\",\n    dataset_type=fo.types.ImageDirectory,\n)\n</code></pre> <p>Alternatively, you can write your own exporter and then export grouped datasets in your custom format using the syntax below:</p> <pre><code># Create an instance of your custom dataset exporter\nexporter = CustomGroupDatasetExporter(...)\n\ndataset_or_view.export(dataset_exporter=exporter, ...)\n</code></pre>"},{"location":"fiftyone_concepts/plots/","title":"Interactive Plots \u00b6","text":"<p>FiftyOne provides a powerful <code>fiftyone.core.plots</code> framework that contains a variety of interactive plotting methods that enable you to visualize your datasets and uncover patterns that are not apparent from inspecting either the raw media files or aggregate statistics.</p> <p>With FiftyOne, you can visualize geolocated data on maps, generate interactive evaluation reports such as confusion matrices and PR curves, create dashboards of custom statistics, and even generate low-dimensional representations of your data that you can use to identify data clusters corresponding to model failure modes, annotation gaps, and more.</p> <p>What do we mean by interactive plots? First, FiftyOne plots are powered by Plotly, which means they are responsive JavaScript-based plots that can be zoomed, panned, and lasso-ed. Second, FiftyOne plots can be linked to the FiftyOne App, so that selecting points in a plot will automatically load the corresponding samples/labels in the App (and vice versa) for you to visualize! Linking plots to their source media is a paradigm that should play a critical part in any visual dataset analysis pipeline.</p> <p>The builtin plots provided by FiftyOne are chosen to help you analyze and improve the quality of your datasets and models, with minimal customization required on your part to get started. At the same time, data/model interpretability is not a narrowly-defined space that can be fully automated. That\u2019s why FiftyOne\u2019s plotting framework is highly customizable and extensible, all by writing pure Python (no JavaScript knowledge required).</p> <p>Note</p> <p>Check out the tutorials page for in-depth walkthroughs that apply the available interactive plotting methods to perform evaluation, identify model failure modes, recommend new samples for annotation, and more!</p>"},{"location":"fiftyone_concepts/plots/#overview","title":"Overview \u00b6","text":"<p>All <code>Session</code> instances provide a <code>plots attribute</code> attribute that you can use to attach <code>ResponsivePlot</code> instances to the FiftyOne App.</p> <p>When <code>ResponsivePlot</code> instances are attached to a <code>Session</code>, they are automatically updated whenever <code>session.view</code> changes for any reason, whether you modify your view in the App, or programmatically change it by setting <code>session.view</code>, or if multiple plots are connected and another plot triggers a <code>Session</code> update!</p> <p>Note</p> <p>Interactive plots are currently only supported in Jupyter notebooks. In the meantime, you can still use FiftyOne\u2019s plotting features in other environments, but you must manually call <code>plot.show()</code> to update the state of a plot to match the state of a connected <code>Session</code>, and any callbacks that would normally be triggered in response to interacting with a plot will not be triggered.</p> <p>See this section for more information.</p> <p>The two main classes of <code>ResponsivePlot</code> are explained next.</p>"},{"location":"fiftyone_concepts/plots/#interactive-plots_1","title":"Interactive plots \u00b6","text":"<p><code>InteractivePlot</code> is a class of plots that are bidirectionally linked to a <code>Session</code> via the IDs of either samples or individual labels in the dataset. When the user performs a selection in the plot, the <code>session.view</code> is automatically updated to select the corresponding samples/labels, and, conversely, when <code>session.view</code> changes, the contents of the current view is automatically selected in the plot.</p> <p>Examples of <code>InteractivePlot</code> types include scatterplots, location scatterplots, and interactive heatmaps.</p> <p></p>"},{"location":"fiftyone_concepts/plots/#view-plots","title":"View plots \u00b6","text":"<p><code>ViewPlot</code> is a class of plots whose state is automatically updated whenever the current <code>session.view</code> changes. View plots can be used to construct dynamic dashboards that update to reflect the contents of your current view.</p> <p>More view plot types are being continually added to the library over time. Current varieties include <code>CategoricalHistogram</code>, <code>NumericalHistogram</code>, and <code>ViewGrid</code>.</p> <p></p>"},{"location":"fiftyone_concepts/plots/#working-in-notebooks","title":"Working in notebooks \u00b6","text":"<p>The recommended way to work with FiftyOne\u2019s interactive plots is in Jupyter notebooks or JupyterLab.</p> <p>In these environments, you can leverage the full power of plots by attaching them to the FiftyOne App and bidirectionally interacting with the plots and the App to identify interesting subsets of your data.</p> <p>Note</p> <p>Support for interactive plots in non-notebook contexts and in Google Colab and Databricks is coming soon! In the meantime, you can still use FiftyOne\u2019s plotting features in these environments, but you must manually call <code>plot.show()</code> to update the state of a plot to match the state of a connected <code>Session</code>, and any callbacks that would normally be triggered in response to interacting with a plot will not be triggered.</p> <p>You can get setup to work in a Jupyter environment by running the commands below for your environment:</p> <p>If you wish to use the <code>matplotlib</code> backend for any interactive plots, refer to this section for setup instructions.</p>"},{"location":"fiftyone_concepts/plots/#visualizing-embeddings","title":"Visualizing embeddings \u00b6","text":"<p>The FiftyOne Brain provides a powerful <code>compute_visualization()</code> method that can be used to generate low-dimensional representations of the samples/object patches in a dataset that can be visualized using interactive FiftyOne plots.</p> <p>To learn more about the available embedding methods, dimensionality reduction techniques, and their applications to dataset analysis, refer to this page. In this section, we\u2019ll just cover the basic mechanics of creating scatterplots and interacting with them.</p> <p>Note</p> <p>The visualizations in this section are rendered under the hood via the <code>scatterplot()</code> method, which you can directly use to generate interactive plots for arbitrary 2D or 3D representations of your data.</p>"},{"location":"fiftyone_concepts/plots/#standalone-plots","title":"Standalone plots \u00b6","text":"<p>Let\u2019s use <code>compute_visualization()</code> to generate a 2D visualization of the images in the test split of the MNIST dataset and then visualize it using the <code>results.visualize()</code> method of the returned results object, where each point is colored by its ground truth label:</p> <pre><code>import cv2\nimport numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"mnist\", split=\"test\")\n\n# Construct a `num_samples x num_pixels` array of images\nimages = np.array([\\\n    cv2.imread(f, cv2.IMREAD_UNCHANGED).ravel()\\\n    for f in dataset.values(\"filepath\")\\\n])\n\n# Compute 2D embeddings\nresults = fob.compute_visualization(dataset, embeddings=images, seed=51)\n\n# Visualize embeddings, colored by ground truth label\nplot = results.visualize(labels=\"ground_truth.label\")\nplot.show(height=720)\n</code></pre> <p></p> <p>As you can see, the 2D embeddings are naturally clustered according to their ground truth label!</p>"},{"location":"fiftyone_concepts/plots/#interactive-plots_2","title":"Interactive plots \u00b6","text":"<p>The full power of <code>compute_visualization()</code> comes when you associate the scatterpoints with the samples or objects in a <code>Dataset</code> and then attach it to a <code>Session</code>.</p> <p>The example below demonstrates setting up an interactive scatterplot for the test split of the MNIST dataset that is attached to the App.</p> <p>In this setup, the scatterplot renders each sample using its corresponding 2D embedding generated by <code>compute_visualization()</code>, colored by the sample\u2019s ground truth label.</p> <p>Since the <code>labels</code> argument to <code>results.visualize()</code> is categorical, each class is rendered as its own trace and you can click on the legend entries to show/hide individual classes, or double-click to show/hide all other classes.</p> <p>When points are lasso-ed in the plot, the corresponding samples are automatically selected in the session\u2019s current <code>view</code>. Likewise, whenever you modify the session\u2019s view, either in the App or by programmatically setting <code>session.view</code>, the corresponding locations will be selected in the scatterplot.</p> <p>Each block in the example code below denotes a separate cell in a Jupyter notebook:</p> <pre><code>import cv2\nimport numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"mnist\", split=\"test\")\n\n# Construct a `num_samples x num_pixels` array of images\nimages = np.array([\\\n    cv2.imread(f, cv2.IMREAD_UNCHANGED).ravel()\\\n    for f in dataset.values(\"filepath\")\\\n])\n\n# Compute 2D embeddings\nresults = fob.compute_visualization(dataset, embeddings=images, seed=51)\n\nsession = fo.launch_app(dataset)\n</code></pre> <pre><code># Visualize embeddings, colored by ground truth label\nplot = results.visualize(labels=\"ground_truth.label\")\nplot.show(height=720)\n\nsession.plots.attach(plot)\n</code></pre> <p>To give a taste of the possible interactions, let\u2019s hide all zero digit images and select the other digits near the zero cluster; this isolates the non-zero digit images in the App that are likely to be confused as zeros:</p> <p></p> <p>Alternatively, let\u2019s hide all classes except the zero digits, and then select the zero digits that are not in the zero cluster; this isolates the zero digit images in the App that are likely to be confused as other digits:</p> <p></p>"},{"location":"fiftyone_concepts/plots/#geolocation-plots","title":"Geolocation plots \u00b6","text":"<p>You can use <code>location_scatterplot()</code> to generate interactive plots of datasets with geolocation data.</p> <p>You can store arbitrary location data in GeoJSON format on your datasets using the <code>GeoLocation</code> and <code>GeoLocations</code> label types. See this section for more information.</p> <p>The <code>location_scatterplot()</code> method only supports simple <code>[longitude, latitude]</code> coordinate points, which can be stored in the <code>point</code> attribute of a <code>GeoLocation</code> field.</p> <p>Note</p> <p>Did you know? You can create location-based views that filter your data by their location!</p>"},{"location":"fiftyone_concepts/plots/#standalone-plots_1","title":"Standalone plots \u00b6","text":"<p>In the simplest case, you can use this method to generate a location scatterplot for a list of <code>[longitude, latitude]</code> coordinates, using the optional <code>labels</code> and <code>sizes</code> parameters to control the color and sizes of each point, respectively.</p> <p>The example below demonstrates this usage using the quickstart-geo dataset from the zoo, which contains <code>GeoLocation</code> data in its <code>location</code> field:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-geo\")\nfob.compute_uniqueness(dataset)\n\n# A list of ``[longitude, latitude]`` coordinates\nlocations = dataset.values(\"location.point.coordinates\")\n\n# Scalar `uniqueness` values for each sample\nuniqueness = dataset.values(\"uniqueness\")\n\n# The number of ground truth objects in each sample\nnum_objects = dataset.values(F(\"ground_truth.detections\").length())\n\n# Create scatterplot\nplot = fo.location_scatterplot(\n    locations=locations,\n    labels=uniqueness,      # color points by their `uniqueness` values\n    sizes=num_objects,      # scale point sizes by number of objects\n    labels_title=\"uniqueness\",\n    sizes_title=\"objects\",\n)\nplot.show()\n</code></pre> <p></p> <p>You can also change the style to <code>style=\"density\"</code> in order to view the data as a density plot:</p> <pre><code># Create density plot\nplot = fo.location_scatterplot(\n    locations=locations,\n    labels=uniqueness,      # color points by their `uniqueness` values\n    sizes=num_objects,      # scale influence by number of objects\n    style=\"density\",\n    radius=10,\n)\nplot.show()\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/plots/#interactive-plots_3","title":"Interactive plots \u00b6","text":"<p>The real power of <code>location_scatterplot()</code> comes when you associate the location coordinates with the samples in a <code>Dataset</code> and then attach it to a <code>Session</code>.</p> <p>The example below demonstrates setting up an interactive location scatterplot for the quickstart-geo dataset that is attached to the App.</p> <p>In this setup, the location plot renders each sample using its corresponding <code>[longitude, latitude]</code> coordinates from the dataset\u2019s only <code>GeoLocation</code> field, <code>location</code>. When points are lasso-ed in the plot, the corresponding samples are automatically selected in the session\u2019s current <code>view</code>. Likewise, whenever you modify the Session\u2019s view, either in the App or by programmatically setting <code>session.view</code>, the corresponding locations will be selected in the scatterplot.</p> <p>Each block in the example code below denotes a separate cell in a Jupyter notebook:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-geo\")\nfob.compute_uniqueness(dataset)\n\nsession = fo.launch_app(dataset)\n</code></pre> <pre><code>from fiftyone import ViewField as F\n\n# Computes the number of ground truth objects in each sample\nnum_objects = F(\"ground_truth.detections\").length()\n\n# Create the scatterplot\nplot = fo.location_scatterplot(\n    samples=dataset,\n    labels=\"uniqueness\",    # color points by their `uniqueness` values\n    sizes=num_objects,      # scale point sizes by number of objects\n    sizes_title=\"objects\",\n)\nplot.show(height=720)\n\nsession.plots.attach(plot)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/plots/#regression-plots","title":"Regression plots \u00b6","text":"<p>When you use evaluation methods such as <code>evaluate_regressions()</code> to evaluate model predictions, the regression plots that you can generate by calling the <code>plot_results()</code> method are responsive plots that can be attached to App instances to interactively explore specific cases of your model\u2019s performance.</p> <p>Note</p> <p>See this page for an in-depth guide to using FiftyOne to evaluate regression models.</p> <p>The example below demonstrates using an interactive regression plot to explore the results of some fake regression data on the quickstart dataset.</p> <p>In this setup, you can lasso scatter points to select the corresponding samples in the App.</p> <p>Likewise, whenever you modify the Session\u2019s view, either in the App or by programmatically setting <code>session.view</code>, the regression plot is automatically updated to select the scatter points that are included in the current view.</p> <p>Each block in the example code below denotes a separate cell in a Jupyter notebook:</p> <pre><code>import random\nimport numpy as np\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\").select_fields().clone()\n\n# Populate some fake regression + weather data\nfor idx, sample in enumerate(dataset, 1):\n    ytrue = random.random() * idx\n    ypred = ytrue + np.random.randn() * np.sqrt(ytrue)\n    confidence = random.random()\n    sample[\"ground_truth\"] = fo.Regression(value=ytrue)\n    sample[\"predictions\"] = fo.Regression(value=ypred, confidence=confidence)\n    sample[\"weather\"] = random.choice([\"sunny\", \"cloudy\", \"rainy\"])\n    sample.save()\n\n# Evaluate the predictions in the `predictions` field with respect to the\n# values in the `ground_truth` field\nresults = dataset.evaluate_regressions(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval\",\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <pre><code># Plot a scatterplot of the results colored by `weather` and scaled by\n# `confidence`\nplot = results.plot_results(labels=\"weather\", sizes=\"predictions.confidence\")\nplot.show()\n\nsession.plots.attach(plot)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/plots/#line-plots","title":"Line plots \u00b6","text":"<p>You can use <code>lines()</code> to generate interactive line plots whose points represent data associated with the samples, frames, or labels of a dataset. These plots can then be attached to App instances to interactively explore specific slices of your dataset based on their corresponding line data.</p> <p>The example below demonstrates using an interactive lines plot to view the frames of the quickstart-video dataset that contain the most vehicles. In this setup, you can lasso scatter points to select the corresponding frames in a frames view in the App.</p> <p>Each block in the example code below denotes a separate cell in a Jupyter notebook:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\").clone()\n\n# Ensure dataset has sampled frames available so we can use frame selection\ndataset.to_frames(sample_frames=True)\n\nsession = fo.launch_app(dataset)\n</code></pre> <pre><code>view = dataset.filter_labels(\"frames.detections\", F(\"label\") == \"vehicle\")\n\n# Plot the number of vehicles in each frame of a video dataset\nplot = fo.lines(\n    x=\"frames.frame_number\",\n    y=F(\"frames.detections.detections\").length(),\n    labels=\"id\",\n    samples=view,\n    xaxis_title=\"frame number\",\n    yaxis_title=\"num vehicles\",\n)\nplot.show()\n\n# When points are selected in the plot, load the corresponding frames in\n# frames views in the App\nplot.selection_mode = \"frames\"\n\nsession.plots.attach(plot)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/plots/#confusion-matrices","title":"Confusion matrices \u00b6","text":"<p>When you use evaluation methods such as <code>evaluate_classifications()</code> and <code>evaluate_detections()</code> to evaluate model predictions, the confusion matrices that you can generate by calling the <code>plot_confusion_matrix()</code> method are responsive plots that can be attached to App instances to interactively explore specific cases of your model\u2019s performance.</p> <p>Note</p> <p>See this page for an in-depth guide to using FiftyOne to evaluate models.</p> <p>The example below demonstrates using an interactive confusion matrix to explore the results of an evaluation on the <code>predictions</code> field of the quickstart dataset.</p> <p>In this setup, you can click on individual cells of the confusion matrix to select the corresponding ground truth and/or predicted <code>Detections</code> in the App. For example, if you click on a diagonal cell of the confusion matrix, you will see the true positive examples of that class in the App.</p> <p>Likewise, whenever you modify the Session\u2019s view, either in the App or by programmatically setting <code>session.view</code>, the confusion matrix is automatically updated to show the cell counts for only those detections that are included in the current view.</p> <p>Each block in the example code below denotes a separate cell in a Jupyter notebook:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Evaluate detections in the `predictions` field\nresults = dataset.evaluate_detections(\"predictions\", gt_field=\"ground_truth\")\n\n# The top-10 most common classes\ncounts = dataset.count_values(\"ground_truth.detections.label\")\nclasses = sorted(counts, key=counts.get, reverse=True)[:10]\n\nsession = fo.launch_app(dataset)\n</code></pre> <pre><code># Plot confusion matrix\nplot = results.plot_confusion_matrix(classes=classes)\nplot.show(height=600)\n\nsession.plots.attach(plot)\n</code></pre> <p></p> <p>When you pass an <code>eval_key</code> to <code>evaluate_detections()</code>, confusion matrices attached to App instances have a different default behavior: when you select cell(s), the corresponding evaluation patches for the run are shown in the App. This allows you to visualize each TP, FP, and FN example in a fine-grained manner:</p> <pre><code>results = dataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\"\n)\n</code></pre> <pre><code># Since these results have an `eval_key`, selecting cells in this plot will\n# load evaluation patch views\nplot = results.plot_confusion_matrix(classes=classes)\nplot.show(height=600)\n\nsession.plots.attach(plot)\n</code></pre> <p></p> <p>If you prefer a different selection behavior, you can simply change the plot\u2019s selection mode.</p>"},{"location":"fiftyone_concepts/plots/#view-plots_1","title":"View plots \u00b6","text":"<p><code>ViewPlot</code> is a class of plots whose state is automatically updated whenever the current <code>session.view</code> changes.</p> <p>Current varieties of view plots include <code>CategoricalHistogram</code>, <code>NumericalHistogram</code>, and <code>ViewGrid</code>.</p> <p>Note</p> <p>New <code>ViewPlot</code> subclasses will be continually added over time, and it is also straightforward to implement your own custom view plots. Contributions are welcome at https://github.com/voxel51/fiftyone!</p> <p>The example below demonstrates the use of <code>ViewGrid</code> to construct a dashboard of histograms of various aspects of a dataset, which can then be attached to a <code>Session</code> in order to automatically see how the statistics change when the session\u2019s <code>view</code> is modified.</p> <p>Each block in the example code below denotes a separate cell in a Jupyter notebook:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\ndataset.compute_metadata()\n\n# Define some interesting plots\nplot1 = fo.NumericalHistogram(F(\"metadata.size_bytes\") / 1024, bins=50, xlabel=\"image size (KB)\")\nplot2 = fo.NumericalHistogram(\"predictions.detections.confidence\", bins=50, range=[0, 1])\nplot3 = fo.CategoricalHistogram(\"ground_truth.detections.label\", order=\"frequency\")\nplot4 = fo.CategoricalHistogram(\"predictions.detections.label\", order=\"frequency\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <pre><code># Construct a custom dashboard of plots\nplot = fo.ViewGrid([plot1, plot2, plot3, plot4], init_view=dataset)\nplot.show(height=720)\n\nsession.plots.attach(plot)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/plots/#attaching-plots-to-the-app","title":"Attaching plots to the App \u00b6","text":"<p>All <code>Session</code> instances provide a <code>plots</code> attribute that you can use to attach <code>ResponsivePlot</code> instances to the FiftyOne App.</p> <p>When <code>ResponsivePlot</code> instances are attached to a <code>Session</code>, they are automatically updated whenever <code>session.view</code> changes for any reason, whether you modify your view in the App, or programmatically change it by setting <code>session.view</code>, or if multiple plots are connected and another plot triggers a <code>Session</code> update!</p> <p>Note</p> <p>Interactive plots are currently only supported in Jupyter notebooks. In the meantime, you can still use FiftyOne\u2019s plotting features in other environments, but you must manually call <code>plot.show()</code> to update the state of a plot to match the state of a connected <code>Session</code>, and any callbacks that would normally be triggered in response to interacting with a plot will not be triggered.</p> <p>See this section for more information.</p>"},{"location":"fiftyone_concepts/plots/#attaching-a-plot","title":"Attaching a plot \u00b6","text":"<p>The code below demonstrates the basic pattern of connecting a <code>ResponsivePlot</code> to a <code>Session</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-geo\")\n\nsession = fo.launch_app(dataset)\n\n# Create a responsive location plot\nplot = fo.location_scatterplot(samples=dataset)\nplot.show()  # show the plot\n\n# Attach the plot to the session\n# Updates will automatically occur when the plot/session are updated\nsession.plots.attach(plot)\n</code></pre> <p>You can view details about the plots attached to a <code>Session</code> by printing it:</p> <pre><code>print(session)\n</code></pre> <pre><code>Dataset:          quickstart-geo\nMedia type:       image\nNum samples:      500\nSelected samples: 0\nSelected labels:  0\nSession URL:      http://localhost:5151/\nConnected plots:\n    plot1: fiftyone.core.plots.plotly.InteractiveScatter\n</code></pre> <p>By default, plots are given sequential names <code>plot1</code>, <code>plot2</code>, etc., but you can customize their names via the optional <code>name</code> parameter of <code>session.plots.attach()</code>.</p> <p>You can retrieve a <code>ResponsivePlot</code> instance from its connected session by its name:</p> <pre><code>same_plot = session.plots[\"plot1\"]\nsame_plot is plot  # True\n</code></pre>"},{"location":"fiftyone_concepts/plots/#connecting-and-disconnecting-plots","title":"Connecting and disconnecting plots \u00b6","text":"<p>By default, when plots are attached to a <code>Session</code>, they are connected, which means that any necessary state updates will happen automatically. If you wish to temporarily suspend updates for an individual plot, you can use <code>plot.disconnect()</code>:</p> <pre><code># Disconnect an individual plot\n# Plot updates will no longer update the session, and vice versa\nplot.disconnect()\n\n# Note that `plot1` is now disconnected\nprint(session)\n</code></pre> <pre><code>Dataset:          quickstart-geo\nMedia type:       image\nNum samples:      500\nSelected samples: 0\nSelected labels:  0\nSession URL:      http://localhost:5151/\nDisconnected plots:\n    plot1: fiftyone.core.plots.plotly.InteractiveScatter\n</code></pre> <p>You can reconnect a plot by calling <code>plot.connect()</code>:</p> <pre><code># Reconnect an individual plot\nplot.connect()\n\n# Note that `plot1` is connected again\nprint(session)\n</code></pre> <pre><code>Dataset:          quickstart-geo\nMedia type:       image\nNum samples:      500\nSelected samples: 0\nSelected labels:  0\nSession URL:      http://localhost:5151/\nConnected plots:\n    plot1: fiftyone.core.plots.plotly.InteractiveScatter\n</code></pre> <p>You can disconnect and reconnect all plots currently attached to a <code>Session</code> via <code>session.plots.disconnect()</code> and <code>session.plots.connect()</code>, respectively.</p>"},{"location":"fiftyone_concepts/plots/#detaching-plots","title":"Detaching plots \u00b6","text":"<p>If you would like to permanently detach a plot from a <code>Session</code>, use <code>session.plots.pop()</code> or <code>session.plots.remove()</code>:</p> <pre><code># Detach plot from its session\nplot = session.plots.pop(\"plot1\")\n\n# Note that `plot1` no longer appears\nprint(session)\n</code></pre> <pre><code>Dataset:          quickstart-geo\nMedia type:       image\nNum samples:      500\nSelected samples: 0\nSelected labels:  0\nSession URL:      http://localhost:5151/\n</code></pre>"},{"location":"fiftyone_concepts/plots/#freezing-plots","title":"Freezing plots \u00b6","text":"<p>Working with interactive plots in notebooks is an amazingly productive experience. However, when you find something particularly interesting that you want to save, or you want to share a notebook with a colleague without requiring them to rerun all of the cells to reproduce your results, you may want to freeze your responsive plots.</p> <p>You can conveniently freeze your currently active App instance and any attached plots by calling <code>session.freeze()</code>:</p> <pre><code># Replace current App instance and all attached plots with static images\nsession.freeze()\n</code></pre> <p>After calling this method, your current App instance and all connected plots will be replaced by static images that will be visible when you save + reopen your notebook later.</p> <p>You can also freeze an individual plot by calling <code>plot.freeze()</code>:</p> <pre><code># Replace a plot with a static image\nplot.freeze()\n</code></pre> <p>You can \u201crevive\u201d frozen App and plot instances by simply rerunning the notebook cells in which they were defined and shown.</p> <p>Note</p> <p><code>session.freeze()</code> and <code>plot.freeze()</code> are only applicable when working in notebook contexts.</p>"},{"location":"fiftyone_concepts/plots/#saving-plots","title":"Saving plots \u00b6","text":"<p>You can use <code>plot.save()</code> to save any <code>InteractivePlot</code> or <code>ViewPlot</code> as a static image or HTML.</p> <p>Consult the documentation of your plot\u2019s <code>save()</code> method for details on configuring the export.</p> <p>For example, you can save a histogram view plot:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nplot = fo.CategoricalHistogram(\n    \"ground_truth.detections.label\",\n    order=\"frequency\",\n    log=True,\n    init_view=dataset,\n)\n\nplot.save(\"./histogram.jpg\", scale=2.0)\n</code></pre> <p></p> <p>Or you can save an embedding scatterplot:</p> <pre><code>import fiftyone.brain as fob\n\nresults = fob.compute_visualization(dataset)\n\nplot = results.visualize(labels=\"uniqueness\", axis_equal=True)\nplot.save(\"./embeddings.png\", height=300, width=800)\n</code></pre> <p></p> <p>You can also save plots generated using the matplotlib backend:</p> <pre><code>plot = results.visualize(\n    labels=\"uniqueness\",\n    backend=\"matplotlib\",\n    ax_equal=True,\n    marker_size=5,\n)\nplot.save(\"./embeddings-matplotlib.png\", dpi=200)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/plots/#advanced-usage","title":"Advanced usage \u00b6","text":""},{"location":"fiftyone_concepts/plots/#customizing-plot-layouts","title":"Customizing plot layouts \u00b6","text":"<p>The <code>plot.show()</code> method used to display plots in FiftyOne supports optional keyword arguments that you can use to customize the look-and-feel of plots.</p> <p>In general, consult the documentation of the relevant <code>plot.show()</code> method for details on the supported parameters.</p> <p>If you are using the default plotly backend, <code>plot.show()</code> will accept any valid keyword arguments for <code>plotly.graph_objects.Figure.update_layout()</code>.</p> <p>The examples below demonstrate some common layout customizations that you may wish to perform:</p> <pre><code># Increase the default height of the figure, in pixels\nplot.show(height=720)\n\n# Equivalent of `axis(\"equal\")` in matplotlib\nplot.show(yaxis_scaleanchor=\"x\")\n</code></pre> <p>Note</p> <p>Refer to the plotly layout documentation for a full list of the supported options.</p>"},{"location":"fiftyone_concepts/plots/#plot-selection-modes","title":"Plot selection modes \u00b6","text":"<p>When working with scatterplots and interactive heatmaps that are linked to frames or labels, you may prefer to see different views loaded in the App when you make a selection in the plot. For example, you may want to see the corresponding objects in a patches view, or you may wish to see the samples containing the objects but with all other labels also visible.</p> <p>You can use the <code>selection_mode</code> property of <code>InteractivePlot</code> instances to change the behavior of App updates when selections are made in connected plots.</p> <p>When a plot is linked to frames, the available <code>selection_mode</code> options are:</p> <ul> <li> <p><code>\"select\"</code> ( default): show video samples with labels only for the selected frames</p> </li> <li> <p><code>\"match\"</code>: show unfiltered video samples containing at least one selected frame</p> </li> <li> <p><code>\"frames\"</code>: show only the selected frames in a frames view</p> </li> </ul> <p>When a plot is linked to labels, the available <code>selection_mode</code> options are:</p> <ul> <li> <p><code>\"patches\"</code> ( default): show the selected labels in a patches view</p> </li> <li> <p><code>\"select\"</code>: show only the selected labels</p> </li> <li> <p><code>\"match\"</code>: show unfiltered samples containing at least one selected label</p> </li> </ul> <p>For example, by default, clicking on cells in a confusion matrix for a detection evaluation will show the corresponding ground truth and predicted objects in an evaluation patches view view in the App. Run the code blocks below in Jupyter notebook cells to see this:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nresults = dataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\"\n)\n\n# Get the 10 most common classes in the dataset\ncounts = dataset.count_values(\"ground_truth.detections.label\")\nclasses = sorted(counts, key=counts.get, reverse=True)[:10]\n\nsession = fo.launch_app(dataset)\n</code></pre> <pre><code>plot = results.plot_confusion_matrix(classes=classes)\nplot.show(height=600)\n\nsession.plots.attach(plot, name=\"eval\")\n</code></pre> <p>However, you can change this behavior by updating the <code>selection_mode</code> property of the plot like so:</p> <pre><code># Selecting cells will now show unfiltered samples containing selected objects\nplot.selection_mode = \"match\"\n</code></pre> <pre><code># Selecting cells will now show filtered samples containing only selected objects\nplot.selection_mode = \"select\"\n</code></pre> <p>Similarly, selecting scatter points in an object embeddings visualization will show the corresponding objects in the App as a patches view:</p> <pre><code># Continuing from the code above\nsession.freeze()\n</code></pre> <pre><code>import fiftyone.brain as fob\n\nresults = fob.compute_visualization(\n    dataset, patches_field=\"ground_truth\", brain_key=\"gt_viz\"\n)\n\n# Restrict visualization to the 10 most common classes\nview = dataset.filter_labels(\"ground_truth\", F(\"label\").is_in(classes))\nresults.use_view(view)\n\nsession.show()\n</code></pre> <pre><code>plot = results.visualize(labels=\"ground_truth.detections.label\")\nplot.show(height=800)\n\nsession.plots.attach(plot, name=\"gt_viz\")\n</code></pre> <p>However, you can change this behavior by updating the <code>selection_mode</code> property of the plot:</p> <pre><code># Selecting points will now show unfiltered samples containing selected objects\nplot.selection_mode = \"match\"\n</code></pre> <pre><code># Selecting points will now show filtered samples containing only selected objects\nplot.selection_mode = \"select\"\n</code></pre> <p>Note</p> <p>The App will immediately update when you set the <code>selection_mode</code> property of an <code>InteractivePlot</code> connected to the App.</p>"},{"location":"fiftyone_concepts/plots/#plotting-backend","title":"Plotting backend \u00b6","text":"<p>Most plotting methods in the <code>fiftyone.core.plots()</code> module provide an optional <code>backend</code> parameter that you can use to control the plotting backend used to render plots.</p> <p>The default plotting backend is <code>plotly</code>, which is highly recommended due to its better performance, look-and-feel, and greater support for interactivity.</p> <p>However, most plot types also support the <code>matplotlib</code> backend. If you chose this backend, plots will be rendered as matplotlib figures. Many matplotlib-powered plot types support interactivity, but you must enable this behavior:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nresults = dataset.evaluate_detections(\"predictions\", gt_field=\"ground_truth\")\n\n# Get the 10 most common classes in the dataset\ncounts = dataset.count_values(\"ground_truth.detections.label\")\nclasses = sorted(counts, key=counts.get, reverse=True)[:10]\n</code></pre> <pre><code># Use the default plotly backend\nplot = results.plot_confusion_matrix(classes=classes)\nplot.show(height=512)\n</code></pre> <p></p> <pre><code>import matplotlib.pyplot as plt\n\n# Use the matplotlib backend instead\nfigure = results.plot_confusion_matrix(\n    classes=classes, backend=\"matplotlib\", figsize=(10, 10)\n)\nplt.show(block=False)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/plots/#interactive-matplotlib-plots","title":"Interactive matplotlib plots \u00b6","text":"<p>If you are using the matplotlib backend, many FiftyOne plots still support interactivity in notebooks, but you must enable this behavior by running the appropriate magic command in your notebook before you generate your first plot.</p> <p>If you forget or choose not to run a magic command, the plots will still display, but they will not be interactive.</p> <p>Follow the instructions for your environment below to enable interactive matplotlib plots:</p>"},{"location":"fiftyone_concepts/running_environments/","title":"FiftyOne Environments \u00b6","text":"<p>This guide describes best practices for using FiftyOne with data stored in various environments, including local machines, remote servers, and cloud storage.</p>"},{"location":"fiftyone_concepts/running_environments/#terminology","title":"Terminology \u00b6","text":"<ul> <li> <p>Local machine: Data is stored on the same computer that will be used to launch the App</p> </li> <li> <p>Remote machine: Data is stored on disk on a separate machine (typically a remote server) from the one that will be used to launch the App</p> </li> <li> <p>Notebooks: You are working from a Jupyter Notebook, Google Colab Notebook, or Databricks Notebook SageMaker Notebook</p> </li> <li> <p>Cloud storage: Data is stored in a cloud bucket (e.g., S3, GCS, or Azure)</p> </li> </ul>"},{"location":"fiftyone_concepts/running_environments/#local-data","title":"Local data \u00b6","text":"<p>When working with data that is stored on disk on a machine with a display, you can directly load a dataset and then launch the App:</p> <pre><code># On local machine\nimport fiftyone as fo\n\ndataset = fo.Dataset(\"my-dataset\")\n\nsession = fo.launch_app(dataset)  # (optional) port=XXXX\n</code></pre> <p>From here, you can explore the dataset interactively from the App and from your Python shell by manipulating the <code>session object</code>.</p> <p>Note</p> <p>You can use custom ports when launching the App in order to operate multiple App instances simultaneously on your machine.</p>"},{"location":"fiftyone_concepts/running_environments/#remote-data","title":"Remote data \u00b6","text":"<p>FiftyOne supports working with data that is stored on a remote machine that you have <code>ssh</code> access to. The basic workflow is to load a dataset on the remote machine via the FiftyOne Python library, launch a remote session, and connect to the session on your local machine where you can then interact with the App.</p> <p>First, <code>ssh</code> into your remote machine and install FiftyOne if necessary.</p> <p>Then load a dataset using Python on the remote machine and launch a remote session:</p> <pre><code># On remote machine\nimport fiftyone as fo\n\ndataset = fo.load_dataset(...)\n\nsession = fo.launch_app(dataset, remote=True)  # optional: port=XXXX\n</code></pre> <p>Leave the Python REPL running and follow the instructions for connecting to this session remotely that were printed to your terminal (also described below).</p> <p>Note</p> <p>You can manipulate the <code>session</code> object on the remote machine as usual to programmatically interact with the App instance that you view locally.</p> <p>To connect to your remote session, open a new terminal window on your local machine and execute the following command to setup port forwarding to connect to your remote session:</p> <pre><code># On local machine\nssh -N -L 5151:127.0.0.1:XXXX [&lt;username&gt;@]&lt;hostname&gt;\n</code></pre> <p>Leave this process running and open http://localhost:5151 in your browser to access the App.</p> <p>In the above, <code>[&lt;username&gt;@]&lt;hostname&gt;</code> specifies the remote machine to connect to, <code>XXXX</code> refers to the port that you chose when you launched the session on your remote machine (the default is 5151), and <code>5151</code> specifies the local port to use to connect to the App (and can be customized).</p> <p>Alternatively, if you have FiftyOne installed on your local machine, you can use the CLI to automatically configure port forwarding and open the App in your browser as follows:</p> <pre><code># On local machine\nfiftyone app connect --destination [&lt;username&gt;@]&lt;hostname&gt;\n</code></pre> <p>If you choose a custom port <code>XXXX</code> on the remote machine, add a <code>--port XXXX</code> flag to the above command.</p> <p>If you would like to use a custom local port, add a <code>--local-port YYYY</code> flag to the above command.</p> <p>Note</p> <p>You can customize the local/remote ports used when launching remote sessions in order to connect/service multiple remote sessions simultaneously.</p> <p>Note</p> <p>If you use ssh keys to connect to your remote machine, you can use the optional <code>--ssh-key</code> argument of the fiftyone app connect command.</p> <p>However, if you are using this key regularly, it is recommended to add it to your <code>~/.ssh/config</code> as the default <code>IdentityFile</code>.</p>"},{"location":"fiftyone_concepts/running_environments/#restricting-the-app-address","title":"Restricting the App address \u00b6","text":"<p>By default, the App will listen on <code>localhost</code>. However, you can provide the optional <code>address</code> parameter to <code>launch_app()</code> to specify a particular IP address or hostname for the App to listen on.</p> <p>Using the default of <code>localhost</code> means the App can only be accessed from the local machine or a machine that was able to setup ssh port forwarding as described in the previous section.</p> <p>An alternative is to set the App address to <code>\"0.0.0.0\"</code> so that the App can be accessed from a remote host or from the local machine itself. Using <code>\"0.0.0.0\"</code> will bind the App to all available interfaces and will allow access to the App from any remote resource with access to your network.</p> <pre><code>import fiftyone as fo\n\ndataset = fo.load_dataset(...)\n\n# Enable connections from remote hosts\nsession = fo.launch_app(dataset, remote=True, address=\"0.0.0.0\")\n</code></pre> <p>If desired, you can permanently configure an App address by setting the <code>default_app_address</code> of your FiftyOne config. You can achieve this by adding the following entry to your <code>~/.fiftyone/config.json</code> file:</p> <pre><code>{\n    \"default_app_address\": \"0.0.0.0\"\n}\n</code></pre> <p>or by setting the following environment variable:</p> <pre><code>export FIFTYONE_DEFAULT_APP_ADDRESS='0.0.0.0'\n</code></pre>"},{"location":"fiftyone_concepts/running_environments/#notebooks","title":"Notebooks \u00b6","text":"<p>FiftyOne officially supports Jupyter Notebooks, Google Colab Notebooks, Databricks Notebooks. App support is also available in SageMaker Notebooks and any cloud notebook that has an accessible network proxy via configured proxy_url.</p> <p>To use FiftyOne in a notebook, simply install <code>fiftyone</code> via <code>pip</code>:</p> <pre><code>!pip install fiftyone\n</code></pre> <p>and load datasets as usual. When you run <code>launch_app()</code> in a notebook, an App window will be opened in the output of your current cell.</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset(\"my-dataset\")\n\n# Creates a session and opens the App in the output of the cell\nsession = fo.launch_app(dataset)\n</code></pre> <p>Any time you update the state of your <code>session</code> object; e.g., by setting <code>session.dataset</code> or <code>session.view</code>, a new App window will be automatically opened in the output of the current cell. The previously active App will be \u201cfrozen\u201d, i.e., replaced with a screenshot of its current state.</p> <pre><code># A new App window will be created in the output of this cell, and the\n# previously active App instance will be replaced with a screenshot\nsession.view = dataset.take(10)\n</code></pre> <p>You can reactivate a frozen App instance from the same notebook session by clicking on the screenshot.</p> <p>Note</p> <p>Reactivating a frozen App instance will load the current state of the <code>session</code> object, not the state in which the screenshot was taken.</p> <p>To reactivate an App instance from a previous session, e.g., when running a notebook downloaded from the web for the first time, you must (re)run the cell.</p> <p>You can manually replace the active App instance with a screenshot by calling <code>session.freeze()</code>. This is useful when you are finished with your notebook and ready to share it with others, as an active App instance itself cannot be viewed outside of the current notebook session.</p> <pre><code># Replace active App instance with screenshot so App state is viewable offline\nsession.freeze()\n</code></pre>"},{"location":"fiftyone_concepts/running_environments/#manually-controlling-app-instances","title":"Manually controlling App instances \u00b6","text":"<p>If you would like to manually control when new App instances are created in a notebook, you can pass the <code>auto=False</code> flag to <code>launch_app()</code>:</p> <pre><code># Creates a session but does not open an App instance\nsession = fo.launch_app(dataset, auto=False)\n</code></pre> <p>When <code>auto=False</code> is provided, a new App window is created only when you call <code>session.show()</code>:</p> <pre><code># Update the session's view; no App window is created\nsession.view = dataset.take(10)\n\n# In another cell\n\n# Now open an App window in the cell's output\nsession.show()\n</code></pre> <p>As usual, this App window will remain connected to your <code>session</code> object, so it will stay in-sync with your session whenever it is active.</p> <p>Note</p> <p>If you run <code>session.show()</code> in multiple cells, only the most recently created App window will be active, i.e., synced with the <code>session</code> object.</p> <p>You can reactivate an older cell by clicking the link in the deactivated App window, or by running the cell again. This will deactivate the previously active cell.</p>"},{"location":"fiftyone_concepts/running_environments/#opening-the-app-in-a-dedicated-tab","title":"Opening the App in a dedicated tab \u00b6","text":"<p>If you are working from a Jupyter notebook, you can open the App in a separate browser tab rather than working with it in cell output(s).</p> <p>To do this, pass the <code>auto=False</code> flag to <code>launch_app()</code> when you launch the App (so that additional App instances will not be created as you work) and then call <code>session.open_tab()</code>:</p> <pre><code># Launch the App in a dedicated browser tab\nsession = fo.launch_app(dataset, auto=False)\nsession.open_tab()\n</code></pre>"},{"location":"fiftyone_concepts/running_environments/#remote-notebooks","title":"Remote notebooks \u00b6","text":"<p>You can also work in a Jupyter notebook in your local browser that is served from a remote machine where your data is located. Follow the instructions below to achieve this.</p> <p>On the remote machine:</p> <p>Start the Jupyter server on a port of your choice:</p> <pre><code># On remote machine\njupyter notebook --no-browser --port=XXXX /path/to/notebook.ipynb\n</code></pre> <p>On your local machine:</p> <p>Back on your local machine, you will need to forward the remote port <code>XXXX</code> to a local port (we\u2019ll also use <code>XXXX</code> here, for consistency):</p> <pre><code># On local machine\nssh -N -L XXXX:localhost:XXXX [&lt;username&gt;@]&lt;hostname&gt;\n</code></pre> <p>Now open <code>localhost:XXXX</code> in your browser and you should find your notebook!</p> <p>If your notebook launches the FiftyOne App, you will also need to forward the port used by the App to your local machine. By default, the App uses port <code>5151</code>, but you can specify any port, say <code>YYYY</code>, not currently in use on your remote machine:</p> <pre><code># On local machine\nssh -N -L 5151:localhost:YYYY [&lt;username&gt;@]&lt;hostname&gt;\n</code></pre> <p>In your Jupyter notebook:</p> <p>When you launch the FiftyOne App in your notebook, you should now see the App as expected!</p> <pre><code># Launch the App in a notebook cell\nsession = fo.launch_app(dataset)  # port=YYYY\n</code></pre> <p>If you chose a port <code>YYYY</code> other than the default <code>5151</code>, you will need to specify it when launching App instances per the commented argument above.</p> <p>Note that you can also open the App in a dedicated tab:</p> <pre><code># Launch the App in a dedicated browser tab\nsession = fo.launch_app(dataset, auto=False)  # port=YYYY\nsession.open_tab()\n</code></pre>"},{"location":"fiftyone_concepts/running_environments/#docker","title":"Docker \u00b6","text":"<p>The FiftyOne repository contains a Dockerfile that you can use/customize to build and run Docker images containing source or release builds of FiftyOne.</p>"},{"location":"fiftyone_concepts/running_environments/#building-an-image","title":"Building an image \u00b6","text":"<p>First, clone the repository:</p> <pre><code>git clone https://github.com/voxel51/fiftyone\ncd fiftyone\n</code></pre> <p>If you want a source install of FiftyOne, then build a wheel:</p> <pre><code>make python\n</code></pre> <p>If you want to install a FiftyOne release, then make the suggested modification in the Dockerfile.</p> <p>Next, build the image:</p> <pre><code>docker build -t voxel51/fiftyone .\n</code></pre> <p>The default image uses Python 3.11, but you can customize these via optional build arguments:</p> <pre><code>docker build \\\n    --build-arg PYTHON_VERSION=3.10 \\\n    -t voxel51/fiftyone .\n</code></pre> <p>Refer to the Dockerfile for additional Python packages that you may wish to include in your build.</p>"},{"location":"fiftyone_concepts/running_environments/#running-an-image","title":"Running an image \u00b6","text":"<p>The image is designed to persist all data in a single <code>/fiftyone</code> directory with the following organization:</p> <pre><code>/fiftyone/\n    db/             # FIFTYONE_DATABASE_DIR\n    default/        # FIFTYONE_DEFAULT_DATASET_DIR\n    zoo/\n        datasets/   # FIFTYONE_DATASET_ZOO_DIR\n        models/     # FIFTYONE_MODEL_ZOO_DIR\n</code></pre> <p>Therefore, to run a container, you should mount <code>/fiftyone</code> as a local volume via <code>--mount</code> or <code>-v</code>, as shown below:</p> <pre><code>SHARED_DIR=/path/to/shared/dir\n\ndocker run -v ${SHARED_DIR}:/fiftyone -p 5151:5151 -it voxel51/fiftyone\n</code></pre> <p>The <code>-p 5151:5151</code> option is required so that when you launch the App from within the container you can connect to it at http://localhost:5151 in your browser.</p> <p>You can also include the <code>-e</code> or <code>--env-file</code> options if you need to further configure FiftyOne.</p> <p>By default, running the image launches an IPython shell, which you can use as normal:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>Any datasets you create inside the Docker image must refer to media files within <code>SHARED_DIR</code> or another mounted volume if you intend to work with datasets between sessions.</p> <p>Note</p> <p>FiftyOne should automatically detect that it is running inside a Docker container. However, if you are unable to load the App in your browser, you may need to manually set the App address to <code>0.0.0.0</code>:</p> <pre><code>session = fo.launch_app(..., address=\"0.0.0.0\")\n</code></pre>"},{"location":"fiftyone_concepts/running_environments/#connecting-to-a-localhost-database","title":"Connecting to a localhost database \u00b6","text":"<p>If you are using a self-managed database that you ordinarily connect to via a URI like <code>mongodb://localhost</code>, then you will need to tweak this slightly when working in Docker. See this question for details.</p> <p>On Linux, include <code>--network=\"host\"</code> in your <code>docker run</code> command and use <code>mongodb://127.0.0.1</code> for your URI.</p> <p>On Mac or Windows, use <code>mongodb://host.docker.internal</code> for your URI.</p>"},{"location":"fiftyone_concepts/running_environments/#cloud-storage","title":"Cloud storage \u00b6","text":"<p>For prototyping, it is possible to work with data in cloud storage buckets in FiftyOne by mounting the buckets as local drives.</p> <p>The following sections describe how to do this in the AWS, Google Cloud, and Microsoft Azure environments.</p> <p>Warning</p> <p>Mounting cloud buckets using the techniques below is not performant and is not recommended or officially supported. It is useful only for prototyping.</p> <p>Our recommended, scalable approach to work with cloud-backed data is FiftyOne Teams, an enterprise deployment of FiftyOne with multiuser collaboration features, native cloud dataset support, and much more!</p>"},{"location":"fiftyone_concepts/running_environments/#aws","title":"AWS \u00b6","text":"<p>If your data is stored in an AWS S3 bucket, you can mount the bucket as a local drive on an EC2 instance and then access the data using the standard workflow for remote data.</p> <p>The steps below outline the process.</p> <p>Step 1</p> <p>Create an EC2 instance.</p> <p>Step 2</p> <p>Now ssh into the instance and install FiftyOne if necessary.</p> <pre><code># On remote machine\npip install fiftyone\n</code></pre> <p>Note</p> <p>You may need to install some system packages on your compute instance instance in order to run FiftyOne.</p> <p>Step 3</p> <p>Mount the S3 bucket as a local drive.</p> <p>You can use s3fs-fuse to do this. You will need to make a <code>.passwd-s3fs</code> file that contains your AWS credentials as outlined in the s3fs-fuse README.</p> <pre><code># On remote machine\ns3fs &lt;bucket-name&gt; /path/to/mount/point \\\n    -o passwd_file=.passwd-s3fs \\\n    -o umask=0007,uid=&lt;your-user-id&gt;\n</code></pre> <p>Step 4</p> <p>Now that you can access your data from the compute instance, start up Python and create a FiftyOne dataset whose filepaths are in the mount point you specified above. Then you can launch the App and work with it locally in your browser using remote sessions.</p>"},{"location":"fiftyone_concepts/running_environments/#google-cloud","title":"Google Cloud \u00b6","text":"<p>If your data is stored in a Google Cloud storage bucket, you can mount the bucket as a local drive on a GC compute instance and then access the data using the standard workflow for remote data.</p> <p>The steps below outline the process.</p> <p>Step 1</p> <p>Create a GC compute instance.</p> <p>Step 2</p> <p>Now ssh into the instance and install FiftyOne if necessary.</p> <pre><code># On remote machine\npip install fiftyone\n</code></pre> <p>Note</p> <p>You may need to install some system packages on your compute instance instance in order to run FiftyOne.</p> <p>Step 3</p> <p>Mount the GCS bucket as a local drive.</p> <p>You can use gcsfuse to do this:</p> <pre><code># On remote machine\ngcsfuse --implicit-dirs my-bucket /path/to/mount\n</code></pre> <p>Step 4</p> <p>Now that you can access your data from the compute instance, start up Python and create a FiftyOne dataset whose filepaths are in the mount point you specified above. Then you can launch the App and work with it locally in your browser using remote sessions.</p>"},{"location":"fiftyone_concepts/running_environments/#microsoft-azure","title":"Microsoft Azure \u00b6","text":"<p>If your data is stored in an Azure storage bucket, you can mount the bucket as a local drive on an Azure compute instance and then access the data using the standard workflow for remote data.</p> <p>The steps below outline the process.</p> <p>Step 1</p> <p>Create an Azure compute instance.</p> <p>Step 2</p> <p>Now ssh into the instance and install FiftyOne if necessary.</p> <pre><code># On remote machine\npip install fiftyone\n</code></pre> <p>Note</p> <p>You may need to install some system packages on your compute instance instance in order to run FiftyOne.</p> <p>Step 3</p> <p>Mount the Azure storage container in the instance.</p> <p>This is fairly straightforward if your data is stored in a blob container. You can use blobfuse for this.</p> <p>Step 4</p> <p>Now that you can access your data from the compute instance, start up Python and create a FiftyOne dataset whose filepaths are in the mount point you specified above. Then you can launch the App and work with it locally in your browser using remote sessions.</p>"},{"location":"fiftyone_concepts/running_environments/#setting-up-a-cloud-instance","title":"Setting up a cloud instance \u00b6","text":"<p>When you create a fresh cloud compute instance, you may need to install some system packages in order to install and use FiftyOne.</p> <p>For example, the script below shows a set of commands that may be used to configure a Debian-like Linux instance, after which you should be able to successfully install FiftyOne.</p> <pre><code># Example setup script for a Debian-like virtual machine\n\n# System packages\nsudo apt update\nsudo apt -y upgrade\nsudo apt install -y build-essential\nsudo apt install -y unzip\nsudo apt install -y cmake\nsudo apt install -y cmake-data\nsudo apt install -y pkg-config\nsudo apt install -y libsm6\nsudo apt install -y libxext6\nsudo apt install -y libssl-dev\nsudo apt install -y libffi-dev\nsudo apt install -y libxml2-dev\nsudo apt install -y libxslt1-dev\nsudo apt install -y zlib1g-dev\nsudo apt install -y python3\nsudo apt install -y python-dev\nsudo apt install -y python3-dev\nsudo apt install -y python3-pip\nsudo apt install -y python3-venv\nsudo apt install -y ffmpeg  # if working with video\n\n# (Recommended) Create a virtual environment\npython3 -m venv fiftyone-env\n. fiftyone-env/bin/activate\n\n# Python packages\npip install --upgrade pip setuptools wheel build\npip install ipython\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/","title":"Using Aggregations \u00b6","text":"<p>Datasets are the core data structure in FiftyOne, allowing you to represent your raw data, labels, and associated metadata. When you query and manipulate a <code>Dataset</code> object using dataset views, a <code>DatasetView</code> object is returned, which represents a filtered view into a subset of the underlying dataset\u2019s contents.</p> <p>Complementary to this data model, one is often interested in computing aggregate statistics about datasets, such as label counts, distributions, and ranges, where each <code>Sample</code> is reduced to a single quantity in the aggregate results.</p> <p>The <code>fiftyone.core.aggregations</code> module offers a declarative and highly-efficient approach to computing summary statistics about your datasets and views.</p>"},{"location":"fiftyone_concepts/using_aggregations/#overview","title":"Overview \u00b6","text":"<p>All builtin aggregations are subclasses of the <code>Aggregation</code> class, each encapsulating the computation of a different statistic about your data.</p> <p>Aggregations are conveniently exposed as methods on all <code>Dataset</code> and <code>DatasetView</code> objects:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# List available aggregations\nprint(dataset.list_aggregations())\n# ['bounds', 'count', 'count_values', 'distinct', ..., 'sum']\n</code></pre> <p>Think of aggregations as more efficient, concise alternatives to writing explicit loops over your dataset to compute a statistic:</p> <pre><code>from collections import defaultdict\n\n# Compute label histogram manually\nmanual_counts = defaultdict(int)\nfor sample in dataset:\n    for detection in sample.ground_truth.detections:\n        manual_counts[detection.label] += 1\n\n# Compute via aggregation\ncounts = dataset.count_values(\"ground_truth.detections.label\")\nprint(counts)  # same as `manual_counts` above\n</code></pre> <p>You can even aggregate on expressions that transform the data in arbitrarily complex ways:</p> <pre><code>from fiftyone import ViewField as F\n\n# Expression that computes the number of predicted objects\nnum_objects = F(\"predictions.detections\").length()\n\n# The `(min, max)` number of predictions per sample\nprint(dataset.bounds(num_objects))\n\n# The average number of predictions per sample\nprint(dataset.mean(num_objects))\n</code></pre> <p>The sections below discuss the available aggregations in more detail. You can also refer to the <code>fiftyone.core.aggregations</code> module documentation for detailed examples of using each aggregation.</p> <p>Note</p> <p>All aggregations can operate on embedded sample fields using the <code>embedded.field.name</code> syntax.</p> <p>Aggregation fields can also include array fields. Most array fields are automatically unwound, but you can always manually unwind an array using the <code>embedded.array[].field</code> syntax. See this section for more details.</p>"},{"location":"fiftyone_concepts/using_aggregations/#compute-bounds","title":"Compute bounds \u00b6","text":"<p>You can use the <code>bounds()</code> aggregation to compute the <code>[min, max]</code> range of a numeric field of a dataset:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Compute the bounds of the `uniqueness` field\nbounds = dataset.bounds(\"uniqueness\")\nprint(bounds)\n# (0.15001302256126986, 1.0)\n\n# Compute the bounds of the detection confidences in the `predictions` field\nbounds = dataset.bounds(\"predictions.detections.confidence\")\nprint(bounds)\n# (0.05003104358911514, 0.9999035596847534)\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/#count-items","title":"Count items \u00b6","text":"<p>You can use the <code>count()</code> aggregation to compute the number of non- <code>None</code> field values in a collection:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Compute the number of samples in the dataset\ncount = dataset.count()\nprint(count)\n# 200\n\n# Compute the number of samples with `predictions`\ncount = dataset.count(\"predictions\")\nprint(count)\n# 200\n\n# Compute the number of detections in the `ground_truth` field\ncount = dataset.count(\"predictions.detections\")\nprint(count)\n# 5620\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/#count-values","title":"Count values \u00b6","text":"<p>You can use the <code>count_values()</code> aggregation to compute the occurrences of field values in a collection:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Compute the number of samples in the dataset\ncounts = dataset.count_values(\"tags\")\nprint(counts)\n# {'validation': 200}\n\n# Compute a histogram of the predicted labels in the `predictions` field\ncounts = dataset.count_values(\"predictions.detections.label\")\nprint(counts)\n# {'bicycle': 13, 'hot dog': 8, ..., 'skis': 52}\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/#distinct-values","title":"Distinct values \u00b6","text":"<p>You can use the <code>distinct()</code> aggregation to compute the distinct values of a field in a collection:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Get the distinct tags on the dataset\nvalues = dataset.distinct(\"tags\")\nprint(values)\n# ['validation']\n\n# Get the distinct labels in the `predictions` field\nvalues = dataset.distinct(\"predictions.detections.label\")\nprint(values)\n# ['airplane', 'apple', 'backpack', ..., 'wine glass', 'zebra']\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/#histogram-values","title":"Histogram values \u00b6","text":"<p>You can use the <code>histogram_values()</code> aggregation to compute the histograms of numeric fields of a collection:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nimport fiftyone.zoo as foz\n\ndef plot_hist(counts, edges):\n    counts = np.asarray(counts)\n    edges = np.asarray(edges)\n    left_edges = edges[:-1]\n    widths = edges[1:] - edges[:-1]\n    plt.bar(left_edges, counts, width=widths, align=\"edge\")\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n#\n# Compute a histogram of the `uniqueness` field\n#\n\ncounts, edges, other = dataset.histogram_values(\"uniqueness\", bins=50)\n\nplot_hist(counts, edges)\nplt.show(block=False)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/using_aggregations/#schema","title":"Schema \u00b6","text":"<p>You can use the <code>schema()</code> aggregation to extract the names and types of the attributes of a specified embedded document field across all samples in a collection.</p> <p>Schema aggregations are useful for detecting the presence and types of dynamic attributes of <code>Label</code> fields across a collection.</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Extract the names and types of all dynamic attributes on the\n# `ground_truth` detections\nprint(dataset.schema(\"ground_truth.detections\", dynamic_only=True))\n</code></pre> <pre><code>{\n    'area': &lt;fiftyone.core.fields.FloatField object at 0x7fc94015fb50&gt;,\n    'iscrowd': &lt;fiftyone.core.fields.FloatField object at 0x7fc964869fd0&gt;,\n}\n</code></pre> <p>You can also use the <code>list_schema()</code> aggregation to extract the value type(s) in a list field across all samples in a collection:</p> <pre><code>from datetime import datetime\nimport fiftyone as fo\n\ndataset = fo.Dataset()\n\nsample1 = fo.Sample(\n    filepath=\"image1.png\",\n    ground_truth=fo.Classification(\n        label=\"cat\",\n        info=[\\\n            fo.DynamicEmbeddedDocument(\\\n                task=\"initial_annotation\",\\\n                author=\"Alice\",\\\n                timestamp=datetime(1970, 1, 1),\\\n                notes=[\"foo\", \"bar\"],\\\n            ),\\\n            fo.DynamicEmbeddedDocument(\\\n                task=\"editing_pass\",\\\n                author=\"Bob\",\\\n                timestamp=datetime.utcnow(),\\\n            ),\\\n        ],\n    ),\n)\n\nsample2 = fo.Sample(\n    filepath=\"image2.png\",\n    ground_truth=fo.Classification(\n        label=\"dog\",\n        info=[\\\n            fo.DynamicEmbeddedDocument(\\\n                task=\"initial_annotation\",\\\n                author=\"Bob\",\\\n                timestamp=datetime(2018, 10, 18),\\\n                notes=[\"spam\", \"eggs\"],\\\n            ),\\\n        ],\n    ),\n)\n\ndataset.add_samples([sample1, sample2])\n\n# Determine that `ground_truth.info` contains embedded documents\nprint(dataset.list_schema(\"ground_truth.info\"))\n# fo.EmbeddedDocumentField\n\n# Determine the fields of the embedded documents in the list\nprint(dataset.schema(\"ground_truth.info[]\"))\n# {'task': StringField, ..., 'notes': ListField}\n\n# Determine the type of the values in the nested `notes` list field\n# Since `ground_truth.info` is not yet declared on the dataset's schema, we\n# must manually include `[]` to unwind the info lists\nprint(dataset.list_schema(\"ground_truth.info[].notes\"))\n# fo.StringField\n\n# Declare the `ground_truth.info` field\ndataset.add_sample_field(\n    \"ground_truth.info\",\n    fo.ListField,\n    subfield=fo.EmbeddedDocumentField,\n    embedded_doc_type=fo.DynamicEmbeddedDocument,\n)\n\n# Now we can inspect the nested `notes` field without unwinding\nprint(dataset.list_schema(\"ground_truth.info.notes\"))\n# fo.StringField\n</code></pre> <p>Note</p> <p>Schema aggregations are used internally by <code>get_dynamic_field_schema()</code> to impute the types of undeclared lists and embedded documents in a dataset.</p>"},{"location":"fiftyone_concepts/using_aggregations/#sum-values","title":"Sum values \u00b6","text":"<p>You can use the <code>sum()</code> aggregation to compute the sum of the (non- <code>None</code>) values of a field in a collection:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Compute average confidence of detections in the `predictions` field\nprint(\n    dataset.sum(\"predictions.detections.confidence\") /\n    dataset.count(\"predictions.detections.confidence\")\n)\n# 0.34994137249820706\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/#min-values","title":"Min values \u00b6","text":"<p>You can use the <code>min()</code> aggregation to compute the minimum of the (non- <code>None</code>) values of a field in a collection:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Compute minimum confidence of detections in the `predictions` field\nprint(dataset.min(\"predictions.detections.confidence\"))\n# 0.05003104358911514\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/#max-values","title":"Max values \u00b6","text":"<p>You can use the <code>max()</code> aggregation to compute the maximum of the (non- <code>None</code>) values of a field in a collection:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Compute maximum confidence of detections in the `predictions` field\nprint(dataset.max(\"predictions.detections.confidence\"))\n# 0.9999035596847534\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/#mean-values","title":"Mean values \u00b6","text":"<p>You can use the <code>mean()</code> aggregation to compute the arithmetic mean of the (non- <code>None</code>) values of a field in a collection:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Compute average confidence of detections in the `predictions` field\nprint(dataset.mean(\"predictions.detections.confidence\"))\n# 0.34994137249820706\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/#quantiles","title":"Quantiles \u00b6","text":"<p>You can use the <code>quantiles()</code> aggregation to compute the quantile(s) of the (non- <code>None</code>) values of a field in a collection:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Compute quantiles of the `uniqueness` field\nprint(dataset.quantiles(\"uniqueness\", [0.25, 0.5, 0.75, 0.9]))\n# [0.22027, 0.33771, 0.62554, 0.69488]\n\n# Compute quantiles of detection confidence in the `predictions` field\nquantiles = dataset.quantiles(\n    \"predictions.detections.confidence\",\n    [0.25, 0.5, 0.75, 0.9],\n)\nprint(quantiles)\n# [0.09231, 0.20251, 0.56273, 0.94354]\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/#standard-deviation","title":"Standard deviation \u00b6","text":"<p>You can use the <code>std()</code> aggregation to compute the standard deviation of the (non- <code>None</code>) values of a field in a collection:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Compute standard deviation of the confidence of detections in the\n# `predictions` field\nprint(dataset.std(\"predictions.detections.confidence\"))\n# 0.3184061813934825\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/#values","title":"Values \u00b6","text":"<p>You can use the <code>values()</code> aggregation to extract a list containing the values of a field across all samples in a collection:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Extract the `uniqueness` values for all samples\nuniqueness = dataset.values(\"uniqueness\")\nprint(len(uniqueness))  # 200\n\n# Extract the labels for all predictions\nlabels = dataset.values(\"predictions.detections.label\")\nprint(len(labels))  # 200\nprint(labels[0]) # ['bird', ..., 'bear', 'sheep']\n</code></pre> <p>Note</p> <p>Unlike other aggregations, <code>values()</code> does not automatically unwind list fields, which ensures that the returned values match the potentially-nested structure of the documents.</p> <p>You can opt-in to unwinding specific list fields using the <code>[]</code> syntax, or you can pass the optional <code>unwind=True</code> parameter to unwind all supported list fields. See Aggregating list fields for more information.</p>"},{"location":"fiftyone_concepts/using_aggregations/#advanced-usage","title":"Advanced usage \u00b6","text":""},{"location":"fiftyone_concepts/using_aggregations/#aggregating-list-fields","title":"Aggregating list fields \u00b6","text":"<p>Aggregations that operate on scalar fields can also be applied to the elements of list fields by appending <code>[]</code> to the list component of the field path.</p> <p>The example below demonstrates this capability:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\ndataset.add_samples(\n    [\\\n        fo.Sample(\\\n            filepath=\"/path/to/image1.png\",\\\n            keypoints=fo.Keypoint(points=[(0, 0), (1, 1)]),\\\n            classes=fo.Classification(\\\n                label=\"cat\", confidence=0.9, friends=[\"dog\", \"squirrel\"]\\\n            ),\\\n        ),\\\n        fo.Sample(\\\n            filepath=\"/path/to/image2.png\",\\\n            keypoints=fo.Keypoint(points=[(0, 0), (0.5, 0.5), (1, 1)]),\\\n            classes=fo.Classification(\\\n                label=\"dog\", confidence=0.8, friends=[\"rabbit\", \"squirrel\"],\\\n            ),\\\n        ),\\\n    ]\n)\n\n#\n# Count the number of keypoints in the dataset\n#\n# The `points` list attribute is declared on the `Keypoint` class, so it is\n# automatically unwound\n#\ncount = dataset.count(\"keypoints.points\")\nprint(count)\n# 5\n\n#\n# Compute the values in the `friends` field of the predictions\n#\n# The `friends` list attribute is a dynamic custom attribute, so we must\n# explicitly request that it be unwound\n#\ncounts = dataset.count_values(\"classes.friends[]\")\nprint(counts)\n# {'dog': 1, 'squirrel': 2, 'rabbit': 1}\n</code></pre> <p>Note</p> <p>FiftyOne will automatically unwind all array fields that are defined in the dataset\u2019s schema without requiring you to explicitly specify this via the <code>[]</code> syntax. This includes the following cases:</p> <p>Top-level list fields: When you write an aggregation that refers to a top-level list field of a dataset; i.e., <code>list_field</code> is automatically coerced to <code>list_field[]</code>, if necessary.</p> <p>Frame fields: When you write an aggregation that refers to a frame-level field of a video dataset; i.e., <code>frames.classification.label</code> is automatically coerced to <code>frames[].classification.label</code> if necessary.</p> <p>Embedded list fields: When you write an aggregation that refers to a list attribute that is declared on a <code>Sample</code>, <code>Frame</code>, or <code>Label</code> class, such as the <code>Classification.tags</code>, <code>Detections.detections</code>, or <code>Keypoint.points</code> attributes; i.e., <code>ground_truth.detections.label</code> is automatically coerced to <code>ground_truth.detections[].label</code>, if necessary.</p>"},{"location":"fiftyone_concepts/using_aggregations/#aggregating-expressions","title":"Aggregating expressions \u00b6","text":"<p>Aggregations also support performing more complex computations on fields via the optional <code>expr</code> argument, which is supported by all aggregations and allows you to specify a <code>ViewExpression</code> defining an arbitrary transformation of the field you\u2019re operating on prior to aggregating.</p> <p>The following examples demonstrate the power of aggregating with expressions:</p> <p>Note</p> <p>When aggregating expressions, field names may contain list fields, and such field paths are handled as explained above.</p> <p>However, there is one important exception when expressions are involved: fields paths that end in array fields are not automatically unwound, you must specify that they should be unwound by appending <code>[]</code>. This change in default behavior allows for the possibility that the <code>ViewExpression</code> you provide is intended to operate on the array as a whole.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Counts the number of predicted objects\n# Here, `predictions.detections` is treated as `predictions.detections[]`\nprint(dataset.count(\"predictions.detections\"))\n\n# Counts the number of predicted objects with confidence &gt; 0.9\n# Here, `predictions.detections` is not automatically unwound\nnum_preds = F(\"predictions.detections\").filter(F(\"confidence\") &gt; 0.9).length()\nprint(dataset.sum(num_preds))\n\n# Computes the (min, max) bounding box area in normalized coordinates\n# Here we must manually specify that we want to unwind terminal list field\n# `predictions.detections` by appending `[]`\nbbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\nprint(dataset.bounds(F(\"ground_truth.detections[]\").apply(bbox_area)))\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/#batching-aggregations","title":"Batching aggregations \u00b6","text":"<p>Rather than computing a single aggregation by invoking methods on a <code>Dataset</code> or <code>DatasetView</code> object, you can also instantiate an <code>Aggregation</code> object directly. In this case, the aggregation is not tied to any dataset or view, only to the parameters such as field name that define it.</p> <pre><code>import fiftyone as fo\n\n# will count the number of samples in a dataset\nsample_count = fo.Count()\n\n# will count the labels in a `ground_truth` detections field\ncount_values = fo.CountValues(\"ground_truth.detections.label\")\n\n# will compute a histogram of the `uniqueness` field\nhistogram_values = fo.HistogramValues(\"uniqueness\", bins=50)\n</code></pre> <p>Instantiating aggregations in this way allows you to execute multiple aggregations on a dataset or view efficiently in a batch via <code>aggregate()</code>:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nresults = dataset.aggregate([sample_count, count_values, histogram_values])\n\nprint(results[0])\n# 200\n\nprint(results[1])\n# {'bowl': 15, 'scissors': 1, 'cup': 21, ..., 'vase': 1, 'sports ball': 3}\n\nprint(results[2][0])  # counts\n# [0, 0, 0, ..., 15, 12, ..., 0, 0]\n\nprint(results[2][1])  # edges\n# [0.0, 0.02, 0.04, ..., 0.98, 1.0]\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/#transforming-data-before-aggregating","title":"Transforming data before aggregating \u00b6","text":"<p>You can use view stages like <code>map_labels()</code> in concert with aggregations to efficiently compute statistics on your datasets.</p> <p>For example, suppose you would like to compute the histogram of the labels in a dataset with certain labels grouped into a single category. You can use <code>map_labels()</code> + <code>count_values()</code> to succinctly express this:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Map `cat` and `dog` to `pet`\nlabels_map = {\"cat\": \"pet\", \"dog\": \"pet\"}\n\ncounts = (\n    dataset\n    .map_labels(\"ground_truth\", labels_map)\n    .count_values(\"ground_truth.detections.label\")\n)\n\nprint(counts)\n# {'toothbrush': 2, 'train': 5, ..., 'pet': 31, ..., 'cow': 22}\n</code></pre> <p>Or, suppose you would like to compute the average confidence of a model\u2019s predictions, ignoring any values less than 0.5. You can use <code>filter_labels()</code> + <code>sum()</code> + <code>count()</code> to succinctly express this:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\navg_conf = (\n    dataset\n    .filter_labels(\"predictions\", F(\"confidence\") &gt;= 0.5)\n    .mean(\"predictions.detections.confidence\")\n)\n\nprint(avg_conf)\n# 0.8170506501060617\n</code></pre>"},{"location":"fiftyone_concepts/using_aggregations/#aggregating-frame-labels","title":"Aggregating frame labels \u00b6","text":"<p>You can compute aggregations on the frame labels of a video dataset by adding the <code>frames</code> prefix to the relevant frame field name:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\n# Count the number of video frames\ncount = dataset.count(\"frames\")\nprint(count)\n# 1279\n\n# Compute a histogram of per-frame object labels\ncounts = dataset.count_values(\"frames.detections.detections.label\")\nprint(counts)\n# {'person': 1108, 'vehicle': 7511, 'road sign': 2726}\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/","title":"Using FiftyOne Datasets \u00b6","text":"<p>After a <code>Dataset</code> has been loaded or created, FiftyOne provides powerful functionality to inspect, search, and modify it from a <code>Dataset</code>-wide down to a <code>Sample</code> level.</p> <p>The following sections provide details of how to use various aspects of a FiftyOne <code>Dataset</code>.</p>"},{"location":"fiftyone_concepts/using_datasets/#datasets","title":"Datasets \u00b6","text":"<p>Instantiating a <code>Dataset</code> object creates a new dataset.</p> <pre><code>import fiftyone as fo\n\ndataset1 = fo.Dataset(\"my_first_dataset\")\ndataset2 = fo.Dataset(\"my_second_dataset\")\ndataset3 = fo.Dataset()  # generates a default unique name\n</code></pre> <p>Check to see what datasets exist at any time via <code>list_datasets()</code>:</p> <pre><code>print(fo.list_datasets())\n# ['my_first_dataset', 'my_second_dataset', '2020.08.04.12.36.29']\n</code></pre> <p>Load a dataset using <code>load_dataset()</code>. Dataset objects are singletons. Cool!</p> <pre><code>_dataset2 = fo.load_dataset(\"my_second_dataset\")\n_dataset2 is dataset2  # True\n</code></pre> <p>If you try to load a dataset via <code>Dataset(...)</code> or create a new dataset via <code>load_dataset()</code> you\u2019re going to have a bad time:</p> <pre><code>_dataset2 = fo.Dataset(\"my_second_dataset\")\n# Dataset 'my_second_dataset' already exists; use `fiftyone.load_dataset()`\n# to load an existing dataset\n\ndataset4 = fo.load_dataset(\"my_fourth_dataset\")\n# DoesNotExistError: Dataset 'my_fourth_dataset' not found\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#dataset-media-type","title":"Dataset media type \u00b6","text":"<p>The media type of a dataset is determined by the media type of the <code>Sample</code> objects that it contains.</p> <p>The <code>media_type</code> property of a dataset is set based on the first sample added to it:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\nprint(dataset.media_type)\n# None\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\ndataset.add_sample(sample)\n\nprint(dataset.media_type)\n# \"image\"\n</code></pre> <p>Note that datasets are homogeneous; they must contain samples of the same media type (except for grouped datasets):</p> <pre><code>sample = fo.Sample(filepath=\"/path/to/video.mp4\")\ndataset.add_sample(sample)\n# MediaTypeError: Sample media type 'video' does not match dataset media type 'image'\n</code></pre> <p>The following media types are available:</p> Media type Description <code>image</code> Datasets that containimages <code>video</code> Datasets that containvideos <code>3d</code> Datasets that contain3D scenes <code>point-cloud</code> Datasets that containpoint clouds <code>group</code> Datasets that containgrouped data slices"},{"location":"fiftyone_concepts/using_datasets/#dataset-persistence","title":"Dataset persistence \u00b6","text":"<p>By default, datasets are non-persistent. Non-persistent datasets are deleted from the database each time the database is shut down. Note that FiftyOne does not store the raw data in datasets directly (only the labels), so your source files on disk are untouched.</p> <p>To make a dataset persistent, set its <code>persistent</code> property to <code>True</code>:</p> <pre><code># Make the dataset persistent\ndataset1.persistent = True\n</code></pre> <p>Without closing your current Python shell, open a new shell and run:</p> <pre><code>import fiftyone as fo\n\n# Verify that both persistent and non-persistent datasets still exist\nprint(fo.list_datasets())\n# ['my_first_dataset', 'my_second_dataset', '2020.08.04.12.36.29']\n</code></pre> <p>All three datasets are still available, since the database connection has not been terminated.</p> <p>However, if you exit all processes with <code>fiftyone</code> imported, then open a new shell and run the command again:</p> <pre><code>import fiftyone as fo\n\n# Verify that non-persistent datasets have been deleted\nprint(fo.list_datasets())\n# ['my_first_dataset']\n</code></pre> <p>you\u2019ll see that the <code>my_second_dataset</code> and <code>2020.08.04.12.36.29</code> datasets have been deleted because they were not persistent.</p>"},{"location":"fiftyone_concepts/using_datasets/#dataset-version","title":"Dataset version \u00b6","text":"<p>The version of the <code>fiftyone</code> package for which a dataset is formatted is stored in the <code>version</code> property of the dataset.</p> <p>If you upgrade your <code>fiftyone</code> package and then load a dataset that was created with an older version of the package, it will be automatically migrated to the new package version (if necessary) the first time you load it.</p>"},{"location":"fiftyone_concepts/using_datasets/#dataset-tags","title":"Dataset tags \u00b6","text":"<p>All <code>Dataset</code> instances have a <code>tags</code> property that you can use to store an arbitrary list of string tags.</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\n# Add some tags\ndataset.tags = [\"test\", \"projectA\"]\n\n# Edit the tags\ndataset.tags.pop()\ndataset.tags.append(\"projectB\")\ndataset.save()  # must save after edits\n</code></pre> <p>Note</p> <p>You must call <code>dataset.save()</code> after updating the dataset\u2019s <code>tags</code> property in-place to save the changes to the database.</p>"},{"location":"fiftyone_concepts/using_datasets/#dataset-stats","title":"Dataset stats \u00b6","text":"<p>You can use the <code>stats()</code> method on a dataset to obtain information about the size of the dataset on disk, including its metadata in the database and optionally the size of the physical media on disk:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfo.pprint(dataset.stats(include_media=True))\n</code></pre> <pre><code>{\n    'samples_count': 200,\n    'samples_bytes': 1290762,\n    'samples_size': '1.2MB',\n    'media_bytes': 24412374,\n    'media_size': '23.3MB',\n    'total_bytes': 25703136,\n    'total_size': '24.5MB',\n}\n</code></pre> <p>You can also invoke <code>stats()</code> on a dataset view to retrieve stats for a specific subset of the dataset:</p> <pre><code>view = dataset[:10].select_fields(\"ground_truth\")\n\nfo.pprint(view.stats(include_media=True))\n</code></pre> <pre><code>{\n    'samples_count': 10,\n    'samples_bytes': 10141,\n    'samples_size': '9.9KB',\n    'media_bytes': 1726296,\n    'media_size': '1.6MB',\n    'total_bytes': 1736437,\n    'total_size': '1.7MB',\n}\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#storing-info","title":"Storing info \u00b6","text":"<p>All <code>Dataset</code> instances have an <code>info</code> property, which contains a dictionary that you can use to store any JSON-serializable information you wish about your dataset.</p> <p>Datasets can also store more specific types of ancillary information such as class lists and mask targets.</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\n# Store a class list in the dataset's info\ndataset.info = {\n    \"dataset_source\": \"https://...\",\n    \"author\": \"...\",\n}\n\n# Edit existing info\ndataset.info[\"owner\"] = \"...\"\ndataset.save()  # must save after edits\n</code></pre> <p>Note</p> <p>You must call <code>dataset.save()</code> after updating the dataset\u2019s <code>info</code> property in-place to save the changes to the database.</p>"},{"location":"fiftyone_concepts/using_datasets/#dataset-app-config","title":"Dataset App config \u00b6","text":"<p>All <code>Dataset</code> instances have an <code>app_config</code> property that contains a <code>DatasetAppConfig</code> that you can use to store dataset-specific settings that customize how the dataset is visualized in the FiftyOne App.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nsession = fo.launch_app(dataset)\n\n# View the dataset's current App config\nprint(dataset.app_config)\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#multiple-media-fields","title":"Multiple media fields \u00b6","text":"<p>You can declare multiple media fields on a dataset and configure which field is used by various components of the App by default:</p> <pre><code>import fiftyone.utils.image as foui\n\n# Generate some thumbnail images\nfoui.transform_images(\n    dataset,\n    size=(-1, 32),\n    output_field=\"thumbnail_path\",\n    output_dir=\"/tmp/thumbnails\",\n)\n\n# Configure when to use each field\ndataset.app_config.media_fields = [\"filepath\", \"thumbnail_path\"]\ndataset.app_config.grid_media_field = \"thumbnail_path\"\ndataset.save()  # must save after edits\n\nsession.refresh()\n</code></pre> <p>You can set <code>media_fallback=True</code> if you want the App to fallback to the <code>filepath</code> field if an alternate media field is missing for a particular sample in the grid and/or modal:</p> <pre><code># Fallback to `filepath` if an alternate media field is missing\ndataset.app_config.media_fallback = True\ndataset.save()\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#custom-color-scheme","title":"Custom color scheme \u00b6","text":"<p>You can store a custom color scheme on a dataset that should be used by default whenever the dataset is loaded in the App:</p> <pre><code>dataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\"\n)\n\n# Store a custom color scheme\ndataset.app_config.color_scheme = fo.ColorScheme(\n    color_pool=[\"#ff0000\", \"#00ff00\", \"#0000ff\", \"pink\", \"yellowgreen\"],\n    color_by=\"value\",\n    fields=[\\\n        {\\\n            \"path\": \"ground_truth\",\\\n            \"colorByAttribute\": \"eval\",\\\n            \"valueColors\": [\\\n                {\"value\": \"fn\", \"color\": \"#0000ff\"},  # false negatives: blue\\\n                {\"value\": \"tp\", \"color\": \"#00ff00\"},  # true positives: green\\\n            ]\\\n        },\\\n        {\\\n            \"path\": \"predictions\",\\\n            \"colorByAttribute\": \"eval\",\\\n            \"valueColors\": [\\\n                {\"value\": \"fp\", \"color\": \"#ff0000\"},  # false positives: red\\\n                {\"value\": \"tp\", \"color\": \"#00ff00\"},  # true positives: green\\\n            ]\\\n        }\\\n    ]\n)\ndataset.save()  # must save after edits\n\n# Setting `color_scheme` to None forces the dataset's default color scheme\n# to be loaded\nsession.color_scheme = None\n</code></pre> <p>Note</p> <p>Refer to the <code>ColorScheme</code> class for documentation of the available customization options.</p> <p>Note</p> <p>Did you know? You can also configure color schemes directly in the App!</p>"},{"location":"fiftyone_concepts/using_datasets/#sidebar-groups","title":"Sidebar groups \u00b6","text":"<p>You can configure the organization and default expansion state of the sidebar\u2019s field groups:</p> <pre><code># Get the default sidebar groups for the dataset\nsidebar_groups = fo.DatasetAppConfig.default_sidebar_groups(dataset)\n\n# Collapse the `metadata` section by default\nprint(sidebar_groups[2].name)  # metadata\nsidebar_groups[2].expanded = False\n\n# Modify the dataset's App config\ndataset.app_config.sidebar_groups = sidebar_groups\ndataset.save()  # must save after edits\n\nsession.refresh()\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#disable-frame-filtering","title":"Disable frame filtering \u00b6","text":"<p>Filtering by frame-level fields of video datasets in the App\u2019s grid view can be expensive when the dataset is large.</p> <p>You can disable frame filtering for a video dataset as follows:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\ndataset.app_config.disable_frame_filtering = True\ndataset.save()  # must save after edits\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>Did you know? You can also globally disable frame filtering for all video datasets via your App config.</p>"},{"location":"fiftyone_concepts/using_datasets/#resetting-a-datasets-app-config","title":"Resetting a dataset\u2019s App config \u00b6","text":"<p>You can conveniently reset any property of a dataset\u2019s App config by setting it to <code>None</code>:</p> <pre><code># Reset the dataset's color scheme\ndataset.app_config.color_scheme = None\ndataset.save()  # must save after edits\n\nprint(dataset.app_config)\n\nsession.refresh()\n</code></pre> <p>or you can reset the entire App config by setting the <code>app_config</code> property to <code>None</code>:</p> <pre><code># Reset App config\ndataset.app_config = None\nprint(dataset.app_config)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>Check out this section for more information about customizing the behavior of the App.</p>"},{"location":"fiftyone_concepts/using_datasets/#storing-class-lists","title":"Storing class lists \u00b6","text":"<p>All <code>Dataset</code> instances have <code>classes</code> and <code>default_classes</code> properties that you can use to store the lists of possible classes for your annotations/models.</p> <p>The <code>classes</code> property is a dictionary mapping field names to class lists for a single <code>Label</code> field of the dataset.</p> <p>If all <code>Label</code> fields in your dataset have the same semantics, you can store a single class list in the store a single target dictionary in the <code>default_classes</code> property of your dataset.</p> <p>You can also pass your class lists to methods such as <code>evaluate_classifications()</code>, <code>evaluate_detections()</code>, and <code>export()</code> that require knowledge of the possible classes in a dataset or field(s).</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\n# Set default classes\ndataset.default_classes = [\"cat\", \"dog\"]\n\n# Edit the default classes\ndataset.default_classes.append(\"other\")\ndataset.save()  # must save after edits\n\n# Set classes for the `ground_truth` and `predictions` fields\ndataset.classes = {\n    \"ground_truth\": [\"cat\", \"dog\"],\n    \"predictions\": [\"cat\", \"dog\", \"other\"],\n}\n\n# Edit a field's classes\ndataset.classes[\"ground_truth\"].append(\"other\")\ndataset.save()  # must save after edits\n</code></pre> <p>Note</p> <p>You must call <code>dataset.save()</code> after updating the dataset\u2019s <code>classes</code> and <code>default_classes</code> properties in-place to save the changes to the database.</p>"},{"location":"fiftyone_concepts/using_datasets/#storing-mask-targets","title":"Storing mask targets \u00b6","text":"<p>All <code>Dataset</code> instances have <code>mask_targets</code> and <code>default_mask_targets</code> properties that you can use to store label strings for the pixel values of <code>Segmentation</code> field masks.</p> <p>The <code>mask_targets</code> property is a dictionary mapping field names to target dicts, each of which is a dictionary defining the mapping between pixel values (2D masks) or RGB hex strings (3D masks) and label strings for the <code>Segmentation</code> masks in the specified field of the dataset.</p> <p>If all <code>Segmentation</code> fields in your dataset have the same semantics, you can store a single target dictionary in the <code>default_mask_targets</code> property of your dataset.</p> <p>When you load datasets with <code>Segmentation</code> fields in the App that have corresponding mask targets, the label strings will appear in the App\u2019s tooltip when you hover over pixels.</p> <p>You can also pass your mask targets to methods such as <code>evaluate_segmentations()</code> and <code>export()</code> that require knowledge of the mask targets for a dataset or field(s).</p> <p>If you are working with 2D segmentation masks, specify target keys as integers:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\n# Set default mask targets\ndataset.default_mask_targets = {1: \"cat\", 2: \"dog\"}\n\n# Edit the default mask targets\ndataset.default_mask_targets[255] = \"other\"\ndataset.save()  # must save after edits\n\n# Set mask targets for the `ground_truth` and `predictions` fields\ndataset.mask_targets = {\n    \"ground_truth\": {1: \"cat\", 2: \"dog\"},\n    \"predictions\": {1: \"cat\", 2: \"dog\", 255: \"other\"},\n}\n\n# Edit an existing mask target\ndataset.mask_targets[\"ground_truth\"][255] = \"other\"\ndataset.save()  # must save after edits\n</code></pre> <p>If you are working with RGB segmentation masks, specify target keys as RGB hex strings:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\n# Set default mask targets\ndataset.default_mask_targets = {\"#499CEF\": \"cat\", \"#6D04FF\": \"dog\"}\n\n# Edit the default mask targets\ndataset.default_mask_targets[\"#FF6D04\"] = \"person\"\ndataset.save()  # must save after edits\n\n# Set mask targets for the `ground_truth` and `predictions` fields\ndataset.mask_targets = {\n    \"ground_truth\": {\"#499CEF\": \"cat\", \"#6D04FF\": \"dog\"},\n    \"predictions\": {\n        \"#499CEF\": \"cat\", \"#6D04FF\": \"dog\", \"#FF6D04\": \"person\"\n    },\n}\n\n# Edit an existing mask target\ndataset.mask_targets[\"ground_truth\"][\"#FF6D04\"] = \"person\"\ndataset.save()  # must save after edits\n</code></pre> <p>Note</p> <p>You must call <code>dataset.save()</code> after updating the dataset\u2019s <code>mask_targets</code> and <code>default_mask_targets</code> properties in-place to save the changes to the database.</p>"},{"location":"fiftyone_concepts/using_datasets/#storing-keypoint-skeletons","title":"Storing keypoint skeletons \u00b6","text":"<p>All <code>Dataset</code> instances have <code>skeletons</code> and <code>default_skeleton</code> properties that you can use to store keypoint skeletons for <code>Keypoint</code> field(s) of a dataset.</p> <p>The <code>skeletons</code> property is a dictionary mapping field names to <code>KeypointSkeleton</code> instances, each of which defines the keypoint label strings and edge connectivity for the <code>Keypoint</code> instances in the specified field of the dataset.</p> <p>If all <code>Keypoint</code> fields in your dataset have the same semantics, you can store a single <code>KeypointSkeleton</code> in the <code>default_skeleton</code> property of your dataset.</p> <p>When you load datasets with <code>Keypoint</code> fields in the App that have corresponding skeletons, the skeletons will automatically be rendered and label strings will appear in the App\u2019s tooltip when you hover over the keypoints.</p> <p>Keypoint skeletons can be associated with <code>Keypoint</code> or <code>Keypoints</code> fields whose <code>points</code> attributes all contain a fixed number of semantically ordered points.</p> <p>The <code>edges</code> argument contains lists of integer indexes that define the connectivity of the points in the skeleton, and the optional <code>labels</code> argument defines the label strings for each node in the skeleton.</p> <p>For example, the skeleton below is defined by edges between the following nodes:</p> <pre><code>left hand &lt;-&gt; left shoulder &lt;-&gt; right shoulder &lt;-&gt; right hand\nleft eye &lt;-&gt; right eye &lt;-&gt; mouth\n</code></pre> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\n# Set keypoint skeleton for the `ground_truth` field\ndataset.skeletons = {\n    \"ground_truth\": fo.KeypointSkeleton(\n        labels=[\\\n            \"left hand\" \"left shoulder\", \"right shoulder\", \"right hand\",\\\n            \"left eye\", \"right eye\", \"mouth\",\\\n        ],\n        edges=[[0, 1, 2, 3], [4, 5, 6]],\n    )\n}\n\n# Edit an existing skeleton\ndataset.skeletons[\"ground_truth\"].labels[-1] = \"lips\"\ndataset.save()  # must save after edits\n</code></pre> <p>Note</p> <p>When using keypoint skeletons, each <code>Keypoint</code> instance\u2019s <code>points</code> list must always respect the indexing defined by the field\u2019s <code>KeypointSkeleton</code>.</p> <p>If a particular keypoint is occluded or missing for an object, use <code>[float(\"nan\"), float(\"nan\")]</code> in its <code>points</code> list.</p> <p>Note</p> <p>You must call <code>dataset.save()</code> after updating the dataset\u2019s <code>skeletons</code> and <code>default_skeleton</code> properties in-place to save the changes to the database.</p>"},{"location":"fiftyone_concepts/using_datasets/#deleting-a-dataset","title":"Deleting a dataset \u00b6","text":"<p>Delete a dataset explicitly via <code>Dataset.delete()</code>. Once a dataset is deleted, any existing reference in memory will be in a volatile state. <code>Dataset.name</code> and <code>Dataset.deleted</code> will still be valid attributes, but calling any other attribute or method will raise a <code>DoesNotExistError</code>.</p> <pre><code>dataset = fo.load_dataset(\"my_first_dataset\")\ndataset.delete()\n\nprint(fo.list_datasets())\n# []\n\nprint(dataset.name)\n# my_first_dataset\n\nprint(dataset.deleted)\n# True\n\nprint(dataset.persistent)\n# DoesNotExistError: Dataset 'my_first_dataset' is deleted\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#samples","title":"Samples \u00b6","text":"<p>An individual <code>Sample</code> is always initialized with a <code>filepath</code> to the corresponding data on disk.</p> <pre><code># An image sample\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\n# A video sample\nanother_sample = fo.Sample(filepath=\"/path/to/video.mp4\")\n</code></pre> <p>Note</p> <p>Creating a new <code>Sample</code> does not load the source data into memory. Source data is read only as needed by the App.</p>"},{"location":"fiftyone_concepts/using_datasets/#adding-samples-to-a-dataset","title":"Adding samples to a dataset \u00b6","text":"<p>A <code>Sample</code> can easily be added to an existing <code>Dataset</code>:</p> <pre><code>dataset = fo.Dataset(\"example_dataset\")\ndataset.add_sample(sample)\n</code></pre> <p>When a sample is added to a dataset, the relevant attributes of the <code>Sample</code> are automatically updated:</p> <pre><code>print(sample.in_dataset)\n# True\n\nprint(sample.dataset_name)\n# example_dataset\n</code></pre> <p>Every sample in a dataset is given a unique ID when it is added:</p> <pre><code>print(sample.id)\n# 5ee0ebd72ceafe13e7741c42\n</code></pre> <p>Multiple samples can be efficiently added to a dataset in batches:</p> <pre><code>print(len(dataset))\n# 1\n\ndataset.add_samples(\n    [\\\n        fo.Sample(filepath=\"/path/to/image1.jpg\"),\\\n        fo.Sample(filepath=\"/path/to/image2.jpg\"),\\\n        fo.Sample(filepath=\"/path/to/image3.jpg\"),\\\n    ]\n)\n\nprint(len(dataset))\n# 4\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#accessing-samples-in-a-dataset","title":"Accessing samples in a dataset \u00b6","text":"<p>FiftyOne provides multiple ways to access a <code>Sample</code> in a <code>Dataset</code>.</p> <p>You can iterate over the samples in a dataset:</p> <pre><code>for sample in dataset:\n    print(sample)\n</code></pre> <p>Use <code>first()</code> and <code>last()</code> to retrieve the first and last samples in a dataset, respectively:</p> <pre><code>first_sample = dataset.first()\nlast_sample = dataset.last()\n</code></pre> <p>Samples can be accessed directly from datasets by their IDs or their filepaths. <code>Sample</code> objects are singletons, so the same <code>Sample</code> instance is returned whenever accessing the sample from the <code>Dataset</code>:</p> <pre><code>same_sample = dataset[sample.id]\nprint(same_sample is sample)\n# True\n\nalso_same_sample = dataset[sample.filepath]\nprint(also_same_sample is sample)\n# True\n</code></pre> <p>You can use dataset views to perform more sophisticated operations on samples like searching, filtering, sorting, and slicing.</p> <p>Note</p> <p>Accessing a sample by its integer index in a <code>Dataset</code> is not allowed. The best practice is to lookup individual samples by ID or filepath, or use array slicing to extract a range of samples, and iterate over samples in a view.</p> <pre><code>dataset[0]\n# KeyError: Accessing dataset samples by numeric index is not supported.\n# Use sample IDs, filepaths, slices, boolean arrays, or a boolean ViewExpression instead\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#deleting-samples-from-a-dataset","title":"Deleting samples from a dataset \u00b6","text":"<p>Samples can be removed from a <code>Dataset</code> through their ID, either one at a time or in batches via <code>delete_samples()</code>:</p> <pre><code>dataset.delete_samples(sample_id)\n\n# equivalent to above\ndel dataset[sample_id]\n\ndataset.delete_samples([sample_id1, sample_id2])\n</code></pre> <p>Samples can also be removed from a <code>Dataset</code> by passing <code>Sample</code> instance(s) or <code>DatasetView</code> instances:</p> <pre><code># Remove a random sample\nsample = dataset.take(1).first()\ndataset.delete_samples(sample)\n\n# Remove 10 random samples\nview = dataset.take(10)\ndataset.delete_samples(view)\n</code></pre> <p>If a <code>Sample</code> object in memory is deleted from a dataset, it will revert to a <code>Sample</code> that has not been added to a <code>Dataset</code>:</p> <pre><code>print(sample.in_dataset)\n# False\n\nprint(sample.dataset_name)\n# None\n\nprint(sample.id)\n# None\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#fields","title":"Fields \u00b6","text":"<p>A <code>Field</code> is an attribute of a <code>Sample</code> that stores information about the sample.</p> <p>Fields can be dynamically created, modified, and deleted from samples on a per-sample basis. When a new <code>Field</code> is assigned to a <code>Sample</code> in a <code>Dataset</code>, it is automatically added to the dataset\u2019s schema and thus accessible on all other samples in the dataset.</p> <p>If a field exists on a dataset but has not been set on a particular sample, its value will be <code>None</code>.</p>"},{"location":"fiftyone_concepts/using_datasets/#default-sample-fields","title":"Default sample fields \u00b6","text":"<p>By default, all <code>Sample</code> instances have the following fields:</p> Field Type Default Description <code>id</code> string <code>None</code> The ID of the sample in its parent dataset, whichis generated automatically when the sample isadded to a dataset, or <code>None</code> if the sample doesnot belong to a dataset <code>filepath</code> string REQUIRED The path to the source data on disk. Must beprovided at sample creation time <code>media_type</code> string N/A The media type of the sample. Computedautomatically from the provided <code>filepath</code> <code>tags</code> list <code>[]</code> A list of string tags for the sample <code>metadata</code> <code>Metadata</code> <code>None</code> Type-specific metadata about the source data <code>created_at</code> datetime <code>None</code> The datetime that the sample was added to itsparent dataset, which is generated automatically,or <code>None</code> if the sample does not belong to adataset <code>last_modified_at</code> datetime <code>None</code> The datetime that the sample was last modified,which is updated automatically, or <code>None</code> if thesample does not belong to a dataset <p>Note</p> <p>The <code>created_at</code> and <code>last_modified_at</code> fields are read-only and are automatically populated/updated when you add samples to datasets and modify them, respectively.</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n}&gt;\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#accessing-fields-of-a-sample","title":"Accessing fields of a sample \u00b6","text":"<p>The names of available fields can be checked on any individual <code>Sample</code>:</p> <pre><code>sample.field_names\n# ('id', 'filepath', 'tags', 'metadata', 'created_at', 'last_modified_at')\n</code></pre> <p>The value of a <code>Field</code> for a given <code>Sample</code> can be accessed either by either attribute or item access:</p> <pre><code>sample.filepath\nsample[\"filepath\"]  # equivalent\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#field-schemas","title":"Field schemas \u00b6","text":"<p>You can use <code>get_field_schema()</code> to retrieve detailed information about the schema of the samples in a dataset:</p> <pre><code>dataset = fo.Dataset(\"a_dataset\")\ndataset.add_sample(sample)\n\ndataset.get_field_schema()\n</code></pre> <pre><code>OrderedDict([\\\n    ('id', &lt;fiftyone.core.fields.ObjectIdField at 0x7fbaa862b358&gt;),\\\n    ('filepath', &lt;fiftyone.core.fields.StringField at 0x11c77ae10&gt;),\\\n    ('tags', &lt;fiftyone.core.fields.ListField at 0x11c790828&gt;),\\\n    ('metadata', &lt;fiftyone.core.fields.EmbeddedDocumentField at 0x11c7907b8&gt;),\\\n    ('created_at', &lt;fiftyone.core.fields.DateTimeField at 0x7fea48361af0&gt;),\\\n    ('last_modified_at', &lt;fiftyone.core.fields.DateTimeField at 0x7fea48361b20&gt;)]),\n])\n</code></pre> <p>You can also view helpful information about a dataset, including its schema, by printing it:</p> <pre><code>print(dataset)\n</code></pre> <pre><code>Name:           a_dataset\nMedia type:     image\nNum samples:    1\nPersistent:     False\nTags:           []\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n</code></pre> <p>Note</p> <p>Did you know? You can store metadata such as descriptions on your dataset\u2019s fields!</p>"},{"location":"fiftyone_concepts/using_datasets/#adding-fields-to-a-sample","title":"Adding fields to a sample \u00b6","text":"<p>New fields can be added to a <code>Sample</code> using item assignment:</p> <pre><code>sample[\"integer_field\"] = 51\nsample.save()\n</code></pre> <p>If the <code>Sample</code> belongs to a <code>Dataset</code>, the dataset\u2019s schema will automatically be updated to reflect the new field:</p> <pre><code>print(dataset)\n</code></pre> <pre><code>Name:           a_dataset\nMedia type:     image\nNum samples:    1\nPersistent:     False\nTags:           []\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    integer_field:    fiftyone.core.fields.IntField\n</code></pre> <p>A <code>Field</code> can be any primitive type, such as <code>bool</code>, <code>int</code>, <code>float</code>, <code>str</code>, <code>date</code>, <code>datetime</code>, <code>list</code>, <code>dict</code>, or more complex data structures like label types:</p> <pre><code>sample[\"animal\"] = fo.Classification(label=\"alligator\")\nsample.save()\n</code></pre> <p>Whenever a new field is added to a sample in a dataset, the field is available on every other sample in the dataset with the value <code>None</code>.</p> <p>Fields must have the same type (or <code>None</code>) across all samples in the dataset. Setting a field to an inappropriate type raises an error:</p> <pre><code>sample2.integer_field = \"a string\"\nsample2.save()\n# ValidationError: a string could not be converted to int\n</code></pre> <p>Note</p> <p>You must call <code>sample.save()</code> in order to persist changes to the database when editing samples that are in datasets.</p>"},{"location":"fiftyone_concepts/using_datasets/#adding-fields-to-a-dataset","title":"Adding fields to a dataset \u00b6","text":"<p>You can also use <code>add_sample_field()</code> to declare new sample fields directly on datasets without explicitly populating any values on its samples:</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(\n    filepath=\"image.jpg\",\n    ground_truth=fo.Classification(label=\"cat\"),\n)\n\ndataset = fo.Dataset()\ndataset.add_sample(sample)\n\n# Declare new primitive fields\ndataset.add_sample_field(\"scene_id\", fo.StringField)\ndataset.add_sample_field(\"quality\", fo.FloatField)\n\n# Declare untyped list fields\ndataset.add_sample_field(\"more_tags\", fo.ListField)\ndataset.add_sample_field(\"info\", fo.ListField)\n\n# Declare a typed list field\ndataset.add_sample_field(\"also_tags\", fo.ListField, subfield=fo.StringField)\n\n# Declare a new Label field\ndataset.add_sample_field(\n    \"predictions\",\n    fo.EmbeddedDocumentField,\n    embedded_doc_type=fo.Classification,\n)\n\nprint(dataset.get_field_schema())\n</code></pre> <pre><code>{\n    'id': &lt;fiftyone.core.fields.ObjectIdField object at 0x7f9280803910&gt;,\n    'filepath': &lt;fiftyone.core.fields.StringField object at 0x7f92d273e0d0&gt;,\n    'tags': &lt;fiftyone.core.fields.ListField object at 0x7f92d2654f70&gt;,\n    'metadata': &lt;fiftyone.core.fields.EmbeddedDocumentField object at 0x7f9280803d90&gt;,\n    'created_at': &lt;fiftyone.core.fields.DateTimeField object at 0x7fea48361af0&gt;,\n    'last_modified_at': &lt;fiftyone.core.fields.DateTimeField object at 0x7fea48361b20&gt;,\n    'ground_truth': &lt;fiftyone.core.fields.EmbeddedDocumentField object at 0x7f92d2605190&gt;,\n    'scene_id': &lt;fiftyone.core.fields.StringField object at 0x7f9280803490&gt;,\n    'quality': &lt;fiftyone.core.fields.FloatField object at 0x7f92d2605bb0&gt;,\n    'more_tags': &lt;fiftyone.core.fields.ListField object at 0x7f92d08e4550&gt;,\n    'info': &lt;fiftyone.core.fields.ListField object at 0x7f92d264f9a0&gt;,\n    'also_tags': &lt;fiftyone.core.fields.ListField object at 0x7f92d264ff70&gt;,\n    'predictions': &lt;fiftyone.core.fields.EmbeddedDocumentField object at 0x7f92d2605640&gt;,\n}\n</code></pre> <p>Whenever a new field is added to a dataset, the field is immediately available on all samples in the dataset with the value <code>None</code>:</p> <pre><code>print(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': '642d8848f291652133df8d3a',\n    'media_type': 'image',\n    'filepath': '/Users/Brian/dev/fiftyone/image.jpg',\n    'tags': [],\n    'metadata': None,\n    'created_at': datetime.datetime(2024, 7, 22, 5, 0, 25, 372399),\n    'last_modified_at': datetime.datetime(2024, 7, 22, 5, 0, 25, 372399),\n    'ground_truth': &lt;Classification: {\n        'id': '642d8848f291652133df8d38',\n        'tags': [],\n        'label': 'cat',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n    'scene_id': None,\n    'quality': None,\n    'more_tags': None,\n    'info': None,\n    'also_tags': None,\n    'predictions': None,\n}&gt;\n</code></pre> <p>You can also declare nested fields on existing embedded documents using dot notation:</p> <pre><code># Declare a new attribute on a `Classification` field\ndataset.add_sample_field(\"predictions.breed\", fo.StringField)\n</code></pre> <p>Note</p> <p>See this section for more options for dynamically expanding the schema of nested lists and embedded documents.</p> <p>You can use <code>get_field()</code> to retrieve a <code>Field</code> instance by its name or <code>embedded.field.name</code>. And, if the field contains an embedded document, you can call <code>get_field_schema()</code> to recursively inspect its nested fields:</p> <pre><code>field = dataset.get_field(\"predictions\")\nprint(field.document_type)\n# &lt;class 'fiftyone.core.labels.Classification'&gt;\n\nprint(set(field.get_field_schema().keys()))\n# {'logits', 'confidence', 'breed', 'tags', 'label', 'id'}\n\n# Directly retrieve a nested field\nfield = dataset.get_field(\"predictions.breed\")\nprint(type(field))\n# &lt;class 'fiftyone.core.fields.StringField'&gt;\n</code></pre> <p>If your dataset contains a <code>ListField</code> with no value type declared, you can add the type later by appending <code>[]</code> to the field path:</p> <pre><code>field = dataset.get_field(\"more_tags\")\nprint(field.field)  # None\n\n# Declare the subfield types of an existing untyped list field\ndataset.add_sample_field(\"more_tags[]\", fo.StringField)\n\nfield = dataset.get_field(\"more_tags\")\nprint(field.field)  # StringField\n\n# List fields can also contain embedded documents\ndataset.add_sample_field(\n    \"info[]\",\n    fo.EmbeddedDocumentField,\n    embedded_doc_type=fo.DynamicEmbeddedDocument,\n)\n\nfield = dataset.get_field(\"info\")\nprint(field.field)  # EmbeddedDocumentField\nprint(field.field.document_type)  # DynamicEmbeddedDocument\n</code></pre> <p>Note</p> <p>Declaring the value type of list fields is required if you want to filter by the list\u2019s values in the App.</p>"},{"location":"fiftyone_concepts/using_datasets/#editing-sample-fields","title":"Editing sample fields \u00b6","text":"<p>You can make any edits you wish to the fields of an existing <code>Sample</code>:</p> <pre><code>sample = fo.Sample(\n    filepath=\"/path/to/image.jpg\",\n    ground_truth=fo.Detections(\n        detections=[\\\n            fo.Detection(label=\"CAT\", bounding_box=[0.1, 0.1, 0.4, 0.4]),\\\n            fo.Detection(label=\"dog\", bounding_box=[0.5, 0.5, 0.4, 0.4]),\\\n        ]\n    )\n)\n\ndetections = sample.ground_truth.detections\n\n# Edit an existing detection\ndetections[0].label = \"cat\"\n\n# Add a new detection\nnew_detection = fo.Detection(label=\"animals\", bounding_box=[0, 0, 1, 1])\ndetections.append(new_detection)\n\nprint(sample)\n\nsample.save()  # if the sample is in a dataset\n</code></pre> <p>Note</p> <p>You must call <code>sample.save()</code> in order to persist changes to the database when editing samples that are in datasets.</p> <p>A common workflow is to iterate over a dataset or view and edit each sample:</p> <pre><code>for sample in dataset:\n    sample[\"new_field\"] = ...\n    sample.save()\n</code></pre> <p>The <code>iter_samples()</code> method is an equivalent way to iterate over a dataset that provides a <code>progress=True</code> option that prints a progress bar tracking the status of the iteration:</p> <pre><code># Prints a progress bar tracking the status of the iteration\nfor sample in dataset.iter_samples(progress=True):\n    sample[\"new_field\"] = ...\n    sample.save()\n</code></pre> <p>The <code>iter_samples()</code> method also provides an <code>autosave=True</code> option that causes all changes to samples emitted by the iterator to be automatically saved using efficient batch updates:</p> <pre><code># Automatically saves sample edits in efficient batches\nfor sample in dataset.iter_samples(autosave=True):\n    sample[\"new_field\"] = ...\n</code></pre> <p>Using <code>autosave=True</code> can significantly improve performance when editing large datasets. See this section for more information on batch update patterns.</p>"},{"location":"fiftyone_concepts/using_datasets/#removing-fields-from-a-sample","title":"Removing fields from a sample \u00b6","text":"<p>A field can be deleted from a <code>Sample</code> using <code>del</code>:</p> <pre><code>del sample[\"integer_field\"]\n</code></pre> <p>If the <code>Sample</code> is not yet in a dataset, deleting a field will remove it from the sample. If the <code>Sample</code> is in a dataset, the field\u2019s value will be <code>None</code>.</p> <p>Fields can also be deleted at the <code>Dataset</code> level, in which case they are removed from every <code>Sample</code> in the dataset:</p> <pre><code>dataset.delete_sample_field(\"integer_field\")\n\nsample.integer_field\n# AttributeError: Sample has no field 'integer_field'\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#storing-field-metadata","title":"Storing field metadata \u00b6","text":"<p>You can store metadata such as descriptions and other info on the fields of your dataset.</p> <p>One approach is to manually declare the field with <code>add_sample_field()</code> with the appropriate metadata provided:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\ndataset.add_sample_field(\n    \"int_field\", fo.IntField, description=\"An integer field\"\n)\n\nfield = dataset.get_field(\"int_field\")\nprint(field.description)  # An integer field\n</code></pre> <p>You can also use <code>get_field()</code> to retrieve a field and update it\u2019s metadata at any time:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\ndataset.add_dynamic_sample_fields()\n\nfield = dataset.get_field(\"ground_truth\")\nfield.description = \"Ground truth annotations\"\nfield.info = {\"url\": \"https://fiftyone.ai\"}\nfield.save()  # must save after edits\n\nfield = dataset.get_field(\"ground_truth.detections.area\")\nfield.description = \"Area of the box, in pixels^2\"\nfield.info = {\"url\": \"https://fiftyone.ai\"}\nfield.save()  # must save after edits\n\ndataset.reload()\n\nfield = dataset.get_field(\"ground_truth\")\nprint(field.description)  # Ground truth annotations\nprint(field.info)  # {'url': 'https://fiftyone.ai'}\n\nfield = dataset.get_field(\"ground_truth.detections.area\")\nprint(field.description)  # Area of the box, in pixels^2\nprint(field.info)  # {'url': 'https://fiftyone.ai'}\n</code></pre> <p>Note</p> <p>You must call <code>field.save()</code> after updating a fields\u2019s <code>description</code> and <code>info</code> attributes in-place to save the changes to the database.</p> <p>Note</p> <p>Did you know? You can view field metadata directly in the App by hovering over fields or attributes in the sidebar!</p>"},{"location":"fiftyone_concepts/using_datasets/#read-only-fields","title":"Read-only fields \u00b6","text":"<p>Certain default sample fields like <code>created_at</code> and <code>last_modified_at</code> are read-only and thus cannot be manually edited:</p> <pre><code>from datetime import datetime\nimport fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.jpg\")\n\ndataset = fo.Dataset()\ndataset.add_sample(sample)\n\nsample.created_at = datetime.utcnow()\n# ValueError: Cannot edit read-only field 'created_at'\n\nsample.last_modified_at = datetime.utcnow()\n# ValueError: Cannot edit read-only field 'last_modified_at'\n</code></pre> <p>You can also manually mark additional fields or embedded fields as read-only at any time:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Declare a new read-only field\ndataset.add_sample_field(\"uuid\", fo.StringField, read_only=True)\n\n# Mark 'filepath' as read-only\nfield = dataset.get_field(\"filepath\")\nfield.read_only = True\nfield.save()  # must save after edits\n\n# Mark a nested field as read-only\nfield = dataset.get_field(\"ground_truth.detections.label\")\nfield.read_only = True\nfield.save()  # must save after edits\n\nsample = dataset.first()\n\nsample.filepath = \"no.jpg\"\n# ValueError: Cannot edit read-only field 'filepath'\n\nsample.ground_truth.detections[0].label = \"no\"\nsample.save()\n# ValueError: Cannot edit read-only field 'ground_truth.detections.label'\n</code></pre> <p>Note</p> <p>You must call <code>field.save()</code> after updating a fields\u2019s <code>read_only</code> attributes in-place to save the changes to the database.</p> <p>Note that read-only fields do not interfere with the ability to add/delete samples from datasets:</p> <pre><code>sample = fo.Sample(filepath=\"/path/to/image.jpg\", uuid=\"1234\")\ndataset.add_sample(sample)\n\ndataset.delete_samples(sample)\n</code></pre> <p>Any fields that you\u2019ve manually marked as read-only may be reverted to editable at any time:</p> <pre><code>sample = dataset.first()\n\n# Revert 'filepath' to editable\nfield = dataset.get_field(\"filepath\")\nfield.read_only = False\nfield.save()  # must save after edits\n\n# Revert nested field to editable\nfield = dataset.get_field(\"ground_truth.detections.label\")\nfield.read_only = False\nfield.save()  # must save after edits\n\nsample.filepath = \"yes.jpg\"\nsample.ground_truth.detections[0].label = \"yes\"\nsample.save()\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#summary-fields","title":"Summary fields \u00b6","text":"<p>Summary fields allow you to efficiently perform queries on large datasets where directly querying the underlying field is prohibitively slow due to the number of objects/frames in the field.</p> <p>For example, suppose you\u2019re working on a video dataset with frame-level objects, and you\u2019re interested in finding videos that contain specific classes of interest, eg <code>person</code>, in at least one frame:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\ndataset.set_field(\"frames.detections.detections.confidence\", F.rand()).save()\n\nsession = fo.launch_app(dataset)\n</code></pre> <p></p> <p>One approach is to directly query the frame-level field ( <code>frames.detections</code> in this case) in the App\u2019s sidebar. However, when the dataset is large, such queries are inefficient, as they cannot unlock query performance and thus require full collection scans over all frames to retrieve the relevant samples.</p> <p>A more efficient approach is to first use <code>create_summary_field()</code> to summarize the relevant input field path(s):</p> <pre><code># Generate a summary field for object labels\nfield_name = dataset.create_summary_field(\"frames.detections.detections.label\")\n\n# The name of the summary field that was created\nprint(field_name)\n# 'frames_detections_label'\n\n# Generate a summary field for [min, max] confidences\ndataset.create_summary_field(\"frames.detections.detections.confidence\")\n</code></pre> <p>Summary fields can be generated for sample-level and frame-level fields, and the input fields can be either categorical or numeric:</p> <p>As the above examples illustrate, summary fields allow you to encode various types of information at the sample-level that you can directly query to find samples that contain specific values.</p> <p>Moreover, summary fields are indexed by default and the App can natively leverage these indexes to provide performant filtering:</p> <p></p> <p>Note</p> <p>Summary fields are automatically added to a <code>summaries</code> sidebar group in the App for easy access and organization.</p> <p>They are also read-only by default, as they are implicitly derived from the contents of their source field and are not intended to be directly modified.</p> <p>You can use <code>list_summary_fields()</code> to list the names of the summary fields on your dataset:</p> <pre><code>print(dataset.list_summary_fields())\n# ['frames_detections_label', 'frames_detections_confidence', ...]\n</code></pre> <p>Since a summary field is derived from the contents of another field, it must be updated whenever there have been modifications to its source field. You can use <code>check_summary_fields()</code> to check for summary fields that may need to be updated:</p> <pre><code># Newly created summary fields don't needed updating\nprint(dataset.check_summary_fields())\n# []\n\n# Modify the dataset\nlabel_upper = F(\"label\").upper()\ndataset.set_field(\"frames.detections.detections.label\", label_upper).save()\n\n# Summary fields now (may) need updating\nprint(dataset.check_summary_fields())\n# ['frames_detections_label', 'frames_detections_confidence', ...]\n</code></pre> <p>Note</p> <p>Note that inclusion in <code>check_summary_fields()</code> is only a heuristic, as any sample modifications may not have affected the summary\u2019s source field.</p> <p>Use <code>update_summary_field()</code> to regenerate a summary field based on the current values of its source field:</p> <pre><code>dataset.update_summary_field(\"frames_detections_label\")\n</code></pre> <p>Finally, use <code>delete_summary_field()</code> or <code>delete_summary_fields()</code> to delete existing summary field(s) that you no longer need:</p> <pre><code>dataset.delete_summary_field(\"frames_detections_label\")\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#media-type","title":"Media type \u00b6","text":"<p>When a <code>Sample</code> is created, its media type is inferred from the <code>filepath</code> to the source media and available via the <code>media_type</code> attribute of the sample, which is read-only.</p> <p>Media type is inferred from the MIME type of the file on disk, as per the table below:</p> MIME type/extension <code>media_type</code> Description <code>image/*</code> <code>image</code> Image sample <code>video/*</code> <code>video</code> Video sample <code>*.fo3d</code> <code>3d</code> 3D sample <code>*.pcd</code> <code>point-cloud</code> Point cloud sample other <code>-</code> Generic sample <p>Note</p> <p>The <code>filepath</code> of a sample can be changed after the sample is created, but the new filepath must have the same media type. In other words, <code>media_type</code> is immutable.</p>"},{"location":"fiftyone_concepts/using_datasets/#tags","title":"Tags \u00b6","text":"<p>All <code>Sample</code> instances have a <code>tags</code> field, which is a string list. By default, this list is empty, but you can use it to store information like dataset splits or application-specific issues like low quality images:</p> <pre><code>dataset = fo.Dataset(\"tagged_dataset\")\n\ndataset.add_samples(\n    [\\\n        fo.Sample(filepath=\"/path/to/image1.png\", tags=[\"train\"]),\\\n        fo.Sample(filepath=\"/path/to/image2.png\", tags=[\"test\", \"low_quality\"]),\\\n    ]\n)\n\nprint(dataset.distinct(\"tags\"))\n# [\"test\", \"low_quality\", \"train\"]\n</code></pre> <p>Note</p> <p>Did you know? You can add, edit, and filter by sample tags directly in the App.</p> <p>The <code>tags</code> field can be used like a standard Python list:</p> <pre><code>sample = dataset.first()\nsample.tags.append(\"new_tag\")\nsample.save()\n</code></pre> <p>Note</p> <p>You must call <code>sample.save()</code> in order to persist changes to the database when editing samples that are in datasets.</p> <p>Datasets and views provide helpful methods such as <code>count_sample_tags()</code>, <code>tag_samples()</code>, <code>untag_samples()</code>, and <code>match_tags()</code> that you can use to perform batch queries and edits to sample tags:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\").clone()\nprint(dataset.count_sample_tags())  # {'validation': 200}\n\n# Tag samples in a view\ntest_view = dataset.limit(100)\ntest_view.untag_samples(\"validation\")\ntest_view.tag_samples(\"test\")\nprint(dataset.count_sample_tags())  # {'validation': 100, 'test': 100}\n\n# Create a view containing samples with a specific tag\nvalidation_view = dataset.match_tags(\"validation\")\nprint(len(validation_view))  # 100\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#metadata","title":"Metadata \u00b6","text":"<p>All <code>Sample</code> instances have a <code>metadata</code> field, which can optionally be populated with a <code>Metadata</code> instance that stores data type-specific metadata about the raw data in the sample. The FiftyOne App and the FiftyOne Brain will use this provided metadata in some workflows when it is available.</p>"},{"location":"fiftyone_concepts/using_datasets/#dates-and-datetimes","title":"Dates and datetimes \u00b6","text":"<p>You can store date information in FiftyOne datasets by populating fields with <code>date</code> or <code>datetime</code> values:</p> <pre><code>from datetime import date, datetime\nimport fiftyone as fo\n\ndataset = fo.Dataset()\ndataset.add_samples(\n    [\\\n        fo.Sample(\\\n            filepath=\"image1.png\",\\\n            acquisition_time=datetime(2021, 8, 24, 21, 18, 7),\\\n            acquisition_date=date(2021, 8, 24),\\\n        ),\\\n        fo.Sample(\\\n            filepath=\"image2.png\",\\\n            acquisition_time=datetime.utcnow(),\\\n            acquisition_date=date.today(),\\\n        ),\\\n    ]\n)\n\nprint(dataset)\nprint(dataset.head())\n</code></pre> <p>Note</p> <p>Did you know? You can create dataset views with date-based queries!</p> <p>Internally, FiftyOne stores all dates as UTC timestamps, but you can provide any valid <code>datetime</code> object when setting a <code>DateTimeField</code> of a sample, including timezone-aware datetimes, which are internally converted to UTC format for safekeeping.</p> <pre><code># A datetime in your local timezone\nnow = datetime.utcnow().astimezone()\n\nsample = fo.Sample(filepath=\"image.png\", acquisition_time=now)\n\ndataset = fo.Dataset()\ndataset.add_sample(sample)\n\n# Samples are singletons, so we reload so `sample` will contain values as\n# loaded from the database\ndataset.reload()\n\nsample.acquisition_time.tzinfo  # None\n</code></pre> <p>By default, when you access a datetime field of a sample in a dataset, it is retrieved as a naive <code>datetime</code> instance expressed in UTC format.</p> <p>However, if you prefer, you can configure FiftyOne to load datetime fields as timezone-aware <code>datetime</code> instances in a timezone of your choice.</p> <p>Warning</p> <p>FiftyOne assumes that all <code>datetime</code> instances with no explicit timezone are stored in UTC format.</p> <p>Therefore, never use <code>datetime.datetime.now()</code> when populating a datetime field of a FiftyOne dataset! Instead, use <code>datetime.datetime.utcnow()</code>.</p>"},{"location":"fiftyone_concepts/using_datasets/#labels","title":"Labels \u00b6","text":"<p>The <code>Label</code> class hierarchy is used to store semantic information about ground truth or predicted labels in a sample.</p> <p>Although such information can be stored in custom sample fields (e.g, in a <code>DictField</code>), it is recommended that you store label information in <code>Label</code> instances so that the FiftyOne App and the FiftyOne Brain can visualize and compute on your labels.</p> <p>Note</p> <p>All <code>Label</code> instances are dynamic! You can add custom fields to your labels to store custom information:</p> <pre><code># Provide some default fields\nlabel = fo.Classification(label=\"cat\", confidence=0.98)\n\n# Add custom fields\nlabel[\"int\"] = 5\nlabel[\"float\"] = 51.0\nlabel[\"list\"] = [1, 2, 3]\nlabel[\"bool\"] = True\nlabel[\"dict\"] = {\"key\": [\"list\", \"of\", \"values\"]}\n</code></pre> <p>You can also declare dynamic attributes on your dataset\u2019s schema, which allows you to enforce type constraints, filter by these custom attributes in the App, and more.</p> <p>FiftyOne provides a dedicated <code>Label</code> subclass for many common tasks. The subsections below describe them.</p>"},{"location":"fiftyone_concepts/using_datasets/#regression","title":"Regression \u00b6","text":"<p>The <code>Regression</code> class represents a numeric regression value for an image. The value itself is stored in the <code>value</code> attribute of the <code>Regression</code> object. This may be a ground truth value or a model prediction.</p> <p>The optional <code>confidence</code> attribute can be used to store a score associated with the model prediction and can be visualized in the App or used, for example, when evaluating regressions.</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\nsample[\"ground_truth\"] = fo.Regression(value=51.0)\nsample[\"prediction\"] = fo.Classification(value=42.0, confidence=0.9)\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'ground_truth': &lt;Regression: {\n        'id': '616c4bef36297ec40a26d112',\n        'tags': [],\n        'value': 51.0,\n        'confidence': None,\n    }&gt;,\n    'prediction': &lt;Classification: {\n        'id': '616c4bef36297ec40a26d113',\n        'tags': [],\n        'label': None,\n        'confidence': 0.9,\n        'logits': None,\n        'value': 42.0,\n    }&gt;,\n}&gt;\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#classification","title":"Classification \u00b6","text":"<p>The <code>Classification</code> class represents a classification label for an image. The label itself is stored in the <code>label</code> attribute of the <code>Classification</code> object. This may be a ground truth label or a model prediction.</p> <p>The optional <code>confidence</code> and <code>logits</code> attributes may be used to store metadata about the model prediction. These additional fields can be visualized in the App or used by Brain methods, e.g., when computing label mistakes.</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\nsample[\"ground_truth\"] = fo.Classification(label=\"sunny\")\nsample[\"prediction\"] = fo.Classification(label=\"sunny\", confidence=0.9)\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'ground_truth': &lt;Classification: {\n        'id': '5f8708db2018186b6ef66821',\n        'label': 'sunny',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n    'prediction': &lt;Classification: {\n        'id': '5f8708db2018186b6ef66822',\n        'label': 'sunny',\n        'confidence': 0.9,\n        'logits': None,\n    }&gt;,\n}&gt;\n</code></pre> <p>Note</p> <p>Did you know? You can store class lists for your models on your datasets.</p>"},{"location":"fiftyone_concepts/using_datasets/#multilabel-classification","title":"Multilabel classification \u00b6","text":"<p>The <code>Classifications</code> class represents a list of classification labels for an image. The typical use case is to represent multilabel annotations/predictions for an image, where multiple labels from a model may apply to a given image. The labels are stored in a <code>classifications</code> attribute of the object, which contains a list of <code>Classification</code> instances.</p> <p>Metadata about individual labels can be stored in the <code>Classification</code> instances as usual; additionally, you can optionally store logits for the overarching model (if applicable) in the <code>logits</code> attribute of the <code>Classifications</code> object.</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\nsample[\"ground_truth\"] = fo.Classifications(\n    classifications=[\\\n        fo.Classification(label=\"animal\"),\\\n        fo.Classification(label=\"cat\"),\\\n        fo.Classification(label=\"tabby\"),\\\n    ]\n)\nsample[\"prediction\"] = fo.Classifications(\n    classifications=[\\\n        fo.Classification(label=\"animal\", confidence=0.99),\\\n        fo.Classification(label=\"cat\", confidence=0.98),\\\n        fo.Classification(label=\"tabby\", confidence=0.72),\\\n    ]\n)\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'ground_truth': &lt;Classifications: {\n        'classifications': [\\\n            &lt;Classification: {\\\n                'id': '5f8708f62018186b6ef66823',\\\n                'label': 'animal',\\\n                'confidence': None,\\\n                'logits': None,\\\n            }&gt;,\\\n            &lt;Classification: {\\\n                'id': '5f8708f62018186b6ef66824',\\\n                'label': 'cat',\\\n                'confidence': None,\\\n                'logits': None,\\\n            }&gt;,\\\n            &lt;Classification: {\\\n                'id': '5f8708f62018186b6ef66825',\\\n                'label': 'tabby',\\\n                'confidence': None,\\\n                'logits': None,\\\n            }&gt;,\\\n        ],\n        'logits': None,\n    }&gt;,\n    'prediction': &lt;Classifications: {\n        'classifications': [\\\n            &lt;Classification: {\\\n                'id': '5f8708f62018186b6ef66826',\\\n                'label': 'animal',\\\n                'confidence': 0.99,\\\n                'logits': None,\\\n            }&gt;,\\\n            &lt;Classification: {\\\n                'id': '5f8708f62018186b6ef66827',\\\n                'label': 'cat',\\\n                'confidence': 0.98,\\\n                'logits': None,\\\n            }&gt;,\\\n            &lt;Classification: {\\\n                'id': '5f8708f62018186b6ef66828',\\\n                'label': 'tabby',\\\n                'confidence': 0.72,\\\n                'logits': None,\\\n            }&gt;,\\\n        ],\n        'logits': None,\n    }&gt;,\n}&gt;\n</code></pre> <p>Note</p> <p>Did you know? You can store class lists for your models on your datasets.</p>"},{"location":"fiftyone_concepts/using_datasets/#object-detection","title":"Object detection \u00b6","text":"<p>The <code>Detections</code> class represents a list of object detections in an image. The detections are stored in the <code>detections</code> attribute of the <code>Detections</code> object.</p> <p>Each individual object detection is represented by a <code>Detection</code> object. The string label of the object should be stored in the <code>label</code> attribute, and the bounding box for the object should be stored in the <code>bounding_box</code> attribute.</p> <p>Note</p> <p>FiftyOne stores box coordinates as floats in <code>[0, 1]</code> relative to the dimensions of the image. Bounding boxes are represented by a length-4 list in the format:</p> <pre><code>[&lt;top-left-x&gt;, &lt;top-left-y&gt;, &lt;width&gt;, &lt;height&gt;]\n</code></pre> <p>Note</p> <p>Did you know? FiftyOne also supports 3D detections!</p> <p>In the case of model predictions, an optional confidence score for each detection can be stored in the <code>confidence</code> attribute.</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\nsample[\"ground_truth\"] = fo.Detections(\n    detections=[fo.Detection(label=\"cat\", bounding_box=[0.5, 0.5, 0.4, 0.3])]\n)\nsample[\"prediction\"] = fo.Detections(\n    detections=[\\\n        fo.Detection(\\\n            label=\"cat\",\\\n            bounding_box=[0.480, 0.513, 0.397, 0.288],\\\n            confidence=0.96,\\\n        ),\\\n    ]\n)\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'ground_truth': &lt;Detections: {\n        'detections': [\\\n            &lt;Detection: {\\\n                'id': '5f8709172018186b6ef66829',\\\n                'attributes': {},\\\n                'label': 'cat',\\\n                'bounding_box': [0.5, 0.5, 0.4, 0.3],\\\n                'mask': None,\\\n                'confidence': None,\\\n                'index': None,\\\n            }&gt;,\\\n        ],\n    }&gt;,\n    'prediction': &lt;Detections: {\n        'detections': [\\\n            &lt;Detection: {\\\n                'id': '5f8709172018186b6ef6682a',\\\n                'attributes': {},\\\n                'label': 'cat',\\\n                'bounding_box': [0.48, 0.513, 0.397, 0.288],\\\n                'mask': None,\\\n                'confidence': 0.96,\\\n                'index': None,\\\n            }&gt;,\\\n        ],\n    }&gt;,\n}&gt;\n</code></pre> <p>Note</p> <p>Did you know? You can store class lists for your models on your datasets.</p> <p>Like all <code>Label</code> types, you can also add custom attributes to your detections by dynamically adding new fields to each <code>Detection</code> instance:</p> <pre><code>import fiftyone as fo\n\ndetection = fo.Detection(\n    label=\"cat\",\n    bounding_box=[0.5, 0.5, 0.4, 0.3],\n    age=51,  # custom attribute\n    mood=\"salty\",  # custom attribute\n)\n\nprint(detection)\n</code></pre> <pre><code>&lt;Detection: {\n    'id': '60f7458c467d81f41c200551',\n    'attributes': {},\n    'tags': [],\n    'label': 'cat',\n    'bounding_box': [0.5, 0.5, 0.4, 0.3],\n    'mask': None,\n    'confidence': None,\n    'index': None,\n    'age': 51,\n    'mood': 'salty',\n}&gt;\n</code></pre> <p>Note</p> <p>Did you know? You can view custom attributes in the App tooltip by hovering over the objects.</p>"},{"location":"fiftyone_concepts/using_datasets/#instance-segmentations","title":"Instance segmentations \u00b6","text":"<p>Object detections stored in <code>Detections</code> may also have instance segmentation masks.</p> <p>These masks can be stored in one of two ways: either directly in the database via the <code>mask</code> attribute, or on disk referenced by the <code>mask_path</code> attribute.</p> <p>Masks stored directly in the database must be 2D numpy arrays containing either booleans or 0/1 integers that encode the extent of the instance mask within the <code>bounding_box</code> of the object.</p> <p>For masks stored on disk, the <code>mask_path</code> attribute should contain the file path to the mask image. We recommend storing masks as single-channel PNG images, where a pixel value of 0 indicates the background (rendered as transparent in the App), and any other value indicates the object.</p> <p>Masks can be of any size; they are stretched as necessary to fill the object\u2019s bounding box when visualizing in the App.</p> <pre><code>import numpy as np\nfrom PIL import Image\n\nimport fiftyone as fo\n\n# Example instance mask\nmask = ((np.random.randn(32, 32) &gt; 0) * 255).astype(np.uint8)\nmask_path = \"/path/to/mask.png\"\nImage.fromarray(mask).save(mask_path)\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\nsample[\"prediction\"] = fo.Detections(\n    detections=[\\\n        fo.Detection(\\\n            label=\"cat\",\\\n            bounding_box=[0.480, 0.513, 0.397, 0.288],\\\n            mask_path=mask_path,\\\n            confidence=0.96,\\\n        ),\\\n    ]\n)\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'prediction': &lt;Detections: {\n        'detections': [\\\n            &lt;Detection: {\\\n                'id': '5f8709282018186b6ef6682b',\\\n                'attributes': {},\\\n                'tags': [],\\\n                'label': 'cat',\\\n                'bounding_box': [0.48, 0.513, 0.397, 0.288],\\\n                'mask': None,\\\n                'mask_path': '/path/to/mask.png',\\\n                'confidence': 0.96,\\\n                'index': None,\\\n            }&gt;,\\\n        ],\n    }&gt;,\n}&gt;\n</code></pre> <p>Like all <code>Label</code> types, you can also add custom attributes to your instance segmentations by dynamically adding new fields to each <code>Detection</code> instance:</p> <pre><code>import numpy as np\nimport fiftyone as fo\n\ndetection = fo.Detection(\n    label=\"cat\",\n    bounding_box=[0.5, 0.5, 0.4, 0.3],\n    mask_path=\"/path/to/mask.png\",\n    age=51,  # custom attribute\n    mood=\"salty\",  # custom attribute\n)\n\nprint(detection)\n</code></pre> <pre><code>&lt;Detection: {\n    'id': '60f74568467d81f41c200550',\n    'attributes': {},\n    'tags': [],\n    'label': 'cat',\n    'bounding_box': [0.5, 0.5, 0.4, 0.3],\n    'mask_path': '/path/to/mask.png',\n    'confidence': None,\n    'index': None,\n    'age': 51,\n    'mood': 'salty',\n}&gt;\n</code></pre> <p>Note</p> <p>Did you know? You can view custom attributes in the App tooltip by hovering over the objects.</p>"},{"location":"fiftyone_concepts/using_datasets/#polylines-and-polygons","title":"Polylines and polygons \u00b6","text":"<p>The <code>Polylines</code> class represents a list of polylines or polygons in an image. The polylines are stored in the <code>polylines</code> attribute of the <code>Polylines</code> object.</p> <p>Each individual polyline is represented by a <code>Polyline</code> object, which represents a set of one or more semantically related shapes in an image. The <code>points</code> attribute contains a list of lists of <code>(x, y)</code> coordinates defining the vertices of each shape in the polyline. If the polyline represents a closed curve, you can set the <code>closed</code> attribute to <code>True</code> to indicate that a line segment should be drawn from the last vertex to the first vertex of each shape in the polyline. If the shapes should be filled when rendering them, you can set the <code>filled</code> attribute to <code>True</code>. Polylines can also have string labels, which are stored in their <code>label</code> attribute.</p> <p>Note</p> <p>FiftyOne stores vertex coordinates as floats in <code>[0, 1]</code> relative to the dimensions of the image.</p> <p>Note</p> <p>Did you know? FiftyOne also supports 3D polylines!</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\n# A simple polyline\npolyline1 = fo.Polyline(\n    points=[[(0.3, 0.3), (0.7, 0.3), (0.7, 0.3)]],\n    closed=False,\n    filled=False,\n)\n\n# A closed, filled polygon with a label\npolyline2 = fo.Polyline(\n    label=\"triangle\",\n    points=[[(0.1, 0.1), (0.3, 0.1), (0.3, 0.3)]],\n    closed=True,\n    filled=True,\n)\n\nsample[\"polylines\"] = fo.Polylines(polylines=[polyline1, polyline2])\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'polylines': &lt;Polylines: {\n        'polylines': [\\\n            &lt;Polyline: {\\\n                'id': '5f87094e2018186b6ef6682e',\\\n                'attributes': {},\\\n                'label': None,\\\n                'points': [[(0.3, 0.3), (0.7, 0.3), (0.7, 0.3)]],\\\n                'index': None,\\\n                'closed': False,\\\n                'filled': False,\\\n            }&gt;,\\\n            &lt;Polyline: {\\\n                'id': '5f87094e2018186b6ef6682f',\\\n                'attributes': {},\\\n                'label': 'triangle',\\\n                'points': [[(0.1, 0.1), (0.3, 0.1), (0.3, 0.3)]],\\\n                'index': None,\\\n                'closed': True,\\\n                'filled': True,\\\n            }&gt;,\\\n        ],\n    }&gt;,\n}&gt;\n</code></pre> <p>Like all <code>Label</code> types, you can also add custom attributes to your polylines by dynamically adding new fields to each <code>Polyline</code> instance:</p> <pre><code>import fiftyone as fo\n\npolyline = fo.Polyline(\n    label=\"triangle\",\n    points=[[(0.1, 0.1), (0.3, 0.1), (0.3, 0.3)]],\n    closed=True,\n    filled=True,\n    kind=\"right\",  # custom attribute\n)\n\nprint(polyline)\n</code></pre> <pre><code>&lt;Polyline: {\n    'id': '60f746b4467d81f41c200555',\n    'attributes': {},\n    'tags': [],\n    'label': 'triangle',\n    'points': [[(0.1, 0.1), (0.3, 0.1), (0.3, 0.3)]],\n    'confidence': None,\n    'index': None,\n    'closed': True,\n    'filled': True,\n    'kind': 'right',\n}&gt;\n</code></pre> <p>Note</p> <p>Did you know? You can view custom attributes in the App tooltip by hovering over the objects.</p>"},{"location":"fiftyone_concepts/using_datasets/#cuboids","title":"Cuboids \u00b6","text":"<p>You can store and visualize cuboids in FiftyOne using the <code>Polyline.from_cuboid()</code> method.</p> <p>The method accepts a list of 8 <code>(x, y)</code> points describing the vertices of the cuboid in the format depicted below:</p> <pre><code>   7--------6\n  /|       /|\n / |      / |\n3--------2  |\n|  4-----|--5\n| /      | /\n|/       |/\n0--------1\n</code></pre> <p>Note</p> <p>FiftyOne stores vertex coordinates as floats in <code>[0, 1]</code> relative to the dimensions of the image.</p> <pre><code>import cv2\nimport numpy as np\nimport fiftyone as fo\n\ndef random_cuboid(frame_size):\n    width, height = frame_size\n    x0, y0 = np.array([width, height]) * ([0, 0.2] + 0.8 * np.random.rand(2))\n    dx, dy = (min(0.8 * width - x0, y0 - 0.2 * height)) * np.random.rand(2)\n    x1, y1 = x0 + dx, y0 - dy\n    w, h = (min(width - x1, y1)) * np.random.rand(2)\n    front = [(x0, y0), (x0 + w, y0), (x0 + w, y0 - h), (x0, y0 - h)]\n    back = [(x1, y1), (x1 + w, y1), (x1 + w, y1 - h), (x1, y1 - h)]\n    vertices = front + back\n    return fo.Polyline.from_cuboid(\n        vertices, frame_size=frame_size, label=\"cuboid\"\n    )\n\nframe_size = (256, 128)\n\nfilepath = \"/tmp/image.png\"\nsize = (frame_size[1], frame_size[0], 3)\ncv2.imwrite(filepath, np.full(size, 255, dtype=np.uint8))\n\ndataset = fo.Dataset(\"cuboids\")\ndataset.add_samples(\n    [\\\n        fo.Sample(filepath=filepath, cuboid=random_cuboid(frame_size))\\\n        for _ in range(51)]\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p></p> <p>Like all <code>Label</code> types, you can also add custom attributes to your cuboids by dynamically adding new fields to each <code>Polyline</code> instance:</p> <pre><code>polyline = fo.Polyline.from_cuboid(\n    vertics, frame_size=frame_size,\n    label=\"vehicle\",\n    filled=True,\n    type=\"sedan\",  # custom attribute\n)\n</code></pre> <p>Note</p> <p>Did you know? You can view custom attributes in the App tooltip by hovering over the objects.</p>"},{"location":"fiftyone_concepts/using_datasets/#rotated-bounding-boxes","title":"Rotated bounding boxes \u00b6","text":"<p>You can store and visualize rotated bounding boxes in FiftyOne using the <code>Polyline.from_rotated_box()</code> method, which accepts rotated boxes described by their center coordinates, width/height, and counter-clockwise rotation, in radians.</p> <p>Note</p> <p>FiftyOne stores vertex coordinates as floats in <code>[0, 1]</code> relative to the dimensions of the image.</p> <pre><code>import cv2\nimport numpy as np\nimport fiftyone as fo\n\ndef random_rotated_box(frame_size):\n    width, height = frame_size\n    xc, yc = np.array([width, height]) * (0.2 + 0.6 * np.random.rand(2))\n    w, h = 1.5 * (min(xc, yc, width - xc, height - yc)) * np.random.rand(2)\n    theta = 2 * np.pi * np.random.rand()\n    return fo.Polyline.from_rotated_box(\n        xc, yc, w, h, theta, frame_size=frame_size, label=\"box\"\n    )\n\nframe_size = (256, 128)\n\nfilepath = \"/tmp/image.png\"\nsize = (frame_size[1], frame_size[0], 3)\ncv2.imwrite(filepath, np.full(size, 255, dtype=np.uint8))\n\ndataset = fo.Dataset(\"rotated-boxes\")\ndataset.add_samples(\n    [\\\n        fo.Sample(filepath=filepath, box=random_rotated_box(frame_size))\\\n        for _ in range(51)\\\n    ]\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p></p> <p>Like all <code>Label</code> types, you can also add custom attributes to your rotated bounding boxes by dynamically adding new fields to each <code>Polyline</code> instance:</p> <pre><code>polyline = fo.Polyline.from_rotated_box(\n    xc, yc, width, height, theta, frame_size=frame_size,\n    label=\"cat\",\n    mood=\"surly\",  # custom attribute\n)\n</code></pre> <p>Note</p> <p>Did you know? You can view custom attributes in the App tooltip by hovering over the objects.</p>"},{"location":"fiftyone_concepts/using_datasets/#keypoints","title":"Keypoints \u00b6","text":"<p>The <code>Keypoints</code> class represents a collection of keypoint groups in an image. The keypoint groups are stored in the <code>keypoints</code> attribute of the <code>Keypoints</code> object. Each element of this list is a <code>Keypoint</code> object whose <code>points</code> attribute contains a list of <code>(x, y)</code> coordinates defining a group of semantically related keypoints in the image.</p> <p>For example, if you are working with a person model that outputs 18 keypoints ( <code>left eye</code>, <code>right eye</code>, <code>nose</code>, etc.) per person, then each <code>Keypoint</code> instance would represent one person, and a <code>Keypoints</code> instance would represent the list of people in the image.</p> <p>Note</p> <p>FiftyOne stores keypoint coordinates as floats in <code>[0, 1]</code> relative to the dimensions of the image.</p> <p>Each <code>Keypoint</code> object can have a string label, which is stored in its <code>label</code> attribute, and it can optionally have a list of per-point confidences in <code>[0, 1]</code> in its <code>confidence</code> attribute:</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\nsample[\"keypoints\"] = fo.Keypoints(\n    keypoints=[\\\n        fo.Keypoint(\\\n            label=\"square\",\\\n            points=[(0.3, 0.3), (0.7, 0.3), (0.7, 0.7), (0.3, 0.7)],\\\n            confidence=[0.6, 0.7, 0.8, 0.9],\\\n        )\\\n    ]\n)\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'keypoints': &lt;Keypoints: {\n        'keypoints': [\\\n            &lt;Keypoint: {\\\n                'id': '5f8709702018186b6ef66831',\\\n                'attributes': {},\\\n                'label': 'square',\\\n                'points': [(0.3, 0.3), (0.7, 0.3), (0.7, 0.7), (0.3, 0.7)],\\\n                'confidence': [0.6, 0.7, 0.8, 0.9],\\\n                'index': None,\\\n            }&gt;,\\\n        ],\n    }&gt;,\n}&gt;\n</code></pre> <p>Like all <code>Label</code> types, you can also add custom attributes to your keypoints by dynamically adding new fields to each <code>Keypoint</code> instance. As a special case, if you add a custom list attribute to a <code>Keypoint</code> instance whose length matches the number of points, these values will be interpreted as per-point attributes and rendered as such in the App:</p> <pre><code>import fiftyone as fo\n\nkeypoint = fo.Keypoint(\n    label=\"rectangle\",\n    kind=\"square\",  # custom object attribute\n    points=[(0.3, 0.3), (0.7, 0.3), (0.7, 0.7), (0.3, 0.7)],\n    confidence=[0.6, 0.7, 0.8, 0.9],\n    occluded=[False, False, True, False],  # custom per-point attributes\n)\n\nprint(keypoint)\n</code></pre> <pre><code>&lt;Keypoint: {\n    'id': '60f74723467d81f41c200556',\n    'attributes': {},\n    'tags': [],\n    'label': 'rectangle',\n    'points': [(0.3, 0.3), (0.7, 0.3), (0.7, 0.7), (0.3, 0.7)],\n    'confidence': [0.6, 0.7, 0.8, 0.9],\n    'index': None,\n    'kind': 'square',\n    'occluded': [False, False, True, False],\n}&gt;\n</code></pre> <p>If your keypoints have semantic meanings, you can store keypoint skeletons on your dataset to encode the meanings.</p> <p>If you are working with keypoint skeletons and a particular point is missing or not visible for an instance, use nan values for its coordinates:</p> <pre><code>keypoint = fo.Keypoint(\n    label=\"rectangle\",\n    points=[\\\n        (0.3, 0.3),\\\n        (float(\"nan\"), float(\"nan\")),  # use nan to encode missing points\\\n        (0.7, 0.7),\\\n        (0.3, 0.7),\\\n    ],\n)\n</code></pre> <p>Note</p> <p>Did you know? When you view datasets with keypoint skeletons in the App, label strings and edges will be drawn when you visualize the keypoint fields.</p>"},{"location":"fiftyone_concepts/using_datasets/#semantic-segmentation","title":"Semantic segmentation \u00b6","text":"<p>The <code>Segmentation</code> class represents a semantic segmentation mask for an image with integer values encoding the semantic labels for each pixel in the image.</p> <p>The mask can either be stored on disk and referenced via the <code>mask_path</code> attribute or stored directly in the database via the <code>mask</code> attribute.</p> <p>Note</p> <p>It is recommended to store segmentations on disk and reference them via the <code>mask_path</code> attribute, for efficiency.</p> <p>Note that <code>mask_path</code> must contain the absolute path to the mask on disk in order to use the dataset from different current working directories in the future.</p> <p>Segmentation masks can be stored in either of these formats:</p> <ul> <li> <p>2D 8-bit or 16-bit images or numpy arrays</p> </li> <li> <p>3D 8-bit RGB images or numpy arrays</p> </li> </ul> <p>Segmentation masks can have any size; they are stretched as necessary to fit the image\u2019s extent when visualizing in the App.</p> <pre><code>import cv2\nimport numpy as np\n\nimport fiftyone as fo\n\n# Example segmentation mask\nmask_path = \"/tmp/segmentation.png\"\nmask = np.random.randint(10, size=(128, 128), dtype=np.uint8)\ncv2.imwrite(mask_path, mask)\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\nsample[\"segmentation1\"] = fo.Segmentation(mask_path=mask_path)\nsample[\"segmentation2\"] = fo.Segmentation(mask=mask)\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'segmentation1': &lt;Segmentation: {\n        'id': '6371d72425de9907b93b2a6b',\n        'tags': [],\n        'mask': None,\n        'mask_path': '/tmp/segmentation.png',\n    }&gt;,\n    'segmentation2': &lt;Segmentation: {\n        'id': '6371d72425de9907b93b2a6c',\n        'tags': [],\n        'mask': array([[8, 5, 5, ..., 9, 8, 5],\\\n               [0, 7, 8, ..., 3, 4, 4],\\\n               [5, 0, 2, ..., 0, 3, 4],\\\n               ...,\\\n               [4, 4, 4, ..., 3, 6, 6],\\\n               [0, 9, 8, ..., 8, 0, 8],\\\n               [0, 6, 8, ..., 2, 9, 1]], dtype=uint8),\n        'mask_path': None,\n    }&gt;,\n}&gt;\n</code></pre> <p>When you load datasets with <code>Segmentation</code> fields containing 2D masks in the App, each pixel value is rendered as a different color (if possible) from the App\u2019s color pool. When you view RGB segmentation masks in the App, the mask colors are always used.</p> <p>Note</p> <p>Did you know? You can store semantic labels for your segmentation fields on your dataset. Then, when you view the dataset in the App, label strings will appear in the App\u2019s tooltip when you hover over pixels.</p> <p>Note</p> <p>The pixel value <code>0</code> and RGB value <code>#000000</code> are reserved \u201cbackground\u201d classes that are always rendered as invisible in the App.</p> <p>If mask targets are provided, all observed values not present in the targets are also rendered as invisible in the App.</p>"},{"location":"fiftyone_concepts/using_datasets/#heatmaps","title":"Heatmaps \u00b6","text":"<p>The <code>Heatmap</code> class represents a continuous-valued heatmap for an image.</p> <p>The map can either be stored on disk and referenced via the <code>map_path</code> attribute or stored directly in the database via the <code>map</code> attribute. When using the <code>map_path</code> attribute, heatmaps may be 8-bit or 16-bit grayscale images. When using the <code>map</code> attribute, heatmaps should be 2D numpy arrays. By default, the map values are assumed to be in <code>[0, 1]</code> for floating point arrays and <code>[0, 255]</code> for integer-valued arrays, but you can specify a custom <code>[min, max]</code> range for a map by setting its optional <code>range</code> attribute.</p> <p>Heatmaps can have any size; they are stretched as necessary to fit the image\u2019s extent when visualizing in the App.</p> <p>Note</p> <p>It is recommended to store heatmaps on disk and reference them via the <code>map_path</code> attribute, for efficiency.</p> <p>Note that <code>map_path</code> must contain the absolute path to the map on disk in order to use the dataset from different current working directories in the future.</p> <pre><code>import cv2\nimport numpy as np\n\nimport fiftyone as fo\n\n# Example heatmap\nmap_path = \"/tmp/heatmap.png\"\nmap = np.random.randint(256, size=(128, 128), dtype=np.uint8)\ncv2.imwrite(map_path, map)\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\nsample[\"heatmap1\"] = fo.Heatmap(map_path=map_path)\nsample[\"heatmap2\"] = fo.Heatmap(map=map)\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'heatmap1': &lt;Heatmap: {\n        'id': '6371d9e425de9907b93b2a6f',\n        'tags': [],\n        'map': None,\n        'map_path': '/tmp/heatmap.png',\n        'range': None,\n    }&gt;,\n    'heatmap2': &lt;Heatmap: {\n        'id': '6371d9e425de9907b93b2a70',\n        'tags': [],\n        'map': array([[179, 249, 119, ...,  94, 213,  68],\\\n               [190, 202, 209, ..., 162,  16,  39],\\\n               [252, 251, 181, ..., 221, 118, 231],\\\n               ...,\\\n               [ 12,  91, 201, ...,  14,  95,  88],\\\n               [164, 118, 171, ...,  21, 170,   5],\\\n               [232, 156, 218, ..., 224,  97,  65]], dtype=uint8),\n        'map_path': None,\n        'range': None,\n    }&gt;,\n}&gt;\n</code></pre> <p>When visualizing heatmaps in the App, when the App is in color-by-field mode, heatmaps are rendered in their field\u2019s color with opacity proportional to the magnitude of the heatmap\u2019s values. For example, for a heatmap whose <code>range</code> is <code>[-10, 10]</code>, pixels with the value +9 will be rendered with 90% opacity, and pixels with the value -3 will be rendered with 30% opacity.</p> <p>When the App is in color-by-value mode, heatmaps are rendered using the colormap defined by the <code>colorscale</code> of your App config, which can be:</p> <ul> <li> <p>The string name of any colorscale recognized by Plotly</p> </li> <li> <p>A manually-defined colorscale like the following:</p> </li> </ul> <pre><code>[\\\n      [0.000, \"rgb(165,0,38)\"],\\\n      [0.111, \"rgb(215,48,39)\"],\\\n      [0.222, \"rgb(244,109,67)\"],\\\n      [0.333, \"rgb(253,174,97)\"],\\\n      [0.444, \"rgb(254,224,144)\"],\\\n      [0.555, \"rgb(224,243,248)\"],\\\n      [0.666, \"rgb(171,217,233)\"],\\\n      [0.777, \"rgb(116,173,209)\"],\\\n      [0.888, \"rgb(69,117,180)\"],\\\n      [1.000, \"rgb(49,54,149)\"],\\\n]\n</code></pre> <p>The example code below demonstrates the possibilities that heatmaps provide by overlaying random gaussian kernels with positive or negative sign on an image dataset and configuring the App\u2019s colorscale in various ways on-the-fly:</p> <pre><code>import os\nimport numpy as np\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\ndef random_kernel(metadata):\n    h = metadata.height // 2\n    w = metadata.width // 2\n    sign = np.sign(np.random.randn())\n    x, y = np.meshgrid(np.linspace(-1, 1, w), np.linspace(-1, 1, h))\n    x0, y0 = np.random.random(2) - 0.5\n    kernel = sign * np.exp(-np.sqrt((x - x0) ** 2 + (y - y0) ** 2))\n    return fo.Heatmap(map=kernel, range=[-1, 1])\n\ndataset = foz.load_zoo_dataset(\"quickstart\").select_fields().clone()\ndataset.compute_metadata()\n\nfor sample in dataset:\n    heatmap = random_kernel(sample.metadata)\n\n    # Convert to on-disk\n    map_path = os.path.join(\"/tmp/heatmaps\", os.path.basename(sample.filepath))\n    heatmap.export_map(map_path, update=True)\n\n    sample[\"heatmap\"] = heatmap\n    sample.save()\n\nsession = fo.launch_app(dataset)\n</code></pre> <pre><code># Select `Settings -&gt; Color by value` in the App\n# Heatmaps will now be rendered using your default colorscale (printed below)\nprint(session.config.colorscale)\n</code></pre> <pre><code># Switch to a different named colorscale\nsession.config.colorscale = \"RdBu\"\nsession.refresh()\n</code></pre> <pre><code># Switch to a custom colorscale\nsession.config.colorscale = [\\\n    [0.00, \"rgb(166,206,227)\"],\\\n    [0.25, \"rgb(31,120,180)\"],\\\n    [0.45, \"rgb(178,223,138)\"],\\\n    [0.65, \"rgb(51,160,44)\"],\\\n    [0.85, \"rgb(251,154,153)\"],\\\n    [1.00, \"rgb(227,26,28)\"],\\\n]\nsession.refresh()\n</code></pre> <p>Note</p> <p>Did you know? You customize your App config in various ways, from environment variables to directly editing a <code>Session</code> object\u2019s config. See this page for more details.</p>"},{"location":"fiftyone_concepts/using_datasets/#temporal-detection","title":"Temporal detection \u00b6","text":"<p>The <code>TemporalDetection</code> class represents an event occurring during a specified range of frames in a video.</p> <p>The <code>label</code> attribute stores the detection label, and the <code>support</code> attribute stores the <code>[first, last]</code> frame range of the detection in the video.</p> <p>The optional <code>confidence</code> attribute can be used to store a model prediction score, and you can add custom attributes as well, which can be visualized in the App.</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/video.mp4\")\nsample[\"events\"] = fo.TemporalDetection(label=\"meeting\", support=[10, 20])\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'video',\n    'filepath': '/path/to/video.mp4',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'events': &lt;TemporalDetection: {\n        'id': '61321c8ea36cb17df655f44f',\n        'tags': [],\n        'label': 'meeting',\n        'support': [10, 20],\n        'confidence': None,\n    }&gt;,\n    'frames': &lt;Frames: 0&gt;,\n}&gt;\n</code></pre> <p>If your temporal detection data is represented as timestamps in seconds, you can use the <code>from_timestamps()</code> factory method to perform the necessary conversion to frames automatically based on the sample\u2019s video metadata:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Download a video to work with\ndataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=1)\nfilepath = dataset.first().filepath\n\nsample = fo.Sample(filepath=filepath)\nsample.compute_metadata()\n\nsample[\"events\"] = fo.TemporalDetection.from_timestamps(\n    [1, 2], label=\"meeting\", sample=sample\n)\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'video',\n    'filepath': '~/fiftyone/quickstart-video/data/Ulcb3AjxM5g_053-1.mp4',\n    'tags': [],\n    'metadata': &lt;VideoMetadata: {\n        'size_bytes': 1758809,\n        'mime_type': 'video/mp4',\n        'frame_width': 1920,\n        'frame_height': 1080,\n        'frame_rate': 29.97002997002997,\n        'total_frame_count': 120,\n        'duration': 4.004,\n        'encoding_str': 'avc1',\n    }&gt;,\n    'created_at': None,\n    'last_modified_at': None,\n    'events': &lt;TemporalDetection: {\n        'id': '61321e498d5f587970b29183',\n        'tags': [],\n        'label': 'meeting',\n        'support': [31, 60],\n        'confidence': None,\n    }&gt;,\n    'frames': &lt;Frames: 0&gt;,\n}&gt;\n</code></pre> <p>The <code>TemporalDetections</code> class holds a list of temporal detections for a sample:</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/video.mp4\")\nsample[\"events\"] = fo.TemporalDetections(\n    detections=[\\\n        fo.TemporalDetection(label=\"meeting\", support=[10, 20]),\\\n        fo.TemporalDetection(label=\"party\", support=[30, 60]),\\\n    ]\n)\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'video',\n    'filepath': '/path/to/video.mp4',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'events': &lt;TemporalDetections: {\n        'detections': [\\\n            &lt;TemporalDetection: {\\\n                'id': '61321ed78d5f587970b29184',\\\n                'tags': [],\\\n                'label': 'meeting',\\\n                'support': [10, 20],\\\n                'confidence': None,\\\n            }&gt;,\\\n            &lt;TemporalDetection: {\\\n                'id': '61321ed78d5f587970b29185',\\\n                'tags': [],\\\n                'label': 'party',\\\n                'support': [30, 60],\\\n                'confidence': None,\\\n            }&gt;,\\\n        ],\n    }&gt;,\n    'frames': &lt;Frames: 0&gt;,\n}&gt;\n</code></pre> <p>Note</p> <p>Did you know? You can store class lists for your models on your datasets.</p>"},{"location":"fiftyone_concepts/using_datasets/#3d-detections","title":"3D detections \u00b6","text":"<p>The App\u2019s 3D visualizer supports rendering 3D object detections represented as <code>Detection</code> instances with their <code>label</code>, <code>location</code>, <code>dimensions</code>, and <code>rotation</code> attributes populated as shown below:</p> <pre><code>import fiftyone as fo\n\n# Object label\nlabel = \"vehicle\"\n\n# Object center `[x, y, z]` in scene coordinates\nlocation = [0.47, 1.49, 69.44]\n\n# Object dimensions `[x, y, z]` in scene units\ndimensions = [2.85, 2.63, 12.34]\n\n# Object rotation `[x, y, z]` around its center, in `[-pi, pi]`\nrotation = [0, -1.56, 0]\n\n# A 3D object detection\ndetection = fo.Detection(\n    label=label,\n    location=location,\n    dimensions=dimensions,\n    rotation=rotation,\n)\n</code></pre> <p>Note</p> <p>Did you know? You can view custom attributes in the App tooltip by hovering over the objects.</p>"},{"location":"fiftyone_concepts/using_datasets/#3d-polylines","title":"3D polylines \u00b6","text":"<p>The App\u2019s 3D visualizer supports rendering 3D polylines represented as <code>Polyline</code> instances with their <code>label</code> and <code>points3d</code> attributes populated as shown below:</p> <pre><code>import fiftyone as fo\n\n# Object label\nlabel = \"lane\"\n\n# A list of lists of `[x, y, z]` points in scene coordinates describing\n# the vertices of each shape in the polyline\npoints3d = [[[-5, -99, -2], [-8, 99, -2]], [[4, -99, -2], [1, 99, -2]]]\n\n# A set of semantically related 3D polylines\npolyline = fo.Polyline(label=label, points3d=points3d)\n</code></pre> <p>Note</p> <p>Did you know? You can view custom attributes in the App tooltip by hovering over the objects.</p>"},{"location":"fiftyone_concepts/using_datasets/#geolocation","title":"Geolocation \u00b6","text":"<p>The <code>GeoLocation</code> class can store single pieces of location data in its properties:</p> <ul> <li> <p><code>point</code>: a <code>[longitude, latitude]</code> point</p> </li> <li> <p><code>line</code>: a line of longitude and latitude coordinates stored in the following format:</p> </li> </ul> <pre><code>[[lon1, lat1], [lon2, lat2], ...]\n</code></pre> <ul> <li><code>polygon</code>: a polygon of longitude and latitude coordinates stored in the format below, where the first element describes the boundary of the polygon and any remaining entries describe holes:</li> </ul> <pre><code>[\\\n      [[lon1, lat1], [lon2, lat2], ...],\\\n      [[lon1, lat1], [lon2, lat2], ...],\\\n      ...\\\n]\n</code></pre> <p>Note</p> <p>All geolocation coordinates are stored in <code>[longitude, latitude]</code> format.</p> <p>If you have multiple geometries of each type that you wish to store on a single sample, then you can use the <code>GeoLocations</code> class and its appropriate properties to do so.</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\nsample[\"location\"] = fo.GeoLocation(\n    point=[-73.9855, 40.7580],\n    polygon=[\\\n        [\\\n            [-73.949701, 40.834487],\\\n            [-73.896611, 40.815076],\\\n            [-73.998083, 40.696534],\\\n            [-74.031751, 40.715273],\\\n            [-73.949701, 40.834487],\\\n        ]\\\n    ],\n)\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'location': &lt;GeoLocation: {\n        'id': '60481f3936dc48428091e926',\n        'tags': [],\n        'point': [-73.9855, 40.758],\n        'line': None,\n        'polygon': [\\\n            [\\\n                [-73.949701, 40.834487],\\\n                [-73.896611, 40.815076],\\\n                [-73.998083, 40.696534],\\\n                [-74.031751, 40.715273],\\\n                [-73.949701, 40.834487],\\\n            ],\\\n        ],\n    }&gt;,\n}&gt;\n</code></pre> <p>Note</p> <p>Did you know? You can create location-based views that filter your data by their location!</p> <p>All location data is stored in GeoJSON format in the database. You can easily retrieve the raw GeoJSON data for a slice of your dataset using the values() aggregation:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-geo\")\n\nvalues = dataset.take(5).values(\"location.point\", _raw=True)\nprint(values)\n</code></pre> <pre><code>[{'type': 'Point', 'coordinates': [-73.9592175465766, 40.71052995514191]},\\\n {'type': 'Point', 'coordinates': [-73.97748118760413, 40.74660360881843]},\\\n {'type': 'Point', 'coordinates': [-73.9508690871987, 40.766631164626]},\\\n {'type': 'Point', 'coordinates': [-73.96569416502996, 40.75449283200206]},\\\n {'type': 'Point', 'coordinates': [-73.97397106211423, 40.67925541341504]}]\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#label-tags","title":"Label tags \u00b6","text":"<p>All <code>Label</code> instances have a <code>tags</code> field, which is a string list. By default, this list is empty, but you can use it to store application-specific information like whether the label is incorrect:</p> <pre><code>detection = fo.Detection(label=\"cat\", bounding_box=[0, 0, 1, 1])\n\ndetection.tags.append(\"mistake\")\n\nprint(detection.tags)\n# [\"mistake\"]\n</code></pre> <p>Note</p> <p>Did you know? You can add, edit, and filter by label tags directly in the App.</p> <p>Datasets and views provide helpful methods such as <code>count_label_tags()</code>, <code>tag_labels()</code>, <code>untag_labels()</code>, <code>match_labels()</code>, and <code>select_labels()</code> that you can use to perform batch queries and edits to label tags:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\").clone()\n\n# Tag all low confidence prediction\nview = dataset.filter_labels(\"predictions\", F(\"confidence\") &lt; 0.1)\nview.tag_labels(\"potential_mistake\", label_fields=\"predictions\")\nprint(dataset.count_label_tags())  # {'potential_mistake': 1555}\n\n# Create a view containing only tagged labels\nview = dataset.select_labels(tags=\"potential_mistake\", fields=\"predictions\")\nprint(len(view))  # 173\nprint(view.count(\"predictions.detections\"))  # 1555\n\n# Create a view containing only samples with at least one tagged label\nview = dataset.match_labels(tags=\"potential_mistake\", fields=\"predictions\")\nprint(len(view))  # 173\nprint(view.count(\"predictions.detections\"))  # 5151\n\ndataset.untag_labels(\"potential_mistake\", label_fields=\"predictions\")\nprint(dataset.count_label_tags())  # {}\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#label-attributes","title":"Label attributes \u00b6","text":"<p>The <code>Detection</code>, <code>Polyline</code>, and <code>Keypoint</code> label types have an optional <code>attributes</code> field that you can use to store custom attributes on the object.</p> <p>The <code>attributes</code> field is a dictionary mapping attribute names to <code>Attribute</code> instances, which contain the <code>value</code> of the attribute and any associated metadata.</p> <p>Warning</p> <p>The <code>attributes</code> field will be removed in an upcoming release.</p> <p>Instead, add custom attributes directly to your <code>Label</code> objects:</p> <pre><code>detection = fo.Detection(label=\"cat\", bounding_box=[0.1, 0.1, 0.8, 0.8])\ndetection[\"custom_attribute\"] = 51\n\n# Equivalent\ndetection = fo.Detection(\n    label=\"cat\",\n    bounding_box=[0.1, 0.1, 0.8, 0.8],\n    custom_attribute=51,\n)\n</code></pre> <p>There are <code>Attribute</code> subclasses for various types of attributes you may want to store. Use the appropriate subclass when possible so that FiftyOne knows the schema of the attributes that you\u2019re storing.</p> Attribute class Value type Description <code>BooleanAttribute</code> <code>bool</code> A boolean attribute <code>CategoricalAttribute</code> <code>string</code> A categorical attribute <code>NumericAttribute</code> <code>float</code> A numeric attribute <code>ListAttribute</code> <code>list</code> A list attribute <code>Attribute</code> arbitrary A generic attribute of any type <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\nsample[\"ground_truth\"] = fo.Detections(\n    detections=[\\\n        fo.Detection(\\\n            label=\"cat\",\\\n            bounding_box=[0.5, 0.5, 0.4, 0.3],\\\n            attributes={\\\n                \"age\": fo.NumericAttribute(value=51),\\\n                \"mood\": fo.CategoricalAttribute(value=\"salty\"),\\\n            },\\\n        ),\\\n    ]\n)\nsample[\"prediction\"] = fo.Detections(\n    detections=[\\\n        fo.Detection(\\\n            label=\"cat\",\\\n            bounding_box=[0.480, 0.513, 0.397, 0.288],\\\n            confidence=0.96,\\\n            attributes={\\\n                \"age\": fo.NumericAttribute(value=51),\\\n                \"mood\": fo.CategoricalAttribute(\\\n                    value=\"surly\", confidence=0.95\\\n                ),\\\n            },\\\n        ),\\\n    ]\n)\n\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': None,\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': None,\n    'last_modified_at': None,\n    'ground_truth': &lt;Detections: {\n        'detections': [\\\n            &lt;Detection: {\\\n                'id': '60f738e7467d81f41c20054c',\\\n                'attributes': {\\\n                    'age': &lt;NumericAttribute: {'value': 51}&gt;,\\\n                    'mood': &lt;CategoricalAttribute: {\\\n                        'value': 'salty', 'confidence': None, 'logits': None\\\n                    }&gt;,\\\n                },\\\n                'tags': [],\\\n                'label': 'cat',\\\n                'bounding_box': [0.5, 0.5, 0.4, 0.3],\\\n                'mask': None,\\\n                'confidence': None,\\\n                'index': None,\\\n            }&gt;,\\\n        ],\n    }&gt;,\n    'prediction': &lt;Detections: {\n        'detections': [\\\n            &lt;Detection: {\\\n                'id': '60f738e7467d81f41c20054d',\\\n                'attributes': {\\\n                    'age': &lt;NumericAttribute: {'value': 51}&gt;,\\\n                    'mood': &lt;CategoricalAttribute: {\\\n                        'value': 'surly', 'confidence': 0.95, 'logits': None\\\n                    }&gt;,\\\n                },\\\n                'tags': [],\\\n                'label': 'cat',\\\n                'bounding_box': [0.48, 0.513, 0.397, 0.288],\\\n                'mask': None,\\\n                'confidence': 0.96,\\\n                'index': None,\\\n            }&gt;,\\\n        ],\n    }&gt;,\n}&gt;\n</code></pre> <p>Note</p> <p>Did you know? You can view attribute values in the App tooltip by hovering over the objects.</p>"},{"location":"fiftyone_concepts/using_datasets/#converting-label-types","title":"Converting label types \u00b6","text":"<p>FiftyOne provides a number of utility methods to convert between different representations of certain label types, such as converting between instance segmentations, semantic segmentations, and polylines.</p> <p>Let\u2019s load some instance segmentations from the COCO dataset to see this in action:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    label_types=[\"segmentations\"],\n    classes=[\"cat\", \"dog\"],\n    label_field=\"instances\",\n    max_samples=25,\n    only_matching=True,\n)\n\nsample = dataset.first()\ndetections = sample[\"instances\"]\n</code></pre> <p>For example, you can use <code>Detections.to_polylines()</code> to convert instance segmentations to polylines:</p> <pre><code># Convert `Detections` to `Polylines`\npolylines = detections.to_polylines(tolerance=2)\nprint(polylines)\n</code></pre> <p>Or you can use <code>Detections.to_segmentation()</code> to convert instance segmentations to semantic segmentation masks:</p> <pre><code>metadata = fo.ImageMetadata.build_for(sample.filepath)\n\n# Convert `Detections` to `Segmentation`\nsegmentation = detections.to_segmentation(\n    frame_size=(metadata.width, metadata.height),\n    mask_targets={1: \"cat\", 2: \"dog\"},\n)\n\n# Export the segmentation to disk\nsegmentation.export_mask(\"/tmp/mask.png\", update=True)\n\nprint(segmentation)\n</code></pre> <p>Methods such as <code>Segmentation.to_detections()</code> and <code>Segmentation.to_polylines()</code> also exist to transform semantic segmentations back into individual shapes.</p> <p>In addition, the <code>fiftyone.utils.labels</code> module contains a variety of utility methods for converting entire collections\u2019 labels between common formats:</p> <pre><code>import fiftyone.utils.labels as foul\n\n# Convert instance segmentations to semantic segmentations stored on disk\nfoul.objects_to_segmentations(\n    dataset,\n    \"instances\",\n    \"segmentations\",\n    output_dir=\"/tmp/segmentations\",\n    mask_targets={1: \"cat\", 2: \"dog\"},\n)\n\n# Convert instance segmentations to polylines format\nfoul.instances_to_polylines(dataset, \"instances\", \"polylines\", tolerance=2)\n\n# Convert semantic segmentations to instance segmentations\nfoul.segmentations_to_detections(\n    dataset,\n    \"segmentations\",\n    \"instances2\",\n    mask_targets={1: \"cat\", 2: \"dog\"},\n    mask_types=\"thing\",  # give each connected region a separate instance\n)\n\nprint(dataset)\n</code></pre> <pre><code>Name:        coco-2017-validation-25\nMedia type:  image\nNum samples: 25\nPersistent:  False\nTags:        []\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    instances:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    segmentations:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Segmentation)\n    polylines:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Polylines)\n    instances2:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n</code></pre> <p>Note that, if your goal is to export the labels to disk, FiftyOne can automatically coerce the labels into the correct format based on the type of the <code>label_field</code> and the <code>dataset_type</code> that you specify for the export without explicitly storing the transformed labels as a new field on your dataset:</p> <pre><code># Export the instance segmentations in the `instances` field as semantic\n# segmentation images on disk\ndataset.export(\n    label_field=\"instances\",\n    dataset_type=fo.types.ImageSegmentationDirectory,\n    labels_path=\"/tmp/masks\",\n    mask_targets={1: \"cat\", 2: \"dog\"},\n)\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#dynamic-attributes","title":"Dynamic attributes \u00b6","text":"<p>Any field(s) of your FiftyOne datasets that contain <code>DynamicEmbeddedDocument</code> values can have arbitrary custom attributes added to their instances.</p> <p>For example, all Label classes and Metadata classes are dynamic, so you can add custom attributes to them as follows:</p> <pre><code># Provide some default attributes\nlabel = fo.Classification(label=\"cat\", confidence=0.98)\n\n# Add custom attributes\nlabel[\"int\"] = 5\nlabel[\"float\"] = 51.0\nlabel[\"list\"] = [1, 2, 3]\nlabel[\"bool\"] = True\nlabel[\"dict\"] = {\"key\": [\"list\", \"of\", \"values\"]}\n</code></pre> <p>By default, dynamic attributes are not included in a dataset\u2019s schema, which means that these attributes may contain arbitrary heterogeneous values across the dataset\u2019s samples.</p> <p>However, FiftyOne provides methods that you can use to formally declare custom dynamic attributes, which allows you to enforce type constraints, filter by these custom attributes in the App, and more.</p> <p>You can use <code>get_dynamic_field_schema()</code> to detect the names and type(s) of any undeclared dynamic embedded document attributes on a dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nprint(dataset.get_dynamic_field_schema())\n</code></pre> <pre><code>{\n    'ground_truth.detections.iscrowd': &lt;fiftyone.core.fields.FloatField&gt;,\n    'ground_truth.detections.area': &lt;fiftyone.core.fields.FloatField&gt;,\n}\n</code></pre> <p>You can then use <code>add_sample_field()</code> to declare a specific dynamic embedded document attribute:</p> <pre><code>dataset.add_sample_field(\"ground_truth.detections.iscrowd\", fo.FloatField)\n</code></pre> <p>or you can use the <code>add_dynamic_sample_fields()</code> method to declare all dynamic embedded document attribute(s) that contain values of a single type:</p> <pre><code>dataset.add_dynamic_sample_fields()\n</code></pre> <p>Note</p> <p>Pass the <code>add_mixed=True</code> option to <code>add_dynamic_sample_fields()</code> if you wish to declare all dynamic attributes that contain mixed values using a generic <code>Field</code> type.</p> <p>You can provide the optional <code>flat=True</code> option to <code>get_field_schema()</code> to retrieve a flattened version of a dataset\u2019s schema that includes all embedded document attributes as top-level keys:</p> <pre><code>print(dataset.get_field_schema(flat=True))\n</code></pre> <pre><code>{\n    'id': &lt;fiftyone.core.fields.ObjectIdField&gt;,\n    'filepath': &lt;fiftyone.core.fields.StringField&gt;,\n    'tags': &lt;fiftyone.core.fields.ListField&gt;,\n    'metadata': &lt;fiftyone.core.fields.EmbeddedDocumentField&gt;,\n    'metadata.size_bytes': &lt;fiftyone.core.fields.IntField&gt;,\n    'metadata.mime_type': &lt;fiftyone.core.fields.StringField&gt;,\n    'metadata.width': &lt;fiftyone.core.fields.IntField&gt;,\n    'metadata.height': &lt;fiftyone.core.fields.IntField&gt;,\n    'metadata.num_channels': &lt;fiftyone.core.fields.IntField&gt;,\n    'created_at': &lt;fiftyone.core.fields.DateTimeField object at 0x7fea584bc730&gt;,\n    'last_modified_at': &lt;fiftyone.core.fields.DateTimeField object at 0x7fea584bc280&gt;,\n    'ground_truth': &lt;fiftyone.core.fields.EmbeddedDocumentField&gt;,\n    'ground_truth.detections': &lt;fiftyone.core.fields.ListField&gt;,\n    'ground_truth.detections.id': &lt;fiftyone.core.fields.ObjectIdField&gt;,\n    'ground_truth.detections.tags': &lt;fiftyone.core.fields.ListField&gt;,\n    ...\n    'ground_truth.detections.iscrowd': &lt;fiftyone.core.fields.FloatField&gt;,\n    'ground_truth.detections.area': &lt;fiftyone.core.fields.FloatField&gt;,\n    ...\n}\n</code></pre> <p>By default, dynamic attributes are not declared on a dataset\u2019s schema when samples are added to it:</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(\n    filepath=\"/path/to/image.jpg\",\n    ground_truth=fo.Detections(\n        detections=[\\\n            fo.Detection(\\\n                label=\"cat\",\\\n                bounding_box=[0.1, 0.1, 0.4, 0.4],\\\n                mood=\"surly\",\\\n            ),\\\n            fo.Detection(\\\n                label=\"dog\",\\\n                bounding_box=[0.5, 0.5, 0.4, 0.4],\\\n                mood=\"happy\",\\\n            )\\\n        ]\n    )\n)\n\ndataset = fo.Dataset()\ndataset.add_sample(sample)\n\nschema = dataset.get_field_schema(flat=True)\n\nassert \"ground_truth.detections.mood\" not in schema\n</code></pre> <p>However, methods such as <code>add_sample()</code>, <code>add_samples()</code>, <code>add_dir()</code>, <code>from_dir()</code>, and <code>merge_samples()</code> provide an optional <code>dynamic=True</code> option that you can provide to automatically declare any dynamic embedded document attributes encountered while importing data:</p> <pre><code>dataset = fo.Dataset()\n\ndataset.add_sample(sample, dynamic=True)\nschema = dataset.get_field_schema(flat=True)\n\nassert \"ground_truth.detections.mood\" in schema\n</code></pre> <p>Note that, when declaring dynamic attributes on non-empty datasets, you must ensure that the attribute\u2019s type is consistent with any existing values in that field, e.g., by first running <code>get_dynamic_field_schema()</code> to check the existing type(s). Methods like <code>add_sample_field()</code> and <code>add_samples(..., dynamic=True)</code> do not validate newly declared field\u2019s types against existing field values:</p> <pre><code>import fiftyone as fo\n\nsample1 = fo.Sample(\n    filepath=\"/path/to/image1.jpg\",\n    ground_truth=fo.Classification(\n        label=\"cat\",\n        mood=\"surly\",\n        age=\"bad-value\",\n    ),\n)\n\nsample2 = fo.Sample(\n    filepath=\"/path/to/image2.jpg\",\n    ground_truth=fo.Classification(\n        label=\"dog\",\n        mood=\"happy\",\n        age=5,\n    ),\n)\n\ndataset = fo.Dataset()\n\ndataset.add_sample(sample1)\n\n# Either of these are problematic\ndataset.add_sample(sample2, dynamic=True)\ndataset.add_sample_field(\"ground_truth.age\", fo.IntField)\n\nsample1.reload()  # ValidationError: bad-value could not be converted to int\n</code></pre> <p>If you declare a dynamic attribute with a type that is not compatible with existing values in that field, you will need to remove that field from the dataset\u2019s schema using <code>remove_dynamic_sample_field()</code> in order for the dataset to be usable again:</p> <pre><code># Removes dynamic field from dataset's schema without deleting the values\ndataset.remove_dynamic_sample_field(\"ground_truth.age\")\n</code></pre> <p>You can use <code>select_fields()</code> and <code>exclude_fields()</code> to create views that select/exclude specific dynamic attributes from your dataset and its schema:</p> <pre><code>dataset.add_sample_field(\"ground_truth.age\", fo.Field)\nsample = dataset.first()\n\nassert \"ground_truth.age\" in dataset.get_field_schema(flat=True)\nassert sample.ground_truth.has_field(\"age\")\n\n# Omits the `age` attribute from the `ground_truth` field\nview = dataset.exclude_fields(\"ground_truth.age\")\nsample = view.first()\n\nassert \"ground_truth.age\" not in view.get_field_schema(flat=True)\nassert not sample.ground_truth.has_field(\"age\")\n\n# Only include `mood` (and default) attributes of the `ground_truth` field\nview = dataset.select_fields(\"ground_truth.mood\")\nsample = view.first()\n\nassert \"ground_truth.age\" not in view.get_field_schema(flat=True)\nassert not sample.ground_truth.has_field(\"age\")\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#custom-embedded-documents","title":"Custom embedded documents \u00b6","text":"<p>If you work with collections of related fields that you would like to organize under a single top-level field, you can achieve this by defining and using custom <code>EmbeddedDocument</code> and <code>DynamicEmbeddedDocument</code> classes to populate your datasetes.</p> <p>Using custom embedded document classes enables you to access your data using the same object-oriented interface enjoyed by FiftyOne\u2019s builtin label types.</p> <p>The <code>EmbeddedDocument</code> class represents a fixed collection of fields with predefined types and optional default values, while the <code>DynamicEmbeddedDocument</code> class supports predefined fields but also allows users to populate arbitrary custom fields at runtime, like FiftyOne\u2019s builtin label types.</p>"},{"location":"fiftyone_concepts/using_datasets/#defining-custom-documents-on-the-fly","title":"Defining custom documents on-the-fly \u00b6","text":"<p>The simplest way to define custom embedded documents on your datasets is to declare empty <code>DynamicEmbeddedDocument</code> field(s) and then incrementally populate new dynamic attributes as needed.</p> <p>To illustrate, let\u2019s start by defining an empty embedded document field:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\n# Define an empty embedded document field\ndataset.add_sample_field(\n    \"camera_info\",\n    fo.EmbeddedDocumentField,\n    embedded_doc_type=fo.DynamicEmbeddedDocument,\n)\n</code></pre> <p>From here, there are a variety of ways to add new embedded attributes to the field.</p> <p>You can explicitly declare new fields using <code>add_sample_field()</code>:</p> <pre><code># Declare a new `camera_id` attribute\ndataset.add_sample_field(\"camera_info.camera_id\", fo.StringField)\n\nassert \"camera_info.camera_id\" in dataset.get_field_schema(flat=True)\n</code></pre> <p>or you can implicitly declare new fields using <code>add_samples()</code> with the <code>dynamic=True</code> flag:</p> <pre><code># Includes a new `quality` attribute\nsample1 = fo.Sample(\n    filepath=\"/path/to/image1.jpg\",\n    camera_info=fo.DynamicEmbeddedDocument(\n        camera_id=\"123456789\",\n        quality=51.0,\n    ),\n)\n\nsample2 = fo.Sample(\n    filepath=\"/path/to/image2.jpg\",\n    camera_info=fo.DynamicEmbeddedDocument(camera_id=\"123456789\"),\n)\n\n# Automatically declares new dynamic attributes as they are encountered\ndataset.add_samples([sample1, sample2], dynamic=True)\n\nassert \"camera_info.quality\" in dataset.get_field_schema(flat=True)\n</code></pre> <p>or you can implicitly declare new fields using <code>set_values()</code> with the <code>dynamic=True</code> flag:</p> <pre><code># Populate a new `description` attribute on each sample in the dataset\ndataset.set_values(\"camera_info.description\", [\"foo\", \"bar\"], dynamic=True)\n\nassert \"camera_info.description\" in dataset.get_field_schema(flat=True)\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#defining-custom-documents-in-modules","title":"Defining custom documents in modules \u00b6","text":"<p>You can also define custom embedded document classes in Python modules and packages that you maintain, using the appropriate types from the <code>fiftyone.core.fields</code> module to declare your fields and their types, defaults, etc.</p> <p>The benefit of this approach over the on-the-fly definition from the previous section is that you can provide extra metadata such as whether fields are <code>required</code> or should have <code>default</code> values if they are not explicitly set during creation.</p> <p>Warning</p> <p>In order to work with datasets containing custom embedded documents defined using this approach, you must configure your <code>module_path</code> in all environments where you intend to work with the datasets that use these classes, not just the environment where you create the dataset.</p> <p>To avoid this requirement, consider defining custom documents on-the-fly instead.</p> <p>For example, suppose you add the following embedded document classes to a <code>foo.bar</code> module:</p> <pre><code>from datetime import datetime\n\nimport fiftyone as fo\n\nclass CameraInfo(fo.EmbeddedDocument):\n    camera_id = fo.StringField(required=True)\n    quality = fo.FloatField()\n    description = fo.StringField()\n\nclass LabelMetadata(fo.DynamicEmbeddedDocument):\n    created_at = fo.DateTimeField(default=datetime.utcnow)\n    model_name = fo.StringField()\n</code></pre> <p>and then <code>foo.bar</code> to FiftyOne\u2019s <code>module_path</code> config setting (see this page for more ways to register this):</p> <pre><code>export FIFTYONE_MODULE_PATH=foo.bar\n\n# Verify module path\nfiftyone config\n</code></pre> <p>You\u2019re now free to use your custom embedded document classes as you please, whether this be top-level sample fields or nested fields:</p> <pre><code>import fiftyone as fo\nimport foo.bar as fb\n\nsample = fo.Sample(\n    filepath=\"/path/to/image.png\",\n    camera_info=fb.CameraInfo(\n        camera_id=\"123456789\",\n        quality=99.0,\n    ),\n    weather=fo.Classification(\n        label=\"sunny\",\n        confidence=0.95,\n        metadata=fb.LabelMetadata(\n            model_name=\"resnet50\",\n            description=\"A dynamic field\",\n        )\n    ),\n)\n\ndataset = fo.Dataset()\ndataset.add_sample(sample)\n\ndataset.name = \"test\"\ndataset.persistent = True\n</code></pre> <p>As long as <code>foo.bar</code> is on your <code>module_path</code>, this dataset can be loaded in future sessions and manipulated as usual:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.load_dataset(\"test\")\nprint(dataset.first())\n</code></pre> <pre><code>&lt;Sample: {\n    'id': '6217b696d181786cff360740',\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n    'last_modified_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n    'camera_info': &lt;CameraInfo: {\n        'camera_id': '123456789',\n        'quality': 99.0,\n        'description': None,\n    }&gt;,\n    'weather': &lt;Classification: {\n        'id': '6217b696d181786cff36073e',\n        'tags': [],\n        'label': 'sunny',\n        'confidence': 0.95,\n        'logits': None,\n        'metadata': &lt;LabelMetadata: {\n            'created_at': datetime.datetime(2022, 2, 24, 16, 47, 18, 10000),\n            'model_name': 'resnet50',\n            'description': 'A dynamic field',\n        }&gt;,\n    }&gt;,\n}&gt;\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#image-datasets","title":"Image datasets \u00b6","text":"<p>Any <code>Sample</code> whose <code>filepath</code> is a file with MIME type <code>image/*</code> is recognized as a image sample, and datasets composed of image samples have media type <code>image</code>:</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/image.png\")\n\ndataset = fo.Dataset()\ndataset.add_sample(sample)\n\nprint(dataset.media_type)  # image\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': '6655ca275e20e244f2c8fe31',\n    'media_type': 'image',\n    'filepath': '/path/to/image.png',\n    'tags': [],\n    'metadata': None,\n    'created_at': datetime.datetime(2024, 7, 22, 5, 15, 8, 122038),\n    'last_modified_at': datetime.datetime(2024, 7, 22, 5, 15, 8, 122038),\n}&gt;\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#example-image-dataset","title":"Example image dataset \u00b6","text":"<p>To get started exploring image datasets, try loading the quickstart dataset from the zoo:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nprint(dataset.count(\"ground_truth.detections\"))  # 1232\nprint(dataset.count(\"predictions.detections\"))  # 5620\nprint(dataset.count_values(\"ground_truth.detections.label\"))\n# {'dog': 15, 'airplane': 24, 'dining table': 15, 'hot dog': 5, ...}\n\nsession = fo.launch_app(dataset)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/using_datasets/#video-datasets","title":"Video datasets \u00b6","text":"<p>Any <code>Sample</code> whose <code>filepath</code> is a file with MIME type <code>video/*</code> is recognized as a video sample, and datasets composed of video samples have media type <code>video</code>:</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/video.mp4\")\n\ndataset = fo.Dataset()\ndataset.add_sample(sample)\n\nprint(dataset.media_type)  # video\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': '6403ccef0a3af5bc780b5a10',\n    'media_type': 'video',\n    'filepath': '/path/to/video.mp4',\n    'tags': [],\n    'metadata': None,\n    'created_at': datetime.datetime(2024, 7, 22, 5, 3, 17, 229263),\n    'last_modified_at': datetime.datetime(2024, 7, 22, 5, 3, 17, 229263),\n    'frames': &lt;Frames: 0&gt;,\n}&gt;\n</code></pre> <p>All video samples have a reserved <code>frames</code> attribute in which you can store frame-level labels and other custom annotations for the video. The <code>frames</code> attribute is a dictionary whose keys are frame numbers and whose values are <code>Frame</code> instances that hold all of the <code>Label</code> instances and other primitive-type fields for the frame.</p> <p>Note</p> <p>FiftyOne uses 1-based indexing for video frame numbers.</p> <p>You can add, modify, and delete labels of any type as well as primitive fields such as integers, strings, and booleans using the same dynamic attribute syntax that you use to interact with samples:</p> <pre><code>frame = fo.Frame(\n    quality=97.12,\n    weather=fo.Classification(label=\"sunny\"),\n    objects=fo.Detections(\n        detections=[\\\n            fo.Detection(label=\"cat\", bounding_box=[0.1, 0.1, 0.2, 0.2]),\\\n            fo.Detection(label=\"dog\", bounding_box=[0.7, 0.7, 0.2, 0.2]),\\\n        ]\n    )\n)\n\n# Add labels to the first frame of the video\nsample.frames[1] = frame\nsample.save()\n</code></pre> <p>Note</p> <p>You must call <code>sample.save()</code> in order to persist changes to the database when editing video samples and/or their frames that are in datasets.</p> <pre><code>&lt;Sample: {\n    'id': '6403ccef0a3af5bc780b5a10',\n    'media_type': 'video',\n    'filepath': '/path/to/video.mp4',\n    'tags': [],\n    'metadata': None,\n    'created_at': datetime.datetime(2024, 7, 22, 5, 3, 17, 229263),\n    'last_modified_at': datetime.datetime(2024, 7, 22, 5, 3, 17, 229263),\n    'frames': &lt;Frames: 1&gt;,    &lt;-- `frames` now contains 1 frame of labels\n}&gt;\n</code></pre> <p>Note</p> <p>The <code>frames</code> attribute of video samples behaves like a defaultdict; a new <code>Frame</code> will be created if the frame number does not exist when you access it.</p> <p>You can iterate over the frames in a video sample using the expected syntax:</p> <pre><code>for frame_number, frame in sample.frames.items():\n    print(frame)\n</code></pre> <pre><code>&lt;Frame: {\n    'id': '6403cd972a54cee076f88bd2',\n    'frame_number': 1,\n    'created_at': datetime.datetime(2024, 7, 22, 5, 3, 40, 839000),\n    'last_modified_at': datetime.datetime(2024, 7, 22, 5, 3, 40, 839000),\n    'quality': 97.12,\n    'weather': &lt;Classification: {\n        'id': '609078d54653b0094e9baa52',\n        'tags': [],\n        'label': 'sunny',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n    'objects': &lt;Detections: {\n        'detections': [\\\n            &lt;Detection: {\\\n                'id': '609078d54653b0094e9baa53',\\\n                'attributes': {},\\\n                'tags': [],\\\n                'label': 'cat',\\\n                'bounding_box': [0.1, 0.1, 0.2, 0.2],\\\n                'mask': None,\\\n                'confidence': None,\\\n                'index': None,\\\n            }&gt;,\\\n            &lt;Detection: {\\\n                'id': '609078d54653b0094e9baa54',\\\n                'attributes': {},\\\n                'tags': [],\\\n                'label': 'dog',\\\n                'bounding_box': [0.7, 0.7, 0.2, 0.2],\\\n                'mask': None,\\\n                'confidence': None,\\\n                'index': None,\\\n            }&gt;,\\\n        ],\n    }&gt;,\n}&gt;\n</code></pre> <p>Notice that the dataset\u2019s summary indicates that the dataset has media type <code>video</code> and includes the schema of any frame fields you add:</p> <pre><code>print(dataset)\n</code></pre> <pre><code>Name:           2021.05.03.18.30.20\nMedia type:     video\nNum samples:    1\nPersistent:     False\nTags:           []\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.VideoMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\nFrame fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    frame_number:     fiftyone.core.fields.FrameNumberField\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    quality:          fiftyone.core.fields.FloatField\n    weather:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n    objects:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n</code></pre> <p>You can retrieve detailed information about the schema of the frames of a video <code>Dataset</code> using <code>dataset.get_frame_field_schema()</code>.</p> <p>The samples in video datasets can be accessed like usual, and the sample\u2019s frame labels can be modified by updating the <code>frames</code> attribute of a <code>Sample</code>:</p> <pre><code>sample = dataset.first()\nfor frame_number, frame in sample.frames.items():\n    frame[\"frame_str\"] = str(frame_number)\n    del frame[\"weather\"]\n    del frame[\"objects\"]\n\nsample.save()\n\nprint(sample.frames[1])\n</code></pre> <pre><code>&lt;Frame: {\n    'id': '6403cd972a54cee076f88bd2',\n    'frame_number': 1,\n    'created_at': datetime.datetime(2024, 7, 22, 5, 3, 40, 839000),\n    'last_modified_at': datetime.datetime(2024, 7, 22, 5, 4, 49, 430051),\n    'quality': 97.12,\n    'weather': None,\n    'objects': None,\n    'frame_str': '1',\n}&gt;\n</code></pre> <p>Note</p> <p>You must call <code>sample.save()</code> in order to persist changes to the database when editing video samples and/or their frames that are in datasets.</p> <p>See this page for more information about building labeled video samples.</p>"},{"location":"fiftyone_concepts/using_datasets/#example-video-dataset","title":"Example video dataset \u00b6","text":"<p>To get started exploring video datasets, try loading the quickstart-video dataset from the zoo:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\nprint(dataset.count(\"frames\"))  # 1279\nprint(dataset.count(\"frames.detections.detections\"))  # 11345\nprint(dataset.count_values(\"frames.detections.detections.label\"))\n# {'vehicle': 7511, 'road sign': 2726, 'person': 1108}\n\nsession = fo.launch_app(dataset)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/using_datasets/#3d-datasets","title":"3D datasets \u00b6","text":"<p>Any <code>Sample</code> whose <code>filepath</code> is a file with extension <code>.fo3d</code> is recognized as a 3D sample, and datasets composed of 3D samples have media type <code>3d</code>.</p> <p>An FO3D file encapsulates a 3D scene constructed using the <code>Scene</code> class, which provides methods to add, remove, and manipulate 3D objects in the scene. A scene is internally represented as a n-ary tree of 3D objects, where each object is a node in the tree. A 3D object is either a 3D mesh, point cloud, or a 3D shape geometry.</p> <p>A scene may be explicitly initialized with additional attributes, such as <code>camera</code>, <code>lights</code>, and <code>background</code>. By default, a scene is created with neutral lighting, and a perspective camera whose <code>up</code> is set to <code>Y</code> axis in a right-handed coordinate system.</p> <p>After a scene is constructed, it should be written to the disk using the <code>scene.write()</code> method, which serializes the scene into an FO3D file.</p> <pre><code>import fiftyone as fo\n\nscene = fo.Scene()\nscene.camera = fo.PerspectiveCamera(up=\"Z\")\n\nmesh = fo.GltfMesh(\"mesh\", \"mesh.glb\")\nmesh.rotation = fo.Euler(90, 0, 0, degrees=True)\n\nsphere1 = fo.SphereGeometry(\"sphere1\", radius=2.0)\nsphere1.position = [-1, 0, 0]\nsphere1.default_material.color = \"red\"\n\nsphere2 = fo.SphereGeometry(\"sphere2\", radius=2.0)\nsphere2.position = [-1, 0, 0]\nsphere2.default_material.color = \"blue\"\n\nscene.add(mesh, sphere1, sphere2)\n\nscene.write(\"/path/to/scene.fo3d\")\n\nsample = fo.Sample(filepath=\"/path/to/scene.fo3d\")\n\ndataset = fo.Dataset()\ndataset.add_sample(sample)\n\nprint(dataset.media_type)  # 3d\n</code></pre> <p>To modify an exising scene, load it via <code>Scene.from_fo3d()</code>, perform any necessary updates, and then re-write it to disk:</p> <pre><code>import fiftyone as fo\n\nscene = fo.Scene.from_fo3d(\"/path/to/scene.fo3d\")\n\nfor node in scene.traverse():\n    if isinstance(node, fo.SphereGeometry):\n        node.visible = False\n\nscene.write(\"/path/to/scene.fo3d\")\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#3d-meshes","title":"3D meshes \u00b6","text":"<p>A 3D mesh is a collection of vertices, edges, and faces that define the shape of a 3D object. Whereas some mesh formats store only the geometry of the mesh, others also store the material properties and textures of the mesh. If a mesh file contains material properties and textures, FiftyOne will automatically load and display them. You may also assign default material for your meshes by setting the <code>default_material</code> attribute of the mesh. In the absence of any material information, meshes are assigned a <code>MeshStandardMaterial</code> with reasonable defaults that can also be dynamically configured from the app. Please refer to <code>material_3d</code> for more details.</p> <p>FiftyOne currently supports <code>GLTF</code>, <code>OBJ</code>, <code>PLY</code>, <code>STL</code>, and <code>FBX 7.x+</code> mesh formats.</p> <p>Note</p> <p>We recommend the <code>GLTF</code> format for 3D meshes where possible, as it is the most compact, efficient, and web-friendly format for storing and transmitting 3D models.</p> <pre><code>import fiftyone as fo\n\nscene = fo.Scene()\n\nmesh1 = fo.GltfMesh(\"mesh1\", \"mesh.glb\")\nmesh1.rotation = fo.Euler(90, 0, 0, degrees=True)\n\nmesh2 = fo.ObjMesh(\"mesh2\", \"mesh.obj\")\nmesh3 = fo.PlyMesh(\"mesh3\", \"mesh.ply\")\nmesh4 = fo.StlMesh(\"mesh4\", \"mesh.stl\")\nmesh5 = fo.FbxMesh(\"mesh5\", \"mesh.fbx\")\n\nscene.add(mesh1, mesh2, mesh3, mesh4, mesh5)\n\nscene.write(\"/path/to/scene.fo3d\")\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#3d-point-clouds","title":"3D point clouds \u00b6","text":"<p>FiftyOne supports the PCD point cloud format. A code snippet to create a PCD object that can be added to a FiftyOne 3D scene is shown below:</p> <pre><code>import fiftyone as fo\n\npcd = fo.PointCloud(\"my-pcd\", \"point-cloud.pcd\")\npcd.default_material.shading_mode = \"custom\"\npcd.default_material.custom_color = \"red\"\npcd.default_material.point_size = 2\n\nscene = fo.Scene()\nscene.add(pcd)\n\nscene.write(\"/path/to/scene.fo3d\")\n</code></pre> <p>You can customize the appearance of a point cloud by setting the <code>default_material</code> attribute of the point cloud object, or dynamically from the app. Please refer to the <code>PointCloudMaterial</code> class for more details.</p> <p>Note</p> <p>If your scene contains multiple point clouds, you can control which point cloud is included in orthographic projections by initializing it with <code>flag_for_projection=True</code>.</p> <p>Here\u2019s how a typical PCD file is structured:</p> <pre><code>import numpy as np\nimport open3d as o3d\n\npoints = np.array([(x1, y1, z1), (x2, y2, z2), ...])\ncolors = np.array([(r1, g1, b1), (r2, g2, b2), ...])\n\npcd = o3d.geometry.PointCloud()\npcd.points = o3d.utility.Vector3dVector(points)\npcd.colors = o3d.utility.Vector3dVector(colors)\n\no3d.io.write_point_cloud(\"/path/to/point-cloud.pcd\", pcd)\n</code></pre> <p>Note</p> <p>When working with modalities such as LIDAR, intensity data is assumed to be encoded in the <code>r</code> channel of the <code>rgb</code> field of the PCD files.</p> <p>When coloring by intensity in the App, the intensity values are automatically scaled to use the full dynamic range of the colorscale.</p>"},{"location":"fiftyone_concepts/using_datasets/#3d-shapes","title":"3D shapes \u00b6","text":"<p>FiftyOne provides a set of primitive 3D shape geometries that can be added to a 3D scene. The following 3D shape geometries are supported:</p> <ul> <li> <p>Box: <code>BoxGeometry</code></p> </li> <li> <p>Sphere: <code>SphereGeometry</code></p> </li> <li> <p>Cylinder: <code>CylinderGeometry</code></p> </li> <li> <p>Plane: <code>PlaneGeometry</code></p> </li> </ul> <p>Similar to meshes and point clouds, shapes can be manipulated by setting their position, rotation, and scale. Their appearance can be customized either by setting the <code>default_material</code> attribute of the shape object, or dynamically from the app.</p> <pre><code>import fiftyone as fo\n\nscene = fo.Scene()\n\nbox = fo.BoxGeometry(\"box\", width=0.5, height=0.5, depth=0.5)\nbox.position = [0, 0, 1]\nbox.default_material.color = \"red\"\n\nsphere = fo.SphereGeometry(\"sphere\", radius=2.0)\nsphere.position = [-1, 0, 0]\nsphere.default_material.color = \"blue\"\n\ncylinder = fo.CylinderGeometry(\"cylinder\", radius_top=0.5, height=1)\ncylinder.position = [0, 1, 0]\n\nplane = fo.PlaneGeometry(\"plane\", width=2, height=2)\nplane.rotation = fo.Euler(90, 0, 0, degrees=True)\n\nscene.add(box, sphere, cylinder, plane)\n\nscene.write(\"/path/to/scene.fo3d\")\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#3d-annotations","title":"3D annotations \u00b6","text":"<p>3D samples may contain any type and number of custom fields, including 3D detections and 3D polylines, which are natively visualizable by the App\u2019s 3D visualizer.</p> <p>Because 3D annotations are stored in dedicated fields of datasets rather than being embedded in FO3D files, they can be queried and filtered via dataset views and in the App just like other primitive/label fields.</p> <pre><code>import fiftyone as fo\n\nscene = fo.Scene()\nscene.add(fo.GltfMesh(\"mesh\", \"mesh.gltf\"))\nscene.write(\"/path/to/scene.fo3d\")\n\ndetection = fo.Detection(\n    label=\"vehicle\",\n    location=[0.47, 1.49, 69.44],\n    dimensions=[2.85, 2.63, 12.34],\n    rotation=[0, -1.56, 0],\n)\n\nsample = fo.Sample(\n    filepath=\"/path/to/scene.fo3d\",\n    ground_truth=fo.Detections(detections=[detection]),\n)\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#orthographic-projection-images","title":"Orthographic projection images \u00b6","text":"<p>In order to visualize 3D datasets in the App\u2019s grid view, you can use <code>compute_orthographic_projection_images()</code> to generate orthographic projection images of each scene:</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.utils3d as fou3d\nimport fiftyone.zoo as foz\n\n# Load an example 3D dataset\ndataset = foz.load_zoo_dataset(\"quickstart-3d\")\n\n# This dataset already has orthographic projections populated, but let's\n# recompute them to demonstrate the idea\nfou3d.compute_orthographic_projection_images(\n    dataset,\n    (-1, 512),  # (width, height) of each image; -1 means aspect-preserving\n    bounds=((-50, -50, -50), (50, 50, 50)),\n    projection_normal=(0, -1, 0),\n    output_dir=\"/tmp/quickstart-3d-proj\",\n    shading_mode=\"height\",\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note that the method also supports grouped datasets that contain 3D slice(s):</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.utils3d as fou3d\nimport fiftyone.zoo as foz\n\n# Load an example group dataset that contains a 3D slice\ndataset = foz.load_zoo_dataset(\"quickstart-groups\")\n\n# Populate orthographic projections\nfou3d.compute_orthographic_projection_images(dataset, (-1, 512), \"/tmp/proj\")\n\ndataset.group_slice = \"pcd\"\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>Orthographic projection images currently only include point clouds, not meshes or 3D shapes.</p> <p>If a scene contains multiple point clouds, you can control which point cloud to project by initializing it with <code>flag_for_projection=True</code>.</p> <p>The above method populates an <code>OrthographicProjectionMetadata</code> field on each sample that contains the path to its projection image and other necessary information to properly visualize it in the App.</p> <p>Refer to the <code>compute_orthographic_projection_images()</code> documentation for available parameters to customize the projections.</p>"},{"location":"fiftyone_concepts/using_datasets/#example-3d-datasets","title":"Example 3D datasets \u00b6","text":"<p>To get started exploring 3D datasets, try loading the quickstart-3d dataset from the zoo:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-3d\")\n\nprint(dataset.count_values(\"ground_truth.label\"))\n# {'bottle': 5, 'stairs': 5, 'keyboard': 5, 'car': 5, ...}\n\nsession = fo.launch_app(dataset)\n</code></pre> <p></p> <p>Also check out the quickstart-groups dataset, which contains a point cloud slice:</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.utils3d as fou3d\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-groups\")\n\n# Populate orthographic projections\nfou3d.compute_orthographic_projection_images(dataset, (-1, 512), \"/tmp/proj\")\n\nprint(dataset.count(\"ground_truth.detections\"))  # 1100\nprint(dataset.count_values(\"ground_truth.detections.label\"))\n# {'Pedestrian': 133, 'Car': 774, ...}\n\ndataset.group_slice = \"pcd\"\nsession = fo.launch_app(dataset)\n</code></pre> <p></p>"},{"location":"fiftyone_concepts/using_datasets/#point-cloud-datasets","title":"Point cloud datasets \u00b6","text":"<p>Warning</p> <p>The <code>point-cloud</code> media type has been deprecated in favor of the 3D media type.</p> <p>While we\u2019ll keep supporting the <code>point-cloud</code> media type for backward compatibility, we recommend using the <code>3d</code> media type for new datasets.</p> <p>Any <code>Sample</code> whose <code>filepath</code> is a PCD file with extension <code>.pcd</code> is recognized as a point cloud sample, and datasets composed of point cloud samples have media type <code>point-cloud</code>:</p> <pre><code>import fiftyone as fo\n\nsample = fo.Sample(filepath=\"/path/to/point-cloud.pcd\")\n\ndataset = fo.Dataset()\ndataset.add_sample(sample)\n\nprint(dataset.media_type)  # point-cloud\nprint(sample)\n</code></pre> <pre><code>&lt;Sample: {\n    'id': '6403ce64c8957c42bc8f9e67',\n    'media_type': 'point-cloud',\n    'filepath': '/path/to/point-cloud.pcd',\n    'tags': [],\n    'metadata': None,\n    'created_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n    'last_modified_at': datetime.datetime(2024, 7, 22, 5, 16, 10, 701907),\n}&gt;\n</code></pre> <p>Point cloud samples may contain any type and number of custom fields, including 3D detections and 3D polylines, which are natively visualizable by the App\u2019s 3D visualizer.</p>"},{"location":"fiftyone_concepts/using_datasets/#datasetviews","title":"DatasetViews \u00b6","text":"<p>Previous sections have demonstrated how to add and interact with <code>Dataset</code> components like samples, fields, and labels. The true power of FiftyOne lies in the ability to search, sort, filter, and explore the contents of a <code>Dataset</code>.</p> <p>Behind this power is the <code>DatasetView</code>. Whenever an operation like <code>match()</code> or <code>sort_by()</code> is applied to a dataset, a <code>DatasetView</code> is returned. As the name implies, a <code>DatasetView</code> is a view into the data in your <code>Dataset</code> that was produced by a series of operations that manipulated your data in different ways.</p> <p>A <code>DatasetView</code> is composed of <code>SampleView</code> objects for a subset of the samples in your dataset. For example, a view may contain only samples with a given tag, or samples whose labels meet a certain criteria.</p> <p>In turn, each <code>SampleView</code> represents a view into the content of the underlying <code>Sample</code> in the dataset. For example, a <code>SampleView</code> may represent the contents of a sample with <code>Detections</code> below a specified threshold filtered out.</p> <p>Learn more about DatasetViews</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\ndataset.compute_metadata()\n\n# Create a view containing the 5 samples from the validation split whose\n# images are &gt;= 48 KB that have the most predictions with confidence &gt; 0.9\ncomplex_view = (\n    dataset\n    .match_tags(\"validation\")\n    .match(F(\"metadata.size_bytes\") &gt;= 48 * 1024)  # &gt;= 48 KB\n    .filter_labels(\"predictions\", F(\"confidence\") &gt; 0.9)\n    .sort_by(F(\"predictions.detections\").length(), reverse=True)\n    .limit(5)\n)\n\n# Check to see how many predictions there are in each matching sample\nprint(complex_view.values(F(\"predictions.detections\").length()))\n# [29, 20, 17, 15, 15]\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#merging-datasets","title":"Merging datasets \u00b6","text":"<p>The <code>Dataset</code> class provides a powerful <code>merge_samples()</code> method that you can use to merge the contents of another <code>Dataset</code> or <code>DatasetView</code> into an existing dataset.</p> <p>By default, samples with the same absolute <code>filepath</code> are merged, and top-level fields from the provided samples are merged in, overwriting any existing values for those fields, with the exception of list fields (e.g., tags) and label list fields (e.g., Detections), in which case the elements of the lists themselves are merged. In the case of label list fields, labels with the same <code>id</code> in both collections are updated rather than duplicated.</p> <p>The <code>merge_samples()</code> method can be configured in numerous ways, including:</p> <ul> <li> <p>Which field to use as a merge key, or an arbitrary function defining the merge key</p> </li> <li> <p>Whether existing samples should be modified or skipped</p> </li> <li> <p>Whether new samples should be added or omitted</p> </li> <li> <p>Whether new fields can be added to the dataset schema</p> </li> <li> <p>Whether list fields should be treated as ordinary fields and merged as a whole rather than merging their elements</p> </li> <li> <p>Whether to merge only specific fields, or all but certain fields</p> </li> <li> <p>Mapping input fields to different field names of this dataset</p> </li> </ul> <p>For example, the following snippet demonstrates merging a new field into an existing dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset1 = foz.load_zoo_dataset(\"quickstart\")\n\n# Create a dataset containing only ground truth objects\ndataset2 = dataset1.select_fields(\"ground_truth\").clone()\n\n# Create a view containing only the predictions\npredictions_view = dataset1.select_fields(\"predictions\")\n\n# Merge the predictions\ndataset2.merge_samples(predictions_view)\n\nprint(dataset1.count(\"ground_truth.detections\"))  # 1232\nprint(dataset2.count(\"ground_truth.detections\"))  # 1232\n\nprint(dataset1.count(\"predictions.detections\"))  # 5620\nprint(dataset2.count(\"predictions.detections\"))  # 5620\n</code></pre> <p>Note that the argument to <code>merge_samples()</code> can be a <code>DatasetView</code>, which means that you can perform possibly-complex transformations to the source dataset to select the desired content to merge.</p> <p>Consider the following variation of the above snippet, which demonstrates a workflow where <code>Detections</code> from another dataset are merged into a dataset with existing <code>Detections</code> in the same field:</p> <pre><code>from fiftyone import ViewField as F\n\n# Create a new dataset that only contains predictions with confidence &gt;= 0.9\ndataset3 = (\n    dataset1\n    .select_fields(\"predictions\")\n    .filter_labels(\"predictions\", F(\"confidence\") &gt; 0.9)\n).clone()\n\n# Create a view that contains only the remaining predictions\nlow_conf_view = dataset1.filter_labels(\"predictions\", F(\"confidence\") &lt; 0.9)\n\n# Merge the low confidence predictions back in\ndataset3.merge_samples(low_conf_view, fields=\"predictions\")\n\nprint(dataset1.count(\"predictions.detections\"))  # 5620\nprint(dataset3.count(\"predictions.detections\"))  # 5620\n</code></pre> <p>Finally, the example below demonstrates the use of a custom merge key to define which samples to merge:</p> <pre><code>import os\n\n# Create a dataset with 100 samples of ground truth labels\ndataset4 = dataset1[50:150].select_fields(\"ground_truth\").clone()\n\n# Create a view with 50 overlapping samples of predictions\npredictions_view = dataset1[:100].select_fields(\"predictions\")\n\n# Merge predictions into dataset, using base filename as merge key and\n# never inserting new samples\ndataset4.merge_samples(\n    predictions_view,\n    key_fcn=lambda sample: os.path.basename(sample.filepath),\n    insert_new=False,\n)\n\nprint(len(dataset4))  # 100\nprint(len(dataset4.exists(\"predictions\")))  # 50\n</code></pre> <p>Note</p> <p>Did you know? You can use <code>merge_dir()</code> to directly merge the contents of a dataset on disk into an existing FiftyOne dataset without first loading it into a temporary dataset and then using <code>merge_samples()</code> to perform the merge.</p>"},{"location":"fiftyone_concepts/using_datasets/#cloning-datasets","title":"Cloning datasets \u00b6","text":"<p>You can use <code>clone()</code> to create a copy of a dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\ndataset2 = dataset.clone()\ndataset2.add_sample_field(\"new_field\", fo.StringField)\n\n# The source dataset is unaffected\nassert \"new_field\" not in dataset.get_field_schema()\n</code></pre> <p>Dataset clones contain deep copies of all samples and dataset-level information in the source dataset. The source media files, however, are not copied.</p> <p>Note</p> <p>Did you know? You can also clone specific subsets of your datasets.</p>"},{"location":"fiftyone_concepts/using_datasets/#batch-updates","title":"Batch updates \u00b6","text":"<p>You are always free to perform any necessary modifications to a <code>Dataset</code> by iterating over it via a Python loop and explicitly performing the edits that you require.</p> <p>However, the <code>Dataset</code> class provides a number of methods that allow you to efficiently perform various common batch actions to your entire dataset.</p>"},{"location":"fiftyone_concepts/using_datasets/#cloning-renaming-clearing-and-deleting-fields","title":"Cloning, renaming, clearing, and deleting fields \u00b6","text":"<p>You can use the <code>clone_sample_field()</code>, <code>rename_sample_field()</code>, <code>clear_sample_field()</code>, and <code>delete_sample_field()</code> methods to efficiently perform common actions on the sample fields of a <code>Dataset</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Clone an existing field\ndataset.clone_sample_field(\"predictions\", \"also_predictions\")\nprint(\"also_predictions\" in dataset.get_field_schema())  # True\n\n# Rename a field\ndataset.rename_sample_field(\"also_predictions\", \"still_predictions\")\nprint(\"still_predictions\" in dataset.get_field_schema())  # True\n\n# Clear a field (sets all values to None)\ndataset.clear_sample_field(\"still_predictions\")\nprint(dataset.count_values(\"still_predictions\"))  # {None: 200}\n\n# Delete a field\ndataset.delete_sample_field(\"still_predictions\")\n</code></pre> <p>You can also use dot notation to manipulate the fields or subfields of embedded documents in your dataset:</p> <pre><code>sample = dataset.first()\n\n# Clone an existing embedded field\ndataset.clone_sample_field(\n    \"predictions.detections.label\",\n    \"predictions.detections.also_label\",\n)\nprint(sample.predictions.detections[0][\"also_label\"])  # \"bird\"\n\n# Rename an embedded field\ndataset.rename_sample_field(\n    \"predictions.detections.also_label\",\n    \"predictions.detections.still_label\",\n)\nprint(sample.predictions.detections[0][\"still_label\"])  # \"bird\"\n\n# Clear an embedded field (sets all values to None)\ndataset.clear_sample_field(\"predictions.detections.still_label\")\nprint(sample.predictions.detections[0][\"still_label\"])  # None\n\n# Delete an embedded field\ndataset.delete_sample_field(\"predictions.detections.still_label\")\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#efficient-batch-edits","title":"Efficient batch edits \u00b6","text":"<p>You are always free to perform arbitrary edits to a <code>Dataset</code> by iterating over its contents and editing the samples directly:</p> <pre><code>import random\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Populate a new field on each sample in the dataset\nfor sample in dataset:\n    sample[\"random\"] = random.random()\n    sample.save()\n\nprint(dataset.count(\"random\"))  # 200\nprint(dataset.bounds(\"random\")) # (0.0007, 0.9987)\n</code></pre> <p>However, the above pattern can be inefficient for large datasets because each <code>sample.save()</code> call makes a new connection to the database.</p> <p>The <code>iter_samples()</code> method provides an <code>autosave=True</code> option that causes all changes to samples emitted by the iterator to be automatically saved using an efficient batch update strategy:</p> <pre><code># Automatically saves sample edits in efficient batches\nfor sample in dataset.select_fields().iter_samples(autosave=True):\n    sample[\"random\"] = random.random()\n</code></pre> <p>Note</p> <p>As the above snippet shows, you should also optimize your iteration by selecting only the required fields.</p> <p>You can configure the default batching strategy that is used via your FiftyOne config, or you can configure the batching strategy on a per-method call basis by passing the optional <code>batch_size</code> and <code>batching_strategy</code> arguments to <code>iter_samples()</code>.</p> <p>You can also use the <code>save_context()</code> method to perform batched edits using the pattern below:</p> <pre><code># Use a context to save sample edits in efficient batches\nwith dataset.save_context() as context:\n    for sample in dataset.select_fields():\n        sample[\"random\"] = random.random()\n        context.save(sample)\n</code></pre> <p>The benefit of the above approach versus passing <code>autosave=True</code> to <code>iter_samples()</code> is that <code>context.save()</code> allows you to be explicit about which samples you are editing, which avoids unnecessary computations if your loop only edits certain samples.</p>"},{"location":"fiftyone_concepts/using_datasets/#setting-values","title":"Setting values \u00b6","text":"<p>Another strategy for performing efficient batch edits is to use <code>set_values()</code> to set a field (or embedded field) on each sample in the dataset in a single batch operation:</p> <pre><code># Delete the field we added earlier\ndataset.delete_sample_field(\"random\")\n\n# Equivalent way to populate the field on each sample in the dataset\nvalues = [random.random() for _ in range(len(dataset))]\ndataset.set_values(\"random\", values)\n\nprint(dataset.count(\"random\"))  # 50\nprint(dataset.bounds(\"random\")) # (0.0041, 0.9973)\n</code></pre> <p>Note</p> <p>When possible, using <code>set_values()</code> is often more efficient than performing the equivalent operation via an explicit iteration over the <code>Dataset</code> because it avoids the need to read <code>Sample</code> instances into memory and sequentially save them.</p> <p>Similarly, you can edit nested sample fields of a <code>Dataset</code> by iterating over the dataset and editing the necessary data:</p> <pre><code># Add a tag to all low confidence predictions in the dataset\nfor sample in dataset:\n    for detection in sample[\"predictions\"].detections:\n        if detection.confidence &lt; 0.06:\n            detection.tags.append(\"low_confidence\")\n\n    sample.save()\n\nprint(dataset.count_label_tags())\n# {'low_confidence': 447}\n</code></pre> <p>However, an equivalent and often more efficient approach is to use <code>values()</code> to extract the slice of data you wish to modify and then use <code>set_values()</code> to save the updated data in a single batch operation:</p> <pre><code># Remove the tags we added in the previous variation\ndataset.untag_labels(\"low_confidence\")\n\n# Load all predicted detections\n# This is a list of lists of `Detection` instances for each sample\ndetections = dataset.values(\"predictions.detections\")\n\n# Add a tag to all low confidence detections\nfor sample_detections in detections:\n    for detection in sample_detections:\n        if detection.confidence &lt; 0.06:\n            detection.tags.append(\"low_confidence\")\n\n# Save the updated predictions\ndataset.set_values(\"predictions.detections\", detections)\n\nprint(dataset.count_label_tags())\n# {'low_confidence': 447}\n</code></pre>"},{"location":"fiftyone_concepts/using_datasets/#setting-label-values","title":"Setting label values \u00b6","text":"<p>Often when working with <code>Label</code> fields, the edits you want to make may be naturally represented as a mapping between label IDs and corresponding attribute values to set on each <code>Label</code> instance. In such cases, you can use <code>set_label_values()</code> to conveniently perform the updates:</p> <pre><code># Grab some random label IDs\nview = dataset.take(5, seed=51)\nlabel_ids = view.values(\"predictions.detections.id\", unwind=True)\n\n# Populate a `random` attribute on all labels\nvalues = {_id: True for _id in label_ids}\ndataset.set_label_values(\"predictions.detections.random\", values)\n\nprint(dataset.count_values(\"predictions.detections.random\"))\n# {True: 111, None: 5509}\n</code></pre>"},{"location":"fiftyone_concepts/using_views/","title":"Dataset Views \u00b6","text":"<p>FiftyOne provides methods that allow you to sort, slice, and search your <code>Dataset</code> using any information that you have added to the <code>Dataset</code>. Performing these actions returns a <code>DatasetView</code> into your <code>Dataset</code> that will show only the samples and labels therein that match your criteria.</p> <p>Note</p> <p><code>DatasetView</code> does not hold its contents in-memory. Views simply store the rule(s) that are applied to extract the content of interest from the underlying <code>Dataset</code> when the view is iterated/aggregated on.</p> <p>This means, for example, that the contents of a <code>DatasetView</code> may change as the underlying <code>Dataset</code> is modified.</p>"},{"location":"fiftyone_concepts/using_views/#overview","title":"Overview \u00b6","text":"<p>A <code>DatasetView</code> is returned whenever any sorting, slicing, or searching operation is performed on a <code>Dataset</code>.</p> <p>You can explicitly create a view that contains an entire dataset via <code>Dataset.view()</code>:</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nview = dataset.view()\n\nprint(view)\n</code></pre> <pre><code>Dataset:        quickstart\nMedia type:     image\nNum samples:    200\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:       fiftyone.core.fields.FloatField\n    predictions:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\nView stages:\n    ---\n</code></pre> <p>You can access specific information about a view in the natural ways:</p> <pre><code>len(view)\n# 200\n\nview.media_type\n# \"image\"\n</code></pre> <p>Like datasets, you access the samples in a view by iterating over it:</p> <pre><code>for sample in view:\n    # Do something with `sample`\n</code></pre> <p>Or, you can access individual samples in a view by their ID or filepath:</p> <pre><code>sample = view.take(1).first()\n\nprint(type(sample))\n# fiftyone.core.sample.SampleView\n\nsame_sample = view[sample.id]\nalso_same_sample = view[sample.filepath]\n\nview[other_sample_id]\n# KeyError: sample non-existent or not in view\n</code></pre> <p>Note</p> <p>Accessing samples in a <code>DatasetView</code> returns <code>SampleView</code> objects, not <code>Sample</code> objects. The two classes are largely interchangeable, but <code>SampleView</code> provides some extra features. See filtering sample contents for more details.</p>"},{"location":"fiftyone_concepts/using_views/#saving-views","title":"Saving views \u00b6","text":"<p>If you find yourself frequently using/recreating certain views, you can use <code>save_view()</code> to save them on your dataset under a name of your choice:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\ndataset.persistent = True\n\n# Create a view\ncats_view = (\n    dataset\n    .select_fields(\"ground_truth\")\n    .filter_labels(\"ground_truth\", F(\"label\") == \"cat\")\n    .sort_by(F(\"ground_truth.detections\").length(), reverse=True)\n)\n\n# Save the view\ndataset.save_view(\"cats-view\", cats_view)\n</code></pre> <p>Then you can conveniently use <code>load_saved_view()</code> to load the view in a future session:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.load_dataset(\"quickstart\")\n\n# Retrieve a saved view\ncats_view = dataset.load_saved_view(\"cats-view\")\nprint(cats_view)\n</code></pre> <p>Note</p> <p>Did you know? You can also save, load, and edit saved views directly from the App!</p> <p>Saved views have certain editable metadata such as a description that you can view via <code>get_saved_view_info()</code> and update via <code>update_saved_view_info()</code>:</p> <pre><code># Get a saved view's editable info\nprint(dataset.get_saved_view_info(\"cats-view\"))\n\n# Update the saved view's name and add a description\ninfo = dict(\n    name=\"still-cats-view\",\n    description=\"a view that only contains cats\",\n)\ndataset.update_saved_view_info(\"cats-view\", info)\n\n# Verify that the info has been updated\nprint(dataset.get_saved_view_info(\"still-cats-view\"))\n</code></pre> <p>You can also use <code>list_saved_views()</code>, <code>has_saved_view()</code>, and <code>delete_saved_view()</code> to manage your saved views.</p> <p>Note</p> <p>Saved views only store the rule(s) used to extract content from the underlying dataset, not the actual content itself. Saving views is cheap. Don\u2019t worry about storage space!</p> <p>Keep in mind, though, that the contents of a saved view may change as the underlying dataset is modified. For example, if a save view contains samples with a certain tag, the view\u2019s contents will change as you add/remove this tag from samples.</p>"},{"location":"fiftyone_concepts/using_views/#view-stages","title":"View stages \u00b6","text":"<p>Dataset views encapsulate a pipeline of logical operations that determine which samples appear in the view (and perhaps what subset of their contents).</p> <p>Each view operation is captured by a <code>ViewStage</code>:</p> <pre><code># List available view operations on a dataset\nprint(dataset.list_view_stages())\n# ['exclude', 'exclude_fields', 'exists', ..., 'skip', 'sort_by', 'take']\n</code></pre> <p>These operations are conveniently exposed as methods on <code>Dataset</code> instances, in which case they create an initial <code>DatasetView</code>:</p> <pre><code># Random set of 100 samples from the dataset\nrandom_view = dataset.take(100)\n\nlen(random_view)\n# 100\n</code></pre> <p>They are also exposed on <code>DatasetView</code> instances, in which case they return another <code>DatasetView</code> with the operation appended to its internal pipeline so that multiple operations can be chained together:</p> <pre><code># Sort `random_view` by filepath\nsorted_random_view = random_view.sort_by(\"filepath\")\n</code></pre> <p>The sections below discuss some interesting view stages in more detail. You can also refer to the <code>fiftyone.core.stages</code> module documentation for examples of using each stage.</p>"},{"location":"fiftyone_concepts/using_views/#slicing","title":"Slicing \u00b6","text":"<p>You can extract a range of <code>Sample</code> instances from a <code>Dataset</code> using <code>skip()</code> and <code>limit()</code> or, equivalently, by using array slicing:</p> <pre><code># Skip the first 2 samples and take the next 3\nrange_view1 = dataset.skip(2).limit(3)\n\n# Equivalently, using array slicing\nrange_view2 = dataset[2:5]\n</code></pre> <p>Samples can be accessed from views in all the same ways as for datasets. This includes using <code>first()</code> and <code>last()</code> to retrieve the first and last samples in a view, respectively, or accessing a sample directly from a <code>DatasetView</code> by its ID or filepath:</p> <pre><code>view = dataset[10:100]\n\nsample10 = view.first()\nsample100 = view.last()\n\nalso_sample10 = view[sample10.id]\nprint(also_sample10.filepath == sample10.filepath)\n# True\n\nalso_sample100 = view[sample100.filepath]\nprint(sample100.id == also_sample100.id)\n# True\n</code></pre> <p>Note that, unlike datasets, <code>SampleView</code> objects are not singletons, since there are an infinite number of possible views into a particular <code>Sample</code>:</p> <pre><code>print(sample10 is also_sample10)\n# False\n</code></pre> <p>Note</p> <p>Accessing a sample by its integer index in a <code>DatasetView</code> is not allowed. The best practice is to lookup individual samples by ID or filepath, or use array slicing to extract a range of samples, and iterate over samples in a view.</p> <pre><code>view[0]\n# KeyError: Accessing samples by numeric index is not supported.\n# Use sample IDs, filepaths, slices, boolean arrays, or a boolean ViewExpression instead\n</code></pre> <p>You can also use boolean array indexing to create a <code>DatasetView</code> into a dataset or view given an array-like of bools defining the samples you wish to extract:</p> <pre><code>import numpy as np\n\n# A boolean array encoding the samples to extract\nbool_array = np.array(dataset.values(\"uniqueness\")) &gt; 0.7\n\nview = dataset[bool_array]\nprint(len(view))\n# 17\n</code></pre> <p>The above syntax is equivalent to the following <code>select()</code> statement:</p> <pre><code>import itertools\n\nids = itertools.compress(dataset.values(\"id\"), bool_array)\nview = dataset.select(ids)\nprint(len(view))\n# 17\n</code></pre> <p>Note that, whenever possible, the above operations are more elegantly implemented using match filters:</p> <pre><code>from fiftyone import ViewField as F\n\n# ViewExpression defining the samples to match\nexpr = F(\"uniqueness\") &gt; 0.7\n\n# Use a match() expression to define the view\nview = dataset.match(expr)\nprint(len(view))\n# 17\n\n# Equivalent: using boolean expression indexing is allowed too\nview = dataset[expr]\nprint(len(view))\n# 17\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#sorting","title":"Sorting \u00b6","text":"<p>You can use <code>sort_by()</code> to sort the samples in a <code>Dataset</code> or <code>DatasetView</code> by a field of interest. The samples in the returned <code>DatasetView</code> can be sorted in ascending or descending order:</p> <pre><code>view = dataset.sort_by(\"filepath\")\nview = dataset.sort_by(\"filepath\", reverse=True)\n</code></pre> <p>You can also sort by expressions!</p> <pre><code>from fiftyone import ViewField as F\n\n# Sort by number of detections in `Detections` field `ground_truth`\nview = dataset.sort_by(F(\"ground_truth.detections\").length(), reverse=True)\n\nprint(len(view.first().ground_truth.detections))  # 39\nprint(len(view.last().ground_truth.detections))  # 0\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#shuffling","title":"Shuffling \u00b6","text":"<p>The samples in a <code>Dataset</code> or <code>DatasetView</code> can be randomly shuffled using <code>shuffle()</code>:</p> <pre><code># Randomly shuffle the order of the samples in the dataset\nview1 = dataset.shuffle()\n</code></pre> <p>An optional <code>seed</code> can be provided to make the shuffle deterministic:</p> <pre><code># Randomly shuffle the samples in the dataset with a fixed seed\n\nview2 = dataset.shuffle(seed=51)\nprint(view2.first().id)\n# 5f31bbfcd0d78c13abe159b1\n\nalso_view2 = dataset.shuffle(seed=51)\nprint(also_view2.first().id)\n# 5f31bbfcd0d78c13abe159b1\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#random-sampling","title":"Random sampling \u00b6","text":"<p>You can extract a random subset of the samples in a <code>Dataset</code> or <code>DatasetView</code> using <code>take()</code>:</p> <pre><code># Take 5 random samples from the dataset\nview1 = dataset.take(5)\n</code></pre> <p>An optional <code>seed</code> can be provided to make the sampling deterministic:</p> <pre><code># Take 5 random samples from the dataset with a fixed seed\n\nview2 = dataset.take(5, seed=51)\nprint(view2.first().id)\n# 5f31bbfcd0d78c13abe159b1\n\nalso_view2 = dataset.take(5, seed=51)\nprint(also_view2.first().id)\n# 5f31bbfcd0d78c13abe159b1\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#filtering","title":"Filtering \u00b6","text":"<p>The real power of <code>DatasetView</code> is the ability to write your own search queries based on your data.</p>"},{"location":"fiftyone_concepts/using_views/#querying-samples","title":"Querying samples \u00b6","text":"<p>You can query for a subset of the samples in a dataset via the <code>match()</code> method. The syntax is:</p> <pre><code>match_view = dataset.match(expression)\n</code></pre> <p>where <code>expression</code> defines the matching expression to use to decide whether to include a sample in the view.</p> <p>FiftyOne provides powerful <code>ViewField</code> and <code>ViewExpression</code> classes that allow you to use native Python operators to define your match expression. Simply wrap the target field of your sample in a <code>ViewField</code> and then apply comparison, logic, arithmetic or array operations to it to create a <code>ViewExpression</code>. You can use dot notation to refer to fields or subfields of the embedded documents in your samples. Any resulting <code>ViewExpression</code> that returns a boolean is a valid expression!</p> <p>The code below shows a few examples. See the API reference for <code>ViewExpression</code> for a full list of supported operations.</p> <pre><code>from fiftyone import ViewField as F\n\n# Populate metadata on all samples\ndataset.compute_metadata()\n\n# Samples whose image is less than 48 KB\nsmall_images_view = dataset.match(F(\"metadata.size_bytes\") &lt; 48 * 1024)\n\n# Samples that contain at least one prediction with confidence above 0.99\n# or whose label ifs \"cat\" or \"dog\"\nmatch = (F(\"confidence\") &gt; 0.99) | (F(\"label\").is_in((\"cat\", \"dog\")))\nmatching_view = dataset.match(\n    F(\"predictions.detections\").filter(match).length() &gt; 0\n)\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#common-filters","title":"Common filters \u00b6","text":"<p>Convenience functions for common queries are also available.</p> <p>Use the <code>match_tags()</code> method to match samples that have the specified tag(s) in their <code>tags</code> field:</p> <pre><code># The validation split of the dataset\nval_view = dataset.match_tags(\"validation\")\n\n# Union of the validation and test splits\nval_test_view = dataset.match_tags((\"validation\", \"test\"))\n</code></pre> <p>Use <code>exists()</code> to only include samples for which a given <code>Field</code> exists and is not <code>None</code>:</p> <pre><code># The subset of samples where predictions have been computed\npredictions_view = dataset.exists(\"predictions\")\n</code></pre> <p>Use <code>select()</code> and <code>exclude()</code> to restrict attention to or exclude samples from a view by their IDs:</p> <pre><code># Get the IDs of two random samples\nsample_ids = [\\\n    dataset.take(1).first().id,\\\n    dataset.take(1).first().id,\\\n]\n\n# Include only samples with the given IDs in the view\nselected_view = dataset.select(sample_ids)\n\n# Exclude samples with the given IDs from the view\nexcluded_view = dataset.exclude(sample_ids)\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#filtering-sample-contents","title":"Filtering sample contents \u00b6","text":"<p>Dataset views can also be used to filter the contents of samples in the view. That\u2019s why <code>DatasetView</code> instances return <code>SampleView</code> objects rather than <code>Sample</code> objects.</p> <p><code>SampleView</code> instances represent the content of your samples in all of the usual ways, with some important caveats:</p> <ul> <li> <p>If you modify the contents of a <code>SampleView</code> and then <code>save()</code> it, any changes that you made to the contents of the <code>SampleView</code> will be reflected in the database.</p> </li> <li> <p>Sample views can exclude fields and filter elements of a field (e.g., omit certain detections from an array of detections in the sample). This means that <code>SampleView</code> instances need not contain all of the information in a sample.</p> </li> <li> <p>Sample views are not singletons and thus you must explicitly <code>reload()</code> them in order to refresh their contents if the underlying sample has been modified elsewhere. However, extracting a <code>SampleView</code> from a <code>DatasetView</code> always returns the updated version of the sample\u2019s contents.</p> </li> </ul> <p>You can use the <code>select_fields()</code> and <code>exclude_fields()</code> stages to select or exclude fields from the returned <code>SampleView</code>:</p> <pre><code>for sample in dataset.select_fields(\"ground_truth\"):\n    print(sample.id)            # OKAY: `id` is always available\n    print(sample.ground_truth)  # OKAY: `ground_truth` was selected\n    print(sample.predictions)   # AttributeError: `predictions` was not selected\n\nfor sample in dataset.exclude_fields(\"predictions\"):\n    print(sample.id)            # OKAY: `id` is always available\n    print(sample.ground_truth)  # OKAY: `ground_truth` was not excluded\n    print(sample.predictions)   # AttributeError: `predictions` was excluded\n</code></pre> <p>The <code>filter_labels()</code> stage is a powerful stage that allows you to filter the contents of <code>Detections</code>, <code>Classifications</code>, <code>Polylines</code>, and <code>Keypoints</code> fields, respectively.</p> <p>Here are some self-contained examples for each task:</p> <p>You can also use the <code>filter_field()</code> stage to filter the contents of arbitrarily-typed fields:</p> <pre><code># Remove tags from samples that don't include the \"validation\" tag\nclean_tags_view = dataset.filter_field(\"tags\", F().contains(\"validation\"))\n</code></pre> <p>Note</p> <p>When you create a <code>DatasetView</code> that contains filtered detections or classifications, the other labels are not removed from the source dataset, even if you <code>save()</code> a <code>SampleView</code> after modifying the filtered detections. This is because each label is updated individually, and other labels in the field are left unchanged.</p> <pre><code>view = dataset.filter_labels(\"predictions\", ...)\n\nfor sample in view:\n    predictions = sample.predictions\n\n    # Modify the detections in the view\n    for detection in predictions.detections:\n        detection[\"new_field\"] = True\n\n    # Other detections in the `predictions` field of the samples that\n    # did not appear in the `view` are not deleted or modified\n    sample.save()\n</code></pre> <p>If you do want to delete data from your samples, assign a new value to the field:</p> <pre><code>view = dataset.filter_labels(\"predictions\", ...)\n\nfor sample in view:\n    sample.predictions = fo.Detections(...)\n\n    # Existing detections in the `predictions` field of the samples\n    # are deleted\n    sample.save()\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#grouping","title":"Grouping \u00b6","text":"<p>You can use <code>group_by()</code> to dynamically group the samples in a collection by a specified field:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\n\n# Take 100 samples and group by ground truth label\nview = dataset.take(100, seed=51).group_by(\"ground_truth.label\")\n\nprint(view.media_type)  # group\nprint(len(view))  # 10\n</code></pre> <p>Note</p> <p>Views generated by <code>group_by()</code> have media type <code>group</code>.</p> <p>By default, the samples in each group are unordered, but you can provide the optional <code>order_by</code> and <code>reverse</code> arguments to <code>group_by()</code> to specify an ordering for the samples in each group:</p> <pre><code># Create an image dataset that contains one sample per frame of the\n# `quickstart-video` dataset\ndataset2 = (\n    foz.load_zoo_dataset(\"quickstart-video\")\n    .to_frames(sample_frames=True)\n    .clone()\n)\n\nprint(len(dataset2))  # 1279\n\n# Group by video ID and order each group by frame number\nview2 = dataset2.group_by(\"sample_id\", order_by=\"frame_number\")\n\nprint(len(view2))  # 10\nprint(view2.values(\"frame_number\"))\n# [1, 1, 1, ..., 1]\n\nsample_id = dataset2.take(1).first().sample_id\nvideo = view2.get_dynamic_group(sample_id)\n\nprint(video.values(\"frame_number\"))\n# [1, 2, 3, ..., 120]\n</code></pre> <p>You can also group by an arbitrary expressions:</p> <pre><code>dataset3 = foz.load_zoo_dataset(\"quickstart\")\n\n# Group samples by the number of ground truth objects they contain\nexpr = F(\"ground_truth.detections\").length()\nview3 = dataset3.group_by(expr)\n\nprint(len(view3))  # 26\nprint(len(dataset3.distinct(expr)))  # 26\n</code></pre> <p>When you iterate over a dynamic grouped view, you get one example from each group. Like any other view, you can chain additional view stages to further refine the view\u2019s contents:</p> <pre><code># Sort the groups by label\nsorted_view = view.sort_by(\"ground_truth.label\")\n\nfor sample in sorted_view:\n    print(sample.ground_truth.label)\n</code></pre> <pre><code>airplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse\nship\ntruck\n</code></pre> <p>In particular, you can use <code>flatten()</code> to unravel the samples in a dynamic grouped view back into a flat view:</p> <pre><code># Unwind the sorted groups back into a flat collection\nflat_sorted_view = sorted_view.flatten()\n\nprint(len(flat_sorted_view))  # 1000\nprint(flat_sorted_view.values(\"ground_truth.label\"))\n# ['airplane', 'airplane', 'airplane', ..., 'truck']\n</code></pre> <p>Note</p> <p>Did you know? When you load dynamic group views in the App, the grid view shows the first example from each group, and you can click on any sample to open the modal and view all samples in the group.</p> <p>You can use <code>get_dynamic_group()</code> to retrieve a view containing the samples with a specific group value of interest:</p> <pre><code>group = view.get_dynamic_group(\"horse\")\nprint(len(group))  # 11\n</code></pre> <p>You can also use <code>iter_dynamic_groups()</code> to iterate over all groups in a dynamic group view:</p> <pre><code>for group in sorted_view.iter_dynamic_groups():\n    print(\"%s: %d\" % (group.first().ground_truth.label, len(group)))\n</code></pre> <pre><code>airplane: 11\nautomobile: 10\nbird: 8\ncat: 12\ndeer: 6\ndog: 7\nfrog: 10\nhorse: 11\nship: 12\ntruck: 13\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#concatenating-views","title":"Concatenating views \u00b6","text":"<p>You can use <code>concat()</code> to concatenate views into the same dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nview1 = dataset.match(F(\"uniqueness\") &lt; 0.2)\nview2 = dataset.match(F(\"uniqueness\") &gt; 0.7)\n\nview = view1.concat(view2)\n\nprint(len(view) == len(view1) + len(view2))  # True\n</code></pre> <p>or you can use the equivalent <code>+</code> syntax sugar:</p> <pre><code>view = view1 + view2\n\nprint(len(view) == len(view1) + len(view2))  # True\n</code></pre> <p>Concatenating generated views such as patches and frames is also allowed:</p> <pre><code>gt_patches = dataset.to_patches(\"ground_truth\")\n\npatches1 = gt_patches[:10]\npatches2 = gt_patches[-10:]\n\npatches = patches1 + patches2\n\nprint(len(patches) == len(patches1) + len(patches2))  # True\n</code></pre> <p>as long as each view is derived from the same root generated view:</p> <pre><code>patches1 = dataset[:10].to_patches(\"ground_truth\")\npatches2 = dataset[-10:].to_patches(\"ground_truth\")\n\npatches = patches1 + patches2  # ERROR: not allowed\n</code></pre> <p>If you concatenate views that use <code>select_fields()</code> or <code>exclude_fields()</code> to manipulate the schema of individual views, the concatenated view will respect the schema of the first view in the chain:</p> <pre><code>view1 = dataset[:10].select_fields()\nview2 = dataset[-10:]\n\nview = view1 + view2\n\n# Fields are omitted from `view2` to match schema of `view1`\nprint(view.last())\n</code></pre> <pre><code>view1 = dataset[:10]\nview2 = dataset[-10:].select_fields()\n\nview = view1 + view2\n\n# Missing fields from `view2` appear as `None` to match schema of `view1`\nprint(view.last())\n</code></pre> <p>Note that <code>concat()</code> will not prevent you from creating concatenated views that contain multiple (possibly filtered) versions of the same <code>Sample</code>, which results in views that contains duplicate sample IDs:</p> <pre><code>sample_id = dataset.first().id\nview = (dataset + dataset).shuffle()\n\nselected_view = view.select(sample_id)\nprint(len(selected_view))  # two samples have the same ID\n</code></pre> <p>Warning</p> <p>The FiftyOne App is not designed to display views with duplicate sample IDs.</p>"},{"location":"fiftyone_concepts/using_views/#date-based-views","title":"Date-based views \u00b6","text":"<p>If your dataset contains date fields, you can construct dataset views that query/filter based on this information by simply writing the appropriate <code>ViewExpression</code>, using <code>date</code>, <code>datetime</code> and <code>timedelta</code> objects to define the required logic.</p> <p>For example, you can use the <code>match()</code> stage to filter a dataset by date as follows:</p> <pre><code>from datetime import datetime, timedelta\n\nimport fiftyone as fo\nfrom fiftyone import ViewField as F\n\ndataset = fo.Dataset()\ndataset.add_samples(\n    [\\\n        fo.Sample(\\\n            filepath=\"image1.png\",\\\n            capture_date=datetime(2021, 8, 24, 1, 0, 0),\\\n        ),\\\n        fo.Sample(\\\n            filepath=\"image2.png\",\\\n            capture_date=datetime(2021, 8, 24, 2, 0, 0),\\\n        ),\\\n        fo.Sample(\\\n            filepath=\"image3.png\",\\\n            capture_date=datetime(2021, 8, 24, 3, 0, 0),\\\n        ),\\\n    ]\n)\n\nquery_date = datetime(2021, 8, 24, 2, 1, 0)\nquery_delta = timedelta(minutes=30)\n\n# Samples with capture date after 2021-08-24 02:01:00\nview = dataset.match(F(\"capture_date\") &gt; query_date)\nprint(view)\n\n# Samples with capture date within 30 minutes of 2021-08-24 02:01:00\nview = dataset.match(abs(F(\"capture_date\") - query_date) &lt; query_delta)\nprint(view)\n</code></pre> <p>Note</p> <p>As the example above demonstrates, <code>ViewExpression</code> instances may contain <code>date</code>, <code>datetime</code> and <code>timedelta</code> objects. Internally, subtracting two dates returns the number of milliseconds between them. Using <code>timedelta</code> allows these units to be abstracted away from the user.</p>"},{"location":"fiftyone_concepts/using_views/#object-patches","title":"Object patches \u00b6","text":"<p>If your dataset contains label list fields like <code>Detections</code> or <code>Polylines</code>, then you can use <code>to_patches()</code> to create views that contain one sample per object patch in a specified label field of your dataset.</p> <p>For example, you can extract patches for all ground truth objects in a detection dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nsession = fo.launch_app(dataset)\n\n# Convert to ground truth patches\ngt_patches = dataset.to_patches(\"ground_truth\")\nprint(gt_patches)\n\n# View patches in the App\nsession.view = gt_patches\n</code></pre> <pre><code>Dataset:     quickstart\nMedia type:  image\nNum patches: 1232\nPatch fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    sample_id:        fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detection)\nView stages:\n    1. ToPatches(field='ground_truth', config=None)\n</code></pre> <p>Note</p> <p>You can pass the optional <code>other_fields</code> parameter to <code>to_patches()</code> to specify additional read-only sample-level fields that each patch should include from their parent samples.</p> <p>Or, you could chain view stages to create a view that contains patches for a filtered set of predictions:</p> <pre><code># Now extract patches for confident person predictions\nperson_patches = (\n    dataset\n    .filter_labels(\n        \"predictions\",\n        (F(\"label\") == \"person\") &amp; (F(\"confidence\") &gt; 0.9)\n    )\n    .to_patches(\"predictions\")\n)\nprint(person_patches)\n\n# View patches in the App\nsession.view = person_patches\n</code></pre> <p>Note</p> <p>Did you know? You can convert to object patches view directly from the App!</p> <p>Object patches views are just like any other dataset view in the sense that:</p> <ul> <li> <p>You can append view stages via the App view bar or views API</p> </li> <li> <p>Any modifications to label tags that you make via the App\u2019s tagging menu or via API methods like <code>tag_labels()</code> and <code>untag_labels()</code> will be reflected on the source dataset</p> </li> <li> <p>Any modifications to the patch labels that you make by iterating over the contents of the view or calling <code>set_values()</code> or <code>set_label_values()</code> will be reflected on the source dataset</p> </li> <li> <p>Calling <code>save()</code>, <code>keep()</code>, or <code>keep_fields()</code> on a patches view (typically one that contains additional view stages that filter or modify its contents) will sync any edits or deletions to the patch labels with the source dataset</p> </li> </ul> <p>However, because object patches views only contain a subset of the contents of a <code>Sample</code> from the source dataset, there are some differences compared to non-patch views:</p> <ul> <li> <p>Tagging or untagging patches (as opposed to their labels) will not affect the tags of the underlying <code>Sample</code></p> </li> <li> <p>Any edits that you make to sample-level fields of object patches views other than the field that defines the patches themselves will not be reflected on the source dataset</p> </li> </ul> <p>Note</p> <p>Did you know? You can export object patches as classification datasets!</p>"},{"location":"fiftyone_concepts/using_views/#evaluation-patches","title":"Evaluation patches \u00b6","text":"<p>If you have run evaluation on predictions from an object detection model, then you can use <code>to_evaluation_patches()</code> to transform the dataset (or a view into it) into a new view that contains one sample for each true positive, false positive, and false negative example.</p> <p>True positive examples will result in samples with both their ground truth and predicted fields populated, while false positive/negative examples will only have one of their corresponding predicted/ground truth fields populated, respectively.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Evaluate `predictions` w.r.t. labels in `ground_truth` field\ndataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\"\n)\n\nsession = fo.launch_app(dataset)\n\n# Convert to evaluation patches\neval_patches = dataset.to_evaluation_patches(\"eval\")\nprint(eval_patches)\n\nprint(eval_patches.count_values(\"type\"))\n# {'fn': 246, 'fp': 4131, 'tp': 986}\n\n# View patches in the App\nsession.view = eval_patches\n</code></pre> <pre><code>Dataset:     quickstart\nMedia type:  image\nNum patches: 5363\nPatch fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    sample_id:        fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    predictions:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    type:             fiftyone.core.fields.StringField\n    iou:              fiftyone.core.fields.FloatField\n    crowd:            fiftyone.core.fields.BooleanField\nView stages:\n    1. ToEvaluationPatches(eval_key='eval', config=None)\n</code></pre> <p>Note</p> <p>You can pass the optional <code>other_fields</code> parameter to <code>to_patches()</code> to specify additional read-only sample-level fields that each patch should include from their parent samples.</p> <p>Refer to the evaluation guide guide for more information about running evaluations and using evaluation patches views to analyze object detection models.</p> <p>Note</p> <p>Did you know? You can convert to evaluation patches view directly from the App!</p> <p>Evaluation patches views are just like any other dataset view in the sense that:</p> <ul> <li> <p>You can append view stages via the App view bar or views API</p> </li> <li> <p>Any modifications to ground truth or predicted label tags that you make via the App\u2019s tagging menu or via API methods like <code>tag_labels()</code> and <code>untag_labels()</code> will be reflected on the source dataset</p> </li> <li> <p>Any modifications to the predicted or ground truth <code>Label</code> elements in the patches view that you make by iterating over the contents of the view or calling <code>set_values()</code> or <code>set_label_values()</code> will be reflected on the source dataset</p> </li> <li> <p>Calling <code>save()</code>, <code>keep()</code>, or <code>keep_fields()</code> on an evaluation patches view (typically one that contains additional view stages that filter or modify its contents) will sync any predicted or ground truth <code>Label</code> edits or deletions with the source dataset</p> </li> </ul> <p>However, because evaluation patches views only contain a subset of the contents of a <code>Sample</code> from the source dataset, there are some differences compared to non-patch views:</p> <ul> <li> <p>Tagging or untagging patches themselves (as opposed to their labels) will not affect the tags of the underlying <code>Sample</code></p> </li> <li> <p>Any edits that you make to sample-level fields of evaluation patches views other than the ground truth/predicted label fields will not be reflected on the source dataset</p> </li> </ul>"},{"location":"fiftyone_concepts/using_views/#video-views","title":"Video views \u00b6","text":"<p>Most view stages naturally support video datasets. For example, stages that refer to fields can be applied to the frame-level fields of video samples by prepending <code>\"frames.\"</code> to the relevant parameters:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\n# Create a view that only contains vehicles\nview = dataset.filter_labels(\"frames.detections\", F(\"label\") == \"vehicle\")\n\n# Compare the number of objects in the view and the source dataset\nprint(dataset.count(\"frames.detections.detections\"))  # 11345\nprint(view.count(\"frames.detections.detections\"))  # 7511\n</code></pre> <p>In addition, FiftyOne provides a variety of dedicated view stages for performing manipulations that are unique to video data.</p>"},{"location":"fiftyone_concepts/using_views/#clip-views","title":"Clip views \u00b6","text":"<p>You can use <code>to_clips()</code> to create views into your video datasets that contain one sample per clip defined by a specific field or expression in a video collection.</p> <p>For example, if you have temporal detection labels on your dataset, then you can create a clips view that contains one sample per temporal segment by simply passing the name of the temporal detection field to <code>to_clips()</code>:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\nsample1 = fo.Sample(\n    filepath=\"video1.mp4\",\n    events=fo.TemporalDetections(\n        detections=[\\\n            fo.TemporalDetection(label=\"meeting\", support=[1, 3]),\\\n            fo.TemporalDetection(label=\"party\", support=[2, 4]),\\\n        ]\n    ),\n)\n\nsample2 = fo.Sample(\n    filepath=\"video2.mp4\",\n    metadata=fo.VideoMetadata(total_frame_count=5),\n    events=fo.TemporalDetections(\n        detections=[\\\n            fo.TemporalDetection(label=\"party\", support=[1, 3]),\\\n            fo.TemporalDetection(label=\"meeting\", support=[3, 5]),\\\n        ]\n    ),\n)\n\ndataset.add_samples([sample1, sample2])\n\n# Create a clips view with one clip per event\nview = dataset.to_clips(\"events\")\nprint(view)\n\n# Verify that one sample per clip was created\nprint(dataset.count(\"events.detections\"))  # 4\nprint(len(view))  # 4\n</code></pre> <pre><code>Dataset:    2021.09.03.09.44.57\nMedia type: video\nNum clips:  4\nClip fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    sample_id:        fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    support:          fiftyone.core.fields.FrameSupportField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.VideoMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    events:           fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\nFrame fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    frame_number:     fiftyone.core.fields.FrameNumberField\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\nView stages:\n    1. ToClips(field_or_expr='events', config=None)\n</code></pre> <p>All clips views contain a top-level <code>support</code> field that contains the <code>[first, last]</code> frame range of the clip within <code>filepath</code>, which points to the source video.</p> <p>Note that the <code>events</code> field, which had type <code>TemporalDetections</code> in the source dataset, now has type <code>Classification</code> in the clips view, since each classification has a one-to-one relationship with its clip.</p> <p>Note</p> <p>You can pass the optional <code>other_fields</code> parameter to <code>to_clips()</code> to specify additional read-only sample-level fields that each clip should include from their parent samples.</p> <p>Note</p> <p>If you edit the <code>support</code> or <code>Classification</code> of a sample in a clips view created from temporal detections, the changes will be applied to the corresponding <code>TemporalDetection</code> in the source dataset.</p> <p>Continuing from the example above, if you would like to see clips only for specific temporal detection labels, you can achieve this by first filtering the labels:</p> <pre><code>from fiftyone import ViewField as F\n\n# Create a clips view with one clip per meeting\nview = (\n    dataset\n    .filter_labels(\"events\", F(\"label\") == \"meeting\")\n    .to_clips(\"events\")\n)\n\nprint(view.values(\"events.label\"))\n# ['meeting', 'meeting']\n</code></pre> <p>Clips views can also be created based on frame-level labels, which provides a powerful query language that you can use to find segments of a video dataset that contain specific frame content of interest.</p> <p>In the simplest case, you can provide the name of a frame-level list field (e.g., <code>Classifications</code> or <code>Detections</code>) to <code>to_clips()</code>, which will create one clip per contiguous range of frames that contain at least one label in the specified field:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\n# Create a view that contains one clip per contiguous range of frames that\n# contains at least one detection\nview = dataset.to_clips(\"frames.detections\")\nprint(view)\n</code></pre> <p>The above view turns out to not be very interesting, since every frame in the <code>quickstart-video</code> dataset contains at least one object. So, instead, lets first filter the objects so that we can construct a clips view that contains one clip per contiguous range of frames that contains at least one person:</p> <pre><code>from fiftyone import ViewField as F\n\n# Create a view that contains one clip per contiguous range of frames that\n# contains at least one person\nview = (\n    dataset\n    .filter_labels(\"frames.detections\", F(\"label\") == \"person\")\n    .to_clips(\"frames.detections\")\n)\nprint(view)\n</code></pre> <pre><code>Dataset:    quickstart-video\nMedia type: video\nNum clips:  8\nClip fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    sample_id:        fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    support:          fiftyone.core.fields.FrameSupportField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.VideoMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\nFrame fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    frame_number:     fiftyone.core.fields.FrameNumberField\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\nView stages:\n    1. FilterLabels(field='frames.detections', filter={'$eq': ['$$this.label', 'person']}, only_matches=True)\n    2. ToClips(field_or_expr='frames.detections', config=None)\n</code></pre> <p>When you iterate over the frames of a sample in a clip view, you will only get the frames within the <code>[first, last]</code> support of each clip:</p> <pre><code>sample = view.last()\n\nprint(sample.support)\n# [116, 120]\n\nframe_numbers = []\nfor frame_number, frame in sample.frames.items():\n    frame_numbers.append(frame_number)\n\nprint(frame_numbers)\n# [116, 117, 118, 119, 120]\n</code></pre> <p>Note</p> <p>Clips views created via <code>to_clips()</code> always contain all frame-level labels from the underlying dataset for their respective frame supports, even if frame-level filtering was applied in previous view stages. In other words, filtering prior to the <code>to_clips()</code> stage only affects the frame supports.</p> <p>You can, however, apply frame-level filtering to clips by appending filtering operations after the <code>to_clips()</code> stage in your view, just like any other view.</p> <p>More generally, you can provide an arbitrary <code>ViewExpression</code> to <code>to_clips()</code> that defines a boolean expression to apply to each frame. In this case, the clips view will contain one clip per contiguous range of frames for which the expression evaluates to true:</p> <pre><code># Create a view that contains one clip per contiguous range of frames that\n# contains at least 10 vehicles\nview = (\n    dataset\n    .filter_labels(\"frames.detections\", F(\"label\") == \"vehicle\")\n    .to_clips(F(\"detections.detections\").length() &gt;= 10)\n)\nprint(view)\n</code></pre> <p>See this section for more information about constructing frame expressions.</p> <p>Note</p> <p>You can pass optional <code>tol</code> and <code>min_len</code> parameters to <code>to_clips()</code> to configure a missing frame tolerance and minimum length for clips generated from frame-level fields or expressions.</p> <p>Clip views are just like any other dataset view in the sense that:</p> <ul> <li> <p>You can append view stages via the App view bar or views API</p> </li> <li> <p>Any modifications to label tags that you make via the App\u2019s tagging menu or via API methods like <code>tag_labels()</code> and <code>untag_labels()</code> will be reflected on the source dataset</p> </li> <li> <p>Any modifications to the frame-level labels in a clips view that you make by iterating over the contents of the view or calling <code>set_values()</code> or <code>set_label_values()</code> will be reflected on the source dataset</p> </li> <li> <p>Calling <code>save()</code>, <code>keep()</code>, or <code>keep_fields()</code> on a clips view (typically one that contains additional view stages that filter or modify its contents) will sync any frame-level edits or deletions with the source dataset</p> </li> </ul> <p>However, because clip views represent only a subset of a <code>Sample</code> from the source dataset, there are some differences compared to non-clip views:</p> <ul> <li> <p>Tagging or untagging clips (as opposed to their labels) will not affect the tags of the underlying <code>Sample</code></p> </li> <li> <p>Any edits that you make to sample-level fields of clip views will not be reflected on the source dataset (except for edits to the <code>support</code> and <code>Classification</code> field populated when generating clip views based on <code>TemporalDetection</code> labels, as described above)</p> </li> </ul>"},{"location":"fiftyone_concepts/using_views/#trajectory-views","title":"Trajectory views \u00b6","text":"<p>You can use <code>to_trajectories()</code> to create views into your video datasets that contain one sample per each unique object trajectory defined by their <code>(label, index)</code> in a frame-level <code>Detections</code> or <code>Polylines</code> field.</p> <p>Trajectory views are a special case of clip views where each clip has been filtered to contain only the identifying object, rather than than all objects with the trajectory\u2019s frame support.</p> <p>For example, if you have frame-level object detections with their <code>index</code> attributes populated, then you can create a trajectories view that contains one clip for each object of a specific type using <code>filter_labels()</code> and <code>to_trajectories()</code> as shown below:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\n# Create a trajectories view for the vehicles in the dataset\ntrajectories = (\n    dataset\n    .filter_labels(\"frames.detections\", F(\"label\") == \"vehicle\")\n    .to_trajectories(\"frames.detections\")\n)\nprint(trajectories)\n\nsession = fo.launch_app(view=trajectories)\n</code></pre> <pre><code>Dataset:    quickstart-video\nMedia type: video\nNum clips:  109\nClip fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    sample_id:        fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    support:          fiftyone.core.fields.FrameSupportField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.VideoMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.odm.embedded_document.DynamicEmbeddedDocument)\nFrame fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    frame_number:     fiftyone.core.fields.FrameNumberField\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\nView stages:\n    1. FilterLabels(field='frames.detections', filter={'$eq': ['$$this.label', 'vehicle']}, only_matches=True, trajectories=False)\n    2. ToTrajectories(field='frames.detections', config=None)\n</code></pre> <p>Warning</p> <p>Trajectory views can contain significantly more frames than their source collection, since the number of frames is now <code>O(# boxes)</code> rather than <code>O(# video frames)</code>.</p>"},{"location":"fiftyone_concepts/using_views/#frame-views","title":"Frame views \u00b6","text":"<p>You can use <code>to_frames()</code> to create image views into your video datasets that contain one sample per frame in the dataset.</p> <p>Note</p> <p>Did you know? Using <code>to_frames()</code> enables you to execute workflows such as model evaluation and Brain methods that only support image collections to the frames of your video datasets!</p> <p>In the simplest case, you can create a view that contains a sample for every frame of the videos in a <code>Dataset</code> or <code>DatasetView</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\nsession = fo.launch_app(dataset)\n\n# Create a frames view for the entire dataset\nframes = dataset.to_frames(sample_frames=True)\nprint(frames)\n\n# Verify that one sample per frame was created\nprint(dataset.sum(\"metadata.total_frame_count\"))  # 1279\nprint(len(frames))  # 1279\n\n# View frames in the App\nsession.view = frames\n</code></pre> <pre><code>Dataset:     quickstart-video\nMedia type:  image\nNum samples: 1279\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    sample_id:        fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    frame_number:     fiftyone.core.fields.FrameNumberField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\nView stages:\n    1. ToFrames(config=None)\n</code></pre> <p>The above example passes the <code>sample_frames=True</code> option to <code>to_frames()</code>, which causes the necessary frames of the input video collection to be sampled into directories of per-frame images on disk when the view is created. For large video datasets, this may take some time and require substantial disk space. The paths to each frame image will also be stored in a <code>filepath</code> field of each <code>Frame</code> of the source collection.</p> <p>Note that, when using the <code>sample_frames=True</code> option, frames that have previously been sampled will not be resampled, so creating frame views into the same dataset will become faster after the frames have been sampled.</p> <p>Note</p> <p>The recommended way to use <code>to_frames()</code> is to first populate the <code>filepath</code> field of each <code>Frame</code> of your dataset offline, either by running it once with the <code>sample_frames=True</code> option or by manually sampling the frames yourself and populating the <code>filepath</code> frame field.</p> <p>Then you can work with frame views efficiently via the default syntax:</p> <pre><code># Creates a view with one sample per frame whose `filepath` is set\nframes = dataset.to_frames()\n</code></pre> <p>More generally, <code>to_frames()</code> exposes a variety of parameters that you can use to configure the behavior of the video-to-image conversion process. You can also combine <code>to_frames()</code> with view stages like <code>match_frames()</code> to achieve fine-grained control over the specific frames you want to study.</p> <p>For example, the snippet below creates a frames view that only contains samples for frames with at least 10 objects, sampling at most one frame per second:</p> <pre><code>from fiftyone import ViewField as F\n\n#\n# Create a frames view that only contains frames with at least 10\n# objects, sampled at a maximum frame rate of 1fps\n#\n\nnum_objects = F(\"detections.detections\").length()\nview = dataset.match_frames(num_objects &gt; 10)\n\nframes = view.to_frames(max_fps=1)\nprint(frames)\n\n# Compare the number of frames in each step\nprint(dataset.count(\"frames\"))  # 1279\nprint(view.count(\"frames\"))  # 354\nprint(len(frames))  # 13\n\n# View frames in the App\nsession.view = frames\n</code></pre> <p>Frame views inherit all frame-level labels from the source video dataset, including their frame number. Each frame sample is also given a <code>sample_id</code> field that records the ID of the parent video sample, and any <code>tags</code> of the parent video sample are also included.</p> <p>Frame views are just like any other image collection view in the sense that:</p> <ul> <li> <p>You can append view stages via the App view bar or views API</p> </li> <li> <p>Any modifications to label tags that you make via the App\u2019s tagging menu or via API methods like <code>tag_labels()</code> and <code>untag_labels()</code> will be reflected on the source dataset</p> </li> <li> <p>Any edits (including additions, modifications, and deletions) to the fields of the samples in a frames view that you make by iterating over the contents of the view or calling <code>set_values()</code> or <code>set_label_values()</code> will be reflected on the source dataset</p> </li> <li> <p>Calling <code>save()</code>, <code>keep()</code>, or <code>keep_fields()</code> on a frames view (typically one that contains additional view stages that filter or modify its contents) will sync any changes to the frames of the underlying video dataset</p> </li> </ul> <p>The only way in which frames views differ from regular image collections is that changes to the <code>tags</code> or <code>metadata</code> fields of frame samples will not be propagated to the frames of the underlying video dataset.</p>"},{"location":"fiftyone_concepts/using_views/#frame-patches-views","title":"Frame patches views \u00b6","text":"<p>Since frame views into video datasets behave just like any other view, you can chain <code>to_frames()</code> and <code>to_patches()</code> to create frame patch views into your video datasets that contain one sample per object patch in the frames of the dataset!</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\nsession = fo.launch_app(dataset)\n\n# Create a frames view\nframes = dataset.to_frames(sample_frames=True)\n\n# Create a frame patches view\nframe_patches = frames.to_patches(\"detections\")\nprint(frame_patches)\n\n# Verify that one sample per object was created\nprint(dataset.count(\"frames.detections.detections\"))  # 11345\nprint(len(frame_patches))  # 11345\n\n# View frame patches in the App\nsession.view = frame_patches\n</code></pre> <pre><code>Dataset:     quickstart-video\nMedia type:  image\nNum patches: 11345\nPatch fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    sample_id:        fiftyone.core.fields.ObjectIdField\n    frame_id:         fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    frame_number:     fiftyone.core.fields.FrameNumberField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detection)\nView stages:\n    1. ToFrames(config=None)\n    2. ToPatches(field='detections', config=None)\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#querying-frames","title":"Querying frames \u00b6","text":"<p>You can query for a subset of the frames in a video dataset via <code>match_frames()</code>. The syntax is:</p> <pre><code>match_view = dataset.match_frames(expression)\n</code></pre> <p>where <code>expression</code> defines the matching expression to use to decide whether to include a frame in the view.</p> <p>FiftyOne provides powerful <code>ViewField</code> and <code>ViewExpression</code> classes that allow you to use native Python operators to define your match expression. Simply wrap the target frame field in a <code>ViewField</code> and then apply comparison, logic, arithmetic or array operations to it to create a <code>ViewExpression</code>. You can use dot notation to refer to fields or subfields of the embedded documents in your frames. Any resulting <code>ViewExpression</code> that returns a boolean is a valid expression!</p> <p>The snippet below demonstrates a possible workflow. See the API reference for <code>ViewExpression</code> for a full list of supported operations.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\n# Create a view that only contains frames with at least 10 objects\nnum_objects = F(\"detections.detections\").length()\nview = dataset.match_frames(num_objects &gt; 10)\n\n# Compare the number of frames in each collection\nprint(dataset.count(\"frames\"))  # 1279\nprint(view.count(\"frames\"))  # 354\n</code></pre> <p>You can also use <code>select_frames()</code> and <code>exclude_frames()</code> to restrict attention to or exclude frames from a view by their IDs:</p> <pre><code># Get the IDs of a couple frames\nframe_ids = [\\\n    dataset.first().frames.first().id,\\\n    dataset.last().frames.last().id,\\\n]\n\n# Select only the specified frames\nselected_view = dataset.select_frames(frame_ids)\n\n# Exclude frames with the given IDs from the view\nexcluded_view = dataset.exclude_frames(frame_ids)\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#similarity-views","title":"Similarity views \u00b6","text":"<p>If your dataset is indexed by similarity, then you can use the <code>sort_by_similarity()</code> stage to programmatically query your data by similarity to image(s) or object patch(es) of interest.</p>"},{"location":"fiftyone_concepts/using_views/#image-similarity","title":"Image similarity \u00b6","text":"<p>The example below indexes a dataset by image similarity using <code>compute_similarity()</code> and then uses <code>sort_by_similarity()</code> to sort the dataset by similarity to a chosen image:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Index the dataset by image similarity\nfob.compute_similarity(dataset, brain_key=\"image_sim\")\n\nsession = fo.launch_app(dataset)\n\n# Select a random query image\nquery_id = dataset.take(1).first().id\n\n# Sort the samples by similarity to the query image\nview = dataset.sort_by_similarity(query_id, brain_key=\"image_sim\")\nprint(view)\n\n# View results in the App\nsession.view = view\n</code></pre> <p>Note</p> <p>Refer to the Brain guide for more information about generating similarity indexes, and check out the App guide to see how to sort images by similarity via point-and-click in the App!</p>"},{"location":"fiftyone_concepts/using_views/#object-similarity","title":"Object similarity \u00b6","text":"<p>The example below indexes the objects in a <code>Detections</code> field of a dataset by similarity using <code>compute_similarity()</code> and then uses <code>sort_by_similarity()</code> to retrieve the 15 most similar objects to a chosen object:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Index the dataset by `ground_truth` object similarity\nfob.compute_similarity(\n    dataset, patches_field=\"ground_truth\", brain_key=\"gt_sim\"\n)\n\n# Convert to ground truth patches view\npatches = dataset.to_patches(\"ground_truth\")\n\n# View patches in the App\nsession = fo.launch_app(view=patches)\n\n# Select a random query object\nquery_id = patches.take(1).first().id\n\n# Retrieve the 15 most similar objects\nsimilar_objects = patches.sort_by_similarity(query_id, k=15, brain_key=\"gt_sim\")\n\n# View results in the App\nsession.view = similar_objects\n</code></pre> <p>Note</p> <p>Refer to the Brain guide for more information about generating similarity indexes, and check out the App guide to see how to sort objects by similarity via point-and-click in the App!</p>"},{"location":"fiftyone_concepts/using_views/#text-similarity","title":"Text similarity \u00b6","text":"<p>When you create a similarity index powered by the CLIP model, you can pass arbitrary natural language queries to <code>sort_by_similarity()</code> along with the <code>brain_key</code> of a compatible similarity index:</p> <p>You can verify that a similarity index supports text queries by checking that it <code>supports_prompts</code>:</p> <pre><code>info = dataset.get_brain_info(brain_key)\nprint(info.config.supports_prompts)  # True\n</code></pre> <p>Note</p> <p>Refer to the Brain guide for more information about generating similarity indexes, and check out the App guide to see how to sort objects by text similarity via point-and-click in the App!</p>"},{"location":"fiftyone_concepts/using_views/#geolocation","title":"Geolocation \u00b6","text":"<p>If your samples have geolocation data, then you can use the <code>geo_near()</code> and <code>geo_within()</code> stages to filter your data based on their location.</p> <p>For example, you can use <code>geo_near()</code> to sort your samples by proximity to a location:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\nTIMES_SQUARE = [-73.9855, 40.7580]\n\ndataset = foz.load_zoo_dataset(\"quickstart-geo\")\n\n# Sort the samples by their proximity to Times Square, and only include\n# samples within 5km\nview = dataset.geo_near(TIMES_SQUARE, max_distance=5000)\n</code></pre> <p>Or, you can use <code>geo_within()</code> to only include samples that lie within a longitude-latitude polygon of your choice:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\nMANHATTAN = [\\\n    [\\\n        [-73.949701, 40.834487],\\\n        [-73.896611, 40.815076],\\\n        [-73.998083, 40.696534],\\\n        [-74.031751, 40.715273],\\\n        [-73.949701, 40.834487],\\\n    ]\\\n]\n\ndataset = foz.load_zoo_dataset(\"quickstart-geo\")\n\n# Only contains samples in Manhattan\nview = dataset.geo_within(MANHATTAN)\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#tagging-contents","title":"Tagging contents \u00b6","text":"<p>You can use the <code>tag_samples()</code> and <code>untag_samples()</code> methods to add or remove sample tags from the samples in a view:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\ndataset.untag_samples(\"validation\") # remove pre-existing tags\n\n# Perform a random 90-10 test-train split\ndataset.take(0.1 * len(dataset)).tag_samples(\"test\")\ndataset.match_tags(\"test\", bool=False).tag_samples(\"train\")\n\nprint(dataset.count_sample_tags())\n# {'train': 180, 'test': 20}\n</code></pre> <p>You can also use the <code>tag_labels()</code> and <code>untag_labels()</code> methods to add or remove label tags from the labels in one or more fields of a view:</p> <pre><code># Add a tag to all low confidence predictions\nview = dataset.filter_labels(\"predictions\", F(\"confidence\") &lt; 0.06)\nview.tag_labels(\"low_confidence\", label_fields=\"predictions\")\n\nprint(dataset.count_label_tags())\n# {'low_confidence': 447}\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#editing-fields","title":"Editing fields \u00b6","text":"<p>You can perform arbitrary edits to a <code>DatasetView</code> by iterating over its contents and editing the samples directly:</p> <pre><code>import random\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nview = dataset.limit(50)\n\n# Populate a new field on each sample in the view\nfor sample in view:\n    sample[\"random\"] = random.random()\n    sample.save()\n\nprint(dataset.count(\"random\"))  # 50\nprint(dataset.bounds(\"random\")) # (0.0005, 0.9928)\n</code></pre> <p>However, the above pattern can be inefficient for large views because each <code>sample.save()</code> call makes a new connection to the database.</p> <p>The <code>iter_samples()</code> method provides an <code>autosave=True</code> option that causes all changes to samples emitted by the iterator to be automatically saved using an efficient batch update strategy:</p> <pre><code># Automatically saves sample edits in efficient batches\nfor sample in view.select_fields().iter_samples(autosave=True):\n    sample[\"random\"] = random.random()\n</code></pre> <p>Note</p> <p>As the above snippet shows, you should also optimize your iteration by selecting only the required fields.</p> <p>You can configure the default batching strategy that is used via your FiftyOne config, or you can configure the batching strategy on a per-method call basis by passing the optional <code>batch_size</code> and <code>batching_strategy</code> arguments to <code>iter_samples()</code>.</p> <p>You can also use the <code>save_context()</code> method to perform batched edits using the pattern below:</p> <pre><code># Use a context to save sample edits in efficient batches\nwith view.save_context() as context:\n    for sample in view.select_fields():\n        sample[\"random\"] = random.random()\n        context.save(sample)\n</code></pre> <p>The benefit of the above approach versus passing <code>autosave=True</code> to <code>iter_samples()</code> is that <code>context.save()</code> allows you to be explicit about which samples you are editing, which avoids unnecessary computations if your loop only edits certain samples.</p> <p>Another strategy for performing efficient batch edits is to use <code>set_values()</code> to set a field (or embedded field) on each sample in the collection in a single batch operation:</p> <pre><code># Delete the field we added earlier\ndataset.delete_sample_field(\"random\")\n\n# Equivalent way to populate a new field on each sample in a view\nvalues = [random.random() for _ in range(len(view))]\nview.set_values(\"random\", values)\n\nprint(dataset.count(\"random\"))  # 50\nprint(dataset.bounds(\"random\")) # (0.0272, 0.9921)\n</code></pre> <p>Note</p> <p>When possible, using <code>set_values()</code> is often more efficient than performing the equivalent operation via an explicit iteration over the <code>DatasetView</code> because it avoids the need to read <code>SampleView</code> instances into memory and sequentially save them.</p> <p>Naturally, you can edit nested sample fields of a <code>DatasetView</code> by iterating over the view and editing the necessary data:</p> <pre><code># Create a view that contains only low confidence predictions\nview = dataset.filter_labels(\"predictions\", F(\"confidence\") &lt; 0.06)\n\n# Add a tag to all predictions in the view\nfor sample in view:\n    for detection in sample[\"predictions\"].detections:\n        detection.tags.append(\"low_confidence\")\n\n    sample.save()\n\nprint(dataset.count_label_tags())\n# {'low_confidence': 447}\n</code></pre> <p>However, an equivalent and often more efficient approach is to use <code>values()</code> to extract the slice of data you wish to modify and then use <code>set_values()</code> to save the updated data in a single batch operation:</p> <pre><code># Remove the tags we added in the previous variation\ndataset.untag_labels(\"low_confidence\")\n\n# Load all predicted detections\n# This is a list of lists of `Detection` instances for each sample\ndetections = view.values(\"predictions.detections\")\n\n# Add a tag to all low confidence detections\nfor sample_detections in detections:\n    for detection in sample_detections:\n        detection.tags.append(\"low_confidence\")\n\n# Save the updated predictions\nview.set_values(\"predictions.detections\", detections)\n\nprint(dataset.count_label_tags())\n# {'low_confidence': 447}\n</code></pre> <p>In the particular case of updating the attributes of a <code>Label</code> field of your dataset, the edits may be most naturally represented as a mapping between label IDs and corresponding attribute values to set on each label. In such cases, you can use <code>set_label_values()</code> to conveniently perform the updates:</p> <pre><code># Grab the IDs of all labels in `view`\nlabel_ids = view.values(\"predictions.detections.id\", unwind=True)\n\n# Populate an `is_low_conf` attribute on all of the labels\nvalues = {_id: True for _id in label_ids}\ndataset.set_label_values(\"predictions.detections.is_low_conf\", values)\n\nprint(dataset.count_values(\"predictions.detections.is_low_conf\"))\n# {True: 447, None: 5173}\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#transforming-fields","title":"Transforming fields \u00b6","text":"<p>In certain situations, you may wish to temporarily modify the values of sample fields in the context of a <code>DatasetView</code> without modifying the underlying dataset. FiftyOne provides the <code>set_field()</code> and <code>map_labels()</code> methods for this purpose.</p> <p>For example, suppose you would like to rename a group of labels to a single category in order to run your evaluation routine. You can use <code>map_labels()</code> to do this:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nANIMALS = [\\\n    \"bear\", \"bird\", \"cat\", \"cow\", \"dog\", \"elephant\", \"giraffe\",\\\n    \"horse\", \"sheep\", \"zebra\"\\\n]\n\n# Replace all animal detection's labels with \"animal\"\nmapping = {k: \"animal\" for k in ANIMALS}\nanimals_view = dataset.map_labels(\"predictions\", mapping)\n\ncounts = animals_view.count_values(\"predictions.detections.label\")\nprint(counts[\"animal\"])\n# 529\n</code></pre> <p>Or, suppose you would like to lower bound all confidences of objects in the <code>predictions</code> field of a dataset. You can use <code>set_field()</code> to do this:</p> <pre><code># Lower bound all confidences in the `predictions` field to 0.5\nbounded_view = dataset.set_field(\n    \"predictions.detections.confidence\",\n    F(\"confidence\").max(0.5),\n)\n\nprint(bounded_view.bounds(\"predictions.detections.confidence\"))\n# (0.5, 0.9999035596847534)\n</code></pre> <p>Note</p> <p>In order to populate a new field using <code>set_field()</code>, you must first declare the new field on the dataset via <code>add_sample_field()</code>:</p> <pre><code># Record the number of predictions in each sample in a new field\ndataset.add_sample_field(\"num_predictions\", fo.IntField)\nview = dataset.set_field(\"num_predictions\", F(\"predictions.detections\").length())\n\nview.save(\"num_predictions\")  # save the new field's values on the dataset\nprint(dataset.bounds(\"num_predictions\"))  # (1, 100)\n</code></pre> <p>The <code>ViewExpression</code> language is quite powerful, allowing you to define complex operations without needing to write an explicit Python loop to perform the desired manipulation.</p> <p>For example, the snippet below visualizes the top-5 highest confidence predictions for each sample in the dataset:</p> <pre><code>from fiftyone import ViewField as F\n\n# Extracts the 5 highest confidence predictions for each sample\ntop5_preds = F(\"detections\").sort(\"confidence\", reverse=True)[:5]\n\ntop5_view = (\n    dataset\n    .set_field(\"predictions.detections\", top5_preds)\n    .select_fields(\"predictions\")\n)\n\nsession = fo.launch_app(view=top5_view)\n</code></pre> <p>If you want to permanently save transformed view fields to the underlying dataset, you can do so by calling <code>save()</code> on the view and optionally passing the name(s) of specific field(s) that you want to save:</p> <pre><code># Saves `predictions` field's contents in the view permanently to dataset\ntop5_view.save(\"predictions\")\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#saving-and-cloning","title":"Saving and cloning \u00b6","text":"<p>Ordinarily, when you define a <code>DatasetView</code> that extracts a specific subset of a dataset and its fields, the underlying <code>Dataset</code> is not modified. However, you can use <code>save()</code> to save the contents of a view you\u2019ve created to the underlying dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Capitalize some labels\nrename_view = dataset.map_labels(\"predictions\", {\"cat\": \"CAT\", \"dog\": \"DOG\"})\nrename_view.save()\n\nprint(dataset.count_values(\"predictions.detections.label\"))\n# {'CAT': 35, 'DOG': 49, ...}\n\n# Discard all predictions with confidence below 0.3\nhigh_conf_view = dataset.filter_labels(\n    \"predictions\", F(\"confidence\") &gt; 0.3, only_matches=False\n)\nhigh_conf_view.save()\n\nprint(dataset.bounds(\"predictions.detections.confidence\"))\n# (0.3001, 0.9999)\n</code></pre> <p>Note that calling <code>save()</code> on a <code>DatasetView</code> will only save modifications to samples that are in the view; all other samples are left unchanged.</p> <p>You can use <code>keep()</code> to delete samples from the underlying dataset that do not appear in a view you created:</p> <pre><code>print(len(dataset))\n# 200\n\n# Discard all samples with no people\npeople_view = dataset.filter_labels(\"ground_truth\", F(\"label\") == \"person\")\npeople_view.keep()\n\nprint(len(dataset))\n# 94\n</code></pre> <p>and you can use <code>keep_fields()</code> to delete any sample/frame fields from the underlying dataset that you have excluded from a view you created:</p> <pre><code># Delete the `predictions` field\nview = dataset.exclude_fields(\"predictions\")\nview.keep_fields()\n\nprint(dataset)\n\n# Delete all non-default fields\nview = dataset.select_fields()\nview.keep_fields()\n\nprint(dataset)\n</code></pre> <p>Alternatively, you can use <code>clone()</code> to create a new <code>Dataset</code> that contains a copy of (only) the contents of a <code>DatasetView</code>:</p> <pre><code># Reload full quickstart dataset\ndataset.delete()\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Create a new dataset that contains only the high confidence predictions\nhigh_conf_view = dataset.filter_labels(\"predictions\", F(\"confidence\") &gt; 0.3)\nhigh_conf_dataset = high_conf_view.clone()\n\nprint(high_conf_dataset.bounds(\"predictions.detections.confidence\"))\n# (0.3001, 0.9999)\n</code></pre> <p>You can also use <code>clone_sample_field()</code> to copy the contents of a view\u2019s field into a new field of the underlying <code>Dataset</code>:</p> <pre><code>print(dataset.count(\"predictions.detections\"))  # 5620\n\n# Make view containing only high confidence predictions\nview = dataset.filter_labels(\"predictions\", F(\"confidence\") &gt; 0.5)\nprint(view.count(\"predictions.detections\"))  # 1564\n\n# Copy high confidence predictions to a new field\nview.clone_sample_field(\"predictions\", \"high_conf_predictions\")\nprint(dataset.count(\"high_conf_predictions.detections\"))  # 1564\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#tips-tricks","title":"Tips &amp; tricks \u00b6","text":""},{"location":"fiftyone_concepts/using_views/#chaining-view-stages","title":"Chaining view stages \u00b6","text":"<p>View stages can be chained together to perform complex operations:</p> <pre><code>from fiftyone import ViewField as F\n\n# Extract the first 5 samples with the \"validation\" tag, alphabetically by\n# filepath, whose images are &gt;= 48 KB\ncomplex_view = (\n    dataset\n    .match_tags(\"validation\")\n    .exists(\"metadata\")\n    .match(F(\"metadata.size_bytes\") &gt;= 48 * 1024)  # &gt;= 48 KB\n    .sort_by(\"filepath\")\n    .limit(5)\n)\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#filtering-detections-by-area","title":"Filtering detections by area \u00b6","text":"<p>Need to filter your detections by bounding box area? Use this <code>ViewExpression</code>!</p> <pre><code>from fiftyone import ViewField as F\n\n# Bboxes are in [top-left-x, top-left-y, width, height] format\nbbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\n\n# Only contains boxes whose area is between 5% and 50% of the image\nmedium_boxes_view = dataset.filter_labels(\n    \"predictions\", (0.05 &lt;= bbox_area) &amp; (bbox_area &lt; 0.5)\n)\n</code></pre> <p>FiftyOne stores bounding box coordinates as relative values in <code>[0, 1]</code>. However, you can use the expression below to filter by absolute pixel area:</p> <pre><code>from fiftyone import ViewField as F\n\ndataset.compute_metadata()\n\n# Computes the area of each bounding box in pixels\nbbox_area = (\n    F(\"$metadata.width\") * F(\"bounding_box\")[2] *\n    F(\"$metadata.height\") * F(\"bounding_box\")[3]\n)\n\n# Only contains boxes whose area is between 32^2 and 96^2 pixels\nmedium_boxes_view = dataset.filter_labels(\n    \"predictions\", (32 ** 2 &lt; bbox_area) &amp; (bbox_area &lt; 96 ** 2)\n)\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#removing-a-batch-of-samples-from-a-dataset","title":"Removing a batch of samples from a dataset \u00b6","text":"<p>You can easily remove a batch of samples from a <code>Dataset</code> by constructing a <code>DatasetView</code> that contains the samples, and then deleting them from the dataset as follows:</p> <pre><code># Choose 10 samples at random\nunlucky_samples = dataset.take(10)\n\ndataset.delete_samples(unlucky_samples)\n</code></pre>"},{"location":"fiftyone_concepts/using_views/#efficiently-iterating-samples","title":"Efficiently iterating samples \u00b6","text":"<p>If you have a dataset with larger fields, such as <code>Classifications</code> or <code>Detections</code>, it can be expensive to load entire samples into memory. If, for a particular use case, you are only interested in a subset of fields, you can use <code>Dataset.select_fields()</code> to load only the fields of interest.</p> <p>Let\u2019s say you have a dataset that looks like this:</p> <pre><code>Name:        open-images-v4-test\nMedia type:  image\nNum samples: 1000\nPersistent:  True\nTags:        []\nSample fields:\n    id:                       fiftyone.core.fields.ObjectIdField\n    filepath:                 fiftyone.core.fields.StringField\n    tags:                     fiftyone.core.fields.ListField(StringField)\n    metadata:                 fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:               fiftyone.core.fields.DateTimeField\n    last_modified_at:         fiftyone.core.fields.DateTimeField\n    open_images_id:           fiftyone.core.fields.StringField\n    groundtruth_image_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n    groundtruth_detections:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    faster_rcnn:              fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    mAP:                      fiftyone.core.fields.FloatField\n    AP_per_class:             fiftyone.core.fields.DictField\n</code></pre> <p>and you want to get a list of <code>open_images_id</code>\u2019s for all samples in the dataset. Loading other fields is unnecessary; in fact, using <code>Dataset.select_fields()</code> to load only the <code>open_images_id</code> field speeds up the operation below by ~200X!</p> <pre><code>import time\n\nstart = time.time()\noids = []\nfor sample in dataset:\n    oids.append(sample.open_images_id)\n\nprint(time.time() - start)\n# 38.212332010269165\n\nstart = time.time()\noids = []\nfor sample in dataset.select_fields(\"open_images_id\"):\n    oids.append(sample.open_images_id)\n\nprint(time.time() - start)\n# 0.20824909210205078\n</code></pre>"},{"location":"fiftyone_concepts/dataset_creation/","title":"Loading data into FiftyOne \u00b6","text":"<p>The first step to using FiftyOne is to load your data into a dataset. FiftyOne supports automatic loading of datasets stored in various common formats. If your dataset is stored in a custom format, don\u2019t worry, FiftyOne also provides support for easily loading datasets in custom formats.</p> <p>Check out the sections below to see which import pattern is the best fit for your data.</p> <p>Note</p> <p>Did you know? You can import media and/or labels from within the FiftyOne App by installing the @voxel51/io plugin!</p> <p>Note</p> <p>When you create a <code>Dataset</code>, its samples and all of their fields (metadata, labels, custom fields, etc.) are written to FiftyOne\u2019s backing database.</p> <p>Important: Samples only store the <code>filepath</code> to the media, not the raw media itself. FiftyOne does not create duplicate copies of your data!</p>"},{"location":"fiftyone_concepts/dataset_creation/#common-formats","title":"Common formats \u00b6","text":"<p>If your data is stored on disk in one of the many common formats supported natively by FiftyOne, then you can automatically load your data into a <code>Dataset</code> via the following simple pattern:</p> <pre><code>import fiftyone as fo\n\n# A name for the dataset\nname = \"my-dataset\"\n\n# The directory containing the dataset to import\ndataset_dir = \"/path/to/dataset\"\n\n# The type of the dataset being imported\ndataset_type = fo.types.COCODetectionDataset  # for example\n\ndataset = fo.Dataset.from_dir(\n    dataset_dir=dataset_dir,\n    dataset_type=dataset_type,\n    name=name,\n)\n</code></pre> <p>Note</p> <p>Check out this page for more details about loading datasets from disk in common formats!</p>"},{"location":"fiftyone_concepts/dataset_creation/#custom-formats","title":"Custom formats \u00b6","text":"<p>The simplest and most flexible approach to loading your data into FiftyOne is to iterate over your data in a simple Python loop, create a <code>Sample</code> for each data + label(s) pair, and then add those samples to a <code>Dataset</code>.</p> <p>FiftyOne provides label types for common tasks such as classification, detection, segmentation, and many more. The examples below give you a sense of the basic workflow for a few tasks:</p> <p>Note that using <code>Dataset.add_samples()</code> to add batches of samples to your datasets can be significantly more efficient than adding samples one-by-one via <code>Dataset.add_sample()</code>.</p> <p>Note</p> <p>If you use the same custom data format frequently in your workflows, then writing a custom dataset importer is a great way to abstract and streamline the loading of your data into FiftyOne.</p>"},{"location":"fiftyone_concepts/dataset_creation/#loading-images","title":"Loading images \u00b6","text":"<p>If you\u2019re just getting started with a project and all you have is a bunch of image files, you can easily load them into a FiftyOne dataset and start visualizing them in the App:</p>"},{"location":"fiftyone_concepts/dataset_creation/#loading-videos","title":"Loading videos \u00b6","text":"<p>If you\u2019re just getting started with a project and all you have is a bunch of video files, you can easily load them into a FiftyOne dataset and start visualizing them in the App:</p>"},{"location":"fiftyone_concepts/dataset_creation/#model-predictions","title":"Model predictions \u00b6","text":"<p>Once you\u2019ve created a dataset and ground truth labels, you can easily add model predictions to take advantage of FiftyOne\u2019s evaluation capabilities.</p>"},{"location":"fiftyone_concepts/dataset_creation/#need-data","title":"Need data? \u00b6","text":"<p>The FiftyOne Dataset Zoo contains dozens of popular public datasets that you can load into FiftyOne in a single line of code:</p> <pre><code>import fiftyone.zoo as foz\n\n# List available datasets\nprint(foz.list_zoo_datasets())\n# ['coco-2014', ...,  'kitti', ..., 'voc-2012', ...]\n\n# Load a split of a zoo dataset\ndataset = foz.load_zoo_dataset(\"cifar10\", split=\"train\")\n</code></pre> <p>Note</p> <p>Check out the available zoo datasets!</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/","title":"Loading Datasets From Disk \u00b6","text":"<p>FiftyOne provides native support for importing datasets from disk in a variety of common formats, and it can be easily extended to import datasets in custom formats.</p> <p>Note</p> <p>Did you know? You can import media and/or labels from within the FiftyOne App by installing the @voxel51/io plugin!</p> <p>Note</p> <p>If your data is in a custom format, writing a simple loop is the easiest way to load your data into FiftyOne.</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The interface for creating a FiftyOne <code>Dataset</code> for your data on disk is conveniently exposed via the Python library and the CLI. The basic recipe is that you simply specify the path(s) to the data on disk and the type of dataset that you\u2019re loading.</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#supported-formats","title":"Supported formats \u00b6","text":"<p>Each supported dataset type is represented by a subclass of <code>fiftyone.types.Dataset</code>, which is used by the Python library and CLI to refer to the corresponding dataset format when reading the dataset from disk.</p> Dataset Type Description ImageDirectory A directory of images. VideoDirectory A directory of videos. MediaDirectory A directory of media files. FiftyOneImageClassificationDataset A labeled dataset consisting of images and their associated classification labelsin a simple JSON format. ImageClassificationDirectoryTree A directory tree whose subfolders define an image classification dataset. VideoClassificationDirectoryTree A directory tree whose subfolders define a video classification dataset. TFImageClassificationDataset A labeled dataset consisting of images and their associated classification labelsstored as TFRecords. FiftyOneImageDetectionDataset A labeled dataset consisting of images and their associated object detectionsstored in a simple JSON format. FiftyOneTemporalDetectionDataset A labeled dataset consisting of videos and their associated temporal detections ina simple JSON format. COCODetectionDataset A labeled dataset consisting of images and their associated object detectionssaved in COCO Object Detection Format. VOCDetectionDataset A labeled dataset consisting of images and their associated object detectionssaved in VOC format. KITTIDetectionDataset A labeled dataset consisting of images and their associated object detectionssaved in KITTI format. YOLOv4Dataset A labeled dataset consisting of images and their associated object detectionssaved in YOLOv4 format. YOLOv5Dataset A labeled dataset consisting of images and their associated object detectionssaved in YOLOv5 format. TFObjectDetectionDataset A labeled dataset consisting of images and their associated object detectionsstored as TFRecords in TF Object Detection API format. ImageSegmentationDirectory A labeled dataset consisting of images and their associated semantic segmentationsstored as images on disk. CVATImageDataset A labeled dataset consisting of images and their associated multitask labelsstored in CVAT image format. CVATVideoDataset A labeled dataset consisting of videos and their associated multitask labelsstored in CVAT video format. OpenLABELImageDataset A labeled dataset consisting of images and their associated multitask labelsstored in OpenLABEL format. OpenLABELVideoDataset A labeled dataset consisting of videos and their associated multitask labelsstored in OpenLABEL format. FiftyOneImageLabelsDataset A labeled dataset consisting of images and their associated multitask predictionsstored in ETA ImageLabels format. BDDDataset A labeled dataset consisting of images and their associated multitask predictionssaved in Berkeley DeepDrive (BDD) format. CSVDataset A labeled dataset consisting of images or videos and their associated field valuesstored as columns of a CSV file. DICOMDataset An image dataset whose image data and optional properties are stored inDICOM format. GeoJSONDataset An image or video dataset whose location data and labels are stored inGeoJSON format. GeoTIFFDataset An image dataset whose image and geolocation data are stored inGeoTIFF format. FiftyOneVideoLabelsDataset A labeled dataset consisting of videos and their associated multitask predictionsstored in ETA VideoLabels format. FiftyOneDataset A dataset consisting of an entire serialized <code>Dataset</code> and its associated sourcemedia. Custom formats Import datasets in custom formats by defining your own <code>Dataset</code> or<code>DatasetImporter</code> class."},{"location":"fiftyone_concepts/dataset_creation/datasets/#imagedirectory","title":"ImageDirectory \u00b6","text":"<p>The <code>fiftyone.types.ImageDirectory</code> type represents a directory of images.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    &lt;filename1&gt;.&lt;ext&gt;\n    &lt;filename2&gt;.&lt;ext&gt;\n</code></pre> <p>where files with non-image MIME types are omitted.</p> <p>By default, the dataset may contain nested subfolders of images, which are recursively listed.</p> <p>Note</p> <p>See <code>ImageDirectoryImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from a directory of images as follows:</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#videodirectory","title":"VideoDirectory \u00b6","text":"<p>The <code>fiftyone.types.VideoDirectory</code> type represents a directory of videos.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    &lt;filename1&gt;.&lt;ext&gt;\n    &lt;filename2&gt;.&lt;ext&gt;\n</code></pre> <p>where files with non-video MIME types are omitted.</p> <p>By default, the dataset may contain nested subfolders of videos, which are recursively listed.</p> <p>Note</p> <p>See <code>VideoDirectoryImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from a directory of videos as follows:</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#mediadirectory","title":"MediaDirectory \u00b6","text":"<p>The <code>fiftyone.types.MediaDirectory</code> type represents a directory of media files.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    &lt;filename1&gt;.&lt;ext&gt;\n    &lt;filename2&gt;.&lt;ext&gt;\n</code></pre> <p>Note</p> <p>All files must have the same media type (image, video, point cloud, etc.)</p> <p>By default, the dataset may contain nested subfolders of media files, which are recursively listed.</p> <p>Note</p> <p>See <code>MediaDirectoryImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from a directory of media files as follows:</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#fiftyoneimageclassificationdataset","title":"FiftyOneImageClassificationDataset \u00b6","text":"<p>The <code>fiftyone.types.FiftyOneImageClassificationDataset</code> type represents a labeled dataset consisting of images and their associated classification label(s) stored in a simple JSON format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels.json\n</code></pre> <p>In the simplest case, <code>labels.json</code> can be a JSON file in the following format:</p> <pre><code>{\n    \"classes\": [\\\n        \"&lt;labelA&gt;\",\\\n        \"&lt;labelB&gt;\",\\\n        ...\\\n    ],\n    \"labels\": {\n        \"&lt;uuid1&gt;\": &lt;target&gt;,\n        \"&lt;uuid2&gt;\": &lt;target&gt;,\n        ...\n    }\n}\n</code></pre> <p>If the <code>classes</code> field is provided, the <code>target</code> values are class IDs that are mapped to class label strings via <code>classes[target]</code>. If no <code>classes</code> field is provided, then the <code>target</code> values directly store the label strings.</p> <p>The target value in <code>labels</code> for unlabeled images is <code>None</code> (or missing).</p> <p>The UUIDs can also be relative paths like <code>path/to/uuid</code>, in which case the images in <code>data/</code> should be arranged in nested subfolders with the corresponding names, or they can be absolute paths, in which case the images may or may not be in <code>data/</code>.</p> <p>Alternatively, <code>labels.json</code> can contain predictions with associated confidences and additional attributes in the following format:</p> <pre><code>{\n    \"classes\": [\\\n        \"&lt;labelA&gt;\",\\\n        \"&lt;labelB&gt;\",\\\n        ...\\\n    ],\n    \"labels\": {\n        \"&lt;uuid1&gt;\": {\n            \"label\": &lt;target&gt;,\n            \"confidence\": &lt;optional-confidence&gt;,\n            \"attributes\": {\n                &lt;optional-name&gt;: &lt;optional-value&gt;,\n                ...\n            }\n        },\n        \"&lt;uuid2&gt;\": {\n            \"label\": &lt;target&gt;,\n            \"confidence\": &lt;optional-confidence&gt;,\n            \"attributes\": {\n                &lt;optional-name&gt;: &lt;optional-value&gt;,\n                ...\n            }\n        },\n        ...\n    }\n}\n</code></pre> <p>You can also load multilabel classifications in this format by storing lists of targets in <code>labels.json</code>:</p> <pre><code>{\n    \"classes\": [\\\n        \"&lt;labelA&gt;\",\\\n        \"&lt;labelB&gt;\",\\\n        ...\\\n    ],\n    \"labels\": {\n        \"&lt;uuid1&gt;\": [&lt;target1&gt;, &lt;target2&gt;, ...],\n        \"&lt;uuid2&gt;\": [&lt;target1&gt;, &lt;target2&gt;, ...],\n        ...\n    }\n}\n</code></pre> <p>where the target values in <code>labels</code> can be class strings, class IDs, or dicts in the format described above defining class labels, confidences, and optional attributes.</p> <p>Note</p> <p>See <code>FiftyOneImageClassificationDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from an image classification dataset stored in the above format as follows:</p> <p>You can also independently specify the locations of the labels and the root directory containing the corresponding media files by providing the <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:</p> <p>Note</p> <p>If the UUIDs in your labels are absolute paths to the source media, then you can omit the <code>data_path</code> parameter from the example above.</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#imageclassificationdirectorytree","title":"ImageClassificationDirectoryTree \u00b6","text":"<p>The <code>fiftyone.types.ImageClassificationDirectoryTree</code> type represents a directory tree whose subfolders define an image classification dataset.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    &lt;classA&gt;/\n        &lt;image1&gt;.&lt;ext&gt;\n        &lt;image2&gt;.&lt;ext&gt;\n        ...\n    &lt;classB&gt;/\n        &lt;image1&gt;.&lt;ext&gt;\n        &lt;image2&gt;.&lt;ext&gt;\n        ...\n    ...\n</code></pre> <p>Unlabeled images are stored in a subdirectory named <code>_unlabeled</code>.</p> <p>Each class folder may contain nested subfolders of images.</p> <p>Note</p> <p>See <code>ImageClassificationDirectoryTreeImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from an image classification directory tree stored in the above format as follows:</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#videoclassificationdirectorytree","title":"VideoClassificationDirectoryTree \u00b6","text":"<p>The <code>fiftyone.types.VideoClassificationDirectoryTree</code> type represents a directory tree whose subfolders define a video classification dataset.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    &lt;classA&gt;/\n        &lt;video1&gt;.&lt;ext&gt;\n        &lt;video2&gt;.&lt;ext&gt;\n        ...\n    &lt;classB&gt;/\n        &lt;video1&gt;.&lt;ext&gt;\n        &lt;video2&gt;.&lt;ext&gt;\n        ...\n    ...\n</code></pre> <p>Unlabeled videos are stored in a subdirectory named <code>_unlabeled</code>.</p> <p>Each class folder may contain nested subfolders of videos.</p> <p>Note</p> <p>See <code>VideoClassificationDirectoryTreeImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from a video classification directory tree stored in the above format as follows:</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#tfimageclassificationdataset","title":"TFImageClassificationDataset \u00b6","text":"<p>The <code>fiftyone.types.TFImageClassificationDataset</code> type represents a labeled dataset consisting of images and their associated classification labels stored as TFRecords.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    tf.records-?????-of-?????\n</code></pre> <p>where the features of the (possibly sharded) TFRecords are stored in the following format:</p> <pre><code>{\n    # Image dimensions\n    \"height\": tf.io.FixedLenFeature([], tf.int64),\n    \"width\": tf.io.FixedLenFeature([], tf.int64),\n    \"depth\": tf.io.FixedLenFeature([], tf.int64),\n    # Image filename\n    \"filename\": tf.io.FixedLenFeature([], tf.int64),\n    # The image extension\n    \"format\": tf.io.FixedLenFeature([], tf.string),\n    # Encoded image bytes\n    \"image_bytes\": tf.io.FixedLenFeature([], tf.string),\n    # Class label string\n    \"label\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n}\n</code></pre> <p>For unlabeled samples, the TFRecords do not contain <code>label</code> features.</p> <p>Note</p> <p>See <code>TFImageClassificationDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from an image classification dataset stored as a directory of TFRecords in the above format as follows:</p> <p>Note</p> <p>You can provide the <code>tf_records_path</code> argument instead of <code>dataset_dir</code> in the examples above to directly specify the path to the TFRecord(s) to load. See <code>TFImageClassificationDatasetImporter</code> for details.</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#fiftyoneimagedetectiondataset","title":"FiftyOneImageDetectionDataset \u00b6","text":"<p>The <code>fiftyone.types.FiftyOneImageDetectionDataset</code> type represents a labeled dataset consisting of images and their associated object detections stored in a simple JSON format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels.json\n</code></pre> <p>where <code>labels.json</code> is a JSON file in the following format:</p> <pre><code>{\n    \"classes\": [\\\n        &lt;labelA&gt;,\\\n        &lt;labelB&gt;,\\\n        ...\\\n    ],\n    \"labels\": {\n        &lt;uuid1&gt;: [\\\n            {\\\n                \"label\": &lt;target&gt;,\\\n                \"bounding_box\": [\\\n                    &lt;top-left-x&gt;, &lt;top-left-y&gt;, &lt;width&gt;, &lt;height&gt;\\\n                ],\\\n                \"confidence\": &lt;optional-confidence&gt;,\\\n                \"attributes\": {\\\n                    &lt;optional-name&gt;: &lt;optional-value&gt;,\\\n                    ...\\\n                }\\\n            },\\\n            ...\\\n        ],\n        &lt;uuid2&gt;: [\\\n            ...\\\n        ],\n        ...\n    }\n}\n</code></pre> <p>and where the bounding box coordinates are expressed as relative values in <code>[0, 1] x [0, 1]</code>.</p> <p>If the <code>classes</code> field is provided, the <code>target</code> values are class IDs that are mapped to class label strings via <code>classes[target]</code>. If no <code>classes</code> field is provided, then the <code>target</code> values directly store the label strings.</p> <p>The target value in <code>labels</code> for unlabeled images is <code>None</code> (or missing).</p> <p>The UUIDs can also be relative paths like <code>path/to/uuid</code>, in which case the images in <code>data/</code> should be arranged in nested subfolders with the corresponding names, or they can be absolute paths, in which case the images may or may not be in <code>data/</code>.</p> <p>Note</p> <p>See <code>FiftyOneImageDetectionDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from an image detection dataset stored in the above format as follows:</p> <p>You can also independently specify the locations of the labels and the root directory containing the corresponding media files by providing the <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:</p> <p>Note</p> <p>If the UUIDs in your labels are absolute paths to the source media, then you can omit the <code>data_path</code> parameter from the example above.</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#fiftyonetemporaldetectiondataset","title":"FiftyOneTemporalDetectionDataset \u00b6","text":"<p>The <code>fiftyone.types.FiftyOneTemporalDetectionDataset</code> type represents a labeled dataset consisting of videos and their associated temporal detections stored in a simple JSON format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels.json\n</code></pre> <p>where <code>labels.json</code> is a JSON file in the following format:</p> <pre><code>{\n    \"classes\": [\\\n        \"&lt;labelA&gt;\",\\\n        \"&lt;labelB&gt;\",\\\n        ...\\\n    ],\n    \"labels\": {\n        \"&lt;uuid1&gt;\": [\\\n            {\\\n                \"label\": &lt;target&gt;,\\\n                \"support\": [&lt;first-frame&gt;, &lt;last-frame&gt;],\\\n                \"confidence\": &lt;optional-confidence&gt;,\\\n                \"attributes\": {\\\n                    &lt;optional-name&gt;: &lt;optional-value&gt;,\\\n                    ...\\\n                }\\\n            },\\\n            {\\\n                \"label\": &lt;target&gt;,\\\n                \"support\": [&lt;first-frame&gt;, &lt;last-frame&gt;],\\\n                \"confidence\": &lt;optional-confidence&gt;,\\\n                \"attributes\": {\\\n                    &lt;optional-name&gt;: &lt;optional-value&gt;,\\\n                    ...\\\n                }\\\n            },\\\n            ...\\\n        ],\n        \"&lt;uuid2&gt;\": [\\\n            {\\\n                \"label\": &lt;target&gt;,\\\n                \"timestamps\": [&lt;start-timestamp&gt;, &lt;stop-timestamp&gt;],\\\n                \"confidence\": &lt;optional-confidence&gt;,\\\n                \"attributes\": {\\\n                    &lt;optional-name&gt;: &lt;optional-value&gt;,\\\n                    ...\\\n                }\\\n            },\\\n            {\\\n                \"label\": &lt;target&gt;,\\\n                \"timestamps\": [&lt;start-timestamp&gt;, &lt;stop-timestamp&gt;],\\\n                \"confidence\": &lt;optional-confidence&gt;,\\\n                \"attributes\": {\\\n                    &lt;optional-name&gt;: &lt;optional-value&gt;,\\\n                    ...\\\n                }\\\n            },\\\n        ],\n        ...\n    }\n}\n</code></pre> <p>The temporal range of each detection can be specified either via the <code>support</code> key, which should contain the <code>[first, last]</code> frame numbers of the detection, or the <code>timestamps</code> key, which should contain the <code>[start, stop]</code> timestamps of the detection in seconds.</p> <p>If the <code>classes</code> field is provided, the <code>target</code> values are class IDs that are mapped to class label strings via <code>classes[target]</code>. If no <code>classes</code> field is provided, then the <code>target</code> values directly store the label strings.</p> <p>Unlabeled videos can have a <code>None</code> (or missing) key in <code>labels</code>.</p> <p>The UUIDs can also be relative paths like <code>path/to/uuid</code>, in which case the images in <code>data/</code> should be arranged in nested subfolders with the corresponding names, or they can be absolute paths, in which case the images may or may not be in <code>data/</code>.</p> <p>Note</p> <p>See <code>FiftyOneTemporalDetectionDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from a temporal detection dataset stored in the above format as follows:</p> <p>You can also independently specify the locations of the labels and the root directory containing the corresponding media files by providing the <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:</p> <p>Note</p> <p>If the UUIDs in your labels are absolute paths to the source media, then you can omit the <code>data_path</code> parameter from the example above.</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#cocodetectiondataset","title":"COCODetectionDataset \u00b6","text":"<p>The <code>fiftyone.types.COCODetectionDataset</code> type represents a labeled dataset consisting of images and their associated object detections saved in COCO Object Detection Format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;filename0&gt;.&lt;ext&gt;\n        &lt;filename1&gt;.&lt;ext&gt;\n        ...\n    labels.json\n</code></pre> <p>where <code>labels.json</code> is a JSON file in the following format:</p> <pre><code>{\n    \"info\": {...},\n    \"licenses\": [\\\n        {\\\n            \"id\": 1,\\\n            \"name\": \"Attribution-NonCommercial-ShareAlike License\",\\\n            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\",\\\n        },\\\n        ...\\\n    ],\n    \"categories\": [\\\n        {\\\n            \"id\": 1,\\\n            \"name\": \"cat\",\\\n            \"supercategory\": \"animal\",\\\n            \"keypoints\": [\"nose\", \"head\", ...],\\\n            \"skeleton\": [[12, 14], [14, 16], ...]\\\n        },\\\n        ...\\\n    ],\n    \"images\": [\\\n        {\\\n            \"id\": 1,\\\n            \"license\": 1,\\\n            \"file_name\": \"&lt;filename0&gt;.&lt;ext&gt;\",\\\n            \"height\": 480,\\\n            \"width\": 640,\\\n            \"date_captured\": null\\\n        },\\\n        ...\\\n    ],\n    \"annotations\": [\\\n        {\\\n            \"id\": 1,\\\n            \"image_id\": 1,\\\n            \"category_id\": 1,\\\n            \"bbox\": [260, 177, 231, 199],\\\n            \"segmentation\": [...],\\\n            \"keypoints\": [224, 226, 2, ...],\\\n            \"num_keypoints\": 10,\\\n            \"score\": 0.95,\\\n            \"area\": 45969,\\\n            \"iscrowd\": 0\\\n        },\\\n        ...\\\n    ]\n}\n</code></pre> <p>See this page for a full specification of the <code>segmentation</code> field.</p> <p>For unlabeled datasets, <code>labels.json</code> does not contain an <code>annotations</code> field.</p> <p>The <code>file_name</code> attribute of the labels file encodes the location of the corresponding images, which can be any of the following:</p> <ul> <li> <p>The filename of an image in the <code>data/</code> folder</p> </li> <li> <p>A relative path like <code>data/sub/folder/filename.ext</code> specifying the relative path to the image in a nested subfolder of <code>data/</code></p> </li> <li> <p>An absolute path to an image, which may or may not be in the <code>data/</code> folder</p> </li> </ul> <p>Note</p> <p>See <code>COCODetectionDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from a COCO detection dataset stored in the above format as follows:</p> <p>Note</p> <p>By default, all supported label types are loaded (detections, segmentations, and keypoints). However, you can choose specific type(s) to load by passing the optional <code>label_types</code> argument to methods like <code>Dataset.from_dir()</code>:</p> <pre><code># Only load bounding boxes\ndataset = fo.Dataset.from_dir(\n    dataset_type=fo.types.COCODetectionDataset,\n    label_types=[\"detections\"],\n    ...\n)\n</code></pre> <p>See <code>COCODetectionDatasetImporter</code> for complete documentation of the available COCO import options.</p> <p>You can also independently specify the locations of the labels and the root directory containing the corresponding media files by providing the <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:</p> <p>Note</p> <p>If the <code>file_name</code> key of your labels contains absolute paths to the source media, then you can omit the <code>data_path</code> parameter from the example above.</p> <p>If you have an existing dataset and corresponding model predictions stored in COCO format, then you can use <code>add_coco_labels()</code> to conveniently add the labels to the dataset. The example below demonstrates a round-trip export and then re-import of both images-and-labels and labels-only data in COCO format:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.utils.coco as fouc\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nclasses = dataset.distinct(\"predictions.detections.label\")\n\n# Export images and ground truth labels to disk\ndataset.export(\n    export_dir=\"/tmp/coco\",\n    dataset_type=fo.types.COCODetectionDataset,\n    label_field=\"ground_truth\",\n    classes=classes,\n)\n\n# Export predictions\ndataset.export(\n    dataset_type=fo.types.COCODetectionDataset,\n    labels_path=\"/tmp/coco/predictions.json\",\n    label_field=\"predictions\",\n    classes=classes,\n)\n\n# Now load ground truth labels into a new dataset\ndataset2 = fo.Dataset.from_dir(\n    dataset_dir=\"/tmp/coco\",\n    dataset_type=fo.types.COCODetectionDataset,\n    label_field=\"ground_truth\",\n)\n\n# And add model predictions\nfouc.add_coco_labels(\n    dataset2,\n    \"predictions\",\n    \"/tmp/coco/predictions.json\",\n    classes,\n)\n\n# Verify that ground truth and predictions were imported as expected\nprint(dataset.count(\"ground_truth.detections\"))\nprint(dataset2.count(\"ground_truth.detections\"))\nprint(dataset.count(\"predictions.detections\"))\nprint(dataset2.count(\"predictions.detections\"))\n</code></pre> <p>Note</p> <p>See <code>add_coco_labels()</code> for a complete description of the available syntaxes for loading COCO-formatted predictions to an existing dataset.</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#vocdetectiondataset","title":"VOCDetectionDataset \u00b6","text":"<p>The <code>fiftyone.types.VOCDetectionDataset</code> type represents a labeled dataset consisting of images and their associated object detections saved in VOC format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;uuid1&gt;.xml\n        &lt;uuid2&gt;.xml\n        ...\n</code></pre> <p>where the labels XML files are in the following format:</p> <pre><code>&lt;annotation&gt;\n    &lt;folder&gt;&lt;/folder&gt;\n    &lt;filename&gt;image.ext&lt;/filename&gt;\n    &lt;path&gt;/path/to/dataset-dir/data/image.ext&lt;/path&gt;\n    &lt;source&gt;\n        &lt;database&gt;&lt;/database&gt;\n    &lt;/source&gt;\n    &lt;size&gt;\n        &lt;width&gt;640&lt;/width&gt;\n        &lt;height&gt;480&lt;/height&gt;\n        &lt;depth&gt;3&lt;/depth&gt;\n    &lt;/size&gt;\n    &lt;segmented&gt;&lt;/segmented&gt;\n    &lt;object&gt;\n        &lt;name&gt;cat&lt;/name&gt;\n        &lt;pose&gt;&lt;/pose&gt;\n        &lt;truncated&gt;0&lt;/truncated&gt;\n        &lt;difficult&gt;0&lt;/difficult&gt;\n        &lt;occluded&gt;0&lt;/occluded&gt;\n        &lt;bndbox&gt;\n            &lt;xmin&gt;256&lt;/xmin&gt;\n            &lt;ymin&gt;200&lt;/ymin&gt;\n            &lt;xmax&gt;450&lt;/xmax&gt;\n            &lt;ymax&gt;400&lt;/ymax&gt;\n        &lt;/bndbox&gt;\n    &lt;/object&gt;\n    &lt;object&gt;\n        &lt;name&gt;dog&lt;/name&gt;\n        &lt;pose&gt;&lt;/pose&gt;\n        &lt;truncated&gt;1&lt;/truncated&gt;\n        &lt;difficult&gt;1&lt;/difficult&gt;\n        &lt;occluded&gt;1&lt;/occluded&gt;\n        &lt;bndbox&gt;\n            &lt;xmin&gt;128&lt;/xmin&gt;\n            &lt;ymin&gt;100&lt;/ymin&gt;\n            &lt;xmax&gt;350&lt;/xmax&gt;\n            &lt;ymax&gt;300&lt;/ymax&gt;\n        &lt;/bndbox&gt;\n    &lt;/object&gt;\n    ...\n&lt;/annotation&gt;\n</code></pre> <p>where either the <code>&lt;filename&gt;</code> and/or <code>&lt;path&gt;</code> field of the annotations may be populated to specify the corresponding source image.</p> <p>Unlabeled images have no corresponding file in <code>labels/</code>.</p> <p>The <code>data/</code> and <code>labels/</code> files may contain nested subfolders of parallelly organized images and masks.</p> <p>Note</p> <p>See <code>VOCDetectionDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from a VOC detection dataset stored in the above format as follows:</p> <p>You can also independently specify the locations of the labels and the root directory containing the corresponding media files by providing the <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:</p> <p>Note</p> <p>If the <code>&lt;path&gt;</code> field of your labels are populated with the absolute paths to the source media, then you can omit the <code>data_path</code> parameter from the example above.</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#kittidetectiondataset","title":"KITTIDetectionDataset \u00b6","text":"<p>The <code>fiftyone.types.KITTIDetectionDataset</code> type represents a labeled dataset consisting of images and their associated object detections saved in KITTI format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;uuid1&gt;.txt\n        &lt;uuid2&gt;.txt\n        ...\n</code></pre> <p>where the labels TXT files are space-delimited files where each row corresponds to an object and the 15 (and optional 16th score) columns have the following meanings:</p> # ofcolumns Name Description Default 1 type The object label 1 truncated A float in <code>[0, 1]</code>, where 0 is non-truncated and1 is fully truncated. Here, truncation refers to the objectleaving image boundaries 0 1 occluded An int in <code>(0, 1, 2, 3)</code> indicating occlusion state,where:- 0 = fully visible- 1 = partly occluded- 2 =largely occluded- 3 = unknown 0 1 alpha Observation angle of the object, in <code>[-pi, pi]</code> 0 4 bbox 2D bounding box of object in the image in pixels, in theformat <code>[xtl, ytl, xbr, ybr]</code> 1 dimensions 3D object dimensions, in meters, in the format<code>[height, width, length]</code> 0 1 location 3D object location <code>(x, y, z)</code> in camera coordinates(in meters) 0 1 rotation_y Rotation around the y-axis in camera coordinates, in<code>[-pi, pi]</code> 0 1 score <code>(optional)</code> A float confidence for the detection <p>When reading datasets of this type, all columns after the four <code>bbox</code> columns are optional.</p> <p>Unlabeled images have no corresponding file in <code>labels/</code>.</p> <p>The <code>data/</code> and <code>labels/</code> files may contain nested subfolders of parallelly organized images and masks.</p> <p>Note</p> <p>See <code>KITTIDetectionDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from a KITTI detection dataset stored in the above format as follows:</p> <p>You can also independently specify the locations of the labels and the root directory containing the corresponding media files by providing the <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#yolov4dataset","title":"YOLOv4Dataset \u00b6","text":"<p>The <code>fiftyone.types.YOLOv4Dataset</code> type represents a labeled dataset consisting of images and their associated object detections saved in YOLOv4 format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    obj.names\n    images.txt\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid1&gt;.txt\n        &lt;uuid2&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.txt\n        ...\n</code></pre> <p>where <code>obj.names</code> contains the object class labels:</p> <pre><code>&lt;label-0&gt;\n&lt;label-1&gt;\n...\n</code></pre> <p>and <code>images.txt</code> contains the list of images in <code>data/</code>:</p> <pre><code>data/&lt;uuid1&gt;.&lt;ext&gt;\ndata/&lt;uuid2&gt;.&lt;ext&gt;\n...\n</code></pre> <p>The image paths in <code>images.txt</code> can be specified as either relative (to the location of file) or as absolute paths. Alternatively, this file can be omitted, in which case the <code>data/</code> directory is listed to determine the available images.</p> <p>The TXT files in <code>data/</code> are space-delimited files where each row corresponds to an object in the image of the same name, in one of the following formats:</p> <pre><code># Detections\n&lt;target&gt; &lt;x-center&gt; &lt;y-center&gt; &lt;width&gt; &lt;height&gt;\n&lt;target&gt; &lt;x-center&gt; &lt;y-center&gt; &lt;width&gt; &lt;height&gt; &lt;confidence&gt;\n\n# Polygons\n&lt;target&gt; &lt;x1&gt; &lt;y1&gt; &lt;x2&gt; &lt;y2&gt; &lt;x3&gt; &lt;y3&gt; ...\n</code></pre> <p>where <code>&lt;target&gt;</code> is the zero-based integer index of the object class label from <code>obj.names</code>, all coordinates are expressed as relative values in <code>[0, 1] x [0, 1]</code>, and <code>&lt;confidence&gt;</code> is an optional confidence in <code>[0, 1]</code>.</p> <p>Unlabeled images have no corresponding TXT file in <code>data/</code>.</p> <p>The <code>data/</code> folder may contain nested subfolders.</p> <p>Note</p> <p>By default, all annotations are loaded as <code>Detections</code>, converting any polylines to tight bounding boxes if necessary. However, you can choose to load YOLO annotations as <code>Polylines</code> by passing the optional <code>label_type</code> argument to methods like <code>Dataset.from_dir()</code>:</p> <pre><code># Load annotations as polygons\ndataset = fo.Dataset.from_dir(\n    dataset_type=fo.types.YOLOv4Dataset,\n    label_type=\"polylines\",\n    ...\n)\n</code></pre> <p>See <code>YOLOv4DatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from a YOLOv4 dataset stored in the above format as follows:</p> <p>You can also independently specify the locations of the labels and the root directory containing the corresponding media files by providing the <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:</p> <p>If you have an existing dataset and corresponding model predictions stored in YOLO format, then you can use <code>add_yolo_labels()</code> to conveniently add the labels to the dataset.</p> <p>The example below demonstrates a round-trip export and then re-import of both images-and-labels and labels-only data in YOLO format:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.utils.yolo as fouy\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nclasses = dataset.distinct(\"predictions.detections.label\")\n\n# Export images and ground truth labels to disk\ndataset.export(\n    export_dir=\"/tmp/yolov4\",\n    dataset_type=fo.types.YOLOv4Dataset,\n    label_field=\"ground_truth\",\n    classes=classes,\n)\n\n# Export predictions\ndataset.export(\n    dataset_type=fo.types.YOLOv4Dataset,\n    labels_path=\"/tmp/yolov4/predictions\",\n    label_field=\"predictions\",\n    classes=classes,\n)\n\n# Now load ground truth labels into a new dataset\ndataset2 = fo.Dataset.from_dir(\n    dataset_dir=\"/tmp/yolov4\",\n    dataset_type=fo.types.YOLOv4Dataset,\n    label_field=\"ground_truth\",\n)\n\n# And add model predictions\nfouy.add_yolo_labels(\n    dataset2,\n    \"predictions\",\n    \"/tmp/yolov4/predictions\",\n    classes,\n)\n\n# Verify that ground truth and predictions were imported as expected\nprint(dataset.count(\"ground_truth.detections\"))\nprint(dataset2.count(\"ground_truth.detections\"))\nprint(dataset.count(\"predictions.detections\"))\nprint(dataset2.count(\"predictions.detections\"))\n</code></pre> <p>Note</p> <p>See <code>add_yolo_labels()</code> for a complete description of the available syntaxes for loading YOLO-formatted predictions to an existing dataset.</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#yolov5dataset","title":"YOLOv5Dataset \u00b6","text":"<p>The <code>fiftyone.types.YOLOv5Dataset</code> type represents a labeled dataset consisting of images and their associated object detections saved in YOLOv5 format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    dataset.yaml\n    images/\n        train/\n            &lt;uuid1&gt;.&lt;ext&gt;\n            &lt;uuid2&gt;.&lt;ext&gt;\n            ...\n        val/\n            &lt;uuid3&gt;.&lt;ext&gt;\n            &lt;uuid4&gt;.&lt;ext&gt;\n            ...\n    labels/\n        train/\n            &lt;uuid1&gt;.txt\n            &lt;uuid2&gt;.txt\n            ...\n        val/\n            &lt;uuid3&gt;.txt\n            &lt;uuid4&gt;.txt\n            ...\n</code></pre> <p>where <code>dataset.yaml</code> contains the following information:</p> <pre><code>path: &lt;dataset_dir&gt;  # optional\ntrain: ./images/train/\nval: ./images/val/\n\nnames:\n  0: list\n  1: of\n  2: classes\n  ...\n</code></pre> <p>See this page for a full description of the possible format of <code>dataset.yaml</code>. In particular, the dataset may contain one or more splits with arbitrary names, as the specific split being imported or exported is specified by the <code>split</code> argument to <code>fiftyone.utils.yolo.YOLOv5DatasetImporter</code>. Also, <code>dataset.yaml</code> can be located outside of <code>&lt;dataset_dir&gt;</code> as long as the optional <code>path</code> is provided.</p> <p>Note</p> <p>Any relative paths in <code>dataset.yaml</code> or per-split TXT files are interpreted relative to the directory containing these files, not your current working directory.</p> <p>The TXT files in <code>labels/</code> are space-delimited files where each row corresponds to an object in the image of the same name, in one of the following formats:</p> <pre><code># Detections\n&lt;target&gt; &lt;x-center&gt; &lt;y-center&gt; &lt;width&gt; &lt;height&gt;\n&lt;target&gt; &lt;x-center&gt; &lt;y-center&gt; &lt;width&gt; &lt;height&gt; &lt;confidence&gt;\n\n# Polygons\n&lt;target&gt; &lt;x1&gt; &lt;y1&gt; &lt;x2&gt; &lt;y2&gt; &lt;x3&gt; &lt;y3&gt; ...\n</code></pre> <p>where <code>&lt;target&gt;</code> is the zero-based integer index of the object class label from <code>names</code>, all coordinates are expressed as relative values in <code>[0, 1] x [0, 1]</code>, and <code>&lt;confidence&gt;</code> is an optional confidence in <code>[0, 1]</code>.</p> <p>Unlabeled images have no corresponding TXT file in <code>labels/</code>. The label file path for each image is obtained by replacing <code>images/</code> with <code>labels/</code> in the respective image path.</p> <p>The image and labels directories for a given split may contain nested subfolders of parallelly organized images and labels.</p> <p>Note</p> <p>By default, all annotations are loaded as <code>Detections</code>, converting any polylines to tight bounding boxes if necessary. However, you can choose to load YOLO annotations as <code>Polylines</code> by passing the optional <code>label_type</code> argument to methods like <code>Dataset.from_dir()</code>:</p> <pre><code># Load annotations as polygons\ndataset = fo.Dataset.from_dir(\n    dataset_type=fo.types.YOLOv5Dataset,\n    label_type=\"polylines\",\n    ...\n)\n</code></pre> <p>See <code>YOLOv5DatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from a YOLOv5 dataset stored in the above format as follows:</p> <pre><code>import fiftyone as fo\n\nname = \"my-dataset\"\ndataset_dir = \"/path/to/yolov5-dataset\"\n\n# The splits to load\nsplits = [\"train\", \"val\"]\n\n# Load the dataset, using tags to mark the samples in each split\ndataset = fo.Dataset(name)\nfor split in splits:\n    dataset.add_dir(\n        dataset_dir=dataset_dir,\n        dataset_type=fo.types.YOLOv5Dataset,\n        split=split,\n        tags=split,\n)\n\n# View summary info about the dataset\nprint(dataset)\n\n# Print the first few samples in the dataset\nprint(dataset.head())\n</code></pre> <p>If you have an existing dataset and corresponding model predictions stored in YOLO format, then you can use <code>add_yolo_labels()</code> to conveniently add the labels to the dataset.</p> <p>The example below demonstrates a round-trip export and then re-import of both images-and-labels and labels-only data in YOLO format:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.utils.yolo as fouy\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nclasses = dataset.distinct(\"predictions.detections.label\")\n\n# YOLOv5 format supports splits, so let's grab only the `validation` split\nview = dataset.match_tags(\"validation\")\n\n# Export images and ground truth labels to disk\nview.export(\n    export_dir=\"/tmp/yolov5\",\n    dataset_type=fo.types.YOLOv5Dataset,\n    split=\"validation\",\n    label_field=\"ground_truth\",\n    classes=classes,\n)\n\n# Export predictions\nview.export(\n    dataset_type=fo.types.YOLOv5Dataset,\n    labels_path=\"/tmp/yolov5/predictions/validation\",\n    label_field=\"predictions\",\n    classes=classes,\n)\n\n# Now load ground truth labels into a new dataset\ndataset2 = fo.Dataset.from_dir(\n    dataset_dir=\"/tmp/yolov5\",\n    dataset_type=fo.types.YOLOv5Dataset,\n    split=\"validation\",\n    label_field=\"ground_truth\",\n)\n\n# And add model predictions\nfouy.add_yolo_labels(\n    dataset2,\n    \"predictions\",\n    \"/tmp/yolov5/predictions/validation\",\n    classes,\n)\n\n# Verify that ground truth and predictions were imported as expected\nprint(view.count(\"ground_truth.detections\"))\nprint(dataset2.count(\"ground_truth.detections\"))\nprint(view.count(\"predictions.detections\"))\nprint(dataset2.count(\"predictions.detections\"))\n</code></pre> <p>Note</p> <p>See <code>add_yolo_labels()</code> for a complete description of the available syntaxes for loading YOLO-formatted predictions to an existing dataset.</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#tfobjectdetectiondataset","title":"TFObjectDetectionDataset \u00b6","text":"<p>The <code>fiftyone.types.TFObjectDetectionDataset</code> type represents a labeled dataset consisting of images and their associated object detections stored as TFRecords in TF Object Detection API format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    tf.records-?????-of-?????\n</code></pre> <p>where the features of the (possibly sharded) TFRecords are stored in the following format:</p> <pre><code>{\n    # Image dimensions\n    \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n    \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n\n    # Image filename is used for both of these when writing\n    \"image/filename\": tf.io.FixedLenFeature([], tf.string),\n    \"image/source_id\": tf.io.FixedLenFeature([], tf.string),\n\n    # Encoded image bytes\n    \"image/encoded\": tf.io.FixedLenFeature([], tf.string),\n\n    # Image format, either `jpeg` or `png`\n    \"image/format\": tf.io.FixedLenFeature([], tf.string),\n\n    # Normalized bounding box coordinates in `[0, 1]`\n    \"image/object/bbox/xmin\": tf.io.FixedLenSequenceFeature(\n        [], tf.float32, allow_missing=True\n    ),\n    \"image/object/bbox/xmax\": tf.io.FixedLenSequenceFeature(\n        [], tf.float32, allow_missing=True\n    ),\n    \"image/object/bbox/ymin\": tf.io.FixedLenSequenceFeature(\n        [], tf.float32, allow_missing=True\n    ),\n    \"image/object/bbox/ymax\": tf.io.FixedLenSequenceFeature(\n        [], tf.float32, allow_missing=True\n    ),\n\n    # Class label string\n    \"image/object/class/text\": tf.io.FixedLenSequenceFeature(\n        [], tf.string, allow_missing=True\n    ),\n\n    # Integer class ID\n    \"image/object/class/label\": tf.io.FixedLenSequenceFeature(\n        [], tf.int64, allow_missing=True\n    ),\n}\n</code></pre> <p>The TFRecords for unlabeled samples do not contain <code>image/object/*</code> features.</p> <p>Note</p> <p>See <code>TFObjectDetectionDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from an object detection dataset stored as a directory of TFRecords in the above format as follows:</p> <p>Note</p> <p>You can provide the <code>tf_records_path</code> argument instead of <code>dataset_dir</code> in the examples above to directly specify the path to the TFRecord(s) to load. See <code>TFObjectDetectionDatasetImporter</code> for details.</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#imagesegmentationdirectory","title":"ImageSegmentationDirectory \u00b6","text":"<p>The <code>fiftyone.types.ImageSegmentationDirectory</code> type represents a labeled dataset consisting of images and their associated semantic segmentations stored as images on disk.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;filename1&gt;.&lt;ext&gt;\n        &lt;filename2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;filename1&gt;.&lt;ext&gt;\n        &lt;filename2&gt;.&lt;ext&gt;\n        ...\n</code></pre> <p>where <code>labels/</code> contains the semantic segmentations stored as images.</p> <p>Unlabeled images have no corresponding file in <code>labels/</code>.</p> <p>The <code>data/</code> and <code>labels/</code> files may contain nested subfolders of parallelly organized images and masks.</p> <p>Note</p> <p>See <code>ImageSegmentationDirectoryImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from an image segmentation dataset stored in the above format as follows:</p> <p>You can also independently specify the locations of the masks and the root directory containing the corresponding media files by providing the <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#cvatimagedataset","title":"CVATImageDataset \u00b6","text":"<p>The <code>fiftyone.types.CVATImageDataset</code> type represents a labeled dataset consisting of images and their associated tags and object detections stored in CVAT image format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels.xml\n</code></pre> <p>where <code>labels.xml</code> is an XML file in the following format:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;annotations&gt;\n    &lt;version&gt;1.1&lt;/version&gt;\n    &lt;meta&gt;\n        &lt;task&gt;\n            &lt;id&gt;0&lt;/id&gt;\n            &lt;name&gt;task-name&lt;/name&gt;\n            &lt;size&gt;51&lt;/size&gt;\n            &lt;mode&gt;annotation&lt;/mode&gt;\n            &lt;overlap&gt;&lt;/overlap&gt;\n            &lt;bugtracker&gt;&lt;/bugtracker&gt;\n            &lt;flipped&gt;False&lt;/flipped&gt;\n            &lt;created&gt;2017-11-20 11:51:51.000000+00:00&lt;/created&gt;\n            &lt;updated&gt;2017-11-20 11:51:51.000000+00:00&lt;/updated&gt;\n            &lt;labels&gt;\n                &lt;label&gt;\n                    &lt;name&gt;car&lt;/name&gt;\n                    &lt;attributes&gt;\n                        &lt;attribute&gt;\n                            &lt;name&gt;type&lt;/name&gt;\n                            &lt;values&gt;coupe\\\\nsedan\\\\ntruck&lt;/values&gt;\n                        &lt;/attribute&gt;\n                        ...\n                    &lt;/attributes&gt;\n                &lt;/label&gt;\n                &lt;label&gt;\n                    &lt;name&gt;traffic_line&lt;/name&gt;\n                    &lt;attributes&gt;\n                        &lt;attribute&gt;\n                            &lt;name&gt;color&lt;/name&gt;\n                            &lt;values&gt;white\\\\nyellow&lt;/values&gt;\n                        &lt;/attribute&gt;\n                        ...\n                    &lt;/attributes&gt;\n                &lt;/label&gt;\n                ...\n            &lt;/labels&gt;\n        &lt;/task&gt;\n        &lt;segments&gt;\n            &lt;segment&gt;\n                &lt;id&gt;0&lt;/id&gt;\n                &lt;start&gt;0&lt;/start&gt;\n                &lt;stop&gt;50&lt;/stop&gt;\n                &lt;url&gt;&lt;/url&gt;\n            &lt;/segment&gt;\n        &lt;/segments&gt;\n        &lt;owner&gt;\n            &lt;username&gt;&lt;/username&gt;\n            &lt;email&gt;&lt;/email&gt;\n        &lt;/owner&gt;\n        &lt;dumped&gt;2017-11-20 11:51:51.000000+00:00&lt;/dumped&gt;\n    &lt;/meta&gt;\n    &lt;image id=\"0\" name=\"&lt;uuid1&gt;.&lt;ext&gt;\" width=\"640\" height=\"480\"&gt;\n        &lt;tag label=\"urban\"&gt;&lt;/tag&gt;\n        ...\n        &lt;box label=\"car\" xtl=\"100\" ytl=\"50\" xbr=\"325\" ybr=\"190\" occluded=\"0\"&gt;\n            &lt;attribute name=\"type\"&gt;sedan&lt;/attribute&gt;\n            ...\n        &lt;/box&gt;\n        ...\n        &lt;polygon label=\"car\" points=\"561.30,916.23;561.30,842.77;...;560.20,966.67\" occluded=\"0\"&gt;\n            &lt;attribute name=\"make\"&gt;Honda&lt;/attribute&gt;\n            ...\n        &lt;/polygon&gt;\n        ...\n        &lt;polyline label=\"traffic_line\" points=\"462.10,0.00;126.80,1200.00\" occluded=\"0\"&gt;\n            &lt;attribute name=\"color\"&gt;yellow&lt;/attribute&gt;\n            ...\n        &lt;/polyline&gt;\n        ...\n        &lt;points label=\"wheel\" points=\"574.90,939.48;1170.16,907.90;...;600.16,459.48\" occluded=\"0\"&gt;\n            &lt;attribute name=\"location\"&gt;front_driver_side&lt;/attribute&gt;\n            ...\n        &lt;/points&gt;\n        ...\n    &lt;/image&gt;\n    ...\n    &lt;image id=\"50\" name=\"&lt;uuid51&gt;.&lt;ext&gt;\" width=\"640\" height=\"480\"&gt;\n        ...\n    &lt;/image&gt;\n&lt;/annotations&gt;\n</code></pre> <p>Unlabeled images have no corresponding <code>image</code> tag in <code>labels.xml</code>.</p> <p>The <code>name</code> field of the <code>&lt;image&gt;</code> tags in the labels file encodes the location of the corresponding images, which can be any of the following:</p> <ul> <li> <p>The filename of an image in the <code>data/</code> folder</p> </li> <li> <p>A relative path like <code>data/sub/folder/filename.ext</code> specifying the relative path to the image in a nested subfolder of <code>data/</code></p> </li> <li> <p>An absolute path to an image, which may or may not be in the <code>data/</code> folder</p> </li> </ul> <p>Note</p> <p>See <code>CVATImageDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from a CVAT image dataset stored in the above format as follows:</p> <p>You can also independently specify the locations of the labels and the root directory containing the corresponding media files by providing the <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:</p> <p>Note</p> <p>If the <code>name</code> key of your labels contains absolute paths to the source media, then you can omit the <code>data_path</code> parameter from the example above.</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#cvatvideodataset","title":"CVATVideoDataset \u00b6","text":"<p>The <code>fiftyone.types.CVATVideoDataset</code> type represents a labeled dataset consisting of videos and their associated object detections stored in CVAT video format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;uuid1&gt;.xml\n        &lt;uuid2&gt;.xml\n        ...\n</code></pre> <p>where the labels XML files are stored in the following format:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;annotations&gt;\n    &lt;version&gt;1.1&lt;/version&gt;\n    &lt;meta&gt;\n        &lt;task&gt;\n            &lt;id&gt;task-id&lt;/id&gt;\n            &lt;name&gt;task-name&lt;/name&gt;\n            &lt;size&gt;51&lt;/size&gt;\n            &lt;mode&gt;interpolation&lt;/mode&gt;\n            &lt;overlap&gt;&lt;/overlap&gt;\n            &lt;bugtracker&gt;&lt;/bugtracker&gt;\n            &lt;flipped&gt;False&lt;/flipped&gt;\n            &lt;created&gt;2017-11-20 11:51:51.000000+00:00&lt;/created&gt;\n            &lt;updated&gt;2017-11-20 11:51:51.000000+00:00&lt;/updated&gt;\n            &lt;labels&gt;\n                &lt;label&gt;\n                    &lt;name&gt;car&lt;/name&gt;\n                    &lt;attributes&gt;\n                        &lt;attribute&gt;\n                            &lt;name&gt;type&lt;/name&gt;\n                            &lt;values&gt;coupe\\\\nsedan\\\\ntruck&lt;/values&gt;\n                        &lt;/attribute&gt;\n                        ...\n                    &lt;/attributes&gt;\n                &lt;/label&gt;\n                &lt;label&gt;\n                    &lt;name&gt;traffic_line&lt;/name&gt;\n                    &lt;attributes&gt;\n                        &lt;attribute&gt;\n                            &lt;name&gt;color&lt;/name&gt;\n                            &lt;values&gt;white\\\\nyellow&lt;/values&gt;\n                        &lt;/attribute&gt;\n                        ...\n                    &lt;/attributes&gt;\n                &lt;/label&gt;\n                ...\n            &lt;/labels&gt;\n        &lt;/task&gt;\n        &lt;segments&gt;\n            &lt;segment&gt;\n                &lt;id&gt;0&lt;/id&gt;\n                &lt;start&gt;0&lt;/start&gt;\n                &lt;stop&gt;50&lt;/stop&gt;\n                &lt;url&gt;&lt;/url&gt;\n            &lt;/segment&gt;\n        &lt;/segments&gt;\n        &lt;owner&gt;\n            &lt;username&gt;&lt;/username&gt;\n            &lt;email&gt;&lt;/email&gt;\n        &lt;/owner&gt;\n        &lt;original_size&gt;\n            &lt;width&gt;640&lt;/width&gt;\n            &lt;height&gt;480&lt;/height&gt;\n        &lt;/original_size&gt;\n        &lt;dumped&gt;2017-11-20 11:51:51.000000+00:00&lt;/dumped&gt;\n    &lt;/meta&gt;\n    &lt;track id=\"0\" label=\"car\"&gt;\n        &lt;box frame=\"0\" xtl=\"100\" ytl=\"50\" xbr=\"325\" ybr=\"190\" outside=\"0\" occluded=\"0\" keyframe=\"1\"&gt;\n            &lt;attribute name=\"type\"&gt;sedan&lt;/attribute&gt;\n            ...\n        &lt;/box&gt;\n        ...\n    &lt;/track&gt;\n    &lt;track id=\"1\" label=\"car\"&gt;\n        &lt;polygon frame=\"0\" points=\"561.30,916.23;561.30,842.77;...;560.20,966.67\" outside=\"0\" occluded=\"0\" keyframe=\"1\"&gt;\n            &lt;attribute name=\"make\"&gt;Honda&lt;/attribute&gt;\n            ...\n        &lt;/polygon&gt;\n        ...\n    &lt;/track&gt;\n    ...\n    &lt;track id=\"10\" label=\"traffic_line\"&gt;\n        &lt;polyline frame=\"10\" points=\"462.10,0.00;126.80,1200.00\" outside=\"0\" occluded=\"0\" keyframe=\"1\"&gt;\n            &lt;attribute name=\"color\"&gt;yellow&lt;/attribute&gt;\n            ...\n        &lt;/polyline&gt;\n        ...\n    &lt;/track&gt;\n    ...\n    &lt;track id=\"88\" label=\"wheel\"&gt;\n        &lt;points frame=\"176\" points=\"574.90,939.48;1170.16,907.90;...;600.16,459.48\" outside=\"0\" occluded=\"0\" keyframe=\"1\"&gt;\n            &lt;attribute name=\"location\"&gt;front_driver_side&lt;/attribute&gt;\n            ...\n        &lt;/points&gt;\n        ...\n    &lt;/track&gt;\n&lt;/annotations&gt;\n</code></pre> <p>Unlabeled videos have no corresponding file in <code>labels/</code>.</p> <p>The <code>data/</code> and <code>labels/</code> files may contain nested subfolders of parallelly organized images and labels.</p> <p>Note</p> <p>See <code>CVATVideoDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from a CVAT video dataset stored in the above format as follows:</p> <p>You can also independently specify the locations of the labels and the root directory containing the corresponding media files by providing the <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#openlabelimagedataset","title":"OpenLABELImageDataset \u00b6","text":"<p>The <code>fiftyone.types.OpenLABELImageDataset</code> type represents a labeled dataset consisting of images and their associated multitask predictions stored = in OpenLABEL format.</p> <p>OpenLABEL is a flexible format which allows labels to be stored in a variety of different ways with respect to the corresponding media files. The following enumerates the possible structures in which media data and OpenLABEL formatted label files can be stored in ways that is understood by FiftyOne:</p> <ol> <li>One label file per image. Each label contains only the metadata and labels associated with the image of the same name. In this case, the <code>labels_path</code> argument is expected to be a directory, if provided:</li> </ol> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;uuid1&gt;.json\n        &lt;uuid2&gt;.json\n        ...\n</code></pre> <ol> <li>One label file for all images. The label file contains all of the metadata and labels associated with every image. In this case, there needs to be additional information provided in the label file to match labels to images. Specifically, the image filepath corresponding to a label must be stored as a stream:</li> </ol> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels.json\n</code></pre> <ol> <li>Multiple label files, each corresponding to one or more images. This case is similar to when there is a single label file, except that the label information may be spread out over multiple files. Since the filenames cannot be used to match labels to images, the image filepaths must again be stored as streams in the labels files:</li> </ol> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;labels-filename1&gt;.json\n        &lt;labels-filename2&gt;.json\n        ...\n</code></pre> <p>As for the actual structure of the labels files themselves, labels are stored in one or more JSON files and can follow a variety of formats. In general following this format:</p> <p>Note</p> <p>All object information stored in the <code>frames</code> key is applied to the corresponding image.</p> <pre><code>{\n    \"openlabel\": {\n        \"metadata\": {\n            \"schema_version\": \"1.0.0\",\n            \"uri\": \"/path/to/&lt;uuid&gt;.&lt;ext&gt;\",\n        },\n        \"objects\": {\n            \"object_uuid1\": {\n                \"name\": \"instance1\",\n                \"type\": \"label1\",\n                \"object_data\": {\n                    \"bbox\": [\\\n                        {\\\n                            \"name\": \"shape\",\\\n                            \"val\": [\\\n                                center-x,\\\n                                center-y,\\\n                                width,\\\n                                height\\\n                            ]\\\n                        }\\\n                    ]\n                }\n            },\n            \"object_uuid2\": {\n                \"name\": \"instance1\",\n                \"type\": \"label2\",\n                \"object_data\": {},  # DEFINED IN FRAMES\n            }\n        },\n        \"frames\": {\n            \"0\": {\n               \"frame_properties\": {\n                  \"streams\": {\n                     \"Camera1\": {\n                        \"uri\": \"&lt;uuid&gt;.&lt;ext&gt;\"\n                     }\n                  }\n               },\n               \"objects\": {\n                  \"object_uuid2\": {\n                     \"object_data\": {\n                        \"poly2d\": [\\\n                           {\\\n                              \"attributes\": {\\\n                                 \"boolean\": [\\\n                                    {\\\n                                       \"name\": \"is_hole\",\\\n                                       \"val\": false\\\n                                    }\\\n                                 ],\\\n                                 \"text\": [\\\n                                    {  # IF NOT PROVIDED OTHERWISE\\\n                                       \"name\": \"stream\",\\\n                                       \"val\": \"Camera1\"\\\n                                    }\\\n                                 ]\\\n                              },\\\n                              \"closed\": true,\\\n                              \"mode\": \"MODE_POLY2D_ABSOLUTE\",\\\n                              \"name\": \"polygon_name\",\\\n                              \"stream\": \"Camera1\",  # IF NOT IN ATTRIBUTES\\\n                              \"val\": [\\\n                                 point1-x,\\\n                                 point1-y,\\\n                                 point2-x,\\\n                                 point2-y,\\\n                                 ...\\\n                              ]\\\n                           }\\\n                        ]\n                     }\n                  }\n              }\n           }\n        },\n        \"streams\": {\n           \"Camera1\": {\n              \"description\": \"\",\n              \"stream_properties\": {\n                 \"height\": 480,\n                 \"width\": 640\n              },\n              \"type\": \"camera\"\n           }\n        },\n        \"ontologies\": ... # NOT PARSED\n        \"relations\": ... # NOT PARSED\n        \"resources\": ... # NOT PARSED\n        \"tags\": ... # NOT PARSED\n    }\n}\n</code></pre> <p>Note</p> <p>See <code>OpenLABELImageDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>If loading <code>Keypoints</code> related to a given <code>KeypointSkeleton</code>, then you can provide a <code>skeleton</code> and <code>skeleton_key</code> argument to the <code>OpenLABELImageDatasetImporter</code> allowing you to match points in your annotations file to labels in the <code>KeypointSkeleton</code> and load the points and their attributes in the correct order.</p> <p>You can create a FiftyOne dataset from a OpenLABEL image dataset stored in the above format as follows:</p> <p>You can also independently specify the locations of the labels and the root directory containing the corresponding media files by providing the <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:</p> <p>Note</p> <p>OpenLABEL is a flexible format that allows for many user-specific decisions about how to represent labels and metadata. If you have OpenLABEL-compliant data in a format not understood by the current importers, please make an issue or contribute a pull request!</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#openlabelvideodataset","title":"OpenLABELVideoDataset \u00b6","text":"<p>The <code>fiftyone.types.OpenLABELVideoDataset</code> type represents a labeled dataset consisting of videos and their associated multitask predictions stored in OpenLABEL format.</p> <p>OpenLABEL is a flexible format which allows labels to be stored in a variety of different ways with respect to the corresponding media files. The following enumerates the possible structures in which media data and OpenLABEL formatted label files can be stored in ways that is understood by FiftyOne:</p> <ol> <li>One label file per video. Each label contains only the metadata and labels associated with the video of the same name. In this case, the <code>labels_path</code> argument is expected to be a directory, if provided:</li> </ol> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;uuid1&gt;.json\n        &lt;uuid2&gt;.json\n        ...\n</code></pre> <ol> <li>One label file for all videos. The label file contains all of the metadata and labels associated with every video. In this case, there needs to be additional information provided in the label file to match labels to videos. Specifically, the video filepath corresponding to a label must be stored as a stream:</li> </ol> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels.json\n</code></pre> <ol> <li>Multiple label files, each corresponding to one or more videos. This case is similar to when there is a single label file, except that the label information may be spread out over multiple files. Since the filenames cannot be used to match labels to videos, the video filepaths must again be stored as streams in the labels files:</li> </ol> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;labaels-filename1&gt;.json\n        &lt;labaels-filename2&gt;.json\n        ...\n</code></pre> <p>As for the actual structure of the labels files themselves, labels are stored in one or more JSON files and can follow a variety of formats. In general following this format:</p> <pre><code>{\n    \"openlabel\": {\n        \"metadata\": {\n            \"schema_version\": \"1.0.0\",\n            \"uri\": \"/path/to/&lt;uuid&gt;.&lt;ext&gt;\",\n        },\n        \"objects\": {\n            \"object_uuid1\": {\n                \"name\": \"instance1\",\n                \"type\": \"label1\",\n                \"object_data\": {\n                    \"bbox\": [\\\n                        {\\\n                            \"name\": \"shape\",\\\n                            \"val\": [\\\n                                center-x,\\\n                                center-y,\\\n                                width,\\\n                                height\\\n                            ]\\\n                        }\\\n                    ]\n                }\n                \"frame_intervals\": [{\"frame_start\": 0, \"frame_end\": 10}],\n            },\n            \"object_uuid2\": {\n                \"name\": \"instance1\",\n                \"type\": \"label2\",\n                \"object_data\": {},  # DEFINED IN FRAMES\n            }\n        },\n        \"frames\": {\n            \"0\": {\n               \"frame_properties\": {\n                  \"streams\": {\n                     \"Camera1\": {\n                        \"uri\":\"&lt;uuid&gt;.&lt;ext&gt;\"\n                     }\n                  }\n               },\n               \"objects\": {\n                  \"object_uuid2\": {\n                     \"object_data\": {\n                        \"poly2d\": [\\\n                           {\\\n                              \"attributes\": {\\\n                                 \"boolean\": [\\\n                                    {\\\n                                       \"name\": \"is_hole\",\\\n                                       \"val\": false\\\n                                    }\\\n                                 ],\\\n                                 \"text\": [\\\n                                    {  # IF NOT PROVIDED OTHERWISE\\\n                                       \"name\": \"stream\",\\\n                                       \"val\": \"Camera1\"\\\n                                    }\\\n                                 ]\\\n                              },\\\n                              \"closed\": true,\\\n                              \"mode\": \"MODE_POLY2D_ABSOLUTE\",\\\n                              \"name\": \"polygon_name\",\\\n                              \"stream\": \"Camera1\",  # IF NOT IN ATTRIBUTES\\\n                              \"val\": [\\\n                                 point1-x,\\\n                                 point1-y,\\\n                                 point2-x,\\\n                                 point2-y,\\\n                                 ...\\\n                              ]\\\n                           }\\\n                        ]\n                     }\n                  }\n              },\n              ...\n           }\n        },\n        \"streams\": {\n           \"Camera1\": {\n              \"description\": \"\",\n              \"stream_properties\": {\n                 \"height\": 480,\n                 \"width\": 640\n              },\n              \"type\": \"camera\"\n           }\n        },\n        \"ontologies\": ...  # NOT PARSED\n        \"relations\" ...  # NOT PARSED\n        \"resources\" ...  # NOT PARSED\n        \"tags\": ...  # NOT PARSED\n    }\n}\n</code></pre> <p>Note</p> <p>See <code>OpenLABELVideoDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>If loading <code>Keypoints</code> related to a given <code>KeypointSkeleton</code>, then you can provide a <code>skeleton</code> and <code>skeleton_key</code> argument to the <code>OpenLABELVideoDatasetImporter</code> allowing you to match points in your annotations file to labels in the <code>KeypointSkeleton</code> and load the points and their attributes in the correct order.</p> <p>You can create a FiftyOne dataset from a OpenLABEL video dataset stored in the above format as follows:</p> <p>You can also independently specify the locations of the labels and the root directory containing the corresponding media files by providing the <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:</p> <p>Note</p> <p>OpenLABEL is a flexible format that allows for many user-specific decisions about how to represent labels and metadata. If you have OpenLABEL-compliant data in a format not understood by the current importers, please make an issue or contribute a pull request!</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#fiftyoneimagelabelsdataset","title":"FiftyOneImageLabelsDataset \u00b6","text":"<p>The <code>fiftyone.types.FiftyOneImageLabelsDataset</code> type represents a labeled dataset consisting of images and their associated multitask predictions stored in ETA ImageLabels format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;uuid1&gt;.json\n        &lt;uuid2&gt;.json\n        ...\n    manifest.json\n</code></pre> <p>where <code>manifest.json</code> is a JSON file in the following format:</p> <pre><code>{\n    \"type\": \"eta.core.datasets.LabeledImageDataset\",\n    \"description\": \"\",\n    \"index\": [\\\n        {\\\n            \"data\": \"data/&lt;uuid1&gt;.&lt;ext&gt;\",\\\n            \"labels\": \"labels/&lt;uuid1&gt;.json\"\\\n        },\\\n        {\\\n            \"data\": \"data/&lt;uuid2&gt;.&lt;ext&gt;\",\\\n            \"labels\": \"labels/&lt;uuid2&gt;.json\"\\\n        },\\\n        ...\\\n    ]\n}\n</code></pre> <p>and where each labels JSON file is stored in ETA ImageLabels format.</p> <p>For unlabeled images, an empty <code>eta.core.image.ImageLabels</code> file is stored.</p> <p>Note</p> <p>See <code>FiftyOneImageLabelsDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from an image labels dataset stored in the above format as follows:</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#fiftyonevideolabelsdataset","title":"FiftyOneVideoLabelsDataset \u00b6","text":"<p>The <code>fiftyone.types.FiftyOneVideoLabelsDataset</code> type represents a labeled dataset consisting of videos and their associated labels stored in ETA VideoLabels format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels/\n        &lt;uuid1&gt;.json\n        &lt;uuid2&gt;.json\n        ...\n    manifest.json\n</code></pre> <p>where <code>manifest.json</code> is a JSON file in the following format:</p> <pre><code>{\n    \"type\": \"eta.core.datasets.LabeledVideoDataset\",\n    \"description\": \"\",\n    \"index\": [\\\n        {\\\n            \"data\": \"data/&lt;uuid1&gt;.&lt;ext&gt;\",\\\n            \"labels\": \"labels/&lt;uuid1&gt;.json\"\\\n        },\\\n        {\\\n            \"data\": \"data/&lt;uuid2&gt;.&lt;ext&gt;\",\\\n            \"labels\": \"labels/&lt;uuid2&gt;.json\"\\\n        },\\\n        ...\\\n    ]\n}\n</code></pre> <p>and where each labels JSON file is stored in ETA VideoLabels format.</p> <p>For unlabeled videos, an empty <code>eta.core.video.VideoLabels</code> file is written.</p> <p>Note</p> <p>See <code>FiftyOneVideoLabelsDatasetImporter</code> for parameters that can be passed to methods like <code>Dataset.from_dir()</code> to customize the import of datasets of this type.</p> <p>You can create a FiftyOne dataset from a video labels dataset stored in the above format as follows:</p>"},{"location":"fiftyone_concepts/dataset_creation/datasets/#bdddataset","title":"BDDDataset \u00b6","text":"<p>The <code>fiftyone.types.BDDDataset</code> type represents a labeled dataset consisting of images and their associated multitask predictions saved in Berkeley DeepDrive (BDD) format.</p> <p>Datasets of this type are read in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;filename0&gt;.&lt;ext&gt;\n        &lt;filename1&gt;.&lt;ext&gt;\n        ...\n    labels.json\n</code></pre> <p>where <code>labels.json</code> is a JSON file in the following format:</p> <p><code>python [\\     {\\         \"name\": \"&lt;filename0&gt;.&lt;ext&gt;\",\\         \"attributes\": {\\             \"scene\": \"city street\",\\             \"timeofday\": \"daytime\",\\             \"weather\": \"overcast\"\\         },\\         \"labels\": [\\             {\\                 \"id\": 0,\\                 \"category\": \"traffic sign\",\\                 \"manualAttributes\": true,\\                 \"manualShape\": true,\\                 \"attributes\": {\\                     \"occluded\": false,\\                     \"trafficLightColor\": \"none\",\\                     \"truncated\": false\\                 },\\                 \"box2d\": {\\                     \"x1\": 1000.698742,\\                     \"x2\": 1040.626872,\\                     \"y1\": 281.992415,\\                     \"y2\": 326.91156\\                 },\\                 \"score\": 0.95\\             },\\             ...\\             {\\                 \"id\": 34,\\                 \"category\": \"drivable area\",\\                 \"manualAttributes\": true,\\                 \"manualShape\": true,\\                 \"attributes\": {\\                     \"areaType\": \"direct\"\\                 },\\                 \"poly2d\": [\\                     {\\                         \"types\": \"LLLLCCC\",\\                         \"closed\": true,\\                         \"vertices\": [\\                             [241.143645, 697.923453],\\                             [541.525255, 380.564983],\\                             ...\\                         ]\\                     }\\                 ],\\                 \"score\": 0.87\\             },\\             ...\\             {\\                 \"id\": 109356,\\                 \"category\": \"lane\",\\                 \"attributes\": {\\                     \"laneDirection\": \"parallel\",\\                     \"laneStyle\": \"dashed\",\\                     \"laneType\": \"single white\"\\                 },\\                 \"manualShape\": true,\\                 \"manualAttributes\": true,\\                 \"poly2d\": [\\                     {\\                         \"types\": \"LL\",\\                         \"closed\": false,\\                         \"vertices\": [\\                             [492.879546, 331.939543],\\                             [0, 471.076658],\\                             ...\\                         ]\\                     }\\                 ],\\                 \"score\": 0.98\\             },\\             ...\\         }\\     }\\     ...\\ ]\\ \\</code>\\ \\ Unlabeled images have no corresponding entry in <code>labels.json</code>.\\ \\ The <code>name</code> attribute of the labels file encodes the location of the\\ corresponding images, which can be any of the following:\\ \\ - The filename of an image in the <code>data/</code> folder\\ \\ - A relative path like <code>data/sub/folder/filename.ext</code> specifying the relative\\ path to the image in a nested subfolder of <code>data/</code>\\ \\ - An absolute path to an image, which may or may not be in the <code>data/</code> folder\\ \\ \\ Note\\ \\ See <code>BDDDatasetImporter</code>\\ for parameters that can be passed to methods like\\ <code>Dataset.from_dir()</code> to\\ customize the import of datasets of this type.\\ \\ You can create a FiftyOne dataset from a BDD dataset stored in the above format\\ as follows:\\ \\ You can also independently specify the locations of the labels and the root\\ directory containing the corresponding media files by providing the\\ <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:\\ \\ Note\\ \\ If the <code>name</code> key of your labels contains absolute paths to the source\\ media, then you can omit the <code>data_path</code> parameter from the example above.\\ \\ ## CSVDataset \u00b6\\ \\ The <code>fiftyone.types.CSVDataset</code> type represents a dataset consisting\\ of images or videos and their associated field values stored as columns of a\\ CSV file.\\ \\ Datasets of this type are read in the following format:\\ \\ <code>\\ &lt;dataset_dir&gt;/\\     data/\\         &lt;filename1&gt;.&lt;ext&gt;\\         &lt;filename2&gt;.&lt;ext&gt;\\         ...\\     labels.csv\\ \\</code>\\ \\ where <code>labels.csv</code> is a CSV file in the following format:\\ \\ <code>\\ field1,field2,field3,...\\ value1,value2,value3,...\\ value1,value2,value3,...\\ ...\\ \\</code>\\ \\ One sample will be generated per row in the CSV file (excluding the header\\ row).\\ \\ One column of the CSV file must contain media paths, which may be either:\\ \\ - filenames or relative paths to media files in <code>data/</code>\\ \\ - absolute paths to media files\\ \\ \\ By default it is assumed that a <code>filepath</code> column exists and contains the\\ media paths, but you can customize this via the optional <code>media_field</code>\\ parameter.\\ \\ By default all columns are loaded as string fields, but you can provide the\\ optional <code>fields</code> parameter to select a subset of columns to load or provide\\ custom parsing functions for each field, as demonstrated below.\\ \\ Note\\ \\ See <code>CSVDatasetImporter</code>\\ for parameters that can be passed to methods like\\ <code>Dataset.from_dir()</code> to\\ customize the import of datasets of this type.\\ \\ You can create a FiftyOne dataset from a CSV dataset stored in the above\\ format as follows:\\ \\ If your CSV file contains absolute media paths, then you can directly specify\\ the path to the CSV file itself by providing the <code>labels_path</code> parameter.\\ \\ Additionally, you can use the <code>fields</code> parameter to customize how each field is\\ parsed, as demonstrated below:\\ \\ ## DICOMDataset \u00b6\\ \\ The <code>fiftyone.types.DICOMDataset</code> type represents a dataset consisting\\ of images and their associated properties stored in\\ DICOM format.\\ \\ Note\\ \\ You must have pydicom&lt;3 installed\\ in order to load DICOM datasets.\\ \\ The standard format for datasets of this type is the following:\\ \\ <code>\\ &lt;dataset_dir&gt;/\\     &lt;filename1&gt;.dcm\\     &lt;filename2&gt;.dcm\\ \\</code>\\ \\ where each <code>.dcm</code> file is a DICOM file that can be read via\\ <code>pydicom.dcmread</code>.\\ \\ Alternatively, rather than providing a <code>dataset_dir</code>, you can provide the\\ <code>dicom_path</code> argument, which can directly specify a glob pattern of DICOM\\ files or the path to a\\ DICOMDIR\\ file.\\ \\ By default, all attributes in the DICOM files discoverable via\\ <code>pydicom.dataset.Dataset.dir()</code> with supported types are loaded\\ into sample-level fields, but you can select only specific attributes by\\ passing the optional <code>keywords</code> argument.\\ \\ Note\\ \\ When importing DICOM datasets, the pixel data are converted to 8-bit\\ images, using the <code>SmallestImagePixelValue</code> and\\ <code>LargestImagePixelValue</code> attributes (if present), to inform the\\ conversion.\\ \\ The images are written to a backing directory that you can configure by\\ passing the <code>images_dir</code> argument. By default, the images are written to\\ <code>dataset_dir</code>.\\ \\ Currently, only single frame images are supported, but a community\\ contribution to support 3D or 4D image types (e.g., CT scans) is welcomed!\\ \\ Note\\ \\ See <code>DICOMDatasetImporter</code>\\ for parameters that can be passed to methods like\\ <code>Dataset.from_dir()</code> to\\ customize the import of datasets of this type.\\ \\ You can create a FiftyOne dataset from a DICOM dataset stored in standard\\ format as follows:\\ \\ You can create a FiftyOne dataset from a glob pattern of DICOM files or the\\ path to a DICOMDIR file as follows:\\ \\ ## GeoJSONDataset \u00b6\\ \\ The <code>fiftyone.types.GeoJSONDataset</code> type represents a dataset consisting\\ of images or videos and their associated geolocation data and optional\\ properties stored in GeoJSON format.\\ \\ Datasets of this type are read in the following format:\\ \\ <code>\\ &lt;dataset_dir&gt;/\\     data/\\         &lt;filename1&gt;.&lt;ext&gt;\\         &lt;filename2&gt;.&lt;ext&gt;\\         ...\\     labels.json\\ \\</code>\\ \\ where <code>labels.json</code> is a GeoJSON file containing a <code>FeatureCollection</code> in the\\ following format:\\ \\ <code>\\ {\\     \"type\": \"FeatureCollection\",\\     \"features\": [\\         {\\             \"type\": \"Feature\",\\             \"geometry\": {\\                 \"type\": \"Point\",\\                 \"coordinates\": [\\                     -73.99496451958454,\\                     40.66338032487842\\                 ]\\             },\\             \"properties\": {\\                 \"filename\": &lt;filename1&gt;.&lt;ext&gt;,\\                 ...\\             }\\         },\\         {\\             \"type\": \"Feature\",\\             \"geometry\": {\\                 \"type\": \"Point\",\\                 \"coordinates\": [\\                     -73.80992143421788,\\                     40.65611832778962\\                 ]\\             },\\             \"properties\": {\\                 \"filename\": &lt;filename2&gt;.&lt;ext&gt;,\\                 ...\\             }\\         },\\         ...\\     ]\\ }\\ \\</code>\\ \\ where the <code>geometry</code> field may contain any valid GeoJSON geometry object, and\\ the <code>filename</code> property encodes the name of the corresponding media in the\\ <code>data/</code> folder. The <code>filename</code> property can also be an absolute path, which\\ may or may not be in the <code>data/</code> folder.\\ \\ Samples with no location data will have a null <code>geometry</code> field.\\ \\ The <code>properties</code> field of each feature can contain additional labels that\\ can be imported.\\ \\ Note\\ \\ See <code>GeoJSONDatasetImporter</code>\\ for parameters that can be passed to methods like\\ <code>Dataset.from_dir()</code> to\\ customize the import of datasets of this type.\\ \\ You can create a FiftyOne dataset from a GeoJSON dataset stored in the above\\ format as follows:\\ \\ You can also independently specify the locations of the labels and the root\\ directory containing the corresponding media files by providing the\\ <code>labels_path</code> and <code>data_path</code> parameters rather than <code>dataset_dir</code>:\\ \\ Note\\ \\ If the <code>filename</code> key of your labels contains absolute paths to the source\\ media, then you can omit the <code>data_path</code> parameter from the example above.\\ \\ ## GeoTIFFDataset \u00b6\\ \\ The <code>fiftyone.types.GeoTIFFDataset</code> type represents a dataset consisting\\ of images and their associated geolocation data stored in\\ GeoTIFF format.\\ \\ Note\\ \\ You must have rasterio installed in\\ order to load GeoTIFF datasets.\\ \\ The standard format for datasets of this type is the following:\\ \\ <code>\\ &lt;dataset_dir&gt;/\\     &lt;filename1&gt;.tif\\     &lt;filename2&gt;.tif\\ \\</code>\\ \\ where each <code>.tif</code> file is a GeoTIFF image that can be read via\\ <code>rasterio.open</code>.\\ \\ Alternatively, rather than providing a <code>dataset_dir</code>, you can provide the\\ <code>image_path</code> argument, which can directly specify a list or glob pattern of\\ GeoTIFF images to load.\\ \\ The dataset will contain a <code>GeoLocation</code> field whose\\ <code>point</code> attribute contains the\\ <code>(longitude, latitude)</code> coordinates of each image center and whose\\ <code>polygon</code> attribute contains\\ the <code>(longitude, latitude)</code> coordinates of the corners of the image (clockwise,\\ starting from the top-left corner).\\ \\ Note\\ \\ See <code>GeoTIFFDatasetImporter</code>\\ for parameters that can be passed to methods like\\ <code>Dataset.from_dir()</code> to\\ customize the import of datasets of this type.\\ \\ You can create a FiftyOne dataset from a GeoTIFF dataset stored in standard\\ format as follows:\\ \\ You can create a FiftyOne dataset from a list or glob pattern of GeoTIFF images\\ as follows:\\ \\ ## FiftyOneDataset \u00b6\\ \\ The <code>fiftyone.types.FiftyOneDataset</code> provides a disk representation of\\ an entire <code>Dataset</code> in a serialized JSON format along with its source media.\\ \\ Datasets of this type are read in the following format:\\ \\ <code>\\ &lt;dataset_dir&gt;/\\     metadata.json\\     samples.json\\     data/\\         &lt;filename1&gt;.&lt;ext&gt;\\         &lt;filename2&gt;.&lt;ext&gt;\\         ...\\     annotations/\\         &lt;anno_key1&gt;.json\\         &lt;anno_key2&gt;.json\\         ...\\     brain/\\         &lt;brain_key1&gt;.json\\         &lt;brain_key2&gt;.json\\         ...\\     evaluations/\\         &lt;eval_key1&gt;.json\\         &lt;eval_key2&gt;.json\\         ...\\ \\</code>\\ \\ where <code>metadata.json</code> is a JSON file containing metadata associated with the\\ dataset, <code>samples.json</code> is a JSON file containing a serialized representation\\ of the samples in the dataset, <code>annotations/</code> contains any serialized\\ <code>AnnotationResults</code>, <code>brain/</code> contains any serialized <code>BrainResults</code>, and\\ <code>evaluations/</code> contains any serialized <code>EvaluationResults</code>.\\ \\ The contents of the <code>data/</code> directory may also be organized in nested\\ subfolders, depending on how the dataset was exported, in which case the\\ filepaths in <code>samples.json</code> should contain corerspondingly nested paths.\\ \\ Video datasets have an additional <code>frames.json</code> file that contains a serialized\\ representation of the frame labels for each video in the dataset.\\ \\ Note\\ \\ See <code>FiftyOneDatasetImporter</code>\\ for parameters that can be passed to methods like\\ <code>Dataset.from_dir()</code> to\\ customize the import of datasets of this type.\\ \\ You can create a FiftyOne dataset from a directory in the above format as\\ follows:\\ \\ If you performed a FiftyOneDataset export\\ using the <code>rel_dir</code> parameter to strip a common prefix from the media filepaths\\ in the dataset, then simply include the <code>rel_dir</code> parameter when importing back\\ into FiftyOne to prepend the appropriate prefix to each media path:\\ \\ Note\\ \\ Exporting in FiftyOneDataset format using\\ the <code>export_media=False</code> and <code>rel_dir</code> parameters is a convenient way to\\ transfer datasets between work environments, since this enables you to\\ store the media files wherever you wish in each environment and then simply\\ provide the appropriate <code>rel_dir</code> value as shown above when importing the\\ dataset into FiftyOne in a new environment.\\ \\ ## Custom formats \u00b6\\ \\ If your data does not follow one of the previous formats, then the simplest and\\ most flexible approach to loading your data into FiftyOne is to iterate over\\ your data in a Python loop and add it to a <code>Dataset</code>.\\ \\ Alternatively, the <code>Dataset</code> class provides a\\ <code>Dataset.from_importer()</code>\\ factory method that can be used to import a dataset using any <code>DatasetImporter</code>\\ instance.\\ \\ This means that you can define your own <code>DatasetImporter</code> class and then import\\ a dataset from disk in your custom format using the following recipe:\\ \\ <code>\\ import fiftyone as fo\\ \\ # Create an instance of your custom dataset importer\\ importer = CustomDatasetImporter(...)\\ \\ # Import the dataset\\ dataset = fo.Dataset.from_importer(importer)\\ \\</code>\\ \\ You can also define a custom <code>Dataset</code> type, which enables you to import\\ datasets in your custom format using the\\ <code>Dataset.from_dir()</code> factory\\ method:\\ \\ <code>\\ import fiftyone as fo\\ \\ # The `fiftyone.types.Dataset` subclass for your custom dataset\\ dataset_type = CustomDataset\\ \\ # Import the dataset\\ dataset = fo.Dataset.from_dir(dataset_type=dataset_type, ...)\\ \\</code>\\ \\ ### Writing a custom DatasetImporter \u00b6\\ \\ <code>DatasetImporter</code> is an abstract interface; the concrete interface that you\\ should implement is determined by the type of dataset that you are importing.\\ \\ ### Importing dataset-level information \u00b6\\ \\ The\\ <code>has_dataset_info</code>\\ property of the importer allows it to declare whether its\\ <code>get_dataset_info()</code>\\ method should be called after all samples have been imported to retrieve a dict\\ of dataset-level information to store in the\\ <code>info</code> property of the dataset.\\ \\ As a special case, if the <code>info</code> dict contains any of the keys listed below,\\ these items are popped and stored in the corresponding dedicated dataset field:\\ \\ - <code>\"classes\"</code> key:\\ <code>Dataset.classes</code>\\ \\ - <code>\"default_classes\"</code> key:\\ <code>Dataset.default_classes</code>\\ \\ - <code>\"mask_targets\"</code> key:\\ <code>Dataset.mask_targets</code>\\ \\ - <code>\"default_mask_targets\"</code> key:\\ <code>Dataset.default_mask_targets</code>\\ \\ - <code>\"skeletons\"</code> key:\\ <code>Dataset.skeletons</code>\\ \\ - <code>\"default_skeleton\"</code> key:\\ <code>Dataset.default_skeleton</code>\\ \\ - <code>\"app_config\"</code> key:\\ <code>Dataset.app_config</code>\\ \\ \\ ### Writing a custom Dataset type \u00b6\\ \\ FiftyOne provides the <code>Dataset</code> type system so that dataset formats can be\\ conveniently referenced by their type when reading/writing datasets on disk.\\ \\ The primary function of the <code>Dataset</code> subclasses is to define the\\ <code>DatasetImporter</code> that should be used to read instances of the dataset from\\ disk and the <code>DatasetExporter</code> that should be used to write instances of the\\ dataset to disk.\\ \\ See this page for more information\\ about defining custom <code>DatasetExporter</code> classes.\\ \\ Custom dataset types can be declared by implementing the <code>Dataset</code> subclass\\ corresponding to the type of dataset that you are working with.\\ \\</p>"},{"location":"fiftyone_concepts/dataset_creation/samples/","title":"Using Sample Parsers \u00b6","text":"<p>This page describes how to use the <code>SampleParser</code> interface to add samples to your FiftyOne datasets.</p> <p>The <code>SampleParser</code> interface provides native support for loading samples in a variety of common formats, and it can be easily extended to import datasets in custom formats, allowing you to automate the dataset loading process.</p> <p>Warning</p> <p>Using the <code>SampleParser</code> interface is not recommended. You\u2019ll likely prefer adding samples manually or using dataset importers to load data into FiftyOne.</p>"},{"location":"fiftyone_concepts/dataset_creation/samples/#adding-samples-to-datasets","title":"Adding samples to datasets \u00b6","text":""},{"location":"fiftyone_concepts/dataset_creation/samples/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The basic recipe for using the <code>SampleParser</code> interface to add samples to a <code>Dataset</code> is to create a parser of the appropriate type and then pass the parser along with an iterable of samples to the appropriate <code>Dataset</code> method.</p> <p>Note</p> <p>A typical use case is that <code>samples</code> in the above recipe is a <code>torch.utils.data.Dataset</code> or an iterable generated by <code>tf.data.Dataset.as_numpy_iterator()</code>.</p>"},{"location":"fiftyone_concepts/dataset_creation/samples/#adding-unlabeled-images","title":"Adding unlabeled images \u00b6","text":"<p>FiftyOne provides a few convenient ways to add unlabeled images in FiftyOne datasets.</p>"},{"location":"fiftyone_concepts/dataset_creation/samples/#adding-a-directory-of-images","title":"Adding a directory of images \u00b6","text":"<p>Use <code>Dataset.add_images_dir()</code> to add a directory of images to a dataset:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\n# A directory of images to add\nimages_dir = \"/path/to/images\"\n\n# Add images to the dataset\ndataset.add_images_dir(images_dir)\n</code></pre>"},{"location":"fiftyone_concepts/dataset_creation/samples/#adding-a-glob-pattern-of-images","title":"Adding a glob pattern of images \u00b6","text":"<p>Use <code>Dataset.add_images_patt()</code> to add a glob pattern of images to a dataset:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\n# A glob pattern of images to add\nimages_patt = \"/path/to/images/*.jpg\"\n\n# Add images to the dataset\ndataset.add_images_patt(images_patt)\n</code></pre>"},{"location":"fiftyone_concepts/dataset_creation/samples/#adding-images-using-a-sampleparser","title":"Adding images using a SampleParser \u00b6","text":"<p>Use <code>Dataset.add_images()</code> to add an iterable of unlabeled images that can be parsed via a specified <code>UnlabeledImageSampleParser</code> to a dataset.</p> <p>Example</p> <p>FiftyOne provides an <code>ImageSampleParser</code> that handles samples that contain either an image that can be converted to numpy format via <code>np.asarray()</code> of the path to an image on disk.</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.data as foud\n\ndataset = fo.Dataset()\n\n# An iterable of images or image paths and the UnlabeledImageSampleParser\n# to use to parse them\nsamples = ...\nsample_parser = foud.ImageSampleParser\n\n# Add images to the dataset\ndataset.add_images(samples, sample_parser)\n</code></pre>"},{"location":"fiftyone_concepts/dataset_creation/samples/#adding-labeled-images","title":"Adding labeled images \u00b6","text":"<p>Use <code>Dataset.add_labeled_images()</code> to add an iterable of samples that can be parsed via a specified <code>LabeledImageSampleParser</code> to a dataset.</p> <p>Example</p> <p>FiftyOne provides an <code>ImageClassificationSampleParser</code> that handles samples that contain <code>(image_or_path, target)</code> tuples, where:</p> <ul> <li> <p><code>image_or_path</code> is either an image that can be converted to numpy format via <code>np.asarray()</code> or the path to an image on disk</p> </li> <li> <p><code>target</code> is either a class ID or a label string</p> </li> </ul> <p>The snippet below adds an iterable of image classification data in the above format to a dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.data as foud\n\ndataset = fo.Dataset()\n\n# An iterable of `(image_or_path, target)` tuples and the\n# LabeledImageSampleParser to use to parse them\nsamples = ...\nsample_parser = foud.ImageClassificationSampleParser\n\n# Add labeled images to the dataset\ndataset.add_labeled_images(samples, sample_parser)\n</code></pre>"},{"location":"fiftyone_concepts/dataset_creation/samples/#adding-unlabeled-videos","title":"Adding unlabeled videos \u00b6","text":"<p>FiftyOne provides a few convenient ways to add unlabeled videos in FiftyOne datasets.</p>"},{"location":"fiftyone_concepts/dataset_creation/samples/#adding-a-directory-of-videos","title":"Adding a directory of videos \u00b6","text":"<p>Use <code>Dataset.add_videos_dir()</code> to add a directory of videos to a dataset:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\n# A directory of videos to add\nvideos_dir = \"/path/to/videos\"\n\n# Add videos to the dataset\ndataset.add_videos_dir(videos_dir)\n</code></pre>"},{"location":"fiftyone_concepts/dataset_creation/samples/#adding-a-glob-pattern-of-videos","title":"Adding a glob pattern of videos \u00b6","text":"<p>Use <code>Dataset.add_videos_patt()</code> to add a glob pattern of videos to a dataset:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\n\n# A glob pattern of videos to add\nvideos_patt = \"/path/to/videos/*.mp4\"\n\n# Add videos to the dataset\ndataset.add_videos_patt(videos_patt)\n</code></pre>"},{"location":"fiftyone_concepts/dataset_creation/samples/#adding-videos-using-a-sampleparser","title":"Adding videos using a SampleParser \u00b6","text":"<p>Use <code>Dataset.add_videos()</code> to add an iterable of unlabeled videos that can be parsed via a specified <code>UnlabeledVideoSampleParser</code> to a dataset.</p> <p>Example</p> <p>FiftyOne provides a <code>VideoSampleParser</code> that handles samples that directly contain the path to the video on disk.</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.data as foud\n\ndataset = fo.Dataset()\n\n# An iterable of video paths and the UnlabeledVideoSampleParser to use to\n# parse them\nsamples = ...\nsample_parser = foud.VideoSampleParser\n\n# Add videos to the dataset\ndataset.add_videos(samples, sample_parser)\n</code></pre>"},{"location":"fiftyone_concepts/dataset_creation/samples/#adding-labeled-videos","title":"Adding labeled videos \u00b6","text":"<p>Use <code>Dataset.add_labeled_videos()</code> to add an iterable of samples that can be parsed via a specified <code>LabeledVideoSampleParser</code> to a dataset.</p> <p>Example</p> <p>FiftyOne provides a <code>VideoLabelsSampleParser</code> that handles samples that contain <code>(video_path, video_labels_or_path)</code> tuples, where:</p> <ul> <li> <p><code>video_path</code> is the path to a video on disk</p> </li> <li> <p><code>video_labels_or_path</code> is an <code>eta.core.video.VideoLabels</code> instance, a serialized dict representation of one, or the path to one on disk</p> </li> </ul> <p>The snippet below adds an iterable of labeled video samples in the above format to a dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.data as foud\n\ndataset = fo.Dataset()\n\n# An iterable of `(video_path, video_labels_or_path)` tuples and the\n# LabeledVideoSampleParser to use to parse them\nsamples = ...\nsample_parser = foud.VideoLabelsSampleParser\n\n# Add labeled videos to the dataset\ndataset.add_labeled_videos(samples, sample_parser)\n</code></pre>"},{"location":"fiftyone_concepts/dataset_creation/samples/#ingesting-samples-into-datasets","title":"Ingesting samples into datasets \u00b6","text":"<p>Creating FiftyOne datasets typically does not create copies of the source media, since <code>Sample</code> instances store the <code>filepath</code> to the media, not the media itself.</p> <p>However, in certain circumstances, such as loading data from binary sources like TFRecords or creating a FiftyOne dataset from unorganized and/or temporary files on disk, it can be desirable to ingest the raw media for each sample into a common backing location.</p> <p>FiftyOne provides support for ingesting samples and their underlying source media in both common formats and can be extended to import datasets in custom formats.</p>"},{"location":"fiftyone_concepts/dataset_creation/samples/#basic-recipe_1","title":"Basic recipe \u00b6","text":"<p>The basic recipe for ingesting samples and their source media into a <code>Dataset</code> is to create a <code>SampleParser</code> of the appropriate type of sample that you\u2019re loading and then pass the parser along with an iterable of samples to the appropriate <code>Dataset</code> method.</p> <p>Note</p> <p>A typical use case is that <code>samples</code> in the above recipe is a <code>torch.utils.data.Dataset</code> or an iterable generated by <code>tf.data.Dataset.as_numpy_iterator()</code>.</p>"},{"location":"fiftyone_concepts/dataset_creation/samples/#ingesting-unlabeled-images","title":"Ingesting unlabeled images \u00b6","text":"<p>Use <code>Dataset.ingest_images()</code> to ingest an iterable of unlabeled images that can be parsed via a specified <code>UnlabeledImageSampleParser</code> into a dataset.</p> <p>The <code>has_image_path</code> property of the parser may either be <code>True</code> or <code>False</code>. If the parser provides image paths, the source images will be directly copied from their source locations into the backing directory for the dataset; otherwise, the image will be read in-memory via <code>get_image()</code> and then written to the backing directory.</p> <p>Example</p> <p>FiftyOne provides an <code>ImageSampleParser</code> that handles samples that contain either an image that can be converted to numpy format via <code>np.asarray()</code> of the path to an image on disk.</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.data as foud\n\ndataset = fo.Dataset()\n\n# An iterable of images or image paths and the UnlabeledImageSampleParser\n# to use to parse them\nsamples = ...\nsample_parser = foud.ImageSampleParser\n\n# A directory in which the images will be written; If `None`, a default directory\n# based on the dataset's `name` will be used\ndataset_dir = ...\n\n# Ingest the images into the dataset\n# The source images are copied into `dataset_dir`\ndataset.ingest_images(samples, sample_parser, dataset_dir=dataset_dir)\n</code></pre>"},{"location":"fiftyone_concepts/dataset_creation/samples/#ingesting-labeled-images","title":"Ingesting labeled images \u00b6","text":"<p>Use <code>Dataset.ingest_labeled_images()</code> to ingest an iterable of samples that can be parsed via a specified <code>LabeledImageSampleParser</code> into a dataset.</p> <p>The <code>has_image_path</code> property of the parser may either be <code>True</code> or <code>False</code>. If the parser provides image paths, the source images will be directly copied from their source locations into the backing directory for the dataset; otherwise, the image will be read in-memory via <code>get_image()</code> and then written to the backing directory.</p> <p>Example</p> <p>FiftyOne provides an <code>ImageClassificationSampleParser</code> that handles samples that contain <code>(image_or_path, target)</code> tuples, where:</p> <ul> <li> <p><code>image_or_path</code> is either an image that can be converted to numpy format via <code>np.asarray()</code> or the path to an image on disk</p> </li> <li> <p><code>target</code> is either a class ID or a label string</p> </li> </ul> <p>The snippet below ingests an iterable of image classification data in the above format intoa a FiftyOne dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.data as foud\n\ndataset = fo.Dataset()\n\n# An iterable of `(image_or_path, target)` tuples and the\n# LabeledImageSampleParser to use to parse them\nsamples = ...\nsample_parser = foud.ImageClassificationSampleParser  # for example\n\n# A directory in which the images will be written; If `None`, a default directory\n# based on the dataset's `name` will be used\ndataset_dir = ...\n\n# Ingest the labeled images into the dataset\n# The source images are copied into `dataset_dir`\ndataset.ingest_labeled_images(samples, sample_parser, dataset_dir=dataset_dir)\n</code></pre>"},{"location":"fiftyone_concepts/dataset_creation/samples/#ingesting-unlabeled-videos","title":"Ingesting unlabeled videos \u00b6","text":"<p>Use <code>Dataset.ingest_videos()</code> to ingest an iterable of unlabeled videos that can be parsed via a specified <code>UnlabeledVideoSampleParser</code> into a dataset.</p> <p>The source videos will be directly copied from their source locations into the backing directory for the dataset.</p> <p>Example</p> <p>FiftyOne provides a <code>VideoSampleParser</code> that handles samples that directly contain the paths to videos on disk.</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.data as foud\n\ndataset = fo.Dataset()\n\n# An iterable of videos or video paths and the UnlabeledVideoSampleParser\n# to use to parse them\nsamples = ...\nsample_parser = foud.VideoSampleParser\n\n# A directory in which the videos will be written; If `None`, a default directory\n# based on the dataset's `name` will be used\ndataset_dir = ...\n\n# Ingest the videos into the dataset\n# The source videos are copied into `dataset_dir`\ndataset.ingest_videos(samples, sample_parser, dataset_dir=dataset_dir)\n</code></pre>"},{"location":"fiftyone_concepts/dataset_creation/samples/#ingesting-labeled-videos","title":"Ingesting labeled videos \u00b6","text":"<p>Use <code>Dataset.ingest_labeled_videos()</code> to ingest an iterable of samples that can be parsed via a specified <code>LabeledVideoSampleParser</code> into a dataset.</p> <p>The source videos will be directly copied from their source locations into the backing directory for the dataset.</p> <p>Example</p> <p>FiftyOne provides a <code>VideoLabelsSampleParser</code> that handles samples that contain <code>(video_path, video_labels_or_path)</code> tuples, where:</p> <ul> <li> <p><code>video_path</code> is the path to a video on disk</p> </li> <li> <p><code>video_labels_or_path</code> is an <code>eta.core.video.VideoLabels</code> instance, a serialized dict representation of one, or the path to one on disk</p> </li> </ul> <p>The snippet below ingests an iterable of labeled videos in the above format into a FiftyOne dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.data as foud\n\ndataset = fo.Dataset()\n\n# An iterable of `(video_path, video_labels_or_path)` tuples and the\n# LabeledVideoSampleParser to use to parse them\nsamples = ...\nsample_parser = foud.VideoLabelsSampleParser  # for example\n\n# A directory in which the videos will be written; If `None`, a default directory\n# based on the dataset's `name` will be used\ndataset_dir = ...\n\n# Ingest the labeled videos into the dataset\n# The source videos are copied into `dataset_dir`\ndataset.ingest_labeled_videos(samples, sample_parser, dataset_dir=dataset_dir)\n</code></pre>"},{"location":"fiftyone_concepts/dataset_creation/samples/#built-in-sampleparser-classes","title":"Built-in SampleParser classes \u00b6","text":"<p>The table below lists the common data formats for which FiftyOne provides built-in <code>SampleParser</code> implementations. You can also write a custom SampleParser to automate the parsing of samples in your own custom data format.</p> <p>You can use a <code>SampleParser</code> to add samples to datasets and ingest samples into datasets.</p> SampleParser Description <code>ImageSampleParser</code> A sample parser that parses raw image samples. <code>VideoSampleParser</code> A sample parser that parses raw video samples. <code>ImageClassificationSampleParser</code> Generic parser for image classification samples whose labels are represented as <code>Classification</code> instances. <code>ImageDetectionSampleParser</code> Generic parser for image detection samples whose labels are represented as <code>Detections</code> instances. <code>ImageLabelsSampleParser</code> Generic parser for image detection samples whose labels are stored inETA ImageLabels format. <code>FiftyOneImageClassificationSampleParser</code> Parser for samples in FiftyOne image classification datasets. See<code>FiftyOneImageClassificationDataset</code> for formatdetails. <code>FiftyOneImageDetectionSampleParser</code> Parser for samples in FiftyOne image detection datasets. See<code>FiftyOneImageDetectionDataset</code> for format details. <code>FiftyOneImageLabelsSampleParser</code> Parser for samples in FiftyOne image labels datasets. See<code>FiftyOneImageLabelsDataset</code> for format details. <code>FiftyOneVideoLabelsSampleParser</code> Parser for samples in FiftyOne video labels datasets. See<code>FiftyOneVideoLabelsDataset</code> for format details. <code>TFImageClassificationSampleParser</code> Parser for image classification samples stored asTFRecords. <code>TFObjectDetectionSampleParser</code> Parser for image detection samples stored inTF Object Detection API format."},{"location":"fiftyone_concepts/dataset_creation/samples/#writing-a-custom-sampleparser","title":"Writing a custom SampleParser \u00b6","text":"<p>FiftyOne provides a variety of built-in SampleParser classes to parse data in common formats. However, if your samples are stored in a custom format, you can provide a custom <code>SampleParser</code> class and provide it to FiftyOne when adding or ingesting samples into your datasets.</p> <p>The <code>SampleParser</code> interface provides a mechanism for defining methods that parse a data sample that is stored in a particular (external to FiftyOne) format and return various elements of the sample in a format that FiftyOne understands.</p> <p><code>SampleParser</code> itself is an abstract interface; the concrete interface that you should implement is determined by the type of samples that you are importing. For example, <code>LabeledImageSampleParser</code> defines an interface for parsing information from a labeled image sample, such as the path to the image on disk, the image itself, metadata about the image, and the label (e.g., classification or object detections) associated with the image.</p>"},{"location":"getting_started/","title":"Getting started with FiftyOne","text":"<p>Welcome to getting started with FiftyOne! The goal of this section is to get you introduced and hands on with some of the basic building blocks for FiftyOne. </p> <p>The basic getting started  will cover essentials ideas and functionality in both the API and the Application. </p> <p>After this getting started you should be on your way to tackling your projects with FiftyOne. You can move on to a getting more specific to your work, try some of the tutorials, or just dive right in.</p> <p>For the bare bones getting started: 1. Install 2. Set up a virtual environment 3. GO!</p>"},{"location":"getting_started/basic/application_tour/","title":"Tour of the Application","text":""},{"location":"getting_started/basic/datasets_samples_fields/","title":"Datasets, Samples, and Fields","text":""},{"location":"getting_started/basic/install/","title":"FiftyOne Installation \u00b6","text":"<p>Note</p> <p>Need to collaborate on your datasets? Check out FiftyOne Teams!</p>"},{"location":"getting_started/basic/install/#prerequisites","title":"Prerequisites \u00b6","text":"<p>You will need a working Python installation. FiftyOne currently requires Python 3.9 - 3.11</p> <p>On Linux, we recommend installing Python through your system package manager (APT, YUM, etc.) if it is available. On other platforms, Python can be downloaded from python.org. To verify that a suitable Python version is installed and accessible, run <code>python --version</code>.</p> <p>We encourage installing FiftyOne in a virtual environment. See setting up a virtual environment for more details.</p>"},{"location":"getting_started/basic/install/#installing-fiftyone","title":"Installing FiftyOne \u00b6","text":"<p>To install FiftyOne, ensure you have activated any virtual environment that you are using, then run:</p> <pre><code>pip install fiftyone\n</code></pre> <p>This will install FiftyOne and all of its dependencies. Once this has completed, you can verify that FiftyOne is installed in your virtual environment by importing the <code>fiftyone</code> package:</p> <pre><code>$ python\n&gt;&gt;&gt;\n&gt;&gt;&gt; import fiftyone as fo\n&gt;&gt;&gt;\n</code></pre> <p>A successful installation of FiftyOne should result in no output when <code>fiftyone</code> is imported. See this section for install troubleshooting tips.</p> <p>If you want to work with video datasets, you\u2019ll also need to install FFmpeg:</p>"},{"location":"getting_started/basic/install/#quickstart","title":"Quickstart \u00b6","text":"<p>Dive right into FiftyOne by opening a Python shell and running the snippet below, which downloads a small dataset and launches the FiftyOne App so you can explore it!</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note that if you are running this code in a script, you must include <code>session.wait()</code> to block execution until you close the App. See this page for more information.</p>"},{"location":"getting_started/basic/install/#troubleshooting","title":"Troubleshooting \u00b6","text":"<p>If you run into any installation issues, review the suggestions below or check the troubleshooting page for more details.</p> <p>Note</p> <p>Most installation issues can be fixed by upgrading some packages and then rerunning the FiftyOne install:</p> <pre><code>pip install --upgrade pip setuptools wheel build\npip install fiftyone\n</code></pre> <p>Mac users:</p> <ul> <li>You must have the XCode Command Line Tools package installed on your machine. You likely already have it, but if you encounter an error message like <code>error: command 'clang' failed with exit status 1</code>, then you may need to install it via <code>xcode-select --install</code>, or see this page for other options.</li> </ul> <p>Linux users:</p> <ul> <li> <p>The <code>psutil</code> package may require Python headers to be installed on your system. On Debian-based distributions, these are available in the <code>python3-dev</code> package.</p> </li> <li> <p>If you encounter an error related to MongoDB failing to start, such as <code>Could not find mongod</code>, you may need to install additional packages. See the alternative Linux builds for details.</p> </li> </ul> <p>Windows users:</p> <ul> <li>If you encounter a <code>psutil.NoSuchProcessExists</code> when importing <code>fiftyone</code>, you will need to install the 64-bit Visual Studio 2015 C++ redistributable library. See here for instructions.</li> </ul>"},{"location":"getting_started/basic/install/#installing-extras","title":"Installing extras \u00b6","text":"<p>Various tutorials and guides that we provide on this site require additional packages in order to run. If you encounter a missing package, you will see helpful instructions on what you need to install. Alternatively, you can preemptively install what you\u2019ll need by installing the following additional packages via <code>pip</code> in your virtual environment:</p> <ul> <li> <p><code>ipython</code> to follow along with interactive examples more easily (note that a system-wide IPython installation will not work in a virtual environment, even if it is accessible)</p> </li> <li> <p><code>torch</code> and <code>torchvision</code> for examples requiring PyTorch. The installation process can vary depending on your system, so consult the PyTorch documentation for specific instructions.</p> </li> <li> <p><code>tensorflow</code> for examples requiring TensorFlow. The installation process can vary depending on your system, so consult the Tensorflow documentation for specific instructions.</p> </li> <li> <p><code>tensorflow-datasets</code> for examples that rely on loading TensorFlow datasets</p> </li> <li> <p>FFmpeg, in order to work with video datasets in FiftyOne. See this page for installation instructions.</p> </li> </ul> <p>Note</p> <p>FiftyOne does not strictly require any of these packages, so you can install only what you need. If you run something that requires an additional package, you will see a helpful message telling you what to install.</p>"},{"location":"getting_started/basic/install/#upgrading-fiftyone","title":"Upgrading FiftyOne \u00b6","text":"<p>You can upgrade an existing FiftyOne installation by passing the <code>--upgrade</code> option to <code>pip install</code>:</p> <pre><code>pip install --upgrade fiftyone\n</code></pre> <p>Note</p> <p>New versions of FiftyOne occasionally introduce data model changes that require database migrations after you upgrade. Rest assured, these migrations will be automatically performed on a per-dataset basis whenever you load a dataset for the first time in a newer version of FiftyOne.</p> <p>Note</p> <p>If you are working with a custom/shared MongoDB database, you can use database admin privileges to control which clients are allowed to upgrade your FiftyOne deployment.</p>"},{"location":"getting_started/basic/install/#downgrading-fiftyone","title":"Downgrading FiftyOne \u00b6","text":"<p>If you need to downgrade to an older version of FiftyOne for any reason, you can do so.</p> <p>Since new releases occasionally introduce backwards-incompatible changes to the data model, you must use the fiftyone migrate command to perform any necessary downward database migrations before installing the older version of FiftyOne.</p> <p>Here\u2019s the workflow for downgrading to an older version of FiftyOne:</p> <pre><code># The version that you wish to downgrade to\nVERSION=0.15.1\n\n# Migrate the database\nfiftyone migrate --all -v $VERSION\n\n# Now install the older version of `fiftyone`\npip install fiftyone==$VERSION\n\n# Optional: verify that your datasets were migrated\nfiftyone migrate --info\n</code></pre> <p>If you are reading this after encountering an error resulting from downgrading your <code>fiftyone</code> package without first running fiftyone migrate, don\u2019t worry, you simply need to reinstall the newer version of FiftyOne and then follow these instructions.</p> <p>See this page if you need to install FiftyOne v0.7.3 or earlier.</p> <p>Note</p> <p>If you are working with a custom/shared MongoDB database, you can use database admin privileges to control which clients are allowed to downgrade your FiftyOne deployment.</p>"},{"location":"getting_started/basic/install/#uninstalling-fiftyone","title":"Uninstalling FiftyOne \u00b6","text":"<p>FiftyOne and all of its subpackages can be uninstalled with:</p> <pre><code>pip uninstall fiftyone fiftyone-brain fiftyone-db\n</code></pre>"},{"location":"getting_started/basic/troubleshooting/","title":"Install Troubleshooting \u00b6","text":"<p>This page lists common issues encountered when installing FiftyOne and possible solutions. If you encounter an issue that this page doesn\u2019t help you resolve, feel free to open an issue on GitHub or contact us on Discord.</p> <p>Note</p> <p>Most installation issues can be fixed by upgrading some packages and then rerunning the FiftyOne install. So, try this first before reading on:</p> <pre><code>pip install --upgrade pip setuptools wheel build\npip install fiftyone\n</code></pre>"},{"location":"getting_started/basic/troubleshooting/#pythonpip-incompatibilities","title":"Python/pip incompatibilities \u00b6","text":""},{"location":"getting_started/basic/troubleshooting/#no-matching-distribution-found","title":"\u201cNo matching distribution found\u201d \u00b6","text":"<p>If you attempt to install FiftyOne with a version of Python or pip that is too old, you may encounter errors like these:</p> <pre><code>ERROR: Could not find a version that satisfies the requirement fiftyone (from versions: none)\nERROR: No matching distribution found for fiftyone\n</code></pre> <pre><code>Could not find a version that satisfies the requirement fiftyone-brain (from versions: )\nNo matching distribution found for fiftyone-brain\n</code></pre> <pre><code>fiftyone requires Python '&gt;=3.9' but the running Python is 3.4.10\n</code></pre> <p>To resolve this, you will need to use Python 3.9 or newer, and pip 19.3 or newer. See the installation guide for details. If you have installed a suitable version of Python in a virtual environment and still encounter this error, ensure that the virtual environment is activated. See the virtual environment setup guide for more details.</p> <p>Note</p> <p>FiftyOne does not support 32-bit platforms.</p>"},{"location":"getting_started/basic/troubleshooting/#package-fiftyone-requires-a-different-python","title":"\u201cPackage \u2018fiftyone\u2019 requires a different Python\u201d \u00b6","text":"<p>This error occurs when attempting to install FiftyOne with an unsupported Python version (either too old or too new). See the installation guide for details on which versions of Python are supported by FiftyOne.</p> <p>If you have multiple Python installations, you may be using <code>pip</code> from an incompatible Python installation. Run <code>pip --version</code> to see which Python version <code>pip</code> is using. If you see an unsupported or unexpected Python version reported, there are several possible causes, including:</p> <ul> <li> <p>You may not have activated a virtual environment in your current shell. Refer to the virtual environment setup guide for details.</p> </li> <li> <p>If you are intentionally using your system Python installation instead of a virtual environment, your system-wide <code>pip</code> may use an unsupported Python version. For instance, on some Linux systems, <code>pip</code> uses Python 2, and <code>pip3</code> uses Python 3. If this is the case, try installing FiftyOne with <code>pip3</code> instead of <code>pip</code>.</p> </li> <li> <p>You may not have a compatible Python version installed. See the installation guide for details.</p> </li> </ul>"},{"location":"getting_started/basic/troubleshooting/#no-module-named-skbuild","title":"\u201cNo module named skbuild\u201d \u00b6","text":"<p>On Linux, this error can occur when attempting to install OpenCV with an old pip version. To fix this, upgrade pip. See the installation guide for instructions, or the opencv-python FAQ for more details.</p>"},{"location":"getting_started/basic/troubleshooting/#videos-do-not-load-in-the-app","title":"Videos do not load in the App \u00b6","text":"<p>You need to install FFmpeg in order to work with video datasets:</p> <p>Without FFmpeg installed, videos may appear in the App, but they will not be rendered with the correct aspect ratio and thus label overlays will not be positioned correctly.</p>"},{"location":"getting_started/basic/troubleshooting/#ipython-installation","title":"IPython installation \u00b6","text":"<p>If you are using IPython and a virtual environment for FiftyOne, IPython must be installed in the virtual environment, per the installation guide. If you attempt to use a system-wide IPython installation in a virtual environment with FiftyOne, you may encounter errors such as:</p> <pre><code>.../IPython/core/interactiveshell.py:935: UserWarning: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.\n</code></pre> <pre><code>File \".../fiftyone/core/../_service_main.py\", line 29, in &lt;module&gt;\n    import psutil\nModuleNotFoundError: No module named 'psutil'\n</code></pre> <pre><code>ServerSelectionTimeoutError: localhost:27017: [Errno 111] Connection refused\n</code></pre> <p>To resolve this, install IPython in your active virtual environment (see the virtual environment guide for more information):</p> <pre><code>pip install ipython\n</code></pre>"},{"location":"getting_started/basic/troubleshooting/#import-and-database-issues","title":"Import and database issues \u00b6","text":"<p>FiftyOne includes a <code>fiftyone-db</code> package wheel for your operating system and hardware. If you have not configured your own database connection, then FiftyOne\u2019s database service will attempt to start up on import using the MongoDB distribution provided by <code>fiftyone-db</code>. If the database fails to start, importing <code>fiftyone</code> will result in exceptions being raised.</p>"},{"location":"getting_started/basic/troubleshooting/#downgrading-to-old-versions","title":"Downgrading to old versions \u00b6","text":"<p>The fiftyone migrate command was introduced in FiftyOne v0.7.3. If you would like to downgrade from a FiftyOne version prior to v0.7.3 (to a yet older version), then you will first need to upgrade to v0.7.3 or later and then downgrade:</p> <pre><code># The version that you wish to downgrade to\nVERSION=0.7.0\n\npip install fiftyone==0.7.3\nfiftyone migrate --all -v $VERSION\npip install fiftyone==$VERSION\n</code></pre> <p>To install a FiftyOne version prior to v0.7.0, you must add <code>--index</code>:</p> <pre><code>pip install --index https://pypi.voxel51.com fiftyone==&lt;version&gt;\n</code></pre>"},{"location":"getting_started/basic/troubleshooting/#database-exits","title":"Database exits \u00b6","text":"<p>On some UNIX systems, the default open file limit setting is too small for FiftyOne\u2019s MongoDB connection. The database service will exit in this case. Running <code>ulimit -n 64000</code> should resolve the issue. 64,000 is the recommended open file limit. MongoDB has full documentation on the issue here.</p>"},{"location":"getting_started/basic/troubleshooting/#troubleshooting-linux-imports","title":"Troubleshooting Linux imports \u00b6","text":"<p><code>fiftyone-db</code> officially supports Amazon Linux 2 and 2023, Debian 9+ (x86_64 only), Ubuntu 18.04+, and RHEL/CentOS 7+ Linux distributions. The correct MongoDB build is downloaded and installed while building the package wheel on your machine.</p> <p>If a suitable MongoDB build is not available or otherwise does not work in your environment, you may encounter a <code>FiftyOneConfigError</code>.</p> <p>If you have output similar to the below, you may just need to install <code>libssl</code> packages.</p> <pre><code>Subprocess ['.../site-packages/fiftyone/db/bin/mongod', ...] exited with error 127:\n.../site-packages/fiftyone/db/bin/mongod: error while loading shared libraries:\n  libcrypto.so.1.1: cannot open shared object file: No such file or directory\n</code></pre> <p>On Ubuntu, <code>libssl</code> packages can be install with the following command:</p> <pre><code>sudo apt install libssl-dev\n</code></pre> <p>If you still face issues with imports, you can follow these instructions to configure FiftyOne to use a MongoDB instance that you have installed yourself.</p>"},{"location":"getting_started/basic/troubleshooting/#troubleshooting-windows-imports","title":"Troubleshooting Windows imports \u00b6","text":"<p>If your encounter a <code>psutil.NoSuchProcessExists</code> exists when importing <code>fiftyone</code>, you are likely missing the C++ libraries MongoDB requires.</p> <pre><code>psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=XXXX)\n</code></pre> <p>Downloading and installing the Microsoft Visual C++ Redistributable from this page should resolve the issue. Specifically, you will want to download the <code>vc_redist.x64.exe</code> redistributable.</p>"},{"location":"getting_started/basic/virtualenv/","title":"Virtual Environment Setup \u00b6","text":"<p>This page describes how to create a Python virtual environment.</p> <p>Using a virtual environment is strongly recommended because it allows maintaining an isolated environment in which FiftyOne and its dependencies can be installed. FiftyOne has a variety of dependencies, some versions of which may conflict with versions already installed on your machine.</p>"},{"location":"getting_started/basic/virtualenv/#creating-a-virtual-environment-using-venv","title":"Creating a virtual environment using <code>venv</code> \u00b6","text":"<p>First, identify a suitable Python executable. On many systems, this will be <code>python3</code> , but it may be <code>python</code> on other systems instead. To confirm your Python version, pass <code>--version</code> to Python. Here is example output from running these commands:</p> <pre><code>$ python --version\nPython 2.7.17\n$ python3 --version\nPython 3.9.20\n</code></pre> <p>In this case, <code>python3</code> should be used in the next step.</p> <p>Navigate to a folder where you would like to create the virtual environment. Using the suitable Python version you have identified, run the following to create a virtual environment called <code>env</code> (you can choose any name):</p> <pre><code>python3 -m venv env\n</code></pre> <p>Replace <code>python3</code> at the beginning of a command if your Python executable has a different name. This will create a new virtual environment in the <code>env</code> folder, with standalone copies of Python and pip, as well as an isolated location to install packages to. However, this environment will not be used until it is activated. To activate the virtual environment, run the following command:</p> <p>After running this command, your shell prompt should begin with <code>(env)</code> , which indicates that the virtual environment has been activated. This state will only affect your current shell, so if you start a new shell, you will need to activate the virtual environment again to use it. When the virtual environment is active, <code>python</code> without any suffix will refer to the Python version you used to create the virtual environment, so you can use this for the remainder of this guide. For example:</p> <pre><code>$ python --version\nPython 3.9.20\n</code></pre> <p>Also note that <code>python</code> and <code>pip</code> live inside the <code>env</code> folder (in this output, the path to the current folder is replaced with <code>...</code>):</p> <p>Before you continue, you should upgrade <code>pip</code> and some related packages in the virtual environment. FiftyOne\u2019s packages rely on some newer pip features, so older pip versions may fail to locate a downloadable version of FiftyOne entirely. To upgrade, run the following command:</p> <pre><code>pip install --upgrade pip setuptools wheel build\n</code></pre> <p>To leave an activated virtual environment and return to using your system-wide Python installation, run <code>deactivate</code>. For more documentation on <code>venv</code>, including additional setup options, see here.</p>"},{"location":"getting_started/basic/virtualenv/#alternatives-to-venv","title":"Alternatives to <code>venv</code> \u00b6","text":"<p>There are lots of ways to set up and work with virtual environments, some of which are listed here. These may be particularly useful to review if you are dealing with virtual environments frequently:</p> <ul> <li> <p>There is a similar virtualenv package ( <code>pip install virtualenv</code>) that supports older Python versions.</p> </li> <li> <p>virtualenvwrapper adds some convenient shell support for creating and managing virtual environments.</p> </li> </ul> <p>Warning</p> <p>We currently discourage using <code>pipenv</code> with FiftyOne, as it has known issues with installing packages from custom package indices.</p>"},{"location":"getting_started/focused_getting_starteds/","title":"Index","text":"<p>Here are all the particular getting starteds beyond the initial foundation. You can find material for: 1. Medical Imaging</p>"},{"location":"getting_started/focused_getting_starteds/medical_imaging/","title":"Index","text":"<p>This will be a getting started flow for getting started with Medical Imaging</p>"},{"location":"how_do_i/","title":"How Do I?","text":"<p>This section covers task size pieces of information or tutorials. It's focused on completing a singular task, such as import photos or update annotations, rather than an entire workflow.</p> <p>I am thinking through tips and tricks in addition to the items here - I am worried about findability. This may need to  get reorganized by subject areas just to guide people a bit more.</p>"},{"location":"how_do_i/#cheat-sheets","title":"Cheat Sheets","text":""},{"location":"how_do_i/#recipes","title":"Recipes","text":""},{"location":"how_do_i/cheat_sheets/","title":"FiftyOne Cheat Sheets \u00b6","text":"<p>Check out the links below for topic-focused, informationally dense documents that will help you get right down to business using FiftyOne.</p>"},{"location":"how_do_i/cheat_sheets/#fiftyone-terminology","title":"FiftyOne terminology","text":"<p>A brief overview of the key terminology in the world of FiftyOne.</p> <p>Learn the lingo</p>"},{"location":"how_do_i/cheat_sheets/#filtering-cheat-sheet","title":"Filtering cheat sheet","text":"<p>Learn the basic syntaxes for matching and filtering in FiftyOne.</p> <p>Write some filters</p>"},{"location":"how_do_i/cheat_sheets/#views-cheat-sheet","title":"Views cheat sheet","text":"<p>Use dataset views to retrieve the subsets of your data you're looking for.</p> <p>Master dataset views</p>"},{"location":"how_do_i/cheat_sheets/#pandas-vs-fiftyone","title":"pandas vs FiftyOne","text":"<p>Perform pandas-style computer vision queries in FiftyOne.</p> <p>Queue the queries</p>"},{"location":"how_do_i/cheat_sheets/fiftyone_terminology/","title":"FiftyOne terminology \u00b6","text":"<p>This cheat sheet introduces the key terminology in the world of FiftyOne.</p>"},{"location":"how_do_i/cheat_sheets/fiftyone_terminology/#the-basics","title":"The basics \u00b6","text":"FiftyOne The open-source framework, the core library,and the Python SDK. FiftyOne App The provided user interface for graphicallyviewing, filtering, and understanding your datasets. Can be launched inthe browser or within notebooks. FiftyOne Teams The enterprise-grade suitebuilt on top of FiftyOne for collaboration, permissioning, and workingwith cloud-backed media."},{"location":"how_do_i/cheat_sheets/fiftyone_terminology/#other-components","title":"Other components \u00b6","text":"Brain Library of ML-powered capabilities forcomputation and visualization. Dataset Zoo Collection of common datasets available fordownload and loading into FiftyOne. Model Zoo Collection of pre-trained models available fordownload and inference. Plugin A module you can use to customize and extendthe behavior of FiftyOne. Operator A plugin subcomponent that defines anoperation that can be executed either directly by users in the Appand/or internally invoked by other plugin components Integration A dataset, ML framework, annotation service, or other tool FiftyOne isdirectly compatible with."},{"location":"how_do_i/cheat_sheets/fiftyone_terminology/#data-model","title":"Data model \u00b6","text":"Dataset Core data structure in FiftyOne, composed of<code>Sample</code> instances. Provides a consistent interface for loadingimages, videos, metadata, annotations, and predictions. The computervision analog of a table of data. Sample The atomic elements of a <code>Dataset</code> that store all the informationrelated to a given piece of data. Every samplehas an associated media file. The computer vision analog of a row oftabular data. DatasetView A view into a <code>Dataset</code>, which can filter,sort, transform, etc. the dataset along various axes to obtain adesired subset of the samples. ViewStage A logical operation, such as filtering, matching,or sorting, which can be used to generate a <code>DatasetView</code>. Field Attributes of <code>Sample</code> instances thatstore customizable information about thesamples. The computer vision analog of a column in a table. Embedded Document Field A collection of related fieldsorganized under a single top-level <code>Field</code>, similar to a nesteddictionary. Label Class hierarchy used tostore semantic information about ground truthor predicted labels in a sample. Builtin <code>Label</code> types include<code>Classification</code>, <code>Detections</code>, <code>Keypoints</code>, and many others. Tag A field containing a list of strings representing relevantinformation. Tags can be assigned to datasets,samples, or labels. Metadata A special <code>Sample</code> field that can be automatically populated withmedia type-specific metadata about the rawmedia associated with the sample. Aggregation A class encapsulating the computation of anaggregate statistic about the contents ofa dataset or view."},{"location":"how_do_i/cheat_sheets/fiftyone_terminology/#fiftyone-app","title":"FiftyOne App \u00b6","text":"Session An instance of the FiftyOne App connected to aspecific dataset, via which you can use to programmatically interactwith the App. Sample grid The rectangular media grid that you can scrollthrough to quickly browse the samples in a dataset. Click on any mediain the grid to open the sample modal. Sample modal The expanded modal that provides detailedinformation and visualization about an individual sample in a dataset. Sidebar Vertical component on left side of Appthat provides convenient options for filtering the dataset andtoggling the visibility of fields in the sample grid. View bar Horizontal bar at the top of the App whereyou can create and compose view stages via point and click operationsto filter your dataset and show only the content of interest."},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/","title":"Filtering Cheat Sheet \u00b6","text":"<p>This cheat sheet shows how to perform common matching and filtering operations in FiftyOne using dataset views.</p>"},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#strings-and-pattern-matching","title":"Strings and pattern matching \u00b6","text":"<p>The formulas in this section use the following example data:</p> <pre><code>import fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\nds = foz.load_zoo_dataset(\"quickstart\")\n</code></pre> Operation Command Filepath starts with \u201c/Users\u201d <code>&lt;br&gt;ds.match(F(\"filepath\").starts_with(\"/Users\"))&lt;br&gt;</code> Filepath ends with \u201c10.jpg\u201d or \u201c10.png\u201d <code>&lt;br&gt;ds.match(F(\"filepath\").ends_with((\"10.jpg\", \"10.png\"))&lt;br&gt;</code> Label contains string \u201cbe\u201d <code>&lt;br&gt;ds.filter_labels(&lt;br&gt;    \"predictions\",&lt;br&gt;    F(\"label\").contains_str(\"be\"),&lt;br&gt;)&lt;br&gt;</code> Filepath contains \u201c088\u201d and is JPEG <code>&lt;br&gt;ds.match(F(\"filepath\").re_match(\"088*.jpg\"))&lt;br&gt;</code> <p>Reference: <code>match()</code> and <code>filter_labels()</code>.</p>"},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#dates-and-times","title":"Dates and times \u00b6","text":"<p>The formulas in this section use the following example data:</p> <pre><code>from datetime import datetime, timedelta\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\nfilepaths = [\"image%d.jpg\" % i for i in range(5)]\ndates = [\\\n    datetime(2021, 8, 24, 1, 0, 0),\\\n    datetime(2021, 8, 24, 2, 0, 0),\\\n    datetime(2021, 8, 25, 3, 11, 12),\\\n    datetime(2021, 9, 25, 4, 22, 23),\\\n    datetime(2022, 9, 27, 5, 0, 0)\\\n]\n\nds = fo.Dataset()\nds.add_samples(\n    [fo.Sample(filepath=f, date=d) for f, d in zip(filepaths, dates)]\n)\n\n# Example data\nquery_date = datetime(2021, 8, 24, 2, 0, 1)\nquery_delta = timedelta(minutes=30)\n</code></pre> Operation Command After 2021-08-24 02:01:00 <code>&lt;br&gt;ds.match(F(\"date\") &gt; query_date)&lt;br&gt;</code> Within 30 minutes of 2021-08-24 02:01:00 <code>&lt;br&gt;ds.match(abs(F(\"date\") - query_date) &lt; query_delta)&lt;br&gt;</code> On the 24th of the month <code>&lt;br&gt;ds.match(F(\"date\").day_of_month() == 24)&lt;br&gt;</code> On even day of the week <code>&lt;br&gt;ds.match(F(\"date\").day_of_week() % 2 == 0)&lt;br&gt;</code> On the 268th day of the year <code>&lt;br&gt;ds.match(F(\"date\").day_of_year() == 268)&lt;br&gt;</code> In the 9th month of the year (September) <code>&lt;br&gt;ds.match(F(\"date\").month() == 9)&lt;br&gt;</code> In the 38th week of the year <code>&lt;br&gt;ds.match(F(\"date\").week() == 38)&lt;br&gt;</code> In the year 2022 <code>&lt;br&gt;ds.match(F(\"date\").year() == 2022)&lt;br&gt;</code> With minute not equal to 0 <code>&lt;br&gt;ds.match(F(\"date\").minute() != 0)&lt;br&gt;</code> <p>Reference: <code>match()</code>.</p>"},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#geospatial","title":"Geospatial \u00b6","text":"<p>The formulas in this section use the following example data:</p> <pre><code>import fiftyone.zoo as foz\n\nTIMES_SQUARE = [-73.9855, 40.7580]\nMANHATTAN = [\\\n    [\\\n        [-73.949701, 40.834487],\\\n        [-73.896611, 40.815076],\\\n        [-73.998083, 40.696534],\\\n        [-74.031751, 40.715273],\\\n        [-73.949701, 40.834487],\\\n    ]\\\n]\n\nds = foz.load_zoo_dataset(\"quickstart-geo\")\n</code></pre> Operation Command Within 5km of Times Square <code>&lt;br&gt;ds.geo_near(TIMES_SQUARE, max_distance=5000)&lt;br&gt;</code> Within Manhattan <code>&lt;br&gt;ds.geo_within(MANHATTAN)&lt;br&gt;</code> <p>Reference: <code>geo_near()</code> and <code>geo_within()</code>.</p>"},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#detections","title":"Detections \u00b6","text":"<p>The formulas in this section use the following example data:</p> <pre><code>import fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\nds = foz.load_zoo_dataset(\"quickstart\")\n</code></pre> Operation Command Predictions with confidence &gt; 0.95 <code>&lt;br&gt;ds.filter_labels(\"predictions\", F(\"confidence\") &gt; 0.95)&lt;br&gt;</code> Exactly 10 ground truth detections <code>&lt;br&gt;ds.match(F(\"ground_truth.detections\").length() == 10)&lt;br&gt;</code> At least one dog <code>&lt;br&gt;ds.match(&lt;br&gt;    F(\"ground_truth.detections.label\").contains(\"dog\")&lt;br&gt;)&lt;br&gt;</code> Images that do not contain dogs <code>&lt;br&gt;ds.match(&lt;br&gt;    ~F(\"ground_truth.detections.label\").contains(\"dog\")&lt;br&gt;)&lt;br&gt;</code> Only dog detections <code>&lt;br&gt;ds.filter_labels(\"ground_truth\", F(\"label\") == \"dog\")&lt;br&gt;</code> Images that only contain dogs <code>&lt;br&gt;ds.match(&lt;br&gt;    F(\"ground_truth.detections.label\").is_subset(&lt;br&gt;        [\"dog\"]&lt;br&gt;    )&lt;br&gt;)&lt;br&gt;</code> Contains either a cat or a dog <code>&lt;br&gt;ds.match(&lt;br&gt;     F(\"predictions.detections.label\").contains(&lt;br&gt;        [\"cat\",\"dog\"]&lt;br&gt;     )&lt;br&gt;)&lt;br&gt;</code> Contains a cat and a dog prediction <code>&lt;br&gt;ds.match(&lt;br&gt;    F(\"predictions.detections.label\").contains(&lt;br&gt;        [\"cat\", \"dog\"], all=True&lt;br&gt;    )&lt;br&gt;)&lt;br&gt;</code> Contains a cat or dog but not both <code>&lt;br&gt;field = \"predictions.detections.label\"&lt;br&gt;one_expr = F(field).contains([\"cat\", \"dog\"])&lt;br&gt;both_expr = F(field).contains([\"cat\", \"dog\"], all=True)&lt;br&gt;ds.match(one_expr &amp; ~both_expr)&lt;br&gt;</code> <p>Reference: <code>match()</code> and <code>filter_labels()</code>.</p>"},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#bounding-boxes","title":"Bounding boxes \u00b6","text":"<p>The formulas in this section assume the following code has been run:</p> <pre><code>import fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\nds = foz.load_zoo_dataset(\"quickstart\")\n\nbox_width, box_height = F(\"bounding_box\")[2], F(\"bounding_box\")[3]\nrel_bbox_area = box_width * box_height\n\nim_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\")\nabs_area = rel_bbox_area * im_width * im_height\n</code></pre> Bounding box query Command Larger than absolute size <code>&lt;br&gt;ds.filter_labels(\"predictions\", abs_area &gt; 96**2)&lt;br&gt;</code> Between two relative sizes <code>&lt;br&gt;good_bboxes = (rel_bbox_area &gt; 0.25) &amp; (rel_bbox_area &lt; 0.75)&lt;br&gt;good_expr = rel_bbox_area.let_in(good_bboxes)&lt;br&gt;ds.filter_labels(\"predictions\", good_expr)&lt;br&gt;</code> Approximately square <code>&lt;br&gt;rectangleness = abs(&lt;br&gt;    box_width * im_width - box_height * im_height&lt;br&gt;)&lt;br&gt;ds.select_fields(\"predictions\").filter_labels(&lt;br&gt;    \"predictions\", rectangleness &lt;= 1&lt;br&gt;)&lt;br&gt;</code> Aspect ratio &gt; 2 <code>&lt;br&gt;aspect_ratio = (&lt;br&gt;    (box_width * im_width) / (box_height * im_height)&lt;br&gt;)&lt;br&gt;ds.select_fields(\"predictions\").filter_labels(&lt;br&gt;    \"predictions\", aspect_ratio &gt; 2&lt;br&gt;)&lt;br&gt;</code> <p>Reference: <code>filter_labels()</code> and <code>select_fields()</code>.</p>"},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#evaluating-detections","title":"Evaluating detections \u00b6","text":"<p>The formulas in this section assume the following code has been run on a dataset <code>ds</code> with detections in its <code>predictions</code> field:</p> <pre><code>import fiftyone.brain as fob\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\nds = foz.load_zoo_dataset(\"quickstart\")\n\nds.evaluate_detections(\"predictions\", eval_key=\"eval\")\n\nfob.compute_uniqueness(ds)\nfob.compute_mistakenness(ds, \"predictions\", label_field=\"ground_truth\")\nep = ds.to_evaluation_patches(\"eval\")\n</code></pre> Operation Command Uniqueness &gt; 0.9 <code>&lt;br&gt;ds.match(F(\"uniqueness\") &gt; 0.9)&lt;br&gt;</code> 10 most unique images <code>&lt;br&gt;ds.sort_by(\"uniqueness\", reverse=True)[:10]&lt;br&gt;</code> Predictions with confidence &gt; 0.95 <code>&lt;br&gt;ds.filter_labels(\"predictions\", F(\"confidence\") &gt; 0.95)&lt;br&gt;</code> 10 most \u201cwrong\u201d predictions <code>&lt;br&gt;ds.sort_by(\"mistakenness\", reverse=True)[:10]&lt;br&gt;</code> Images with more than 10 false positives <code>&lt;br&gt;ds.match(F(\"eval_fp\") &gt; 10)&lt;br&gt;</code> False positive \u201cdog\u201d detections <code>&lt;br&gt;ep.match_labels(&lt;br&gt;   filter=(F(\"eval\") == \"fp\") &amp; (F(\"label\") == \"dog\"),&lt;br&gt;   fields=\"predictions\",&lt;br&gt;)&lt;br&gt;</code> Predictions with IoU &gt; 0.9 <code>&lt;br&gt;ep.match(F(\"iou\") &gt; 0.9)&lt;br&gt;</code> <p>Reference: <code>match()</code>, <code>sort_by()</code>, <code>filter_labels()</code>, and <code>match_labels()</code>.</p>"},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#classifications","title":"Classifications \u00b6","text":""},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#evaluating-classifications","title":"Evaluating classifications \u00b6","text":"<p>The formulas in the following table assumes the following code has been run on a dataset <code>ds</code>, where the <code>predictions</code> field is populated with classification predictions that have their <code>logits</code> attribute set:</p> <pre><code>import fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\nds = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\n\n# TODO: add your own predicted classifications\n\nds.evaluate_classifications(\"predictions\", gt_field=\"ground_truth\")\n\nfob.compute_uniqueness(ds)\nfob.compute_hardness(ds, \"predictions\")\nfob.compute_mistakenness(ds, \"predictions\", label_field=\"ground_truth\")\n</code></pre> Operation Command 10 most unique incorrect predictions <code>&lt;br&gt;ds.match(&lt;br&gt;    F(\"predictions.label\") != F(\"ground_truth.label\")&lt;br&gt;).sort_by(\"uniqueness\", reverse=True)[:10]&lt;br&gt;</code> 10 most \u201cwrong\u201d predictions <code>&lt;br&gt;ds.sort_by(\"mistakenness\", reverse=True)[:10]&lt;br&gt;</code> 10 most likely annotation mistakes <code>&lt;br&gt;ds.match_tags(\"train\").sort_by(&lt;br&gt;    \"mistakenness\", reverse=True&lt;br&gt;)[:10]&lt;br&gt;</code> <p>Reference: <code>match()</code>, <code>sort_by()</code>, and <code>match_tags()</code>.</p>"},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#built-in-filter-and-match-functions","title":"Built-in filter and match functions \u00b6","text":"<p>FiftyOne has special methods for matching and filtering on specific data types. Take a look at the examples in this section to see how various operations can be performed via these special purpose methods, and compare that to the brute force implementation of the same operation that follows.</p> <p>The tables in this section use the following example data:</p> <pre><code>from bson import ObjectId\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\nds = foz.load_zoo_dataset(\"quickstart\")\n\n# Tag a few random samples\nds.take(3).tag_labels(\"potential_mistake\", label_fields=\"predictions\")\n\n# Grab a few label IDs\nlabel_ids = [\\\n    dataset.first().ground_truth.detections[0].id,\\\n    dataset.last().predictions.detections[0].id,\\\n]\nds.select_labels(ids=label_ids).tag_labels(\"error\")\n\nlen_filter = F(\"label\").strlen() &lt; 3\nid_filter = F(\"_id\").is_in([ObjectId(_id) for _id in label_ids])\n</code></pre>"},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#filtering-labels","title":"Filtering labels \u00b6","text":"Operation Get predicted detections that have confidence &gt; 0.9 Idiomatic <code>&lt;br&gt;ds.filter_labels(\"predictions\", F(\"confidence\") &gt; 0.9)&lt;br&gt;</code> Brute force <code>&lt;br&gt;ds.set_field(&lt;br&gt;    \"predictions.detections\",&lt;br&gt;    F(\"detections\").filter(F(\"confidence\") &gt; 0.9)),&lt;br&gt;)&lt;br&gt;</code> <p>Reference: <code>filter_labels()</code>.</p>"},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#matching-labels","title":"Matching labels \u00b6","text":"Operation Samples that have labels with id\u2019s in the list <code>label_ids</code> Idiomatic <code>&lt;br&gt;ds.match_labels(ids=label_ids)&lt;br&gt;</code> Brute force <code>&lt;br&gt;pred_expr = F(\"predictions.detections\").filter(id_filter).length() &gt; 0&lt;br&gt;gt_expr = F(\"ground_truth.detections\").filter(id_filter).length() &gt; 0&lt;br&gt;ds.match(pred_expr | gt_expr)&lt;br&gt;</code> Operation Samples that have labels satisfying <code>len_filter</code> in <code>predictions</code> or <code>ground_truth</code> field Idiomatic <code>&lt;br&gt;ds.match_labels(&lt;br&gt;    filter=len_filter,&lt;br&gt;    fields=[\"predictions\", \"ground_truth\"],&lt;br&gt;)&lt;br&gt;</code> Brute force <code>&lt;br&gt;pred_expr = F(\"predictions.detections\").filter(len_filter).length() &gt; 0&lt;br&gt;gt_expr = F(\"ground_truth.detections\").filter(len_filter).length() &gt; 0&lt;br&gt;ds.match(pred_expr | gt_expr)&lt;br&gt;</code> Operation Samples that have labels with tag \u201cerror\u201d in <code>predictions</code> or <code>ground_truth</code> field Idiomatic <code>&lt;br&gt;ds.match_labels(tags=\"error\")&lt;br&gt;</code> Brute force <code>&lt;br&gt;tag_expr = F(\"tags\").contains(\"error\")&lt;br&gt;pred_expr = F(\"predictions.detections\").filter(tag_expr).length() &gt; 0&lt;br&gt;gt_expr = F(\"ground_truth.detections\").filter(tag_expr).length() &gt; 0&lt;br&gt;ds.match(pred_expr | gt_expr)&lt;br&gt;</code> <p>Reference: <code>match_labels()</code>.</p>"},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#matching-tags","title":"Matching tags \u00b6","text":"Operation Samples that have tag <code>validation</code> Idiomatic <code>&lt;br&gt;ds.match_tags(\"validation\")&lt;br&gt;</code> Brute force <code>&lt;br&gt;ds.match(F(\"tags\").contains(\"validation\"))&lt;br&gt;</code> <p>Reference: <code>match_tags()</code>.</p>"},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#matching-frames","title":"Matching frames \u00b6","text":"<p>The following table uses this example data:</p> <pre><code>import fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\nds = foz.load_zoo_dataset(\"quickstart-video\")\nnum_objects = F(\"detections.detections\").length()\n</code></pre> Operation Frames with at least 10 detections Idiomatic <code>&lt;br&gt;ds.match_frames(num_objects &gt; 10)&lt;br&gt;</code> Brute force <code>&lt;br&gt;ds.match(F(\"frames\").filter(num_objects &gt; 10).length() &gt; 0)&lt;br&gt;</code> <p>Reference: <code>match_frames()</code>.</p>"},{"location":"how_do_i/cheat_sheets/filtering_cheat_sheet/#filtering-keypoints","title":"Filtering keypoints \u00b6","text":"<p>You can use <code>filter_keypoints()</code> to retrieve individual keypoints within a <code>Keypoint</code> instance that match a specified condition.</p> <p>The following table uses this example data:</p> <pre><code>import fiftyone as fo\nfrom fiftyone import ViewField as F\n\nds = fo.Dataset()\nds.add_samples(\n    [\\\n        fo.Sample(\\\n            filepath=\"image1.jpg\",\\\n            predictions=fo.Keypoints(\\\n                keypoints=[\\\n                    fo.Keypoint(\\\n                        label=\"person\",\\\n                        points=[(0.1, 0.1), (0.1, 0.9), (0.9, 0.9), (0.9, 0.1)],\\\n                        confidence=[0.7, 0.8, 0.95, 0.99],\\\n                    )\\\n                ]\\\n            )\\\n        ),\\\n        fo.Sample(filepath=\"image2.jpg\"),\\\n    ]\n)\n\nds.default_skeleton = fo.KeypointSkeleton(\n    labels=[\"nose\", \"left eye\", \"right eye\", \"left ear\", \"right ear\"],\n    edges=[[0, 1, 2, 0], [0, 3], [0, 4]],\n)\n</code></pre> Operation Only include predicted keypoints with confidence &gt; 0.9 Idiomatic <code>&lt;br&gt;ds.filter_keypoints(\"predictions\", filter=F(\"confidence\") &gt; 0.9)&lt;br&gt;</code> Brute force <code>&lt;br&gt;tmp = ds.clone()&lt;br&gt;for sample in tmp.iter_samples(autosave=True):&lt;br&gt;    if sample.predictions is None:&lt;br&gt;        continue&lt;br&gt;    for keypoint in sample.predictions.keypoints:&lt;br&gt;        for i, confidence in enumerate(keypoint.confidence):&lt;br&gt;            if confidence &lt;= 0.9:&lt;br&gt;                keypoint.points[i] = [None, None]&lt;br&gt;</code> <p>Reference: <code>match_frames()</code>.</p>"},{"location":"how_do_i/cheat_sheets/pandas_vs_fiftyone/","title":"pandas vs FiftyOne \u00b6","text":"<p>This cheat sheet shows how to translate common pandas operations into FiftyOne.</p>"},{"location":"how_do_i/cheat_sheets/pandas_vs_fiftyone/#nomenclature","title":"Nomenclature \u00b6","text":"pandas FiftyOne DataFrame ( <code>df</code>) Dataset ( <code>ds</code>) Row Sample Column Field"},{"location":"how_do_i/cheat_sheets/pandas_vs_fiftyone/#getting-started","title":"Getting started \u00b6","text":"pandas FiftyOne Importing the packages <code>import pandas as pd</code> <code>import fiftyone as fo</code> Create empty dataset <code>df = pd.DataFrame()</code> <code>ds = fo.Dataset()</code> Load dataset <code>df = pd.read_csv(*)</code> <code>ds = fo.Dataset.from_dir(*)</code>"},{"location":"how_do_i/cheat_sheets/pandas_vs_fiftyone/#basics","title":"Basics \u00b6","text":"pandas FiftyOne First row/sample <code>df.iloc[0]</code> or <code>df.head(1)</code> <code>ds.first()</code> or <code>ds.head(1)</code> Last row/sample <code>df.iloc[-1]</code> or <code>df.tail(1)</code> <code>ds.last()</code> or <code>ds.tail(1)</code> First few rows/samples <code>df.head()</code> <code>ds.head()</code> Last few rows/samples <code>df.tail()</code> <code>ds.tail()</code> Get specific row/sample <code>df.loc[j]</code> <code>ds[sample_id]</code> Number of rows/samples <code>len(df)</code> <code>len(ds)</code> Column names/field schema <code>df.columns</code> <code>ds.get_field_schema()</code> Get all values in column/field <code>df[*].tolist()</code> <code>ds.values(*)</code>"},{"location":"how_do_i/cheat_sheets/pandas_vs_fiftyone/#view-stages","title":"View stages \u00b6","text":"pandas FiftyOne Make a copy <code>df.copy()</code> <code>ds.clone()</code> Slice <code>df[start:end]</code> <code>ds[start:end]</code> Random sample <code>df.sample(n=n)</code> <code>ds.take(n)</code> Shuffle data <code>df.sample(frac=1)</code> <code>ds.shuffle()</code> Filter by column/field value <code>df[df[*] &gt; threshold]</code> <code>ds.match(F(*) &gt; threshold)</code> Sort values <code>df.sort_values()</code> <code>ds.sort_by(*)</code> Delete all <code>import gc</code><code>del df; gc.collect()</code> <code>ds.delete()</code>"},{"location":"how_do_i/cheat_sheets/pandas_vs_fiftyone/#aggregations","title":"Aggregations \u00b6","text":"pandas FiftyOne Count <code>df[*].count()</code> <code>ds.count(*)</code> Sum <code>df[*].sum()</code> <code>ds.sum(*)</code> Unique values <code>df[*].unique()</code> <code>ds.distinct(*)</code> Bounds <code>min = df[*].min()</code><code>max = df[*].max()</code> <code>min, max = ds.bounds(*)</code> Mean <code>df[*].mean()</code> <code>ds.mean(*)</code> Standard deviation <code>df[*].std()</code> <code>ds.std(*)</code> Quantile <code>df[*].quantile(values)</code> <code>ds.quantiles(*, values))</code>"},{"location":"how_do_i/cheat_sheets/pandas_vs_fiftyone/#structural-changes","title":"Structural changes \u00b6","text":"pandas FiftyOne New column/field as constant value <code>df[\"col\"] = value</code> <code>ds.add_sample_field(\"field\", fo.StringField)</code><code>ds.set_field(\"field\", value).save()</code> New column/field from external data <code>df[\"col\"] = data</code> <code>ds.set_values(\"field\", data)</code> New column/field from existing columns/fields <code>df[\"col\"] = df.apply(fcn, axis=1)</code> <code>ds.add_sample_field(\"field\", fo.FloatField)</code><code>ds.set_field(\"field\", expression).save()</code> Remove a column/field <code>df = df.drop([\"col\"], axis=1)</code> <code>ds.delete_sample_fields([\"field\"])</code> or<code>ds.exclude_fields([\"field\"]).keep_fields()</code> Keep only specified columns/fields <code>df[\"col1\", \"col2\"]</code> <code>ds.select_fields([\"field1\", \"field2\"])</code> Concatenate DataFrames or DatasetViews <code>pd.concat([df1, df2])</code> <code>view1.concat(view2)</code> Add a single row/sample <code>df.append(row, ignore_index=True)</code> <code>ds.add_sample(sample)</code> Remove rows/samples <code>df.drop(rows)</code> <code>ds.delete_samples(sample_ids)</code> or<code>ds.exclude(samples).keep()</code> Keep only specified rows/samples <code>df.iloc[rows]</code> <code>ds.select(sample_ids)</code> Rename column/field <code>df.rename(columns={\"old\": \"new\"})</code> <code>ds.rename_sample_field(\"old\", \"new\")</code>"},{"location":"how_do_i/cheat_sheets/pandas_vs_fiftyone/#expressions","title":"Expressions \u00b6","text":"pandas FiftyOne Exact equality <code>df[df[*] == value]</code> <code>ds.match(F(*) == value)</code> Less than or equal to <code>new_df = df[df[*] &lt;= value]</code> <code>new_view = ds.match(F(*) &lt;= value)</code> Logical complement <code>new_df = df[~(df[*] &lt;= value)]</code> <code>new_view = ds.match(~(F(*) &lt;= value))</code> Logical AND <code>df[pd_cond1 &amp; pd_cond2]</code> <code>ds.match(fo_cond1 &amp; fo_cond2)</code> Logical OR <code>df[pd_cond1 | pd_cond2]</code> <code>ds.match(fo_cond1 | fo_cond2)</code> Is in <code>df[*].isin(cols)</code> <code>ds.filter_labels(*, F(\"label\").is_in(fields))</code> Contains string <code>df[*].str.contains(substr)</code> <code>ds.filter_labels(*, F(\"label\").contains_str(substr))</code> Check for numerics <code>pdt.is_numeric_dtype(df[*])</code> <code>isinstance(ds.get_field_schema()[*], (fo.FloatField, fo.IntField))</code> or<code>len(ds.match(F(*).is_number())) &gt; 0</code> Check for strings <code>pdt.is_string_dtype(df[*])</code> <code>isinstance(ds.get_field_schema()[*], fo.StringField)</code> or<code>len(ds.match(F(*).is_string())) &gt; 0</code> Check for null entries <code>df.isna().any()</code> <code>len(ds.match(F(*) == None)) &gt; 0</code> <p>Note</p> <p>The table above assumes you have imported:</p> <pre><code>import pandas.api.types as pdt\nfrom fiftyone import ViewField as F\n</code></pre>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/","title":"Views Cheat Sheet \u00b6","text":"<p>This cheat sheet shows how to use dataset views to retrieve the specific subset of data you\u2019re looking for.</p>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#the-six-basic-operations","title":"The six basic operations \u00b6","text":"<p>If you run:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset()\nprint(dataset.list_view_stages())\n</code></pre> <p>you\u2019ll see a list of all <code>ViewStage</code> methods that you can use to construct views into your datasets.</p> <p>With a few exceptions (which we\u2019ll cover later), these methods can be organized into six categories: matching, filtering, selection, exclusion, indexing, and conversion.</p>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#matching","title":"Matching \u00b6","text":"<p>These stages select certain subsets of the input collection that match a given condition, without modifying the contents of the samples.</p> <code>match()</code> <code>match_frames()</code> <code>match_labels()</code> <code>match_tags()</code>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#filtering","title":"Filtering \u00b6","text":"<p>These stages filter the contents of the samples in the input collection based on a given condition.</p> <code>filter_field()</code> <code>filter_labels()</code> <code>filter_keypoints()</code>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#selection","title":"Selection \u00b6","text":"<p>These stages select certain subsets of the input collection that contain a given identifier.</p> <p>Selection is similar to matching/filtering, but with a simpler syntax that supports specific selection criteria (common identifiers like IDs and tags) rather than arbitrary expressions.</p> <code>select()</code> <code>select_by()</code> <code>select_fields()</code> <code>select_frames()</code> <code>select_groups()</code> <code>select_group_slices()</code> <code>select_labels()</code>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#exclusion","title":"Exclusion \u00b6","text":"<p>These stages exclude data from the input collection based on the given criteria.</p> <code>exclude()</code> <code>exclude_by()</code> <code>exclude_fields()</code> <code>exclude_frames()</code> <code>exclude_groups()</code> <code>exclude_labels()</code>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#sorting","title":"Sorting \u00b6","text":"<p>These stages sort the samples in the input collection based on a given condition.</p> <code>sort_by()</code> <code>sort_by_similarity()</code>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#indexing","title":"Indexing \u00b6","text":"<p>These stages slice and reorder the samples in the input collection.</p> <code>limit()</code> <code>shuffle()</code> <code>skip()</code> <code>take()</code>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#conversion","title":"Conversion \u00b6","text":"<p>These stages create views that transform the contents of the input collection into a different shape.</p> <code>to_patches()</code> <code>to_evaluation_patches()</code> <code>to_clips()</code> <code>to_frames()</code>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#miscellaneous","title":"Miscellaneous \u00b6","text":"<p>Other stages that do not fit into the six buckets above include:</p> <code>concat()</code> <code>exists()</code> <code>geo_near()</code> <code>geo_within()</code> <code>group_by()</code> <code>map_labels()</code> <code>mongo()</code> <code>set_field()</code>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#filtering-matching-selecting-and-excluding","title":"Filtering, matching, selecting, and excluding \u00b6","text":"<p>FiftyOne\u2019s goal is to help you perform your computer vision workflows as simply and efficiently as possible, without the need to manually iterate through all the samples in your dataset.</p> <p>To achieve this, FiftyOne provides builtin view stages tailored for each primitive data type (samples, labels, fields, tags, frames, and groups) that provide concise syntaxes for performing the desired operation against that primitive\u2019s attributes.</p> Match Filter Select Exclude Samples <code>match()</code> <code>select()</code> <code>exclude()</code> Labels <code>match_labels()</code> <code>filter_labels()</code> <code>select_labels()</code> <code>exclude_labels()</code> Fields <code>filter_field()</code> <code>select_fields()</code> <code>exclude_fields()</code> Tags <code>match_tags()</code> Frames <code>match_frames()</code> <code>select_frames()</code> <code>exclude_frames()</code> Groups <code>select_groups()</code> <code>exclude_groups()</code> <p>From the table above, we see that most operations on each primitive type are directly supported via tailored methods. The empty cells in the table fall into two categories:</p> <ul> <li> <p>the operation does not make sense on the primitive</p> </li> <li> <p>the operation can easily achieved on the primitive via another base method</p> </li> </ul> <p>In the following sections, we\u2019ll fill in the gaps in the table.</p>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#samples","title":"Samples \u00b6","text":"<p>The only method missing from the <code>Samples</code> row of the table is a \u201cfilter\u201d method. This is because filtering operations create a view with contents of the primitive to which they are applied. However, as samples are comprised of fields, the <code>filter_field()</code> method provides all of the desired functionality.</p>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#labels","title":"Labels \u00b6","text":"<p>While all of the methods in the <code>Labels</code> row are filled in, there is one subtlety: filtering by <code>id</code>.</p> <p>The <code>match_labels()</code>, <code>select_labels()</code>, and <code>exclude_labels()</code> methods all allow you to pass in a list of IDs to define a view. However, in order to filter by ID using <code>filter_labels()</code>, you\u2019ll need to make two changes when creating your filter expression:</p> <ul> <li> <p>use <code>\"_id\"</code> rather than <code>\"id\"</code> when referencing ID fields</p> </li> <li> <p>cast ID strings to <code>ObjectId()</code></p> </li> </ul> <p>The following example demonstrates how to correctly filter by label IDs:</p> <pre><code>from bson import ObjectId\n\nimport fiftone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nlabel_id = dataset.first().predictions.detections[0].id\n\nview = dataset.filter_labels(\"predictions\", F(\"_id\") == ObjectId(label_id))\n</code></pre>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#fields","title":"Fields \u00b6","text":"<p>The only missing entry in the <code>Fields</code> row is a \u201cmatch\u201d stage. Such a method would absolutely make sense: matches on fields are common. However, a dedicated method is not necessary because you can easily achieve this using the existing <code>match()</code> method.</p> <p>A hypothetical match fields method would take as input a <code>field</code> and a <code>filter</code> to apply to it, but we can achieve this in multiple ways via simple <code>match()</code> expressions:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfield = \"ground_truth.detections\"\nfilter = F().length() &gt; 0\n\n# What a `match_fields()` method would look like\n# view = dataset.match_fields(field, filter)\n\n# Option 1: directly apply the filter to the field\nview = dataset.match(F(field).length() &gt; 0)\n\n# Option 2: apply() the filter to the field\nview = dataset.match(F(field).apply(filter))\n</code></pre> <p>Note that <code>exists()</code> is also a special case of field matching:</p> <pre><code>dataset.take(100).set_field(\"uniqueness\", None).save()\n\n# Concise syntax via `exists()`\nview = dataset.exists(\"uniqueness\")\n\n# Equivalent syntax via `match()`\nview = dataset.match(F(\"uniqueness\") != None)\n</code></pre>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#tags","title":"Tags \u00b6","text":"<p>All three of the missing <code>Tag</code> methods can be created with relative ease.</p> <p>Here\u2019s how selecting tags can be achieved:</p> <pre><code>from fiftyone import ViewField as F\n\n# What a select_tags() method would look like\n# view = dataset.select_tags(tags)\n\n# How to achieve this with existing methods\nview = dataset.set_field(\"tags\", F(\"tags\").intersection(tags))\n</code></pre> <p>Here\u2019s how excluding tags can be achieved:</p> <pre><code>from fiftyone import ViewField as F\n\n# What an exclude_tags() method would look like\n# view = dataset.exclude_tags(tags)\n\n# How to achieve this with existing methods\nview = dataset.set_field(\"tags\", F(\"tags\").difference(tags))\n</code></pre> <p>And here\u2019s how filtering tags can be achieved:</p> <pre><code>from fiftyone import ViewField as F\n\n# What a filter_tags() method would look like\n# view = dataset.filter_tags(expr)\n\n# How to achieve this with existing methods\nview = dataset.set_field(\"tags\", F(\"tags\").filter(expr))\n</code></pre> <p>Note</p> <p>The above examples use the set <code>intersection()</code> and <code>difference()</code> view expressions.</p>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#frames-and-groups","title":"Frames and groups \u00b6","text":"<p>When working with frame-level and group-level data in FiftyOne, all applicable view stages naturally support querying against frame- or group-level fields by prepending <code>\"frames.\"</code> or <code>\"groups.\"</code> to field paths, respectively.</p> <p>For example, you can retrieve the frame-level object detections in the \u201cdetections\u201d field of the quickstart-video dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\nview = dataset.filter_labels(\"frames.detections\", F(\"label\") == \"vehicle\")\n</code></pre> <p>Or when working with grouped datasets, you can match groups based on whether they contain a given group slice:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\n# Load a dataset with 200 groups, each with \"left\", \"right\", \"pcd\" elements\ndataset = foz.load_zoo_dataset(\"quickstart-groups\")\n\n# Add 50 new groups with only \"left\" slice samples\ndataset.add_samples(\n    [\\\n        fo.Sample(\\\n            filepath=\"image%d.png\" % i,\\\n            group=fo.Group().element(\"left\"),\\\n        )\\\n        for i in range(50)\\\n    ]\n)\n\n# Match groups that have \"pcd\" elements\nview = dataset.match(F(\"groups.pcd\") != None)\n</code></pre>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#conversion_1","title":"Conversion \u00b6","text":"<p>FiftyOne provides a variety of convenient methods for converting your data from one format to another. Some of these conversions are accomplished as view stages that return generated views, which are views that contain a different shape of data than the original <code>Dataset</code> or <code>DatasetView</code> to which the view stage was applied.</p> <p>Let\u2019s briefly cover the transformation that each generated view performs.</p>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#images-to-object-patches","title":"Images to object patches \u00b6","text":"<p>If your dataset contains label list fields like <code>Detections</code> or <code>Polylines</code>, then you can use <code>to_patches()</code> to create views that contain one sample per object patch in a specified label field of your dataset.</p> <p>For example, you can extract patches for all ground truth objects in a detection dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\ngt_patches = dataset.to_patches(\"ground_truth\")\nprint(gt_patches)\n</code></pre> <pre><code>Dataset:     quickstart\nMedia type:  image\nNum patches: 1232\nPatch fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    sample_id:        fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detection)\nView stages:\n    1. ToPatches(field='ground_truth', config=None)\n</code></pre>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#images-to-evaluation-patches","title":"Images to evaluation patches \u00b6","text":"<p>If you have run evaluation on predictions from an object detection model, then you can use <code>to_evaluation_patches()</code> to transform the dataset (or a view into it) into a new view that contains one sample for each true positive, false positive, and false negative example.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Evaluate `predictions` w.r.t. labels in `ground_truth` field\ndataset.evaluate_detections(\n \"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\"\n)\n\n# Convert to evaluation patches\neval_patches = dataset.to_evaluation_patches(\"eval\")\nprint(eval_patches)\n\nprint(eval_patches.count_values(\"type\"))\n# {'fn': 246, 'fp': 4131, 'tp': 986}\n</code></pre> <pre><code>Dataset:     quickstart\nMedia type:  image\nNum patches: 5363\nPatch fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    sample_id:        fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    predictions:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    type:             fiftyone.core.fields.StringField\n    iou:              fiftyone.core.fields.FloatField\n    crowd:            fiftyone.core.fields.BooleanField\nView stages:\n    1. ToEvaluationPatches(eval_key='eval', config=None)\n</code></pre>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#videos-to-clips","title":"Videos to clips \u00b6","text":"<p>You can use <code>to_clips()</code> to create views into your video datasets that contain one sample per clip defined by a specific field or expression in a video collection.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\n# Create a clips view that contains one clip for each contiguous segment\n# that contains at least one road sign in every frame\nclips = (\n    dataset\n    .filter_labels(\"frames.detections\", F(\"label\") == \"road sign\")\n    .to_clips(\"frames.detections\")\n)\nprint(clips)\n</code></pre> <pre><code>Dataset:    quickstart-video\nMedia type: video\nNum clips:  11\nClip fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    sample_id:        fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    support:          fiftyone.core.fields.FrameSupportField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.VideoMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\nFrame fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    frame_number:     fiftyone.core.fields.FrameNumberField\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\nView stages:\n    1. FilterLabels(field='frames.detections', ...)\n    2. ToClips(field_or_expr='frames.detections', config=None)\n</code></pre>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#videos-to-images","title":"Videos to images \u00b6","text":"<p>You can use <code>to_frames()</code> to create image views into your video datasets that contain one sample per frame in the input collection.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\n# Create a view with one sample per frame\nframes = dataset.to_frames(sample_frames=True)\nprint(frames)\n</code></pre> <pre><code>Dataset:     quickstart-video\nMedia type:  image\nNum samples: 1279\nSample fields:\n   id:               fiftyone.core.fields.ObjectIdField\n   sample_id:        fiftyone.core.fields.ObjectIdField\n   frame_number:     fiftyone.core.fields.FrameNumberField\n   filepath:         fiftyone.core.fields.StringField\n   tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n   metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n   created_at:       fiftyone.core.fields.DateTimeField\n   last_modified_at: fiftyone.core.fields.DateTimeField\n   detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\nView stages:\n  1. ToFrames(config=None)\n</code></pre>"},{"location":"how_do_i/cheat_sheets/views_cheat_sheet/#grouped-to-non-grouped","title":"Grouped to non-grouped \u00b6","text":"<p>You can use the <code>select_group_slices()</code> to generate a <code>DatasetView</code> that contains a flattened collection of samples from specific slice(s) of a grouped dataset.</p> <p>For example, the following code creates an image collection from the \u201cleft\u201d and \u201cright\u201d group slices of the quickstart-groups dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-groups\")\n\nimage_view = dataset.select_group_slices([\"left\", \"right\"])\nprint(image_view)\n</code></pre> <pre><code>Dataset:     groups-overview\nMedia type:  image\nNum samples: 400\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    group:            fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.groups.Group)\n    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\nView stages:\n    1. SelectGroupSlices(slices=['left', 'right'])\n</code></pre> <p>Note</p> <p>All group slice(s) you select must have the same media type, since the <code>media_type</code> of the returned collection is the media type of the slices you select.</p>"},{"location":"how_do_i/recipes/","title":"FiftyOne Recipes \u00b6","text":"<p>FiftyOne turbocharges your current workflows, transforming hours of scripting into minutes so that you can focus on your models. Browse the recipes below to see how you can leverage FiftyOne to enhance key parts of your machine learning workflows.</p>"},{"location":"how_do_i/recipes/#creating-views-and-using-view-expressions","title":"Creating views and using view expressions","text":"<p>Create views to easily query and explore your datasets in FiftyOne.</p> <p>Basics,Dataset-Curation</p> <p></p>"},{"location":"how_do_i/recipes/#removing-duplicate-images-from-a-dataset","title":"Removing duplicate images from a dataset","text":"<p>Automatically find and remove duplicate and near-duplicate images from your FiftyOne datasets.</p> <p>Basics,Dataset-Curation</p> <p></p>"},{"location":"how_do_i/recipes/#removing-duplicate-objects-from-a-dataset","title":"Removing duplicate objects from a dataset","text":"<p>Check out some common workflows for finding and removing duplicate objects from your FiftyOne datasets.</p> <p>Basics,Dataset-Curation</p> <p></p>"},{"location":"how_do_i/recipes/#adding-classifier-predictions-to-a-dataset","title":"Adding classifier predictions to a dataset","text":"<p>Add FiftyOne to your model training and analysis loop to visualize and analyze your classifier's predictions.</p> <p>Basics,Model-Training</p> <p></p>"},{"location":"how_do_i/recipes/#adding-object-detections-to-a-dataset","title":"Adding object detections to a dataset","text":"<p>Use FiftyOne to store your object detections and use the FiftyOne App to analyze them.</p> <p>Basics,Model-Training</p> <p></p>"},{"location":"how_do_i/recipes/#draw-labels-on-samples","title":"Draw labels on samples","text":"<p>Render labels on the samples in your FiftyOne Dataset with a single line of code.</p> <p>Basics,Visualization</p> <p></p>"},{"location":"how_do_i/recipes/#convert-dataset-formats-on-disk","title":"Convert dataset formats on disk","text":"<p>Use FiftyOne's powerful dataset import/export features to convert your datasets on disk between standard (or custom) formats.</p> <p>Basics,I/O</p> <p></p>"},{"location":"how_do_i/recipes/#merging-datasets","title":"Merging datasets","text":"<p>Easily merge datasets on disk or in-memory using FiftyOne; e.g., to add a new set of model predictions to a dataset.</p> <p>Basics,I/O</p> <p></p>"},{"location":"how_do_i/recipes/#import-datasets-in-custom-formats","title":"Import datasets in custom formats","text":"<p>Write your own custom DatasetImporter and use it to import datasets in your custom format into FiftyOne.</p> <p>Advanced,I/O</p> <p></p>"},{"location":"how_do_i/recipes/#export-datasets-in-custom-formats","title":"Export datasets in custom formats","text":"<p>Write your own custom DatasetExporter and use it to export a FiftyOne Dataset to disk in your custom format.</p> <p>Advanced,I/O</p> <p></p>"},{"location":"how_do_i/recipes/#parse-samples-in-custom-formats","title":"Parse samples in custom formats","text":"<p>Write your own custom SampleParser and use it to add samples in your custom format to a FiftyOne Dataset.</p> <p>Advanced,I/O</p> <p></p> <p>Note</p> <p>Check out the fiftyone-examples repository for more examples of using FiftyOne!</p>"},{"location":"how_do_i/recipes/adding_classifications/","title":"Adding Classifier Predictions to a Dataset","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>You'll also need to install <code>torch</code> and <code>torchvision</code>, if necessary:</p> In\u00a0[1]: Copied! <pre>!pip install torch torchvision\n</pre> !pip install torch torchvision <p>In this example, we'll work with the test split of the CIFAR-10 dataset, which is conveniently available for download from the FiftyOne Dataset Zoo:</p> In\u00a0[2]: Copied! <pre># Downloads the test split of CIFAR-10\n!fiftyone zoo datasets download cifar10 --splits test\n</pre> # Downloads the test split of CIFAR-10 !fiftyone zoo datasets download cifar10 --splits test <pre>Downloading split 'test' to '/Users/Brian/fiftyone/cifar10/test'\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /Users/Brian/fiftyone/cifar10/tmp-download/cifar-10-python.tar.gz\n170500096it [00:05, 30536650.39it/s]                                            \nExtracting /Users/Brian/fiftyone/cifar10/tmp-download/cifar-10-python.tar.gz to /Users/Brian/fiftyone/cifar10/tmp-download\n 100% |\u2588\u2588\u2588| 10000/10000 [5.7s elapsed, 0s remaining, 1.9K samples/s]      \nDataset info written to '/Users/Brian/fiftyone/cifar10/info.json'\n</pre> <p>We'll also download a pre-trained CIFAR-10 PyTorch model that we'll use to generate some predictions:</p> In\u00a0[1]: Copied! <pre># Download the software\n!git clone --depth 1 --branch v2.1 https://github.com/huyvnphan/PyTorch_CIFAR10.git\n\n# Download the pretrained model (90MB)\n!eta gdrive download --public \\\n    1dGfpeFK_QG0kV-U6QDHMX2EOGXPqaNzu \\\n    PyTorch_CIFAR10/cifar10_models/state_dicts/resnet50.pt\n</pre> # Download the software !git clone --depth 1 --branch v2.1 https://github.com/huyvnphan/PyTorch_CIFAR10.git  # Download the pretrained model (90MB) !eta gdrive download --public \\     1dGfpeFK_QG0kV-U6QDHMX2EOGXPqaNzu \\     PyTorch_CIFAR10/cifar10_models/state_dicts/resnet50.pt <pre>Cloning into 'PyTorch_CIFAR10'...\nremote: Enumerating objects: 82, done.\nremote: Counting objects: 100% (82/82), done.\nremote: Compressing objects: 100% (57/57), done.\nremote: Total 82 (delta 13), reused 59 (delta 12), pack-reused 0\nUnpacking objects: 100% (82/82), done.\nNote: checking out '2a2e76a56f943b70403796387d968704e74971ae'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by performing another checkout.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -b with the checkout command again. Example:\n\n  git checkout -b &lt;new-branch-name&gt;\n\nDownloading '1dGfpeFK_QG0kV-U6QDHMX2EOGXPqaNzu' to 'PyTorch_CIFAR10/cifar10_models/state_dicts/resnet50.pt'\n 100% |\u2588\u2588\u2588\u2588|  719.8Mb/719.8Mb [2.7s elapsed, 0s remaining, 276.2Mb/s]      \n</pre> In\u00a0[2]: Copied! <pre>import json\nimport os\n\n# The location of the dataset on disk that you downloaded above\ndataset_dir = os.path.expanduser(\"~/fiftyone/cifar10/test\")\n\n# Maps image UUIDs to image paths\nimages_dir = os.path.join(dataset_dir, \"data\")\nimage_uuids_to_paths = {\n    os.path.splitext(n)[0]: os.path.join(images_dir, n)\n    for n in os.listdir(images_dir)\n}\n\nlabels_path = os.path.join(dataset_dir, \"labels.json\")\nwith open(labels_path, \"rt\") as f:\n    _labels = json.load(f)\n\n# Get classes\nclasses = _labels[\"classes\"]\n\n# Maps image UUIDs to int targets\nlabels = _labels[\"labels\"]\n\n# Make a list of (image_path, label) samples\ndata = [(image_uuids_to_paths[u], classes[t]) for u, t in labels.items()]\n\n# Print a few data\nfor sample in data[:5]:\n    print(sample)\n</pre> import json import os  # The location of the dataset on disk that you downloaded above dataset_dir = os.path.expanduser(\"~/fiftyone/cifar10/test\")  # Maps image UUIDs to image paths images_dir = os.path.join(dataset_dir, \"data\") image_uuids_to_paths = {     os.path.splitext(n)[0]: os.path.join(images_dir, n)     for n in os.listdir(images_dir) }  labels_path = os.path.join(dataset_dir, \"labels.json\") with open(labels_path, \"rt\") as f:     _labels = json.load(f)  # Get classes classes = _labels[\"classes\"]  # Maps image UUIDs to int targets labels = _labels[\"labels\"]  # Make a list of (image_path, label) samples data = [(image_uuids_to_paths[u], classes[t]) for u, t in labels.items()]  # Print a few data for sample in data[:5]:     print(sample) <pre>('/Users/Brian/fiftyone/cifar10/test/data/000001.jpg', 'cat')\n('/Users/Brian/fiftyone/cifar10/test/data/000002.jpg', 'ship')\n('/Users/Brian/fiftyone/cifar10/test/data/000003.jpg', 'ship')\n('/Users/Brian/fiftyone/cifar10/test/data/000004.jpg', 'airplane')\n('/Users/Brian/fiftyone/cifar10/test/data/000005.jpg', 'frog')\n</pre> <p>Building a FiftyOne dataset from your samples in this format is simple:</p> In\u00a0[3]: Copied! <pre>import fiftyone as fo\n\n# Load the data into FiftyOne samples\nsamples = []\nfor image_path, label in data:\n    samples.append(\n        fo.Sample(\n            filepath=image_path,\n            ground_truth=fo.Classification(label=label),\n        )\n    )\n\n# Add the samples to a dataset\ndataset = fo.Dataset(\"classifier-recipe\")\ndataset.add_samples(samples)\n\n# Print some information about the dataset\nprint(dataset)\n</pre> import fiftyone as fo  # Load the data into FiftyOne samples samples = [] for image_path, label in data:     samples.append(         fo.Sample(             filepath=image_path,             ground_truth=fo.Classification(label=label),         )     )  # Add the samples to a dataset dataset = fo.Dataset(\"classifier-recipe\") dataset.add_samples(samples)  # Print some information about the dataset print(dataset) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [44.9s elapsed, 0s remaining, 221.4 samples/s]      \nName:           classifier-recipe\nMedia type:     image\nNum samples:    10000\nPersistent:     False\nInfo:           {}\nTags:           []\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n</pre> In\u00a0[4]: Copied! <pre># Print a sample from the dataset\nprint(dataset.first())\n</pre> # Print a sample from the dataset print(dataset.first()) <pre>&lt;Sample: {\n    'id': '603031c01ec865586e477d8e',\n    'media_type': 'image',\n    'filepath': '/Users/Brian/fiftyone/cifar10/test/data/000001.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '603031bf1ec865586e47567d',\n        'label': 'cat',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n}&gt;\n</pre> In\u00a0[5]: Copied! <pre># Used to write view expressions that involve sample fields\nfrom fiftyone import ViewField as F\n\n# Gets five airplanes from the dataset\nview = (\n    dataset.match(F(\"ground_truth.label\") == \"airplane\")\n    .limit(5)\n)\n\n# Print some information about the view you created\nprint(view)\n</pre> # Used to write view expressions that involve sample fields from fiftyone import ViewField as F  # Gets five airplanes from the dataset view = (     dataset.match(F(\"ground_truth.label\") == \"airplane\")     .limit(5) )  # Print some information about the view you created print(view) <pre>Dataset:        classifier-recipe\nMedia type:     image\nNum samples:    5\nTags:           []\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\nView stages:\n    1. Match(filter={'$expr': {'$eq': [...]}})\n    2. Limit(limit=5)\n</pre> In\u00a0[6]: Copied! <pre># Print a sample from the view\nprint(view.first())\n</pre> # Print a sample from the view print(view.first()) <pre>&lt;SampleView: {\n    'id': '603031c01ec865586e477d94',\n    'media_type': 'image',\n    'filepath': '/Users/Brian/fiftyone/cifar10/test/data/000004.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '603031bf1ec865586e475680',\n        'label': 'airplane',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n}&gt;\n</pre> <p>Iterating over the samples in a view is easy:</p> In\u00a0[7]: Copied! <pre>for sample in view:\n    print(sample.filepath)\n</pre> for sample in view:     print(sample.filepath) <pre>/Users/Brian/fiftyone/cifar10/test/data/000004.jpg\n/Users/Brian/fiftyone/cifar10/test/data/000011.jpg\n/Users/Brian/fiftyone/cifar10/test/data/000022.jpg\n/Users/Brian/fiftyone/cifar10/test/data/000028.jpg\n/Users/Brian/fiftyone/cifar10/test/data/000045.jpg\n</pre> In\u00a0[8]: Copied! <pre>import sys\n\nimport numpy as np\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader\n\nimport fiftyone.utils.torch as fout\n\nsys.path.insert(1, \"PyTorch_CIFAR10\")\nfrom cifar10_models import resnet50\n\n\ndef make_cifar10_data_loader(image_paths, sample_ids, batch_size):\n    mean = [0.4914, 0.4822, 0.4465]\n    std = [0.2023, 0.1994, 0.2010]\n    transforms = torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize(mean, std),\n        ]\n    )\n    dataset = fout.TorchImageDataset(\n        image_paths, sample_ids=sample_ids, transform=transforms\n    )\n    return DataLoader(dataset, batch_size=batch_size, num_workers=4)\n\n\ndef predict(model, imgs):\n    logits = model(imgs).detach().cpu().numpy()\n    predictions = np.argmax(logits, axis=1)\n    odds = np.exp(logits)\n    confidences = np.max(odds, axis=1) / np.sum(odds, axis=1)\n    return predictions, confidences\n\n\n#\n# Load model\n#\n# Model performance numbers are available at:\n#   https://github.com/huyvnphan/PyTorch_CIFAR10\n#\n\nmodel = resnet50(pretrained=True)\n\n#\n# Extract a few images to process\n#\n\nnum_samples = 25\nbatch_size = 5\n\nview = dataset.take(num_samples, seed=51)\n\nimage_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view])\ndata_loader = make_cifar10_data_loader(image_paths, sample_ids, batch_size)\n\n#\n# Perform prediction and store results in dataset\n#\n\nwith fo.ProgressBar() as pb:\n    for imgs, sample_ids in pb(data_loader):\n        predictions, confidences = predict(model, imgs)\n\n        # Add predictions to your FiftyOne dataset\n        for sample_id, prediction, confidence in zip(\n            sample_ids, predictions, confidences\n        ):\n            sample = dataset[sample_id]\n            sample[\"predictions\"] = fo.Classification(\n                label=classes[prediction],\n                confidence=confidence,\n            )\n            sample.save()\n</pre> import sys  import numpy as np import torch import torchvision from torch.utils.data import DataLoader  import fiftyone.utils.torch as fout  sys.path.insert(1, \"PyTorch_CIFAR10\") from cifar10_models import resnet50   def make_cifar10_data_loader(image_paths, sample_ids, batch_size):     mean = [0.4914, 0.4822, 0.4465]     std = [0.2023, 0.1994, 0.2010]     transforms = torchvision.transforms.Compose(         [             torchvision.transforms.ToTensor(),             torchvision.transforms.Normalize(mean, std),         ]     )     dataset = fout.TorchImageDataset(         image_paths, sample_ids=sample_ids, transform=transforms     )     return DataLoader(dataset, batch_size=batch_size, num_workers=4)   def predict(model, imgs):     logits = model(imgs).detach().cpu().numpy()     predictions = np.argmax(logits, axis=1)     odds = np.exp(logits)     confidences = np.max(odds, axis=1) / np.sum(odds, axis=1)     return predictions, confidences   # # Load model # # Model performance numbers are available at: #   https://github.com/huyvnphan/PyTorch_CIFAR10 #  model = resnet50(pretrained=True)  # # Extract a few images to process #  num_samples = 25 batch_size = 5  view = dataset.take(num_samples, seed=51)  image_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view]) data_loader = make_cifar10_data_loader(image_paths, sample_ids, batch_size)  # # Perform prediction and store results in dataset #  with fo.ProgressBar() as pb:     for imgs, sample_ids in pb(data_loader):         predictions, confidences = predict(model, imgs)          # Add predictions to your FiftyOne dataset         for sample_id, prediction, confidence in zip(             sample_ids, predictions, confidences         ):             sample = dataset[sample_id]             sample[\"predictions\"] = fo.Classification(                 label=classes[prediction],                 confidence=confidence,             )             sample.save() <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [1.4s elapsed, 0s remaining, 3.6 batches/s]         \n</pre> <p>We can print our dataset to verify that a <code>predictions</code> field has been added to its schema:</p> In\u00a0[8]: Copied! <pre>print(dataset)\n</pre> print(dataset) <pre>Name:           classifier-recipe\nMedia type:     image\nNum samples:    10000\nPersistent:     False\nInfo:           {}\nTags:           []\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n    predictions:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n</pre> <p>Let's explore the predictions we added by creating a view that sorts the samples in order of prediction confidence:</p> In\u00a0[15]: Copied! <pre>pred_view = (\n    dataset\n    .exists(\"predictions\")\n    .sort_by(\"predictions.confidence\", reverse=True)\n)\n\nprint(\"Number of samples with predictions: %s\\n\" % len(pred_view))\nprint(\"Highest confidence prediction:\\n\")\nprint(pred_view.first())\n</pre> pred_view = (     dataset     .exists(\"predictions\")     .sort_by(\"predictions.confidence\", reverse=True) )  print(\"Number of samples with predictions: %s\\n\" % len(pred_view)) print(\"Highest confidence prediction:\\n\") print(pred_view.first()) <pre>Number of samples with predictions: 25\n\nHighest confidence prediction:\n\n&lt;SampleView: {\n    'id': '602fe4dcb6fdaf68aad0cd8c',\n    'media_type': 'image',\n    'filepath': '/Users/Brian/fiftyone/cifar10/test/data/005771.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '602fe4c0b6fdaf68aad08ff1',\n        'label': 'ship',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n    'predictions': &lt;Classification: {\n        'id': '602fe4feb6fdaf68aad0eed7',\n        'label': 'ship',\n        'confidence': 0.8228137493133545,\n        'logits': None,\n    }&gt;,\n}&gt;\n</pre> In\u00a0[16]: Copied! <pre># Open the dataset in the App\nsession = fo.launch_app(dataset)\n</pre> # Open the dataset in the App session = fo.launch_app(dataset) Activate In\u00a0[17]: Copied! <pre># Show five random samples in the App\nsession.view = dataset.take(5)\n</pre> # Show five random samples in the App session.view = dataset.take(5) Activate In\u00a0[18]: Copied! <pre># Show the samples for which we added predictions above\nsession.view = pred_view\n</pre> # Show the samples for which we added predictions above session.view = pred_view Activate In\u00a0[19]: Copied! <pre># Show the full dataset again\nsession.view = None\n</pre> # Show the full dataset again session.view = None Activate <p>You can select samples in the App by clicking on the images. Try it!</p> <p>After you've selected some images in the App, you can hop back over to Python and make a view that contains those samples!</p> In\u00a0[20]: Copied! <pre># Make a view containing the currently selected samples in the App\nselected_view = dataset.select(session.selected)\n\n# Print details about the selected samples\nprint(selected_view)\n</pre> # Make a view containing the currently selected samples in the App selected_view = dataset.select(session.selected)  # Print details about the selected samples print(selected_view) <pre>Dataset:        classifier-recipe\nMedia type:     image\nNum samples:    8\nTags:           []\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n    predictions:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\nView stages:\n    1. Select(sample_ids=['602fe4c1b6fdaf68aad0a07a', '602fe4c1b6fdaf68aad0a082', '602fe4c1b6fdaf68aad0a086', ...])\n</pre> In\u00a0[21]: Copied! <pre>session.freeze() # screenshot the active App for sharing\n</pre> session.freeze() # screenshot the active App for sharing"},{"location":"how_do_i/recipes/adding_classifications/#adding-classifier-predictions-to-a-dataset","title":"Adding Classifier Predictions to a Dataset\u00b6","text":"<p>This recipe provides a glimpse into the possibilities for integrating FiftyOne into your ML workflows. Specifically, it covers:</p> <ul> <li>Loading an image classification dataset in FiftyOne</li> <li>Adding classifier predictions to a dataset</li> <li>Launching the FiftyOne App and visualizing/exploring your data</li> <li>Integrating the App into your data analysis workflow</li> </ul>"},{"location":"how_do_i/recipes/adding_classifications/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"how_do_i/recipes/adding_classifications/#loading-an-image-classification-dataset","title":"Loading an image classification dataset\u00b6","text":"<p>Suppose you have an image classification dataset on disk in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;uuid1&gt;.&lt;ext&gt;\n        &lt;uuid2&gt;.&lt;ext&gt;\n        ...\n    labels.json\n</code></pre> <p>where <code>labels.json</code> is a JSON file in the following format:</p> <pre><code>{\n    \"classes\": [\n        &lt;labelA&gt;,\n        &lt;labelB&gt;,\n        ...\n    ],\n    \"labels\": {\n        &lt;uuid1&gt;: &lt;target1&gt;,\n        &lt;uuid2&gt;: &lt;target2&gt;,\n        ...\n    }\n}\n</code></pre> <p>In your current workflow, you may parse this data into a list of <code>(image_path, label)</code> tuples as follows:</p>"},{"location":"how_do_i/recipes/adding_classifications/#working-with-views","title":"Working with views\u00b6","text":"<p>FiftyOne provides a powerful notion of dataset views that you can use to explore subsets of the samples in your dataset.</p> <p>Here's an example operation:</p>"},{"location":"how_do_i/recipes/adding_classifications/#adding-model-predictions","title":"Adding model predictions\u00b6","text":"<p>Now let's add our classifier's predictions to our FiftyOne dataset in a new <code>predictions</code> field:</p>"},{"location":"how_do_i/recipes/adding_classifications/#using-the-fiftyone-app","title":"Using the FiftyOne App\u00b6","text":"<p>The FiftyOne App allows you easily visualize, explore, search, filter, your datasets.</p> <p>You can explore the App interactively through the GUI, and you can even interact with it in real-time from your Python interpreter!</p>"},{"location":"how_do_i/recipes/adding_detections/","title":"Adding Object Detections to a Dataset","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>In this tutorial, we'll use an off-the-shelf Faster R-CNN detection model provided by PyTorch. To use it, you'll need to install <code>torch</code> and <code>torchvision</code>, if necessary.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install torch torchvision\n</pre> !pip install torch torchvision In\u00a0[2]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=\"detector-recipe\",\n)\n</pre> import fiftyone as fo import fiftyone.zoo as foz  dataset = foz.load_zoo_dataset(     \"coco-2017\",     split=\"validation\",     dataset_name=\"detector-recipe\", ) <pre>Split 'validation' already downloaded\nLoading 'coco-2017' split 'validation'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [43.3s elapsed, 0s remaining, 114.9 samples/s]      \nDataset 'detector-recipe' created\n</pre> <p>Let's inspect the dataset to see what we downloaded:</p> In\u00a0[3]: Copied! <pre># Print some information about the dataset\nprint(dataset)\n</pre> # Print some information about the dataset print(dataset) <pre>Name:           detector-recipe\nMedia type:     image\nNum samples:    5000\nPersistent:     False\nInfo:           {'classes': ['0', 'person', 'bicycle', ...]}\nTags:           ['validation']\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n</pre> In\u00a0[4]: Copied! <pre># Print a ground truth detection\nsample = dataset.first()\nprint(sample.ground_truth.detections[0])\n</pre> # Print a ground truth detection sample = dataset.first() print(sample.ground_truth.detections[0]) <pre>&lt;Detection: {\n    'id': '602fea44db78a9b44e6ae129',\n    'attributes': BaseDict({}),\n    'label': 'potted plant',\n    'bounding_box': BaseList([\n        0.37028125,\n        0.3345305164319249,\n        0.038593749999999996,\n        0.16314553990610328,\n    ]),\n    'mask': None,\n    'confidence': None,\n    'index': None,\n    'area': 531.8071000000001,\n    'iscrowd': 0.0,\n}&gt;\n</pre> <p>Note that the ground truth detections are stored in the <code>ground_truth</code> field of the samples.</p> <p>Before we go further, let's launch the FiftyOne App and use the GUI to explore the dataset visually:</p> In\u00a0[5]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) Activate In\u00a0[1]: Copied! <pre>import torch\nimport torchvision\n\n# Run the model on GPU if it is available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Load a pre-trained Faster R-CNN model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nmodel.to(device)\nmodel.eval()\n\nprint(\"Model ready\")\n</pre> import torch import torchvision  # Run the model on GPU if it is available device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # Load a pre-trained Faster R-CNN model model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) model.to(device) model.eval()  print(\"Model ready\") <pre>Model ready\n</pre> <p>The code below performs inference with the model on a randomly chosen subset of 100 samples from the dataset and stores the predictions in a <code>predictions</code> field of the samples.</p> In\u00a0[6]: Copied! <pre># Choose a random subset of 100 samples to add predictions to\npredictions_view = dataset.take(100, seed=51)\n</pre> # Choose a random subset of 100 samples to add predictions to predictions_view = dataset.take(100, seed=51) In\u00a0[7]: Copied! <pre>from PIL import Image\nfrom torchvision.transforms import functional as func\n\nimport fiftyone as fo\n\n# Get class list\nclasses = dataset.default_classes\n\n# Add predictions to samples\nwith fo.ProgressBar() as pb:\n    for sample in pb(predictions_view):\n        # Load image\n        image = Image.open(sample.filepath)\n        image = func.to_tensor(image).to(device)\n        c, h, w = image.shape\n        \n        # Perform inference\n        preds = model([image])[0]\n        labels = preds[\"labels\"].cpu().detach().numpy()\n        scores = preds[\"scores\"].cpu().detach().numpy()\n        boxes = preds[\"boxes\"].cpu().detach().numpy()\n        \n        # Convert detections to FiftyOne format\n        detections = []\n        for label, score, box in zip(labels, scores, boxes):\n            # Convert to [top-left-x, top-left-y, width, height]\n            # in relative coordinates in [0, 1] x [0, 1]\n            x1, y1, x2, y2 = box\n            rel_box = [x1 / w, y1 / h, (x2 - x1) / w, (y2 - y1) / h]\n\n            detections.append(\n                fo.Detection(\n                    label=classes[label],\n                    bounding_box=rel_box,\n                    confidence=score\n                )\n            )\n        \n        # Save predictions to dataset\n        sample[\"predictions\"] = fo.Detections(detections=detections)\n        sample.save()\n</pre> from PIL import Image from torchvision.transforms import functional as func  import fiftyone as fo  # Get class list classes = dataset.default_classes  # Add predictions to samples with fo.ProgressBar() as pb:     for sample in pb(predictions_view):         # Load image         image = Image.open(sample.filepath)         image = func.to_tensor(image).to(device)         c, h, w = image.shape                  # Perform inference         preds = model([image])[0]         labels = preds[\"labels\"].cpu().detach().numpy()         scores = preds[\"scores\"].cpu().detach().numpy()         boxes = preds[\"boxes\"].cpu().detach().numpy()                  # Convert detections to FiftyOne format         detections = []         for label, score, box in zip(labels, scores, boxes):             # Convert to [top-left-x, top-left-y, width, height]             # in relative coordinates in [0, 1] x [0, 1]             x1, y1, x2, y2 = box             rel_box = [x1 / w, y1 / h, (x2 - x1) / w, (y2 - y1) / h]              detections.append(                 fo.Detection(                     label=classes[label],                     bounding_box=rel_box,                     confidence=score                 )             )                  # Save predictions to dataset         sample[\"predictions\"] = fo.Detections(detections=detections)         sample.save() <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [12.7m elapsed, 0s remaining, 0.1 samples/s]    \n</pre> <p>Let's load <code>predictions_view</code> in the App to visualize the predictions that we added:</p> In\u00a0[11]: Copied! <pre>session.view = predictions_view\n</pre> session.view = predictions_view Activate In\u00a0[12]: Copied! <pre>session.show()\n</pre> session.show() Activate In\u00a0[2]: Copied! <pre>patches_view = predictions_view.to_patches(\"ground_truth\")\nprint(patches_view)\n</pre> patches_view = predictions_view.to_patches(\"ground_truth\") print(patches_view) <pre>Dataset:     detector-recipe\nMedia type:  image\nNum patches: 849\nTags:        ['validation']\nPatch fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    sample_id:    fiftyone.core.fields.StringField\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detection)\nView stages:\n    1. Exists(field='predictions', bool=True)\n    2. ToPatches(field='ground_truth')\n</pre> <p>Let's use the App to create the same view as above. To do so, we just need to click the patches button in the App and select <code>ground_truth</code>.</p> In\u00a0[3]: Copied! <pre>session = fo.launch_app(view=predictions_view)\n</pre> session = fo.launch_app(view=predictions_view) Activate In\u00a0[5]: Copied! <pre>session = fo.launch_app(view=predictions_view)\n</pre> session = fo.launch_app(view=predictions_view) Activate In\u00a0[13]: Copied! <pre># Click the down caret on the `predictions` field of Fields Sidebar\n# and apply a confidence threshold\nsession.show()\n</pre> # Click the down caret on the `predictions` field of Fields Sidebar # and apply a confidence threshold session.show() Activate In\u00a0[15]: Copied! <pre>from fiftyone import ViewField as F\n\n# Only contains detections with confidence &gt;= 0.75\nhigh_conf_view = predictions_view.filter_labels(\"predictions\", F(\"confidence\") &gt; 0.75)\n</pre> from fiftyone import ViewField as F  # Only contains detections with confidence &gt;= 0.75 high_conf_view = predictions_view.filter_labels(\"predictions\", F(\"confidence\") &gt; 0.75) In\u00a0[16]: Copied! <pre># Print some information about the view\nprint(high_conf_view)\n</pre> # Print some information about the view print(high_conf_view) <pre>Dataset:        detector-recipe\nMedia type:     image\nNum samples:    100\nTags:           ['validation']\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    predictions:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\nView stages:\n    1. Take(size=100, seed=51)\n    2. FilterLabels(field='predictions', filter={'$gt': ['$$this.confidence', 0.75]}, only_matches=True)\n</pre> In\u00a0[18]: Copied! <pre># Print a prediction from the view to verify that its confidence is &gt; 0.75\nsample = high_conf_view.first()\nprint(sample.predictions.detections[0])\n</pre> # Print a prediction from the view to verify that its confidence is &gt; 0.75 sample = high_conf_view.first() print(sample.predictions.detections[0]) <pre>&lt;Detection: {\n    'id': '602feaf5db78a9b44e6c1423',\n    'attributes': BaseDict({}),\n    'label': 'giraffe',\n    'bounding_box': BaseList([\n        0.24742321968078612,\n        0.24475666681925456,\n        0.5395549297332763,\n        0.742965825398763,\n    ]),\n    'mask': None,\n    'confidence': 0.9984311461448669,\n    'index': None,\n}&gt;\n</pre> <p>Now let's load our view in the App to view the predictions that we programmatically selected:</p> In\u00a0[19]: Copied! <pre># Load high confidence view in the App\nsession.view = high_conf_view\n</pre> # Load high confidence view in the App session.view = high_conf_view Activate In\u00a0[20]: Copied! <pre>session.show()\n</pre> session.show() Activate In\u00a0[21]: Copied! <pre>session.freeze() # screenshot the active App for sharing\n</pre> session.freeze() # screenshot the active App for sharing"},{"location":"how_do_i/recipes/adding_detections/#adding-object-detections-to-a-dataset","title":"Adding Object Detections to a Dataset\u00b6","text":"<p>This recipe provides a glimpse into the possibilities for integrating FiftyOne into your ML workflows. Specifically, it covers:</p> <ul> <li>Loading an object detection dataset from the Dataset Zoo</li> <li>Adding predictions from an object detector to the dataset</li> <li>Launching the FiftyOne App and visualizing/exploring your data</li> <li>Integrating the App into your data analysis workflow</li> </ul>"},{"location":"how_do_i/recipes/adding_detections/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"how_do_i/recipes/adding_detections/#loading-a-detection-dataset","title":"Loading a detection dataset\u00b6","text":"<p>In this recipe, we'll work with the validation split of the COCO dataset, which is conveniently available for download via the FiftyOne Dataset Zoo.</p> <p>The snippet below will download the validation split and load it into FiftyOne.</p>"},{"location":"how_do_i/recipes/adding_detections/#adding-model-predictions","title":"Adding model predictions\u00b6","text":"<p>Now let's add some predictions from an object detector to the dataset.</p> <p>We'll use an off-the-shelf Faster R-CNN detection model provided by PyTorch. The following cell downloads the model and loads it:</p>"},{"location":"how_do_i/recipes/adding_detections/#using-the-fiftyone-app","title":"Using the FiftyOne App\u00b6","text":"<p>Now let's use the App to analyze the predictions we've added to our dataset in more detail.</p>"},{"location":"how_do_i/recipes/adding_detections/#visualizing-bounding-boxes","title":"Visualizing bounding boxes\u00b6","text":"<p>Each field of the samples are shown as togglable checkboxes on the left sidebar which can be used to control whether ground truth or predicted boxes are rendered on the images.</p> <p>You can also double-click on an image to view individual samples in more detail:</p>"},{"location":"how_do_i/recipes/adding_detections/#visualizing-object-patches","title":"Visualizing object patches\u00b6","text":"<p>It can be beneficial to view every object as an individual sample, especially when there are multiple overlapping detections like in the image above.</p> <p>In FiftyOne this is called a patches view and can be created through Python or directly in the App.</p>"},{"location":"how_do_i/recipes/adding_detections/#confidence-thresholding-in-the-app","title":"Confidence thresholding in the App\u00b6","text":"<p>From the App instance above, it looks like our detector is generating some spurious low-quality detections. Let's use the App to interactively filter the predictions by <code>confidence</code> to identify a reasonable confidence threshold for our model:</p>"},{"location":"how_do_i/recipes/adding_detections/#confidence-thresholding-in-python","title":"Confidence thresholding in Python\u00b6","text":"<p>FiftyOne also provides the ability to write expressions that match, filter, and sort detections based on their attributes. See using DatasetViews for full details.</p> <p>For example, we can programmatically generate a view that contains only detections whose <code>confidence</code> is at least <code>0.75</code> as follows:</p>"},{"location":"how_do_i/recipes/adding_detections/#selecting-samples-of-interest","title":"Selecting samples of interest\u00b6","text":"<p>You can select images in the App by clicking on them. Then, you can create a view that contains only those samples by opening the selected samples dropdown in the top left corner of the image grid and clicking <code>Only show selected</code>.</p>"},{"location":"how_do_i/recipes/convert_datasets/","title":"Convert Dataset Formats","text":"<p>If you haven't already, install FiftyOne:</p> In\u00a0[\u00a0]: Copied! <pre>pip install fiftyone\n</pre> pip install fiftyone <p>This notebook contains bash commands. To run it as a notebook, you must install the Jupyter bash kernel via the command below.</p> <p>Alternatively, you can just copy + paste the code blocks into your shell.</p> In\u00a0[1]: Copied! <pre>pip install bash_kernel\npython -m bash_kernel.install\n</pre> pip install bash_kernel python -m bash_kernel.install <p>In this recipe we'll use the FiftyOne Dataset Zoo to download some open source datasets to work with.</p> <p>Specifically, we'll need TensorFlow and TensorFlow Datasets installed to access the datasets:</p> In\u00a0[2]: Copied! <pre>pip install tensorflow tensorflow-datasets\n</pre> pip install tensorflow tensorflow-datasets <p>Download the test split of the CIFAR-10 dataset from the FiftyOne Dataset Zoo using the command below:</p> In\u00a0[1]: Copied! <pre># Download the test split of CIFAR-10\nfiftyone zoo datasets download cifar10 --split test\n</pre> # Download the test split of CIFAR-10 fiftyone zoo datasets download cifar10 --split test <pre>Downloading split 'test' to '~/fiftyone/cifar10/test'\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ~/fiftyone/cifar10/tmp-download/cifar-10-python.tar.gz\n170500096it [00:04, 35887670.65it/s]                                            \nExtracting ~/fiftyone/cifar10/tmp-download/cifar-10-python.tar.gz to ~/fiftyone/cifar10/tmp-download\n 100% |\u2588\u2588\u2588| 10000/10000 [5.2s elapsed, 0s remaining, 1.8K samples/s]      \nDataset info written to '~/fiftyone/cifar10/info.json'\n</pre> <p>Download the validation split of the KITTI dataset from the FiftyOne Dataset Zoo using the command below:</p> In\u00a0[1]: Copied! <pre># Download the validation split of KITTI\nfiftyone zoo datasets download kitti --split validation\n</pre> # Download the validation split of KITTI fiftyone zoo datasets download kitti --split validation <pre>Split 'validation' already downloaded\n</pre> <p>The FiftyOne CLI provides a number of utilities for importing and exporting datasets in a variety of common (or custom) formats.</p> <p>Specifically, the <code>fiftyone convert</code> command provides a convenient way to convert datasets on disk between formats by specifying the fiftyone.types.Dataset type of the input and desired output.</p> <p>FiftyOne provides a collection of builtin types that you can use to read/write datasets in common formats out-of-the-box:</p> Dataset format Import Supported? Export Supported? Conversion Supported? ImageDirectory \u2713 \u2713 \u2713 VideoDirectory \u2713 \u2713 \u2713 FiftyOneImageClassificationDataset \u2713 \u2713 \u2713 ImageClassificationDirectoryTree \u2713 \u2713 \u2713 TFImageClassificationDataset \u2713 \u2713 \u2713 FiftyOneImageDetectionDataset \u2713 \u2713 \u2713 COCODetectionDataset \u2713 \u2713 \u2713 VOCDetectionDataset \u2713 \u2713 \u2713 KITTIDetectionDataset \u2713 \u2713 \u2713 YOLODataset \u2713 \u2713 \u2713 TFObjectDetectionDataset \u2713 \u2713 \u2713 CVATImageDataset \u2713 \u2713 \u2713 CVATVideoDataset \u2713 \u2713 \u2713 FiftyOneImageLabelsDataset \u2713 \u2713 \u2713 FiftyOneVideoLabelsDataset \u2713 \u2713 \u2713 BDDDataset \u2713 \u2713 \u2713 <p>In addition, you can define your own custom dataset types to read/write datasets in your own formats.</p> <p>The usage of the <code>fiftyone convert</code> command is as follows:</p> In\u00a0[1]: Copied! <pre>fiftyone convert -h\n</pre> fiftyone convert -h <pre>usage: fiftyone convert [-h] --input-type INPUT_TYPE --output-type OUTPUT_TYPE\n                        [--input-dir INPUT_DIR]\n                        [--input-kwargs KEY=VAL [KEY=VAL ...]]\n                        [--output-dir OUTPUT_DIR]\n                        [--output-kwargs KEY=VAL [KEY=VAL ...]] [-o]\n\nConvert datasets on disk between supported formats.\n\n    Examples::\n\n        # Convert an image classification directory tree to TFRecords format\n        fiftyone convert \\\n            --input-dir /path/to/image-classification-directory-tree \\\n            --input-type fiftyone.types.ImageClassificationDirectoryTree \\\n            --output-dir /path/for/tf-image-classification-dataset \\\n            --output-type fiftyone.types.TFImageClassificationDataset\n\n        # Convert a COCO detection dataset to CVAT image format\n        fiftyone convert \\\n            --input-dir /path/to/coco-detection-dataset \\\n            --input-type fiftyone.types.COCODetectionDataset \\\n            --output-dir /path/for/cvat-image-dataset \\\n            --output-type fiftyone.types.CVATImageDataset\n\n        # Perform a customized conversion via optional kwargs\n        fiftyone convert \\\n            --input-dir /path/to/coco-detection-dataset \\\n            --input-type fiftyone.types.COCODetectionDataset \\\n            --input-kwargs max_samples=100 shuffle=True \\\n            --output-dir /path/for/cvat-image-dataset \\\n            --output-type fiftyone.types.TFObjectDetectionDataset \\\n            --output-kwargs force_rgb=True \\\n            --overwrite\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --input-dir INPUT_DIR\n                        the directory containing the dataset\n  --input-kwargs KEY=VAL [KEY=VAL ...]\n                        additional keyword arguments for `fiftyone.utils.data.convert_dataset(..., input_kwargs=)`\n  --output-dir OUTPUT_DIR\n                        the directory to which to write the output dataset\n  --output-kwargs KEY=VAL [KEY=VAL ...]\n                        additional keyword arguments for `fiftyone.utils.data.convert_dataset(..., output_kwargs=)`\n  -o, --overwrite       whether to overwrite an existing output directory\n\nrequired arguments:\n  --input-type INPUT_TYPE\n                        the fiftyone.types.Dataset type of the input dataset\n  --output-type OUTPUT_TYPE\n                        the fiftyone.types.Dataset type to output\n</pre> <p>When you downloaded the test split of the CIFAR-10 dataset above, it was written to disk as a dataset in fiftyone.types.FiftyOneImageClassificationDataset format.</p> <p>You can verify this by printing information about the downloaded dataset:</p> In\u00a0[6]: Copied! <pre>fiftyone zoo datasets info cifar10\n</pre> fiftyone zoo datasets info cifar10 <pre>***** Dataset description *****\nThe CIFAR-10 dataset consists of 60000 32 x 32 color images in 10\n    classes, with 6000 images per class. There are 50000 training images and\n    10000 test images.\n\n    Dataset size:\n        132.40 MiB\n\n    Source:\n        https://www.cs.toronto.edu/~kriz/cifar.html\n    \n***** Supported splits *****\ntest, train\n\n***** Dataset location *****\n~/fiftyone/cifar10\n\n***** Dataset info *****\n{\n    \"name\": \"cifar10\",\n    \"zoo_dataset\": \"fiftyone.zoo.datasets.torch.CIFAR10Dataset\",\n    \"dataset_type\": \"fiftyone.types.dataset_types.FiftyOneImageClassificationDataset\",\n    \"num_samples\": 10000,\n    \"downloaded_splits\": {\n        \"test\": {\n            \"split\": \"test\",\n            \"num_samples\": 10000\n        }\n    },\n    \"classes\": [\n        \"airplane\",\n        \"automobile\",\n        \"bird\",\n        \"cat\",\n        \"deer\",\n        \"dog\",\n        \"frog\",\n        \"horse\",\n        \"ship\",\n        \"truck\"\n    ]\n}\n</pre> <p>The snippet below uses <code>fiftyone convert</code> to convert the test split of the CIFAR-10 dataset to fiftyone.types.ImageClassificationDirectoryTree format, which stores classification datasets on disk in a directory tree structure with images organized per-class:</p> <pre><code>&lt;dataset_dir&gt;\n\u251c\u2500\u2500 &lt;classA&gt;/\n\u2502   \u251c\u2500\u2500 &lt;image1&gt;.&lt;ext&gt;\n\u2502   \u251c\u2500\u2500 &lt;image2&gt;.&lt;ext&gt;\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 &lt;classB&gt;/\n\u2502   \u251c\u2500\u2500 &lt;image1&gt;.&lt;ext&gt;\n\u2502   \u251c\u2500\u2500 &lt;image2&gt;.&lt;ext&gt;\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre> In\u00a0[7]: Copied! <pre>INPUT_DIR=$(fiftyone zoo datasets find cifar10 --split test)\nOUTPUT_DIR=/tmp/fiftyone/cifar10-dir-tree\n\nfiftyone convert \\\n    --input-dir ${INPUT_DIR} --input-type fiftyone.types.FiftyOneImageClassificationDataset \\\n    --output-dir ${OUTPUT_DIR} --output-type fiftyone.types.ImageClassificationDirectoryTree\n</pre> INPUT_DIR=$(fiftyone zoo datasets find cifar10 --split test) OUTPUT_DIR=/tmp/fiftyone/cifar10-dir-tree  fiftyone convert \\     --input-dir ${INPUT_DIR} --input-type fiftyone.types.FiftyOneImageClassificationDataset \\     --output-dir ${OUTPUT_DIR} --output-type fiftyone.types.ImageClassificationDirectoryTree <pre>Loading dataset from '~/fiftyone/cifar10/test'\nInput format 'fiftyone.types.dataset_types.FiftyOneImageClassificationDataset'\n 100% |\u2588\u2588\u2588| 10000/10000 [4.2s elapsed, 0s remaining, 2.4K samples/s]      \nImport complete\nExporting dataset to '/tmp/fiftyone/cifar10-dir-tree'\nExport format 'fiftyone.types.dataset_types.ImageClassificationDirectoryTree'\n 100% |\u2588\u2588\u2588| 10000/10000 [6.2s elapsed, 0s remaining, 1.7K samples/s]        \nExport complete\n</pre> <p>Let's verify that the conversion happened as expected:</p> In\u00a0[8]: Copied! <pre>ls -lah /tmp/fiftyone/cifar10-dir-tree/\n</pre> ls -lah /tmp/fiftyone/cifar10-dir-tree/ <pre>total 0\ndrwxr-xr-x    12 voxel51  wheel   384B Jul 14 11:08 .\ndrwxr-xr-x     3 voxel51  wheel    96B Jul 14 11:08 ..\ndrwxr-xr-x  1002 voxel51  wheel    31K Jul 14 11:08 airplane\ndrwxr-xr-x  1002 voxel51  wheel    31K Jul 14 11:08 automobile\ndrwxr-xr-x  1002 voxel51  wheel    31K Jul 14 11:08 bird\ndrwxr-xr-x  1002 voxel51  wheel    31K Jul 14 11:08 cat\ndrwxr-xr-x  1002 voxel51  wheel    31K Jul 14 11:08 deer\ndrwxr-xr-x  1002 voxel51  wheel    31K Jul 14 11:08 dog\ndrwxr-xr-x  1002 voxel51  wheel    31K Jul 14 11:08 frog\ndrwxr-xr-x  1002 voxel51  wheel    31K Jul 14 11:08 horse\ndrwxr-xr-x  1002 voxel51  wheel    31K Jul 14 11:08 ship\ndrwxr-xr-x  1002 voxel51  wheel    31K Jul 14 11:08 truck\n</pre> In\u00a0[9]: Copied! <pre>ls -lah /tmp/fiftyone/cifar10-dir-tree/airplane/ | head\n</pre> ls -lah /tmp/fiftyone/cifar10-dir-tree/airplane/ | head <pre>total 8000\ndrwxr-xr-x  1002 voxel51  wheel    31K Jul 14 11:08 .\ndrwxr-xr-x    12 voxel51  wheel   384B Jul 14 11:08 ..\n-rw-r--r--     1 voxel51  wheel   1.2K Jul 14 11:23 000004.jpg\n-rw-r--r--     1 voxel51  wheel   1.1K Jul 14 11:23 000011.jpg\n-rw-r--r--     1 voxel51  wheel   1.1K Jul 14 11:23 000022.jpg\n-rw-r--r--     1 voxel51  wheel   1.3K Jul 14 11:23 000028.jpg\n-rw-r--r--     1 voxel51  wheel   1.2K Jul 14 11:23 000045.jpg\n-rw-r--r--     1 voxel51  wheel   1.2K Jul 14 11:23 000053.jpg\n-rw-r--r--     1 voxel51  wheel   1.3K Jul 14 11:23 000075.jpg\n</pre> <p>Now let's convert the classification directory tree to TFRecords format!</p> In\u00a0[10]: Copied! <pre>INPUT_DIR=/tmp/fiftyone/cifar10-dir-tree\nOUTPUT_DIR=/tmp/fiftyone/cifar10-tfrecords\n\nfiftyone convert \\\n    --input-dir ${INPUT_DIR} --input-type fiftyone.types.ImageClassificationDirectoryTree \\\n    --output-dir ${OUTPUT_DIR} --output-type fiftyone.types.TFImageClassificationDataset\n</pre> INPUT_DIR=/tmp/fiftyone/cifar10-dir-tree OUTPUT_DIR=/tmp/fiftyone/cifar10-tfrecords  fiftyone convert \\     --input-dir ${INPUT_DIR} --input-type fiftyone.types.ImageClassificationDirectoryTree \\     --output-dir ${OUTPUT_DIR} --output-type fiftyone.types.TFImageClassificationDataset <pre>Loading dataset from '/tmp/fiftyone/cifar10-dir-tree'\nInput format 'fiftyone.types.dataset_types.ImageClassificationDirectoryTree'\n 100% |\u2588\u2588\u2588| 10000/10000 [4.0s elapsed, 0s remaining, 2.5K samples/s]      \nImport complete\nExporting dataset to '/tmp/fiftyone/cifar10-tfrecords'\nExport format 'fiftyone.types.dataset_types.TFImageClassificationDataset'\n   0% ||--|     1/10000 [23.2ms elapsed, 3.9m remaining, 43.2 samples/s] 2020-07-14 11:24:15.187387: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2020-07-14 11:24:15.201384: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f83df428f60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2020-07-14 11:24:15.201405: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n 100% |\u2588\u2588\u2588| 10000/10000 [8.2s elapsed, 0s remaining, 1.3K samples/s]        \nExport complete\n</pre> <p>Let's verify that the conversion happened as expected:</p> In\u00a0[11]: Copied! <pre>ls -lah /tmp/fiftyone/cifar10-tfrecords\n</pre> ls -lah /tmp/fiftyone/cifar10-tfrecords <pre>total 29696\ndrwxr-xr-x  3 voxel51  wheel    96B Jul 14 11:24 .\ndrwxr-xr-x  4 voxel51  wheel   128B Jul 14 11:24 ..\n-rw-r--r--  1 voxel51  wheel    14M Jul 14 11:24 tf.records\n</pre> <p>When you downloaded the validation split of the KITTI dataset above, it was written to disk as a dataset in fiftyone.types.FiftyOneImageDetectionDataset format.</p> <p>You can verify this by printing information about the downloaded dataset:</p> In\u00a0[12]: Copied! <pre>fiftyone zoo datasets info kitti\n</pre> fiftyone zoo datasets info kitti <pre>***** Dataset description *****\nKITTI contains a suite of vision tasks built using an autonomous\n    driving platform.\n\n    The full benchmark contains many tasks such as stereo, optical flow, visual\n    odometry, etc. This dataset contains the object detection dataset,\n    including the monocular images and bounding boxes. The dataset contains\n    7481 training images annotated with 3D bounding boxes. A full description\n    of the annotations can be found in the README of the object development kit\n    on the KITTI homepage.\n\n    Dataset size:\n        5.27 GiB\n\n    Source:\n        http://www.cvlibs.net/datasets/kitti\n    \n***** Supported splits *****\ntest, train, validation\n\n***** Dataset location *****\n~/fiftyone/kitti\n\n***** Dataset info *****\n{\n    \"name\": \"kitti\",\n    \"zoo_dataset\": \"fiftyone.zoo.datasets.tf.KITTIDataset\",\n    \"dataset_type\": \"fiftyone.types.dataset_types.FiftyOneImageDetectionDataset\",\n    \"num_samples\": 423,\n    \"downloaded_splits\": {\n        \"validation\": {\n            \"split\": \"validation\",\n            \"num_samples\": 423\n        }\n    },\n    \"classes\": [\n        \"Car\",\n        \"Van\",\n        \"Truck\",\n        \"Pedestrian\",\n        \"Person_sitting\",\n        \"Cyclist\",\n        \"Tram\",\n        \"Misc\"\n    ]\n}\n</pre> <p>The snippet below uses <code>fiftyone convert</code> to convert the test split of the CIFAR-10 dataset to fiftyone.types.COCODetectionDataset format, which writes the dataset to disk with annotations in COCO format.</p> In\u00a0[13]: Copied! <pre>INPUT_DIR=$(fiftyone zoo datasets find kitti --split validation)\nOUTPUT_DIR=/tmp/fiftyone/kitti-coco\n\nfiftyone convert \\\n    --input-dir ${INPUT_DIR} --input-type fiftyone.types.FiftyOneImageDetectionDataset \\\n    --output-dir ${OUTPUT_DIR} --output-type fiftyone.types.COCODetectionDataset\n</pre> INPUT_DIR=$(fiftyone zoo datasets find kitti --split validation) OUTPUT_DIR=/tmp/fiftyone/kitti-coco  fiftyone convert \\     --input-dir ${INPUT_DIR} --input-type fiftyone.types.FiftyOneImageDetectionDataset \\     --output-dir ${OUTPUT_DIR} --output-type fiftyone.types.COCODetectionDataset <pre>Loading dataset from '~/fiftyone/kitti/validation'\nInput format 'fiftyone.types.dataset_types.FiftyOneImageDetectionDataset'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 423/423 [1.2s elapsed, 0s remaining, 351.0 samples/s]         \nImport complete\nExporting dataset to '/tmp/fiftyone/kitti-coco'\nExport format 'fiftyone.types.dataset_types.COCODetectionDataset'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 423/423 [4.4s elapsed, 0s remaining, 96.1 samples/s]       \nExport complete\n</pre> <p>Let's verify that the conversion happened as expected:</p> In\u00a0[14]: Copied! <pre>ls -lah /tmp/fiftyone/kitti-coco/\n</pre> ls -lah /tmp/fiftyone/kitti-coco/ <pre>total 880\ndrwxr-xr-x    4 voxel51  wheel   128B Jul 14 11:24 .\ndrwxr-xr-x    5 voxel51  wheel   160B Jul 14 11:24 ..\ndrwxr-xr-x  425 voxel51  wheel    13K Jul 14 11:24 data\n-rw-r--r--    1 voxel51  wheel   437K Jul 14 11:24 labels.json\n</pre> In\u00a0[15]: Copied! <pre>ls -lah /tmp/fiftyone/kitti-coco/data | head\n</pre> ls -lah /tmp/fiftyone/kitti-coco/data | head <pre>total 171008\ndrwxr-xr-x  425 voxel51  wheel    13K Jul 14 11:24 .\ndrwxr-xr-x    4 voxel51  wheel   128B Jul 14 11:24 ..\n-rw-r--r--    1 voxel51  wheel   195K Jul 14 11:24 000001.jpg\n-rw-r--r--    1 voxel51  wheel   191K Jul 14 11:24 000002.jpg\n-rw-r--r--    1 voxel51  wheel   167K Jul 14 11:24 000003.jpg\n-rw-r--r--    1 voxel51  wheel   196K Jul 14 11:24 000004.jpg\n-rw-r--r--    1 voxel51  wheel   224K Jul 14 11:24 000005.jpg\n-rw-r--r--    1 voxel51  wheel   195K Jul 14 11:24 000006.jpg\n-rw-r--r--    1 voxel51  wheel   177K Jul 14 11:24 000007.jpg\n</pre> In\u00a0[19]: Copied! <pre>cat /tmp/fiftyone/kitti-coco/labels.json | python -m json.tool 2&gt; /dev/null | head -20\necho \"...\"\ncat /tmp/fiftyone/kitti-coco/labels.json | python -m json.tool 2&gt; /dev/null | tail -20\n</pre> cat /tmp/fiftyone/kitti-coco/labels.json | python -m json.tool 2&gt; /dev/null | head -20 echo \"...\" cat /tmp/fiftyone/kitti-coco/labels.json | python -m json.tool 2&gt; /dev/null | tail -20 <pre>{\n    \"info\": {\n        \"year\": \"\",\n        \"version\": \"\",\n        \"description\": \"Exported from FiftyOne\",\n        \"contributor\": \"\",\n        \"url\": \"https://voxel51.com/fiftyone\",\n        \"date_created\": \"2020-07-14T11:24:40\"\n    },\n    \"licenses\": [],\n    \"categories\": [\n        {\n            \"id\": 0,\n            \"name\": \"Car\",\n            \"supercategory\": \"none\"\n        },\n        {\n            \"id\": 1,\n            \"name\": \"Cyclist\",\n            \"supercategory\": \"none\"\n...\n            \"area\": 4545.8,\n            \"segmentation\": null,\n            \"iscrowd\": 0\n        },\n        {\n            \"id\": 3196,\n            \"image_id\": 422,\n            \"category_id\": 3,\n            \"bbox\": [\n                367.2,\n                107.3,\n                36.2,\n                105.2\n            ],\n            \"area\": 3808.2,\n            \"segmentation\": null,\n            \"iscrowd\": 0\n        }\n    ]\n}\n</pre> <p>Now let's convert from COCO format to CVAT Image format format!</p> In\u00a0[20]: Copied! <pre>INPUT_DIR=/tmp/fiftyone/kitti-coco\nOUTPUT_DIR=/tmp/fiftyone/kitti-cvat\n\nfiftyone convert \\\n    --input-dir ${INPUT_DIR} --input-type fiftyone.types.COCODetectionDataset \\\n    --output-dir ${OUTPUT_DIR} --output-type fiftyone.types.CVATImageDataset\n</pre> INPUT_DIR=/tmp/fiftyone/kitti-coco OUTPUT_DIR=/tmp/fiftyone/kitti-cvat  fiftyone convert \\     --input-dir ${INPUT_DIR} --input-type fiftyone.types.COCODetectionDataset \\     --output-dir ${OUTPUT_DIR} --output-type fiftyone.types.CVATImageDataset <pre>Loading dataset from '/tmp/fiftyone/kitti-coco'\nInput format 'fiftyone.types.dataset_types.COCODetectionDataset'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 423/423 [2.0s elapsed, 0s remaining, 206.4 samples/s]      \nImport complete\nExporting dataset to '/tmp/fiftyone/kitti-cvat'\nExport format 'fiftyone.types.dataset_types.CVATImageDataset'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 423/423 [1.3s elapsed, 0s remaining, 323.7 samples/s]         \nExport complete\n</pre> <p>Let's verify that the conversion happened as expected:</p> In\u00a0[21]: Copied! <pre>ls -lah /tmp/fiftyone/kitti-cvat\n</pre> ls -lah /tmp/fiftyone/kitti-cvat <pre>total 584\ndrwxr-xr-x    4 voxel51  wheel   128B Jul 14 11:25 .\ndrwxr-xr-x    6 voxel51  wheel   192B Jul 14 11:25 ..\ndrwxr-xr-x  425 voxel51  wheel    13K Jul 14 11:25 data\n-rw-r--r--    1 voxel51  wheel   289K Jul 14 11:25 labels.xml\n</pre> In\u00a0[22]: Copied! <pre>cat /tmp/fiftyone/kitti-cvat/labels.xml | head -20\necho \"...\"\ncat /tmp/fiftyone/kitti-cvat/labels.xml | tail -20\n</pre> cat /tmp/fiftyone/kitti-cvat/labels.xml | head -20 echo \"...\" cat /tmp/fiftyone/kitti-cvat/labels.xml | tail -20 <pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;annotations&gt;\n    &lt;version&gt;1.1&lt;/version&gt;\n    &lt;meta&gt;\n        &lt;task&gt;\n            &lt;size&gt;423&lt;/size&gt;\n            &lt;mode&gt;annotation&lt;/mode&gt;\n            &lt;labels&gt;\n                &lt;label&gt;\n                    &lt;name&gt;Car&lt;/name&gt;\n                    &lt;attributes&gt;\n                    &lt;/attributes&gt;\n                &lt;/label&gt;\n                &lt;label&gt;\n                    &lt;name&gt;Cyclist&lt;/name&gt;\n                    &lt;attributes&gt;\n                    &lt;/attributes&gt;\n                &lt;/label&gt;\n                &lt;label&gt;\n                    &lt;name&gt;Misc&lt;/name&gt;\n...\n        &lt;box label=\"Pedestrian\" xtl=\"360\" ytl=\"116\" xbr=\"402\" ybr=\"212\"&gt;\n        &lt;/box&gt;\n        &lt;box label=\"Pedestrian\" xtl=\"396\" ytl=\"120\" xbr=\"430\" ybr=\"212\"&gt;\n        &lt;/box&gt;\n        &lt;box label=\"Pedestrian\" xtl=\"413\" ytl=\"112\" xbr=\"483\" ybr=\"212\"&gt;\n        &lt;/box&gt;\n        &lt;box label=\"Pedestrian\" xtl=\"585\" ytl=\"80\" xbr=\"646\" ybr=\"215\"&gt;\n        &lt;/box&gt;\n        &lt;box label=\"Pedestrian\" xtl=\"635\" ytl=\"94\" xbr=\"688\" ybr=\"212\"&gt;\n        &lt;/box&gt;\n        &lt;box label=\"Pedestrian\" xtl=\"422\" ytl=\"85\" xbr=\"469\" ybr=\"210\"&gt;\n        &lt;/box&gt;\n        &lt;box label=\"Pedestrian\" xtl=\"457\" ytl=\"93\" xbr=\"520\" ybr=\"213\"&gt;\n        &lt;/box&gt;\n        &lt;box label=\"Pedestrian\" xtl=\"505\" ytl=\"101\" xbr=\"548\" ybr=\"206\"&gt;\n        &lt;/box&gt;\n        &lt;box label=\"Pedestrian\" xtl=\"367\" ytl=\"107\" xbr=\"403\" ybr=\"212\"&gt;\n        &lt;/box&gt;\n    &lt;/image&gt;\n&lt;/annotations&gt;</pre> In\u00a0[23]: Copied! <pre>rm -rf /tmp/fiftyone\n</pre> rm -rf /tmp/fiftyone"},{"location":"how_do_i/recipes/convert_datasets/#convert-dataset-formats","title":"Convert Dataset Formats\u00b6","text":"<p>This recipe demonstrates how to use FiftyOne to convert datasets on disk between common formats.</p>"},{"location":"how_do_i/recipes/convert_datasets/#setup","title":"Setup\u00b6","text":""},{"location":"how_do_i/recipes/convert_datasets/#download-datasets","title":"Download datasets\u00b6","text":""},{"location":"how_do_i/recipes/convert_datasets/#the-fiftyone-convert-command","title":"The fiftyone convert command\u00b6","text":""},{"location":"how_do_i/recipes/convert_datasets/#convert-cifar-10-dataset","title":"Convert CIFAR-10 dataset\u00b6","text":""},{"location":"how_do_i/recipes/convert_datasets/#convert-kitti-dataset","title":"Convert KITTI dataset\u00b6","text":""},{"location":"how_do_i/recipes/convert_datasets/#cleanup","title":"Cleanup\u00b6","text":"<p>You can cleanup the files generated by this recipe by running the command below:</p>"},{"location":"how_do_i/recipes/creating_views/","title":"Creating Views","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone In\u00a0[\u00a0]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n</pre> import fiftyone as fo import fiftyone.zoo as foz In\u00a0[\u00a0]: Copied! <pre>dataset = foz.load_zoo_dataset(\"quickstart\")\ndataset.evaluate_detections(\"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\")\n</pre> dataset = foz.load_zoo_dataset(\"quickstart\") dataset.evaluate_detections(\"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\") <p>Dataset views can range from as simple as \"select a slice of the dataset\" to \"filter sample that have at least two large bounding boxes of people or dogs with high confidence and that were evaluated to be a false positive, then crop all images to those bounding boxes\":</p> In\u00a0[\u00a0]: Copied! <pre>from fiftyone import ViewField as F\n\n# Slice dataset\nsimple_view = dataset[51:151]\n\n# Complex filtering and conversion\ncomplex_view = (\n    dataset\n    .filter_labels(\n        \"predictions\", (\n            (F(\"confidence\") &gt; 0.7)\n            &amp; ((F(\"bounding_box\")[2] * F(\"bounding_box\")[3]) &gt; 0.3)\n            &amp; (F(\"eval\") == \"fp\")\n            &amp; (F(\"label\").is_in([\"person\", \"dog\"]))\n        )\n    ).match(\n        F(\"predictions.detections\").length() &gt; 2\n    ).to_patches(\"predictions\")\n)\n</pre> from fiftyone import ViewField as F  # Slice dataset simple_view = dataset[51:151]  # Complex filtering and conversion complex_view = (     dataset     .filter_labels(         \"predictions\", (             (F(\"confidence\") &gt; 0.7)             &amp; ((F(\"bounding_box\")[2] * F(\"bounding_box\")[3]) &gt; 0.3)             &amp; (F(\"eval\") == \"fp\")             &amp; (F(\"label\").is_in([\"person\", \"dog\"]))         )     ).match(         F(\"predictions.detections\").length() &gt; 2     ).to_patches(\"predictions\") ) <p>The goal is that, by the end of this notebook, creating complex views like the one above will be as straight forward as the simple views.</p> In\u00a0[\u00a0]: Copied! <pre># A view that contains the entire dataset\nview = dataset.view()\n</pre> # A view that contains the entire dataset view = dataset.view() <p>Within FiftyOne, views and datasets are largely interchangable in nearly all operations. Anything you can do to a dataset, you can also do to a view.</p> In\u00a0[\u00a0]: Copied! <pre>print(view)\n</pre> print(view) <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 200\nTags:        ['validation']\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    is_cloudy:       fiftyone.core.fields.BooleanField\n    classification:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n    classifications: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\nView stages:\n    ---\n</pre> <p>To create some more interesting views, you need to apply a view stage operation to the dataset. The list of available view stages can be printed as follows:</p> In\u00a0[\u00a0]: Copied! <pre>dataset.list_view_stages()\n</pre> dataset.list_view_stages() Out[\u00a0]: <pre>['exclude',\n 'exclude_by',\n 'exclude_fields',\n 'exclude_frames',\n 'exclude_labels',\n 'exists',\n 'filter_field',\n 'filter_labels',\n 'filter_classifications',\n 'filter_detections',\n 'filter_polylines',\n 'filter_keypoints',\n 'geo_near',\n 'geo_within',\n 'group_by',\n 'limit',\n 'limit_labels',\n 'map_labels',\n 'set_field',\n 'match',\n 'match_frames',\n 'match_labels',\n 'match_tags',\n 'mongo',\n 'select',\n 'select_by',\n 'select_fields',\n 'select_frames',\n 'select_labels',\n 'shuffle',\n 'skip',\n 'sort_by',\n 'sort_by_similarity',\n 'take',\n 'to_patches',\n 'to_evaluation_patches',\n 'to_clips',\n 'to_frames']</pre> <p>These view stages allow you to perform many useful operations on datasets like slicing, sorting, shuffling, filtering, and more.</p> <p>For example, the take() stage lets you extract a random subset of samples from the dataset:</p> In\u00a0[\u00a0]: Copied! <pre>random_view = dataset.take(100)\n\nprint(len(random_view))\n</pre> random_view = dataset.take(100)  print(len(random_view)) <pre>100\n</pre> <p>These view stages can also be chained together, each operating on the view returned by the previous stage:</p> In\u00a0[\u00a0]: Copied! <pre>sorted_random_view = random_view.sort_by(\"filepath\")\nsliced_sorted_random_view = sorted_random_view[10:51]\n</pre> sorted_random_view = random_view.sort_by(\"filepath\") sliced_sorted_random_view = sorted_random_view[10:51] <p>Note that the slicing syntax is simply a different representation of the skip() and limit() stages:</p> In\u00a0[\u00a0]: Copied! <pre>sliced_sorted_random_view = sorted_random_view.skip(10).limit(41)\n</pre> sliced_sorted_random_view = sorted_random_view.skip(10).limit(41) <p>An example of one of the stages used in this notebook is match(). This stage will keep or remove samples in the dataset one by one based on if some expression applied to the sample resolves to True or False.</p> <p>For example, we can create a view that includes all samples with a uniqueness greater than 0.5:</p> In\u00a0[\u00a0]: Copied! <pre>matched_view = dataset.match(F(\"uniqueness\") &gt; 0.5)\n</pre> matched_view = dataset.match(F(\"uniqueness\") &gt; 0.5) <p>Another useful view stage is set_field(). This stage will actually modify a field in your dataset based on the provided expression. Note that this modification is only within the resulting <code>DatasetView</code> and will not modify the underlying dataset.</p> <p>For example, lets set a boolean field called <code>is_cloudy</code> to True for all samples in the dataset. Note that when using <code>set_field()</code>, you need to ensure that the field exists on the dataset first.</p> In\u00a0[\u00a0]: Copied! <pre>dataset.add_sample_field(\"is_cloudy\", fo.BooleanField)\ncloudy_view = dataset.set_field(\"is_cloudy\", True)\n\ndataset.set_values(\"is_cloudy\", [True]*len(dataset))\n</pre> dataset.add_sample_field(\"is_cloudy\", fo.BooleanField) cloudy_view = dataset.set_field(\"is_cloudy\", True)  dataset.set_values(\"is_cloudy\", [True]*len(dataset)) <p>Most view stages accept a ViewExpression as input. View stages that seemingly operate on fields can also accept expressions! For example, sort_by() can accept a field name or an expression:</p> In\u00a0[\u00a0]: Copied! <pre># Sort by filepaths\ndataset.sort_by(\"filepath\")\n\n# Sort by the number of predicted objects per sample\ndataset.sort_by(F(\"predictions.detections\").length())\n</pre> # Sort by filepaths dataset.sort_by(\"filepath\")  # Sort by the number of predicted objects per sample dataset.sort_by(F(\"predictions.detections\").length()) Out[\u00a0]: <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 200\nTags:        ['validation']\nSample fields:\n    id:           fiftyone.core.fields.ObjectIdField\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:   fiftyone.core.fields.FloatField\n    predictions:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:      fiftyone.core.fields.IntField\n    eval_fp:      fiftyone.core.fields.IntField\n    eval_fn:      fiftyone.core.fields.IntField\n    is_cloudy:    fiftyone.core.fields.BooleanField\nView stages:\n    1. SortBy(field_or_expr={'$size': {'$ifNull': [...]}}, reverse=False)</pre> <p>The idea is to think about what is expected by a view stage, then provide the input that is needed in the form of a string or an expression.</p> <p>sort_by() operates on a sample-level, meaning we can either provide it the name of a sample-level field to use for sorting (<code>filepath</code>) or we can provide it an expression that resolves to a sample-level value. In this case the expression is counting the number of predicted objects for each sample and using those integers to sort the dataset.</p> <p>In our dataset, after performing evaluation, we populated the field <code>eval_tp</code> on each sample with is an integer containing the number of true positive predictions exist in the sample. There are multiple ways to match samples based on the <code>eval_tp</code> field.</p> <p>The way to think about view expressions in this case is the same as the expressions for the if-statement in Python that resolve in a boolean context.</p> In\u00a0[\u00a0]: Copied! <pre>a = True\nb = 51\n\nif a: # Nothing else needed\n    pass\n\nif b &gt; 4:\n    # True if b &gt; 4\n    pass\n\nif b:\n    # True if b != 0\n    pass\n</pre> a = True b = 51  if a: # Nothing else needed     pass  if b &gt; 4:     # True if b &gt; 4     pass  if b:     # True if b != 0     pass In\u00a0[\u00a0]: Copied! <pre>tp_view = dataset.match(F(\"eval_tp\") &gt; 4)\n\nprint(len(tp_view))\n</pre> tp_view = dataset.match(F(\"eval_tp\") &gt; 4)  print(len(tp_view)) <pre>69\n</pre> <p>When providing just an integer in the expression in a Python if-statement, then the statement is True as long as the integer is not zero. The same logic applies with view expressions in this case:</p> In\u00a0[\u00a0]: Copied! <pre>nonzero_tp_view = dataset.match(F(\"eval_tp\"))\n\nprint(len(nonzero_tp_view))\n</pre> nonzero_tp_view = dataset.match(F(\"eval_tp\"))  print(len(nonzero_tp_view)) <pre>198\n</pre> <p>We can also use <code>~</code> to negate an expression:</p> In\u00a0[\u00a0]: Copied! <pre>zero_tp_view = dataset.match(~F(\"eval_tp\"))\n\nprint(zero_tp_view.values(\"eval_tp\"))\n</pre> zero_tp_view = dataset.match(~F(\"eval_tp\"))  print(zero_tp_view.values(\"eval_tp\")) <pre>[0, 0]\n</pre> In\u00a0[\u00a0]: Copied! <pre>sample = fo.Sample(\n    filepath=\"example.png\",\n    ground_truth=fo.Detections(\n        detections=[\n            fo.Detection(label=\"cat\", bounding_box=[0.1, 0.1, 0.8, 0.8])\n        ]\n    ),\n)\n\nfo.pprint(sample.to_dict())\n</pre> sample = fo.Sample(     filepath=\"example.png\",     ground_truth=fo.Detections(         detections=[             fo.Detection(label=\"cat\", bounding_box=[0.1, 0.1, 0.8, 0.8])         ]     ), )  fo.pprint(sample.to_dict())  <pre>{\n    'filepath': '/content/example.png',\n    'tags': [],\n    'metadata': None,\n    'ground_truth': {\n        '_cls': 'Detections',\n        'detections': [\n            {\n                '_id': {'$oid': '622f67345627ae9fa020e6f9'},\n                '_cls': 'Detection',\n                'attributes': {},\n                'tags': [],\n                'label': 'cat',\n                'bounding_box': [0.1, 0.1, 0.8, 0.8],\n            },\n        ],\n    },\n}\n</pre> <p>Here you can see that <code>ground_truth.detections</code> is a list.</p> <p>If you have a field containing a primitive value, then it rarely requires more than one operation to get the value that is needed by the view stage. However, when working with a list of values in a field, then there can be multiple different operations that need to be performed to get to the desired value.</p> <p>The most important operations for working with lists are:</p> <ul> <li>filter(): apply a boolean to each element of a list to determine what to keep, resolving to a list</li> <li>map(): apply a function to each element of a list, resolving to a list</li> <li>reduce(): operates on a list and resolves to a single value</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Only include predictions with `confidence` of at least 0.9\nview = dataset.set_field(\n    \"predictions.detections\",\n    F(\"detections\").filter(F(\"confidence\") &gt; 0.9)\n)\n</pre> # Only include predictions with `confidence` of at least 0.9 view = dataset.set_field(     \"predictions.detections\",     F(\"detections\").filter(F(\"confidence\") &gt; 0.9) ) <p>Note that the filter_labels() operation is simply a simplification of the filter operation and set_field(). This operation will automatically apply the given expression to the corresponding list field of the label if applicable (<code>Detections</code>, <code>Classifications</code>, etc) or will apply the expression as a match operation for non-list labels (<code>Detection</code>, <code>Classification</code>, etc).</p> In\u00a0[\u00a0]: Copied! <pre># Filter detections\nview1 = dataset.filter_labels(\"ground_truth\", F(\"label\") == \"cat\")\n\n# Equivalently\nview2 = (\n    dataset\n    .set_field(\"ground_truth.detections\", F(\"detections\").filter(F(\"label\") == \"cat\"))\n    .match(F(\"ground_truth.detections\").length() &gt; 0)\n)\n\nprint(len(view1))\nprint(len(view2))\n</pre> # Filter detections view1 = dataset.filter_labels(\"ground_truth\", F(\"label\") == \"cat\")  # Equivalently view2 = (     dataset     .set_field(\"ground_truth.detections\", F(\"detections\").filter(F(\"label\") == \"cat\"))     .match(F(\"ground_truth.detections\").length() &gt; 0) )  print(len(view1)) print(len(view2)) <pre>14\n14\n</pre> <p>The match operation above was added since by default, filter_labels() sets the keyword argument <code>only_matches=True</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Add example classification labels\ndataset.set_values(\"classifications\", [fo.Classification(label=\"cat\")]*len(dataset))\n\n# Filter classification\nview1 = dataset.filter_labels(\"classifications\", F(\"label\") == \"cat\")\n\n# Equivalently\nview2 = dataset.match(F(\"classifications.label\") == \"cat\")\n\nprint(len(view1))\nprint(len(view2))\n</pre> # Add example classification labels dataset.set_values(\"classifications\", [fo.Classification(label=\"cat\")]*len(dataset))  # Filter classification view1 = dataset.filter_labels(\"classifications\", F(\"label\") == \"cat\")  # Equivalently view2 = dataset.match(F(\"classifications.label\") == \"cat\")  print(len(view1)) print(len(view2)) <pre>200\n200\n</pre> In\u00a0[\u00a0]: Copied! <pre>transform_tag = F().upper()\nview = dataset.set_field(\"tags\", F(\"tags\").map(transform_tag))\n\nprint(view)\n</pre> transform_tag = F().upper() view = dataset.set_field(\"tags\", F(\"tags\").map(transform_tag))  print(view) <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 200\nTags:        ['VALIDATION']\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    is_cloudy:       fiftyone.core.fields.BooleanField\n    classification:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n    classifications: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\nView stages:\n    1. SetField(field='tags', expr={'$map': {'as': 'this', 'in': {...}, 'input': '$tags'}})\n</pre> <p>Note that the <code>F()</code> above is empty, indicating that upper() is applied to the primitives stored in each element of the field. In this case, the primitives are the string tags. In general, <code>F()</code> references the root of the current context.</p> In\u00a0[\u00a0]: Copied! <pre>from fiftyone.core.expressions import VALUE\n</pre> from fiftyone.core.expressions import VALUE In\u00a0[\u00a0]: Copied! <pre># Get all of the matched gt object ids\nview = (\n    dataset\n    .set_field(\n        \"predictions.gt_ids\",\n        F(\"detections\")\n        .filter(F(\"eval\") == \"tp\")\n        .reduce(VALUE.append(F(\"eval_id\")), init_val=[])\n    )\n)\nview.first().predictions.gt_ids\n</pre> # Get all of the matched gt object ids view = (     dataset     .set_field(         \"predictions.gt_ids\",         F(\"detections\")         .filter(F(\"eval\") == \"tp\")         .reduce(VALUE.append(F(\"eval_id\")), init_val=[])     ) ) view.first().predictions.gt_ids Out[\u00a0]: <pre>['5f452471ef00e6374aac53c8', '5f452471ef00e6374aac53ca']</pre> In\u00a0[\u00a0]: Copied! <pre>dataset.compute_metadata()\n\n# Computes the area of each bounding box in pixels\nbbox_area = (\n    F(\"$metadata.width\") * F(\"bounding_box\")[2] *\n    F(\"$metadata.height\") * F(\"bounding_box\")[3]\n)\n\n# Only contains boxes whose area is between 32^2 and 96^2 pixels\nmedium_boxes_view = dataset.filter_labels(\n    \"predictions\", (32 ** 2 &lt; bbox_area) &amp; (bbox_area &lt; 96 ** 2)\n)\n</pre> dataset.compute_metadata()  # Computes the area of each bounding box in pixels bbox_area = (     F(\"$metadata.width\") * F(\"bounding_box\")[2] *     F(\"$metadata.height\") * F(\"bounding_box\")[3] )  # Only contains boxes whose area is between 32^2 and 96^2 pixels medium_boxes_view = dataset.filter_labels(     \"predictions\", (32 ** 2 &lt; bbox_area) &amp; (bbox_area &lt; 96 ** 2) ) <p>For a complete listing of all operations that can be performed to create view expressions and examples of each, check out the API documentation.</p> In\u00a0[\u00a0]: Copied! <pre>dataset.list_aggregations()\n</pre> dataset.list_aggregations() Out[\u00a0]: <pre>['bounds',\n 'count',\n 'count_values',\n 'distinct',\n 'histogram_values',\n 'mean',\n 'std',\n 'sum',\n 'values']</pre> <p>The documentation already contains plenty of detailed information about aggregations. This section just highlights how view expressions can be used with aggregations.</p> <p>In the simplest case, aggregations can be performed by providing the name of a field you want to compute on:</p> In\u00a0[\u00a0]: Copied! <pre>print(dataset.distinct(\"predictions.detections.label\"))\n</pre> print(dataset.distinct(\"predictions.detections.label\")) <pre>['airplane', 'apple', 'backpack', 'banana', 'baseball glove', 'bear', 'bed', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'bowl', 'broccoli', 'bus', 'cake', 'car', 'carrot', 'cat', 'cell phone', 'chair', 'clock', 'couch', 'cow', 'cup', 'dining table', 'dog', 'donut', 'elephant', 'fire hydrant', 'fork', 'frisbee', 'giraffe', 'hair drier', 'handbag', 'horse', 'hot dog', 'keyboard', 'kite', 'knife', 'laptop', 'microwave', 'motorcycle', 'mouse', 'orange', 'oven', 'parking meter', 'person', 'pizza', 'potted plant', 'refrigerator', 'remote', 'sandwich', 'scissors', 'sheep', 'sink', 'skateboard', 'skis', 'snowboard', 'spoon', 'sports ball', 'stop sign', 'suitcase', 'surfboard', 'teddy bear', 'tennis racket', 'tie', 'toaster', 'toilet', 'toothbrush', 'traffic light', 'train', 'truck', 'tv', 'umbrella', 'vase', 'wine glass', 'zebra']\n</pre> <p>However, you can also pass a ViewExpression to the aggregation method, in which case the expression will be evaluated and then aggregated as requested:</p> In\u00a0[\u00a0]: Copied! <pre>print(dataset.distinct(F(\"uniqueness\").round(2)))\n</pre> print(dataset.distinct(F(\"uniqueness\").round(2))) <pre>[0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.34, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.72, 0.73, 0.74, 0.75, 0.78, 0.8, 0.82, 0.92, 1.0]\n</pre>"},{"location":"how_do_i/recipes/creating_views/#creating-views","title":"Creating Views\u00b6","text":"<p>FiftyOne datasets provide the flexibility to store large, complex data. While it is helpful that data can be imported and exported easily, the real potential of FiftyOne comes from its powerful query language that you can use to define custom views into your datasets.</p> <p>A dataset view can be thought of as a pipeline of operations that is applied to a dataset to extract a subset of the dataset whose samples and fields are filtered, sorted, shuffled, etc. Check out this page for an extended discussion of dataset views.</p> <p>In this notebook, we'll do a brief walkthrough of creating and using dataset views.</p>"},{"location":"how_do_i/recipes/creating_views/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"how_do_i/recipes/creating_views/#overview","title":"Overview\u00b6","text":"<p>To start out, lets import FiftyOne, load up a dataset, and evaluate some predicted object detections.</p>"},{"location":"how_do_i/recipes/creating_views/#view-basics","title":"View basics\u00b6","text":"<p>\"Creating a view from a dataset\" is simply the process of performing an operation on a dataset that returns a <code>DatasetView</code>. The most basic way to turn a <code>Dataset</code> into a <code>DatasetView</code> is to just call <code>view()</code>.</p>"},{"location":"how_do_i/recipes/creating_views/#view-expressions","title":"View expressions\u00b6","text":"<p>At this point, you might be wondering \"what is this <code>F</code> that I keep seeing everywhere\"? That <code>F</code> defines a ViewField which can be used to write a ViewExpression. These expressions are what give you the power to write custom queries based on information that exists in your dataset.</p> <p>In this section, we go over what some view expression operations and how to write more complex views.</p>"},{"location":"how_do_i/recipes/creating_views/#view-fields","title":"View fields\u00b6","text":"<p>As mentioned, view expressions are built around view fields. A ViewField is how you inject the information stored in a specific field of your dataset into a view expression.</p> <p>For example, if you had a boolean field on your dataset called <code>is_cloudy</code> indicating if the image contains cloudy or not, then for each sample, <code>F(\"is_cloudy\")</code> can be thought of as being replaced with the value of <code>\"is_cloudy\"</code> for that sample. Since values in the field are themselves boolean, the view to match samples where <code>\"is_cloudy\"</code> is True is simply:</p> <pre>cloudy_view = dataset.match(F(\"is_cloudy\"))\n</pre>"},{"location":"how_do_i/recipes/creating_views/#nested-lists","title":"Nested lists\u00b6","text":"<p>The most difficult/subtle aspect of creating view expressions is how to handle nested lists.</p> <p>To get a better idea of which samples contain lists, you can print out your sample as a dictionary:</p>"},{"location":"how_do_i/recipes/creating_views/#filtering-list-fields","title":"Filtering list fields\u00b6","text":"<p>The filter() operation is quite useful to allow for fine-grained access to the information that is to be kept and removed from the view.</p>"},{"location":"how_do_i/recipes/creating_views/#mapping-list-fields","title":"Mapping list fields\u00b6","text":"<p>The map() operation can be used to apply an expression to every element of a list. For example, we can update the tags to set every tag to uppercase:</p>"},{"location":"how_do_i/recipes/creating_views/#reducing-list-fields","title":"Reducing list fields\u00b6","text":"<p>The reduce() operation lets you take a list, operate on each element of it, and return some value. Reduce expressions generally involve some <code>VALUE</code> that is being aggregated as each element is iterated over. For example, this could be some float that values are added to, a string that gets concatenated each iteration, or even a list to which elements are appended.</p> <p>Say that we want to set a field on our predictions containing the IDs of the corresponding ground truth objects that were matched to the true positives. We can use filter and reduce to accomplish this as follows:</p>"},{"location":"how_do_i/recipes/creating_views/#referencing-root-fields","title":"Referencing root fields\u00b6","text":"<p>Another useful property of expressions is prepending your field names with <code>$</code> to refer to the root of the document. This can be used, for example, to use sample-level information like <code>metadata</code> when filtering at a detection-level:</p>"},{"location":"how_do_i/recipes/creating_views/#aggregations","title":"Aggregations\u00b6","text":"<p>Aggregations provide a convenient syntax to compute aggregate statistics or extract values across a dataset or view.</p> <p>For example, you can use aggregations to get information like:</p> <ul> <li>The boundary values of a field</li> <li>The unique label names in your dataset</li> <li>The standard deviation of a value across your samples</li> <li>Extract a slice of field values across a view</li> </ul> <p>You can view the available aggregations like so:</p>"},{"location":"how_do_i/recipes/creating_views/#summary","title":"Summary\u00b6","text":"<p>Dataset views and the view expressions language are powerful and flexible aspects of FiftyOne.</p> <p>Getting comfortable with using views and expressions to slice and dice your datasets based on the questions you have will allow you to work efficiently to curate high quality datasets.</p>"},{"location":"how_do_i/recipes/custom_exporter/","title":"Writing Custom Dataset Exporters","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>In this recipe we'll use the FiftyOne Dataset Zoo to download the CIFAR-10 dataset to use as sample data to feed our custom exporter.</p> <p>Behind the scenes, FiftyOne uses either the TensorFlow Datasets or TorchVision Datasets libraries to wrangle the datasets, depending on which ML library you have installed.</p> <p>You can, for example, install PyTorch as follows:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install torch torchvision\n</pre> !pip install torch torchvision <p>Here's the complete definition of the <code>DatasetExporter</code>:</p> In\u00a0[1]: Copied! <pre>import csv\nimport os\n\nimport fiftyone as fo\nimport fiftyone.utils.data as foud\n\n\nclass CSVImageClassificationDatasetExporter(foud.LabeledImageDatasetExporter):\n    \"\"\"Exporter for image classification datasets whose labels and image\n    metadata are stored on disk in a CSV file.\n\n    Datasets of this type are exported in the following format:\n\n        &lt;dataset_dir&gt;/\n            data/\n                &lt;filename1&gt;.&lt;ext&gt;\n                &lt;filename2&gt;.&lt;ext&gt;\n                ...\n            labels.csv\n\n    where ``labels.csv`` is a CSV file in the following format::\n\n        filepath,size_bytes,mime_type,width,height,num_channels,label\n        &lt;filepath&gt;,&lt;size_bytes&gt;,&lt;mime_type&gt;,&lt;width&gt;,&lt;height&gt;,&lt;num_channels&gt;,&lt;label&gt;\n        &lt;filepath&gt;,&lt;size_bytes&gt;,&lt;mime_type&gt;,&lt;width&gt;,&lt;height&gt;,&lt;num_channels&gt;,&lt;label&gt;\n        ...\n\n    Args:\n        export_dir: the directory to write the export\n    \"\"\"\n\n    def __init__(self, export_dir):\n        super().__init__(export_dir=export_dir)\n        self._data_dir = None\n        self._labels_path = None\n        self._labels = None\n        self._image_exporter = None\n        \n    @property\n    def requires_image_metadata(self):\n        \"\"\"Whether this exporter requires\n        :class:`fiftyone.core.metadata.ImageMetadata` instances for each sample\n        being exported.\n        \"\"\"\n        return True\n\n    @property\n    def label_cls(self):\n        \"\"\"The :class:`fiftyone.core.labels.Label` class(es) exported by this\n        exporter.\n\n        This can be any of the following:\n\n        -   a :class:`fiftyone.core.labels.Label` class. In this case, the\n            exporter directly exports labels of this type\n        -   a list or tuple of :class:`fiftyone.core.labels.Label` classes. In\n            this case, the exporter can export a single label field of any of\n            these types\n        -   a dict mapping keys to :class:`fiftyone.core.labels.Label` classes.\n            In this case, the exporter can handle label dictionaries with\n            value-types specified by this dictionary. Not all keys need be\n            present in the exported label dicts\n        -   ``None``. In this case, the exporter makes no guarantees about the\n            labels that it can export\n        \"\"\"\n        return fo.Classification\n\n    def setup(self):\n        \"\"\"Performs any necessary setup before exporting the first sample in\n        the dataset.\n\n        This method is called when the exporter's context manager interface is\n        entered, :func:`DatasetExporter.__enter__`.\n        \"\"\"\n        self._data_dir = os.path.join(self.export_dir, \"data\")\n        self._labels_path = os.path.join(self.export_dir, \"labels.csv\")\n        self._labels = []\n        \n        # The `ImageExporter` utility class provides an `export()` method\n        # that exports images to an output directory with automatic handling\n        # of things like name conflicts\n        self._image_exporter = foud.ImageExporter(\n            True, export_path=self._data_dir, default_ext=\".jpg\",\n        )\n        self._image_exporter.setup()\n        \n    def export_sample(self, image_or_path, label, metadata=None):\n        \"\"\"Exports the given sample to the dataset.\n\n        Args:\n            image_or_path: an image or the path to the image on disk\n            label: an instance of :meth:`label_cls`, or a dictionary mapping\n                field names to :class:`fiftyone.core.labels.Label` instances,\n                or ``None`` if the sample is unlabeled\n            metadata (None): a :class:`fiftyone.core.metadata.ImageMetadata`\n                instance for the sample. Only required when\n                :meth:`requires_image_metadata` is ``True``\n        \"\"\"\n        out_image_path, _ = self._image_exporter.export(image_or_path)\n\n        if metadata is None:\n            metadata = fo.ImageMetadata.build_for(image_or_path)\n\n        self._labels.append((\n            out_image_path,\n            metadata.size_bytes,\n            metadata.mime_type,\n            metadata.width,\n            metadata.height,\n            metadata.num_channels,\n            label.label,  # here, `label` is a `Classification` instance\n        ))\n\n    def close(self, *args):\n        \"\"\"Performs any necessary actions after the last sample has been\n        exported.\n\n        This method is called when the exporter's context manager interface is\n        exited, :func:`DatasetExporter.__exit__`.\n\n        Args:\n            *args: the arguments to :func:`DatasetExporter.__exit__`\n        \"\"\"\n        # Ensure the base output directory exists\n        basedir = os.path.dirname(self._labels_path)\n        if basedir and not os.path.isdir(basedir):\n            os.makedirs(basedir)\n\n        # Write the labels CSV file\n        with open(self._labels_path, \"w\") as f:\n            writer = csv.writer(f)\n            writer.writerow([\n                \"filepath\",\n                \"size_bytes\",\n                \"mime_type\",\n                \"width\",\n                \"height\",\n                \"num_channels\",\n                \"label\",\n            ])\n            for row in self._labels:\n                writer.writerow(row)\n</pre> import csv import os  import fiftyone as fo import fiftyone.utils.data as foud   class CSVImageClassificationDatasetExporter(foud.LabeledImageDatasetExporter):     \"\"\"Exporter for image classification datasets whose labels and image     metadata are stored on disk in a CSV file.      Datasets of this type are exported in the following format:          /             data/                 . .                 ...             labels.csv      where ``labels.csv`` is a CSV file in the following format::          filepath,size_bytes,mime_type,width,height,num_channels,label         ,,,,,, ,,,,,,         ...      Args:         export_dir: the directory to write the export     \"\"\"      def __init__(self, export_dir):         super().__init__(export_dir=export_dir)         self._data_dir = None         self._labels_path = None         self._labels = None         self._image_exporter = None              @property     def requires_image_metadata(self):         \"\"\"Whether this exporter requires         :class:`fiftyone.core.metadata.ImageMetadata` instances for each sample         being exported.         \"\"\"         return True      @property     def label_cls(self):         \"\"\"The :class:`fiftyone.core.labels.Label` class(es) exported by this         exporter.          This can be any of the following:          -   a :class:`fiftyone.core.labels.Label` class. In this case, the             exporter directly exports labels of this type         -   a list or tuple of :class:`fiftyone.core.labels.Label` classes. In             this case, the exporter can export a single label field of any of             these types         -   a dict mapping keys to :class:`fiftyone.core.labels.Label` classes.             In this case, the exporter can handle label dictionaries with             value-types specified by this dictionary. Not all keys need be             present in the exported label dicts         -   ``None``. In this case, the exporter makes no guarantees about the             labels that it can export         \"\"\"         return fo.Classification      def setup(self):         \"\"\"Performs any necessary setup before exporting the first sample in         the dataset.          This method is called when the exporter's context manager interface is         entered, :func:`DatasetExporter.__enter__`.         \"\"\"         self._data_dir = os.path.join(self.export_dir, \"data\")         self._labels_path = os.path.join(self.export_dir, \"labels.csv\")         self._labels = []                  # The `ImageExporter` utility class provides an `export()` method         # that exports images to an output directory with automatic handling         # of things like name conflicts         self._image_exporter = foud.ImageExporter(             True, export_path=self._data_dir, default_ext=\".jpg\",         )         self._image_exporter.setup()              def export_sample(self, image_or_path, label, metadata=None):         \"\"\"Exports the given sample to the dataset.          Args:             image_or_path: an image or the path to the image on disk             label: an instance of :meth:`label_cls`, or a dictionary mapping                 field names to :class:`fiftyone.core.labels.Label` instances,                 or ``None`` if the sample is unlabeled             metadata (None): a :class:`fiftyone.core.metadata.ImageMetadata`                 instance for the sample. Only required when                 :meth:`requires_image_metadata` is ``True``         \"\"\"         out_image_path, _ = self._image_exporter.export(image_or_path)          if metadata is None:             metadata = fo.ImageMetadata.build_for(image_or_path)          self._labels.append((             out_image_path,             metadata.size_bytes,             metadata.mime_type,             metadata.width,             metadata.height,             metadata.num_channels,             label.label,  # here, `label` is a `Classification` instance         ))      def close(self, *args):         \"\"\"Performs any necessary actions after the last sample has been         exported.          This method is called when the exporter's context manager interface is         exited, :func:`DatasetExporter.__exit__`.          Args:             *args: the arguments to :func:`DatasetExporter.__exit__`         \"\"\"         # Ensure the base output directory exists         basedir = os.path.dirname(self._labels_path)         if basedir and not os.path.isdir(basedir):             os.makedirs(basedir)          # Write the labels CSV file         with open(self._labels_path, \"w\") as f:             writer = csv.writer(f)             writer.writerow([                 \"filepath\",                 \"size_bytes\",                 \"mime_type\",                 \"width\",                 \"height\",                 \"num_channels\",                 \"label\",             ])             for row in self._labels:                 writer.writerow(row)  <p>In order to use <code>CSVImageClassificationDatasetExporter</code>, we need some labeled image samples to work with.</p> <p>Let's use some samples from the test split of CIFAR-10:</p> In\u00a0[2]: Copied! <pre>import fiftyone.zoo as foz\n\nnum_samples = 1000\n\n#\n# Load `num_samples` from CIFAR-10\n#\n# This command will download the test split of CIFAR-10 from the web the first\n# time it is executed, if necessary\n#\ncifar10_test = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\nsamples = cifar10_test.limit(num_samples)\n</pre> import fiftyone.zoo as foz  num_samples = 1000  # # Load `num_samples` from CIFAR-10 # # This command will download the test split of CIFAR-10 from the web the first # time it is executed, if necessary # cifar10_test = foz.load_zoo_dataset(\"cifar10\", split=\"test\") samples = cifar10_test.limit(num_samples) <pre>Split 'test' already downloaded\nLoading 'cifar10' split 'test'\n 100% |\u2588\u2588\u2588| 10000/10000 [4.4s elapsed, 0s remaining, 2.2K samples/s]      \n</pre> In\u00a0[3]: Copied! <pre># Print summary information about the samples\nprint(samples)\n</pre> # Print summary information about the samples print(samples) <pre>Dataset:        cifar10-test\nNum samples:    1000\nTags:           ['test']\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\nPipeline stages:\n    1. Limit(limit=1000)\n</pre> In\u00a0[4]: Copied! <pre># Print a sample\nprint(samples.first())\n</pre> # Print a sample print(samples.first()) <pre>&lt;Sample: {\n    'dataset_name': 'cifar10-test',\n    'id': '5f0e6d7f503bf2b87254061c',\n    'filepath': '~/fiftyone/cifar10/test/data/000001.jpg',\n    'tags': BaseList(['test']),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {'label': 'cat', 'confidence': None, 'logits': None}&gt;,\n}&gt;\n</pre> <p>With our samples and <code>DatasetExporter</code> in-hand, exporting the samples to disk in our custom format is as simple as follows:</p> In\u00a0[5]: Copied! <pre>export_dir = \"/tmp/fiftyone/custom-dataset-exporter\"\n\n# Export the dataset\nprint(\"Exporting %d samples to '%s'\" % (len(samples), export_dir))\nexporter = CSVImageClassificationDatasetExporter(export_dir)\nsamples.export(dataset_exporter=exporter)\n</pre> export_dir = \"/tmp/fiftyone/custom-dataset-exporter\"  # Export the dataset print(\"Exporting %d samples to '%s'\" % (len(samples), export_dir)) exporter = CSVImageClassificationDatasetExporter(export_dir) samples.export(dataset_exporter=exporter) <pre>Exporting 1000 samples to '/tmp/fiftyone/custom-dataset-exporter'\n 100% |\u2588\u2588\u2588\u2588\u2588| 1000/1000 [1.0s elapsed, 0s remaining, 1.0K samples/s]          \n</pre> <p>Let's inspect the contents of the exported dataset to verify that it was written in the correct format:</p> In\u00a0[9]: Copied! <pre>!ls -lah /tmp/fiftyone/custom-dataset-exporter\n</pre> !ls -lah /tmp/fiftyone/custom-dataset-exporter <pre>total 168\r\ndrwxr-xr-x     4 voxel51  wheel   128B Jul 14 22:46 .\r\ndrwxr-xr-x     3 voxel51  wheel    96B Jul 14 22:46 ..\r\ndrwxr-xr-x  1002 voxel51  wheel    31K Jul 14 22:46 data\r\n-rw-r--r--     1 voxel51  wheel    83K Jul 14 22:46 labels.csv\r\n</pre> In\u00a0[10]: Copied! <pre>!ls -lah /tmp/fiftyone/custom-dataset-exporter/data | head -n 10\n</pre> !ls -lah /tmp/fiftyone/custom-dataset-exporter/data | head -n 10 <pre>total 8000\r\ndrwxr-xr-x  1002 voxel51  wheel    31K Jul 14 22:46 .\r\ndrwxr-xr-x     4 voxel51  wheel   128B Jul 14 22:46 ..\r\n-rw-r--r--     1 voxel51  wheel   1.4K Jul 14 22:46 000001.jpg\r\n-rw-r--r--     1 voxel51  wheel   1.3K Jul 14 22:46 000002.jpg\r\n-rw-r--r--     1 voxel51  wheel   1.2K Jul 14 22:46 000003.jpg\r\n-rw-r--r--     1 voxel51  wheel   1.2K Jul 14 22:46 000004.jpg\r\n-rw-r--r--     1 voxel51  wheel   1.4K Jul 14 22:46 000005.jpg\r\n-rw-r--r--     1 voxel51  wheel   1.3K Jul 14 22:46 000006.jpg\r\n-rw-r--r--     1 voxel51  wheel   1.4K Jul 14 22:46 000007.jpg\r\n</pre> In\u00a0[11]: Copied! <pre>!head -n 10 /tmp/fiftyone/custom-dataset-exporter/labels.csv\n</pre> !head -n 10 /tmp/fiftyone/custom-dataset-exporter/labels.csv <pre>filepath,size_bytes,mime_type,width,height,num_channels,label\r\n/tmp/fiftyone/custom-dataset-exporter/data/000001.jpg,1422,image/jpeg,32,32,3,cat\r\n/tmp/fiftyone/custom-dataset-exporter/data/000002.jpg,1285,image/jpeg,32,32,3,ship\r\n/tmp/fiftyone/custom-dataset-exporter/data/000003.jpg,1258,image/jpeg,32,32,3,ship\r\n/tmp/fiftyone/custom-dataset-exporter/data/000004.jpg,1244,image/jpeg,32,32,3,airplane\r\n/tmp/fiftyone/custom-dataset-exporter/data/000005.jpg,1388,image/jpeg,32,32,3,frog\r\n/tmp/fiftyone/custom-dataset-exporter/data/000006.jpg,1311,image/jpeg,32,32,3,frog\r\n/tmp/fiftyone/custom-dataset-exporter/data/000007.jpg,1412,image/jpeg,32,32,3,automobile\r\n/tmp/fiftyone/custom-dataset-exporter/data/000008.jpg,1218,image/jpeg,32,32,3,frog\r\n/tmp/fiftyone/custom-dataset-exporter/data/000009.jpg,1262,image/jpeg,32,32,3,cat\r\n</pre> In\u00a0[12]: Copied! <pre>!rm -rf /tmp/fiftyone\n</pre> !rm -rf /tmp/fiftyone"},{"location":"how_do_i/recipes/custom_exporter/#writing-custom-dataset-exporters","title":"Writing Custom Dataset Exporters\u00b6","text":"<p>This recipe demonstrates how to write a custom DatasetExporter and use it to export a FiftyOne dataset to disk in your custom format.</p>"},{"location":"how_do_i/recipes/custom_exporter/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"how_do_i/recipes/custom_exporter/#writing-a-datasetexporter","title":"Writing a DatasetExporter\u00b6","text":"<p>FiftyOne provides a DatasetExporter interface that defines how it exports datasets to disk when methods such as Dataset.export() are used.</p> <p><code>DatasetExporter</code> itself is an abstract interface; the concrete interface that you should implement is determined by the type of dataset that you are exporting. See writing a custom DatasetExporter for full details.</p> <p>In this recipe, we'll write a custom LabeledImageDatasetExporter that can export an image classification dataset to disk in the following format:</p> <pre><code>&lt;dataset_dir&gt;/\n    data/\n        &lt;filename1&gt;.&lt;ext&gt;\n        &lt;filename2&gt;.&lt;ext&gt;\n        ...\n    labels.csv\n</code></pre> <p>where <code>labels.csv</code> is a CSV file that contains the image metadata and associated labels in the following format:</p> <pre><code>filepath,size_bytes,mime_type,width,height,num_channels,label\n&lt;filepath&gt;,&lt;size_bytes&gt;,&lt;mime_type&gt;,&lt;width&gt;,&lt;height&gt;,&lt;num_channels&gt;,&lt;label&gt;\n&lt;filepath&gt;,&lt;size_bytes&gt;,&lt;mime_type&gt;,&lt;width&gt;,&lt;height&gt;,&lt;num_channels&gt;,&lt;label&gt;\n...\n</code></pre>"},{"location":"how_do_i/recipes/custom_exporter/#generating-a-sample-dataset","title":"Generating a sample dataset\u00b6","text":""},{"location":"how_do_i/recipes/custom_exporter/#exporting-a-dataset","title":"Exporting a dataset\u00b6","text":""},{"location":"how_do_i/recipes/custom_exporter/#cleanup","title":"Cleanup\u00b6","text":"<p>You can cleanup the files generated by this recipe by running:</p>"},{"location":"how_do_i/recipes/custom_importer/","title":"Writing Custom Dataset Importers","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>In this recipe we'll use the FiftyOne Dataset Zoo to download the CIFAR-10 dataset to use as sample data to feed our custom importer.</p> <p>Behind the scenes, FiftyOne either uses the TensorFlow Datasets or TorchVision Datasets libraries to wrangle the datasets, depending on which ML library you have installed.</p> <p>You can, for example, install PyTorch as follows:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install torch torchvision\n</pre> !pip install torch torchvision <p>Here's the complete definition of the <code>DatasetImporter</code>:</p> In\u00a0[3]: Copied! <pre>import csv\nimport os\n\nimport fiftyone as fo\nimport fiftyone.utils.data as foud\n\n\nclass CSVImageClassificationDatasetImporter(foud.LabeledImageDatasetImporter):\n    \"\"\"Importer for image classification datasets whose filepaths and labels\n    are stored on disk in a CSV file.\n\n    Datasets of this type should contain a ``labels.csv`` file in their\n    dataset directories in the following format::\n\n        filepath,size_bytes,mime_type,width,height,num_channels,label\n        &lt;filepath&gt;,&lt;size_bytes&gt;,&lt;mime_type&gt;,&lt;width&gt;,&lt;height&gt;,&lt;num_channels&gt;,&lt;label&gt;\n        &lt;filepath&gt;,&lt;size_bytes&gt;,&lt;mime_type&gt;,&lt;width&gt;,&lt;height&gt;,&lt;num_channels&gt;,&lt;label&gt;\n        ...\n\n    Args:\n        dataset_dir: the dataset directory\n        shuffle (False): whether to randomly shuffle the order in which the\n            samples are imported\n        seed (None): a random seed to use when shuffling\n        max_samples (None): a maximum number of samples to import. By default,\n            all samples are imported\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_dir,\n        shuffle=False,\n        seed=None,\n        max_samples=None,\n    ):\n        super().__init__(\n            dataset_dir=dataset_dir,\n            shuffle=shuffle,\n            seed=seed,\n            max_samples=max_samples\n        )\n        self._labels_file = None\n        self._labels = None\n        self._iter_labels = None\n\n    def __iter__(self):\n        self._iter_labels = iter(self._labels)\n        return self\n\n    def __next__(self):\n        \"\"\"Returns information about the next sample in the dataset.\n\n        Returns:\n            an  ``(image_path, image_metadata, label)`` tuple, where\n\n            -   ``image_path``: the path to the image on disk\n            -   ``image_metadata``: an\n                :class:`fiftyone.core.metadata.ImageMetadata` instances for the\n                image, or ``None`` if :meth:`has_image_metadata` is ``False``\n            -   ``label``: an instance of :meth:`label_cls`, or a dictionary\n                mapping field names to :class:`fiftyone.core.labels.Label`\n                instances, or ``None`` if the sample is unlabeled\n\n        Raises:\n            StopIteration: if there are no more samples to import\n        \"\"\"\n        (\n            filepath,\n            size_bytes,\n            mime_type,\n            width,\n            height,\n            num_channels,\n            label,\n        ) = next(self._iter_labels)\n\n        image_metadata = fo.ImageMetadata(\n            size_bytes=size_bytes,\n            mime_type=mime_type,\n            width=width,\n            height=height,\n            num_channels=num_channels,\n        )\n\n        label = fo.Classification(label=label)\n        return filepath, image_metadata, label\n\n    def __len__(self):\n        \"\"\"The total number of samples that will be imported.\n\n        Raises:\n            TypeError: if the total number is not known\n        \"\"\"\n        return len(self._labels)\n\n    @property\n    def has_dataset_info(self):\n        \"\"\"Whether this importer produces a dataset info dictionary.\"\"\"\n        return False\n\n    @property\n    def has_image_metadata(self):\n        \"\"\"Whether this importer produces\n        :class:`fiftyone.core.metadata.ImageMetadata` instances for each image.\n        \"\"\"\n        return True\n\n    @property\n    def label_cls(self):\n        \"\"\"The :class:`fiftyone.core.labels.Label` class(es) returned by this\n        importer.\n\n        This can be any of the following:\n\n        -   a :class:`fiftyone.core.labels.Label` class. In this case, the\n            importer is guaranteed to return labels of this type\n        -   a list or tuple of :class:`fiftyone.core.labels.Label` classes. In\n            this case, the importer can produce a single label field of any of\n            these types\n        -   a dict mapping keys to :class:`fiftyone.core.labels.Label` classes.\n            In this case, the importer will return label dictionaries with keys\n            and value-types specified by this dictionary. Not all keys need be\n            present in the imported labels\n        -   ``None``. In this case, the importer makes no guarantees about the\n            labels that it may return\n        \"\"\"\n        return fo.Classification\n\n    def setup(self):\n        \"\"\"Performs any necessary setup before importing the first sample in\n        the dataset.\n\n        This method is called when the importer's context manager interface is\n        entered, :func:`DatasetImporter.__enter__`.\n        \"\"\"\n        labels_path = os.path.join(self.dataset_dir, \"labels.csv\")\n\n        labels = []\n        with open(labels_path, \"r\") as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                labels.append((\n                    row[\"filepath\"],\n                    row[\"size_bytes\"],\n                    row[\"mime_type\"],\n                    row[\"width\"],\n                    row[\"height\"],\n                    row[\"num_channels\"],\n                    row[\"label\"],\n                ))\n\n        # The `_preprocess_list()` function is provided by the base class\n        # and handles shuffling/max sample limits\n        self._labels = self._preprocess_list(labels)\n\n    def close(self, *args):\n        \"\"\"Performs any necessary actions after the last sample has been\n        imported.\n\n        This method is called when the importer's context manager interface is\n        exited, :func:`DatasetImporter.__exit__`.\n\n        Args:\n            *args: the arguments to :func:`DatasetImporter.__exit__`\n        \"\"\"\n        pass\n</pre> import csv import os  import fiftyone as fo import fiftyone.utils.data as foud   class CSVImageClassificationDatasetImporter(foud.LabeledImageDatasetImporter):     \"\"\"Importer for image classification datasets whose filepaths and labels     are stored on disk in a CSV file.      Datasets of this type should contain a ``labels.csv`` file in their     dataset directories in the following format::          filepath,size_bytes,mime_type,width,height,num_channels,label         ,,,,,, ,,,,,,         ...      Args:         dataset_dir: the dataset directory         shuffle (False): whether to randomly shuffle the order in which the             samples are imported         seed (None): a random seed to use when shuffling         max_samples (None): a maximum number of samples to import. By default,             all samples are imported     \"\"\"      def __init__(         self,         dataset_dir,         shuffle=False,         seed=None,         max_samples=None,     ):         super().__init__(             dataset_dir=dataset_dir,             shuffle=shuffle,             seed=seed,             max_samples=max_samples         )         self._labels_file = None         self._labels = None         self._iter_labels = None      def __iter__(self):         self._iter_labels = iter(self._labels)         return self      def __next__(self):         \"\"\"Returns information about the next sample in the dataset.          Returns:             an  ``(image_path, image_metadata, label)`` tuple, where              -   ``image_path``: the path to the image on disk             -   ``image_metadata``: an                 :class:`fiftyone.core.metadata.ImageMetadata` instances for the                 image, or ``None`` if :meth:`has_image_metadata` is ``False``             -   ``label``: an instance of :meth:`label_cls`, or a dictionary                 mapping field names to :class:`fiftyone.core.labels.Label`                 instances, or ``None`` if the sample is unlabeled          Raises:             StopIteration: if there are no more samples to import         \"\"\"         (             filepath,             size_bytes,             mime_type,             width,             height,             num_channels,             label,         ) = next(self._iter_labels)          image_metadata = fo.ImageMetadata(             size_bytes=size_bytes,             mime_type=mime_type,             width=width,             height=height,             num_channels=num_channels,         )          label = fo.Classification(label=label)         return filepath, image_metadata, label      def __len__(self):         \"\"\"The total number of samples that will be imported.          Raises:             TypeError: if the total number is not known         \"\"\"         return len(self._labels)      @property     def has_dataset_info(self):         \"\"\"Whether this importer produces a dataset info dictionary.\"\"\"         return False      @property     def has_image_metadata(self):         \"\"\"Whether this importer produces         :class:`fiftyone.core.metadata.ImageMetadata` instances for each image.         \"\"\"         return True      @property     def label_cls(self):         \"\"\"The :class:`fiftyone.core.labels.Label` class(es) returned by this         importer.          This can be any of the following:          -   a :class:`fiftyone.core.labels.Label` class. In this case, the             importer is guaranteed to return labels of this type         -   a list or tuple of :class:`fiftyone.core.labels.Label` classes. In             this case, the importer can produce a single label field of any of             these types         -   a dict mapping keys to :class:`fiftyone.core.labels.Label` classes.             In this case, the importer will return label dictionaries with keys             and value-types specified by this dictionary. Not all keys need be             present in the imported labels         -   ``None``. In this case, the importer makes no guarantees about the             labels that it may return         \"\"\"         return fo.Classification      def setup(self):         \"\"\"Performs any necessary setup before importing the first sample in         the dataset.          This method is called when the importer's context manager interface is         entered, :func:`DatasetImporter.__enter__`.         \"\"\"         labels_path = os.path.join(self.dataset_dir, \"labels.csv\")          labels = []         with open(labels_path, \"r\") as f:             reader = csv.DictReader(f)             for row in reader:                 labels.append((                     row[\"filepath\"],                     row[\"size_bytes\"],                     row[\"mime_type\"],                     row[\"width\"],                     row[\"height\"],                     row[\"num_channels\"],                     row[\"label\"],                 ))          # The `_preprocess_list()` function is provided by the base class         # and handles shuffling/max sample limits         self._labels = self._preprocess_list(labels)      def close(self, *args):         \"\"\"Performs any necessary actions after the last sample has been         imported.          This method is called when the importer's context manager interface is         exited, :func:`DatasetImporter.__exit__`.          Args:             *args: the arguments to :func:`DatasetImporter.__exit__`         \"\"\"         pass  <p>In order to use <code>CSVImageClassificationDatasetImporter</code>, we need to generate a sample dataset in the required format.</p> <p>Let's first write a small utility to populate a <code>labels.csv</code> file in the required format.</p> In\u00a0[4]: Copied! <pre>def write_csv_labels(samples, csv_path, label_field=\"ground_truth\"):\n    \"\"\"Writes a labels CSV format for the given samples in the format expected\n    by :class:`CSVImageClassificationDatasetImporter`.\n\n    Args:\n        samples: an iterable of :class:`fiftyone.core.sample.Sample` instances\n        csv_path: the path to write the CSV file\n        label_field (\"ground_truth\"): the label field of the samples to write\n    \"\"\"\n    # Ensure base directory exists\n    basedir = os.path.dirname(csv_path)\n    if basedir and not os.path.isdir(basedir):\n        os.makedirs(basedir)\n\n    # Write the labels\n    with open(csv_path, \"w\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\n            \"filepath\",\n            \"size_bytes\",\n            \"mime_type\",\n            \"width\",\n            \"height\",\n            \"num_channels\",\n            \"label\",\n        ])\n        for sample in samples:\n            filepath = sample.filepath\n            metadata = sample.metadata\n            if metadata is None:\n                metadata = fo.ImageMetadata.build_for(filepath)\n\n            label = sample[label_field].label\n            writer.writerow([\n                filepath,\n                metadata.size_bytes,\n                metadata.mime_type,\n                metadata.width,\n                metadata.height,\n                metadata.num_channels,\n                label,\n            ])\n</pre> def write_csv_labels(samples, csv_path, label_field=\"ground_truth\"):     \"\"\"Writes a labels CSV format for the given samples in the format expected     by :class:`CSVImageClassificationDatasetImporter`.      Args:         samples: an iterable of :class:`fiftyone.core.sample.Sample` instances         csv_path: the path to write the CSV file         label_field (\"ground_truth\"): the label field of the samples to write     \"\"\"     # Ensure base directory exists     basedir = os.path.dirname(csv_path)     if basedir and not os.path.isdir(basedir):         os.makedirs(basedir)      # Write the labels     with open(csv_path, \"w\") as f:         writer = csv.writer(f)         writer.writerow([             \"filepath\",             \"size_bytes\",             \"mime_type\",             \"width\",             \"height\",             \"num_channels\",             \"label\",         ])         for sample in samples:             filepath = sample.filepath             metadata = sample.metadata             if metadata is None:                 metadata = fo.ImageMetadata.build_for(filepath)              label = sample[label_field].label             writer.writerow([                 filepath,                 metadata.size_bytes,                 metadata.mime_type,                 metadata.width,                 metadata.height,                 metadata.num_channels,                 label,             ])  <p>Now let's populate a directory with a <code>labels.csv</code> file in the format required by <code>CSVImageClassificationDatasetImporter</code> with some samples from the test split of CIFAR-10:</p> In\u00a0[7]: Copied! <pre>import fiftyone.zoo as foz\n\n\ndataset_dir = \"/tmp/fiftyone/custom-dataset-importer\"\nnum_samples = 1000\n\n#\n# Load `num_samples` from CIFAR-10\n#\n# This command will download the test split of CIFAR-10 from the web the first\n# time it is executed, if necessary\n#\ncifar10_test = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\nsamples = cifar10_test.limit(num_samples)\n\n# This dataset format requires samples to have their `metadata` fields populated\nprint(\"Computing metadata for samples\")\nsamples.compute_metadata()\n\n# Write labels to disk in CSV format\ncsv_path = os.path.join(dataset_dir, \"labels.csv\")\nprint(\"Writing labels for %d samples to '%s'\" % (num_samples, csv_path))\nwrite_csv_labels(samples, csv_path)\n</pre> import fiftyone.zoo as foz   dataset_dir = \"/tmp/fiftyone/custom-dataset-importer\" num_samples = 1000  # # Load `num_samples` from CIFAR-10 # # This command will download the test split of CIFAR-10 from the web the first # time it is executed, if necessary # cifar10_test = foz.load_zoo_dataset(\"cifar10\", split=\"test\") samples = cifar10_test.limit(num_samples)  # This dataset format requires samples to have their `metadata` fields populated print(\"Computing metadata for samples\") samples.compute_metadata()  # Write labels to disk in CSV format csv_path = os.path.join(dataset_dir, \"labels.csv\") print(\"Writing labels for %d samples to '%s'\" % (num_samples, csv_path)) write_csv_labels(samples, csv_path) <pre>Split 'test' already downloaded\nLoading existing dataset 'cifar10-test'. To reload from disk, first delete the existing dataset\nComputing metadata for samples\n 100% |\u2588\u2588\u2588\u2588\u2588| 1000/1000 [421.2ms elapsed, 0s remaining, 2.4K samples/s]      \nWriting labels for 1000 samples to '/tmp/fiftyone/custom-dataset-importer/labels.csv'\n</pre> <p>Let's inspect the contents of the labels CSV to ensure they're in the correct format:</p> In\u00a0[13]: Copied! <pre>!head -n 10 /tmp/fiftyone/custom-dataset-importer/labels.csv\n</pre> !head -n 10 /tmp/fiftyone/custom-dataset-importer/labels.csv <pre>filepath,size_bytes,mime_type,width,height,num_channels,label\r\n~/fiftyone/cifar10/test/data/000001.jpg,1422,image/jpeg,32,32,3,cat\r\n~/fiftyone/cifar10/test/data/000002.jpg,1285,image/jpeg,32,32,3,ship\r\n~/fiftyone/cifar10/test/data/000003.jpg,1258,image/jpeg,32,32,3,ship\r\n~/fiftyone/cifar10/test/data/000004.jpg,1244,image/jpeg,32,32,3,airplane\r\n~/fiftyone/cifar10/test/data/000005.jpg,1388,image/jpeg,32,32,3,frog\r\n~/fiftyone/cifar10/test/data/000006.jpg,1311,image/jpeg,32,32,3,frog\r\n~/fiftyone/cifar10/test/data/000007.jpg,1412,image/jpeg,32,32,3,automobile\r\n~/fiftyone/cifar10/test/data/000008.jpg,1218,image/jpeg,32,32,3,frog\r\n~/fiftyone/cifar10/test/data/000009.jpg,1262,image/jpeg,32,32,3,cat\r\n</pre> <p>With our dataset and <code>DatasetImporter</code> in-hand, loading the data as a FiftyOne dataset is as simple as follows:</p> In\u00a0[14]: Copied! <pre># Import the dataset\nprint(\"Importing dataset from '%s'\" % dataset_dir)\nimporter = CSVImageClassificationDatasetImporter(dataset_dir)\ndataset = fo.Dataset.from_importer(importer)\n</pre> # Import the dataset print(\"Importing dataset from '%s'\" % dataset_dir) importer = CSVImageClassificationDatasetImporter(dataset_dir) dataset = fo.Dataset.from_importer(importer) <pre>Importing dataset from '/tmp/fiftyone/custom-dataset-importer'\n 100% |\u2588\u2588\u2588\u2588\u2588| 1000/1000 [780.7ms elapsed, 0s remaining, 1.3K samples/s]      \n</pre> In\u00a0[15]: Copied! <pre># Print summary information about the dataset\nprint(dataset)\n</pre> # Print summary information about the dataset print(dataset) <pre>Name:           2020.07.14.22.33.01\nPersistent:     False\nNum samples:    1000\nTags:           []\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n</pre> In\u00a0[16]: Copied! <pre># Print a sample\nprint(dataset.first())\n</pre> # Print a sample print(dataset.first()) <pre>&lt;Sample: {\n    'dataset_name': '2020.07.14.22.33.01',\n    'id': '5f0e6add1dfd5f8c299ac528',\n    'filepath': '~/fiftyone/cifar10/test/data/000001.jpg',\n    'tags': BaseList([]),\n    'metadata': &lt;ImageMetadata: {\n        'size_bytes': 1422,\n        'mime_type': 'image/jpeg',\n        'width': 32,\n        'height': 32,\n        'num_channels': 3,\n    }&gt;,\n    'ground_truth': &lt;Classification: {'label': 'cat', 'confidence': None, 'logits': None}&gt;,\n}&gt;\n</pre> In\u00a0[17]: Copied! <pre>!rm -rf /tmp/fiftyone\n</pre> !rm -rf /tmp/fiftyone"},{"location":"how_do_i/recipes/custom_importer/#writing-custom-dataset-importers","title":"Writing Custom Dataset Importers\u00b6","text":"<p>This recipe demonstrates how to write a custom DatasetImporter and use it to load a dataset from disk in your custom format into FiftyOne.</p>"},{"location":"how_do_i/recipes/custom_importer/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"how_do_i/recipes/custom_importer/#writing-a-datasetimporter","title":"Writing a DatasetImporter\u00b6","text":"<p>FiftyOne provides a DatasetImporter interface that defines how it imports datasets from disk when methods such as Dataset.from_importer() are used.</p> <p><code>DatasetImporter</code> itself is an abstract interface; the concrete interface that you should implement is determined by the type of dataset that you are importing. See writing a custom DatasetImporter for full details.</p> <p>In this recipe, we'll write a custom LabeledImageDatasetImporter that can import an image classification dataset whose image metadata and labels are stored in a <code>labels.csv</code> file in the dataset directory with the following format:</p> <pre><code>filepath,size_bytes,mime_type,width,height,num_channels,label\n&lt;filepath&gt;,&lt;size_bytes&gt;,&lt;mime_type&gt;,&lt;width&gt;,&lt;height&gt;,&lt;num_channels&gt;,&lt;label&gt;\n&lt;filepath&gt;,&lt;size_bytes&gt;,&lt;mime_type&gt;,&lt;width&gt;,&lt;height&gt;,&lt;num_channels&gt;,&lt;label&gt;\n...\n</code></pre>"},{"location":"how_do_i/recipes/custom_importer/#generating-a-sample-dataset","title":"Generating a sample dataset\u00b6","text":""},{"location":"how_do_i/recipes/custom_importer/#importing-a-dataset","title":"Importing a dataset\u00b6","text":""},{"location":"how_do_i/recipes/custom_importer/#cleanup","title":"Cleanup\u00b6","text":"<p>You can cleanup the files generated by this recipe by running:</p>"},{"location":"how_do_i/recipes/custom_parser/","title":"Writing Custom Sample Parsers","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>In this receipe we'll use the TorchVision Datasets library to download the CIFAR-10 dataset to use as sample data to feed our custom parser.</p> <p>You can install the necessary packages, if necessary, as follows:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install torch torchvision\n</pre> !pip install torch torchvision <p>Here's the complete definition of the <code>SampleParser</code>:</p> In\u00a0[1]: Copied! <pre>import fiftyone as fo\nimport fiftyone.utils.data as foud\n\n\nclass PyTorchClassificationDatasetSampleParser(foud.LabeledImageSampleParser):\n    \"\"\"Parser for image classification samples loaded from a PyTorch dataset.\n\n    This parser can parse samples from a ``torch.utils.data.DataLoader`` that\n    emits ``(img_tensor, target)`` tuples, where::\n\n        - `img_tensor`: is a PyTorch Tensor containing the image\n        - `target`: the integer index of the target class\n\n    Args:\n        classes: the list of class label strings\n    \"\"\"\n\n    def __init__(self, classes):\n        self.classes = classes\n\n    @property\n    def has_image_path(self):\n        \"\"\"Whether this parser produces paths to images on disk for samples\n        that it parses.\n        \"\"\"\n        return False\n\n    @property\n    def has_image_metadata(self):\n        \"\"\"Whether this parser produces\n        :class:`fiftyone.core.metadata.ImageMetadata` instances for samples\n        that it parses.\n        \"\"\"\n        return False\n\n    @property\n    def label_cls(self):\n        \"\"\"The :class:`fiftyone.core.labels.Label` class(es) returned by this\n        parser.\n\n        This can be any of the following:\n\n        -   a :class:`fiftyone.core.labels.Label` class. In this case, the\n            parser is guaranteed to return labels of this type\n        -   a list or tuple of :class:`fiftyone.core.labels.Label` classes. In\n            this case, the parser can produce a single label field of any of\n            these types\n        -   a dict mapping keys to :class:`fiftyone.core.labels.Label` classes.\n            In this case, the parser will return label dictionaries with keys\n            and value-types specified by this dictionary. Not all keys need be\n            present in the imported labels\n        -   ``None``. In this case, the parser makes no guarantees about the\n            labels that it may return\n        \"\"\"\n        return fo.Classification\n\n    def get_image(self):\n        \"\"\"Returns the image from the current sample.\n\n        Returns:\n            a numpy image\n        \"\"\"\n        img_tensor = self.current_sample[0]\n        return img_tensor.cpu().numpy()\n\n    def get_label(self):\n        \"\"\"Returns the label for the current sample.\n\n        Returns:\n            a :class:`fiftyone.core.labels.Label` instance, or a dictionary\n            mapping field names to :class:`fiftyone.core.labels.Label`\n            instances, or ``None`` if the sample is unlabeled\n        \"\"\"\n        target = self.current_sample[1]\n        return fo.Classification(label=self.classes[int(target)])\n</pre> import fiftyone as fo import fiftyone.utils.data as foud   class PyTorchClassificationDatasetSampleParser(foud.LabeledImageSampleParser):     \"\"\"Parser for image classification samples loaded from a PyTorch dataset.      This parser can parse samples from a ``torch.utils.data.DataLoader`` that     emits ``(img_tensor, target)`` tuples, where::          - `img_tensor`: is a PyTorch Tensor containing the image         - `target`: the integer index of the target class      Args:         classes: the list of class label strings     \"\"\"      def __init__(self, classes):         self.classes = classes      @property     def has_image_path(self):         \"\"\"Whether this parser produces paths to images on disk for samples         that it parses.         \"\"\"         return False      @property     def has_image_metadata(self):         \"\"\"Whether this parser produces         :class:`fiftyone.core.metadata.ImageMetadata` instances for samples         that it parses.         \"\"\"         return False      @property     def label_cls(self):         \"\"\"The :class:`fiftyone.core.labels.Label` class(es) returned by this         parser.          This can be any of the following:          -   a :class:`fiftyone.core.labels.Label` class. In this case, the             parser is guaranteed to return labels of this type         -   a list or tuple of :class:`fiftyone.core.labels.Label` classes. In             this case, the parser can produce a single label field of any of             these types         -   a dict mapping keys to :class:`fiftyone.core.labels.Label` classes.             In this case, the parser will return label dictionaries with keys             and value-types specified by this dictionary. Not all keys need be             present in the imported labels         -   ``None``. In this case, the parser makes no guarantees about the             labels that it may return         \"\"\"         return fo.Classification      def get_image(self):         \"\"\"Returns the image from the current sample.          Returns:             a numpy image         \"\"\"         img_tensor = self.current_sample[0]         return img_tensor.cpu().numpy()      def get_label(self):         \"\"\"Returns the label for the current sample.          Returns:             a :class:`fiftyone.core.labels.Label` instance, or a dictionary             mapping field names to :class:`fiftyone.core.labels.Label`             instances, or ``None`` if the sample is unlabeled         \"\"\"         target = self.current_sample[1]         return fo.Classification(label=self.classes[int(target)]) <p>Note that <code>PyTorchClassificationDatasetSampleParser</code> specifies <code>has_image_path == False</code> and <code>has_image_metadata == False</code>, because the PyTorch dataset directly provides the in-memory image, not its path on disk.</p> <p>In order to use <code>PyTorchClassificationDatasetSampleParser</code>, we need a PyTorch Dataset from which to feed it samples.</p> <p>Let's use the CIFAR-10 dataset from the TorchVision Datasets library:</p> In\u00a0[2]: Copied! <pre>import torch\nimport torchvision\n\n\n# Downloads the test split of the CIFAR-10 dataset and prepares it for loading\n# in a DataLoader\ndataset = torchvision.datasets.CIFAR10(\n    \"/tmp/fiftyone/custom-parser/pytorch\",\n    train=False,\n    download=True,\n    transform=torchvision.transforms.ToTensor(),\n)\nclasses = dataset.classes\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n</pre> import torch import torchvision   # Downloads the test split of the CIFAR-10 dataset and prepares it for loading # in a DataLoader dataset = torchvision.datasets.CIFAR10(     \"/tmp/fiftyone/custom-parser/pytorch\",     train=False,     download=True,     transform=torchvision.transforms.ToTensor(), ) classes = dataset.classes data_loader = torch.utils.data.DataLoader(dataset, batch_size=1) <pre>Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /tmp/fiftyone/custom-parser/pytorch/cifar-10-python.tar.gz\n</pre> <pre>HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))</pre> <pre>Extracting /tmp/fiftyone/custom-parser/pytorch/cifar-10-python.tar.gz to /tmp/fiftyone/custom-parser/pytorch\n</pre> <p>Now we can load the samples into the dataset. Since our custom sample parser declares <code>has_image_path == False</code>, we must use the Dataset.ingest_labeled_images() method to load the samples into a FiftyOne dataset, which will write the individual images to disk as they are ingested so that FiftyOne can access them.</p> In\u00a0[3]: Copied! <pre>dataset = fo.Dataset(\"cifar10-samples\")\n\nsample_parser = PyTorchClassificationDatasetSampleParser(classes)\n\n# The directory to use to store the individual images on disk\ndataset_dir = \"/tmp/fiftyone/custom-parser/fiftyone\"\n\n# Ingest the samples from the data loader\ndataset.ingest_labeled_images(data_loader, sample_parser, dataset_dir=dataset_dir)\n\nprint(\"Loaded %d samples\" % len(dataset))\n</pre> dataset = fo.Dataset(\"cifar10-samples\")  sample_parser = PyTorchClassificationDatasetSampleParser(classes)  # The directory to use to store the individual images on disk dataset_dir = \"/tmp/fiftyone/custom-parser/fiftyone\"  # Ingest the samples from the data loader dataset.ingest_labeled_images(data_loader, sample_parser, dataset_dir=dataset_dir)  print(\"Loaded %d samples\" % len(dataset)) <pre> 100% |\u2588\u2588\u2588| 10000/10000 [6.7s elapsed, 0s remaining, 1.5K samples/s]      \nLoaded 10000 samples\n</pre> <p>Let's inspect the contents of the dataset to verify that the samples were loaded as expected:</p> In\u00a0[4]: Copied! <pre># Print summary information about the dataset\nprint(dataset)\n</pre> # Print summary information about the dataset print(dataset) <pre>Name:           cifar10-samples\nPersistent:     False\nNum samples:    10000\nTags:           []\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.StringField\n</pre> In\u00a0[5]: Copied! <pre># Print a few samples from the dataset\nprint(dataset.head())\n</pre> # Print a few samples from the dataset print(dataset.head()) <pre>&lt;Sample: {\n    'dataset_name': 'cifar10-samples',\n    'id': '5f15aeab6d4e59654468a14e',\n    'filepath': '/tmp/fiftyone/custom-parser/fiftyone/000001.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'ground_truth': 'cat',\n}&gt;\n&lt;Sample: {\n    'dataset_name': 'cifar10-samples',\n    'id': '5f15aeab6d4e59654468a14f',\n    'filepath': '/tmp/fiftyone/custom-parser/fiftyone/000002.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'ground_truth': 'ship',\n}&gt;\n&lt;Sample: {\n    'dataset_name': 'cifar10-samples',\n    'id': '5f15aeab6d4e59654468a150',\n    'filepath': '/tmp/fiftyone/custom-parser/fiftyone/000003.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'ground_truth': 'ship',\n}&gt;\n</pre> <p>We can also verify that the ingested images were written to disk as expected:</p> In\u00a0[27]: Copied! <pre>!ls -lah /tmp/fiftyone/custom-parser/fiftyone | head -n 10\n</pre> !ls -lah /tmp/fiftyone/custom-parser/fiftyone | head -n 10 <pre>total 0\r\ndrwxr-xr-x  10002 voxel51  wheel   313K Jul 20 10:34 .\r\ndrwxr-xr-x      4 voxel51  wheel   128B Jul 20 10:34 ..\r\n-rw-r--r--      1 voxel51  wheel     0B Jul 20 10:34 000001.jpg\r\n-rw-r--r--      1 voxel51  wheel     0B Jul 20 10:34 000002.jpg\r\n-rw-r--r--      1 voxel51  wheel     0B Jul 20 10:34 000003.jpg\r\n-rw-r--r--      1 voxel51  wheel     0B Jul 20 10:34 000004.jpg\r\n-rw-r--r--      1 voxel51  wheel     0B Jul 20 10:34 000005.jpg\r\n-rw-r--r--      1 voxel51  wheel     0B Jul 20 10:34 000006.jpg\r\n-rw-r--r--      1 voxel51  wheel     0B Jul 20 10:34 000007.jpg\r\n</pre> <p>If our <code>LabeledImageSampleParser</code> declared <code>has_image_path == True</code>, then we could use Dataset.add_labeled_images() to add samples to FiftyOne datasets without creating a copy of the source images on disk.</p> <p>However, our sample parser does not provide image paths, so an informative error message is raised if we try to use it in an unsupported way:</p> In\u00a0[6]: Copied! <pre>dataset = fo.Dataset()\n\nsample_parser = PyTorchClassificationDatasetSampleParser(classes)\n\n# Won't work because our SampleParser does not provide paths to its source images on disk\ndataset.add_labeled_images(data_loader, sample_parser)\n</pre> dataset = fo.Dataset()  sample_parser = PyTorchClassificationDatasetSampleParser(classes)  # Won't work because our SampleParser does not provide paths to its source images on disk dataset.add_labeled_images(data_loader, sample_parser) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-6-a3d739e371af&gt; in &lt;module&gt;\n      4 \n      5 # Won't work because our SampleParser does not provide paths to its source images on disk\n----&gt; 6 dataset.add_labeled_images(data_loader, sample_parser)\n\n~/dev/fiftyone/fiftyone/core/dataset.py in add_labeled_images(self, samples, sample_parser, label_field, tags, expand_schema)\n    729         if not sample_parser.has_image_path:\n    730             raise ValueError(\n--&gt; 731                 \"Sample parser must have `has_image_path == True` to add its \"\n    732                 \"samples to the dataset\"\n    733             )\n\nValueError: Sample parser must have `has_image_path == True` to add its samples to the dataset</pre> In\u00a0[7]: Copied! <pre>!rm -rf /tmp/fiftyone\n</pre> !rm -rf /tmp/fiftyone"},{"location":"how_do_i/recipes/custom_parser/#writing-custom-sample-parsers","title":"Writing Custom Sample Parsers\u00b6","text":"<p>This recipe demonstrates how to write a custom SampleParser and use it to add samples in your custom format to a FiftyOne Dataset.</p>"},{"location":"how_do_i/recipes/custom_parser/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"how_do_i/recipes/custom_parser/#writing-a-sampleparser","title":"Writing a SampleParser\u00b6","text":"<p>FiftyOne provides a SampleParser interface that defines how it parses provided samples when methods such as Dataset.add_labeled_images() and Dataset.ingest_labeled_images() are used.</p> <p><code>SampleParser</code> itself is an abstract interface; the concrete interface that you should implement is determined by the type of samples that you are importing. See writing a custom SampleParser for full details.</p> <p>In this recipe, we'll write a custom LabeledImageSampleParser that can parse labeled images from a PyTorch Dataset.</p>"},{"location":"how_do_i/recipes/custom_parser/#ingesting-samples-into-a-dataset","title":"Ingesting samples into a dataset\u00b6","text":""},{"location":"how_do_i/recipes/custom_parser/#adding-samples-to-a-dataset","title":"Adding samples to a dataset\u00b6","text":""},{"location":"how_do_i/recipes/custom_parser/#cleanup","title":"Cleanup\u00b6","text":"<p>You can cleanup the files generated by this recipe by running:</p>"},{"location":"how_do_i/recipes/draw_labels/","title":"Drawing Labels on Samples","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>In this recipe we'll use the FiftyOne Dataset Zoo to download some labeled datasets to use as sample data for drawing labels.</p> <p>Behind the scenes, FiftyOne uses either the TensorFlow Datasets or TorchVision Datasets libraries to wrangle the datasets, depending on which ML library you have installed.</p> <p>You can, for example, install PyTorch as follows (we'll also need <code>pycocotools</code> to load the COCO dataset, in particular):</p> In\u00a0[1]: Copied! <pre>!pip install torch torchvision\n!pip install pycocotools\n</pre> !pip install torch torchvision !pip install pycocotools In\u00a0[1]: Copied! <pre>!fiftyone zoo datasets download coco-2017 --splits validation\n</pre> !fiftyone zoo datasets download coco-2017 --splits validation <pre>Split 'validation' already downloaded\r\n</pre> <p>Now let's load the dataset, extract a DatasetView that contains 100 images from the dataset, and render them as annotated images with their ground truth labels overlaid:</p> In\u00a0[2]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.utils.annotations as foua\n\n# Directory to write the output annotations\nanno_dir = \"/tmp/fiftyone/draw_labels/coco-2017-validation-anno\"\n\n# Load the validation split of the COCO-2017 dataset\ndataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\")\n\n# Extract some samples\nview = dataset.limit(100)\n\n#\n# You can customize the look-and-feel of the annotations\n# For more information, see:\n# https://voxel51.com/docs/fiftyone/user_guide/draw_labels.html#customizing-label-rendering\n#\nconfig = foua.DrawConfig({\n    \"per_object_label_colors\": True\n})\n\n# Render the labels\nprint(\"Writing annotated images to '%s'\" % anno_dir)\nview.draw_labels(anno_dir, config=config)\nprint(\"Annotation complete\")\n</pre> import fiftyone as fo import fiftyone.zoo as foz import fiftyone.utils.annotations as foua  # Directory to write the output annotations anno_dir = \"/tmp/fiftyone/draw_labels/coco-2017-validation-anno\"  # Load the validation split of the COCO-2017 dataset dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\")  # Extract some samples view = dataset.limit(100)  # # You can customize the look-and-feel of the annotations # For more information, see: # https://voxel51.com/docs/fiftyone/user_guide/draw_labels.html#customizing-label-rendering # config = foua.DrawConfig({     \"per_object_label_colors\": True })  # Render the labels print(\"Writing annotated images to '%s'\" % anno_dir) view.draw_labels(anno_dir, config=config) print(\"Annotation complete\") <pre>Split 'validation' already downloaded\nLoading 'coco-2017' split 'validation'\n 100% |\u2588\u2588\u2588\u2588\u2588| 5000/5000 [14.8s elapsed, 0s remaining, 339.4 samples/s]      \nWriting annotated images to '/tmp/fiftyone/draw_labels/coco-2017-validation-anno'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [7.3s elapsed, 0s remaining, 11.9 samples/s]        \nAnnotation complete\n</pre> <p>Let's list the output directory to verify that the annotations have been generated:</p> In\u00a0[3]: Copied! <pre>!ls -lah /tmp/fiftyone/draw_labels/coco-2017-validation-anno | head\n</pre> !ls -lah /tmp/fiftyone/draw_labels/coco-2017-validation-anno | head <pre>total 51976\r\ndrwxr-xr-x  202 Brian  wheel   6.3K Jul 27 18:36 .\r\ndrwxr-xr-x    5 Brian  wheel   160B Jul 27 15:59 ..\r\n-rw-r--r--    1 Brian  wheel   115K Jul 27 18:36 000001-2.jpg\r\n-rw-r--r--@   1 Brian  wheel   116K Jul 27 12:51 000001.jpg\r\n-rw-r--r--    1 Brian  wheel   243K Jul 27 18:36 000002-2.jpg\r\n-rw-r--r--    1 Brian  wheel   243K Jul 27 12:51 000002.jpg\r\n-rw-r--r--    1 Brian  wheel   177K Jul 27 18:36 000003-2.jpg\r\n-rw-r--r--@   1 Brian  wheel   177K Jul 27 12:51 000003.jpg\r\n-rw-r--r--    1 Brian  wheel   101K Jul 27 18:36 000004-2.jpg\r\n</pre> <p>Here's an example of an annotated image that was generated:</p> <p></p> In\u00a0[4]: Copied! <pre>!fiftyone zoo datasets download caltech101 --splits test\n</pre> !fiftyone zoo datasets download caltech101 --splits test <pre>Split 'test' already downloaded\n</pre> <p>Now let's load the dataset, extract a DatasetView that contains 100 images from the dataset, and render them as annotated images with their ground truth labels overlaid:</p> In\u00a0[5]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.utils.annotations as foua\n\n# Directory to write the output annotations\nanno_dir = \"/tmp/fiftyone/draw_labels/caltech101-test-anno\"\n\n# Load the test split of the Caltech 101 dataset\ndataset = foz.load_zoo_dataset(\"caltech101\", split=\"test\")\n\n# Extract some samples\nview = dataset.limit(100)\n\n#\n# You can customize the look-and-feel of the annotations\n# For more information, see:\n# https://voxel51.com/docs/fiftyone/user_guide/draw_labels.html#customizing-label-rendering\n#\nconfig = foua.DrawConfig({\n    \"font_size\": 36\n})\n\n# Render the labels\nprint(\"Writing annotated images to '%s'\" % anno_dir)\nview.draw_labels(anno_dir, config=config)\nprint(\"Annotation complete\")\n</pre> import fiftyone as fo import fiftyone.zoo as foz import fiftyone.utils.annotations as foua  # Directory to write the output annotations anno_dir = \"/tmp/fiftyone/draw_labels/caltech101-test-anno\"  # Load the test split of the Caltech 101 dataset dataset = foz.load_zoo_dataset(\"caltech101\", split=\"test\")  # Extract some samples view = dataset.limit(100)  # # You can customize the look-and-feel of the annotations # For more information, see: # https://voxel51.com/docs/fiftyone/user_guide/draw_labels.html#customizing-label-rendering # config = foua.DrawConfig({     \"font_size\": 36 })  # Render the labels print(\"Writing annotated images to '%s'\" % anno_dir) view.draw_labels(anno_dir, config=config) print(\"Annotation complete\") <pre>Split 'test' already downloaded\nLoading 'caltech101' split 'test'\n 100% |\u2588\u2588\u2588\u2588\u2588| 9145/9145 [4.8s elapsed, 0s remaining, 1.9K samples/s]      \nWriting annotated images to '/tmp/fiftyone/draw_labels/caltech101-test-anno'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [2.6s elapsed, 0s remaining, 37.4 samples/s]        \nAnnotation complete\n</pre> <p>Let's list the output directory to verify that the annotations have been generated:</p> In\u00a0[6]: Copied! <pre>!ls -lah /tmp/fiftyone/draw_labels/caltech101-test-anno | head\n</pre> !ls -lah /tmp/fiftyone/draw_labels/caltech101-test-anno | head <pre>total 17456\r\ndrwxr-xr-x  182 Brian  wheel   5.7K Jul 27 18:37 .\r\ndrwxr-xr-x    5 Brian  wheel   160B Jul 27 15:59 ..\r\n-rw-r--r--@   1 Brian  wheel    13K Jul 27 18:37 image_0001-2.jpg\r\n-rw-r--r--    1 Brian  wheel    41K Jul 27 15:59 image_0001.jpg\r\n-rw-r--r--    1 Brian  wheel   197K Jul 27 18:37 image_0002.jpg\r\n-rw-r--r--    1 Brian  wheel   5.9K Jul 27 18:37 image_0003.jpg\r\n-rw-r--r--    1 Brian  wheel    19K Jul 27 18:37 image_0004-2.jpg\r\n-rw-r--r--    1 Brian  wheel    33K Jul 27 15:59 image_0004.jpg\r\n-rw-r--r--    1 Brian  wheel    18K Jul 27 18:37 image_0005-2.jpg\r\n</pre> <p>Here's an example of an annotated image that was generated:</p> In\u00a0[2]: Copied! <pre>import fiftyone.zoo as foz\n\n# Load a small video dataset\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\nprint(dataset)\n</pre> import fiftyone.zoo as foz  # Load a small video dataset dataset = foz.load_zoo_dataset(\"quickstart-video\")  print(dataset) <pre>Dataset already downloaded\nLoading 'quickstart-video'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [15.9s elapsed, 0s remaining, 0.6 samples/s]     \nName:           quickstart-video\nMedia type      video\nNum samples:    10\nPersistent:     False\nInfo:           {'description': 'quickstart-video'}\nTags:           []\nSample fields:\n    media_type: fiftyone.core.fields.StringField\n    filepath:   fiftyone.core.fields.StringField\n    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    frames:     fiftyone.core.fields.FramesField\nFrame fields:\n    frame_number: fiftyone.core.fields.FrameNumberField\n    objs:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n</pre> <p>Note that the dataset contains frame-level detections in the <code>objs</code> field of each frame.</p> <p>Let's make a DatasetView that contains a couple random videos from the dataset and render them as annotated videos with the frame-level detections overlaid:</p> In\u00a0[3]: Copied! <pre>import fiftyone.utils.annotations as foua\n\n# Directory to write the output annotations\nanno_dir = \"/tmp/fiftyone/draw_labels/quickstart-video-anno\"\n\n# Extract two random samples\nview = dataset.take(2)\n\n#\n# You can customize the look-and-feel of the annotations\n# For more information, see:\n# https://voxel51.com/docs/fiftyone/user_guide/draw_labels.html#customizing-label-rendering\n#\nconfig = foua.DrawConfig({\n    \"per_object_label_colors\": True\n})\n\n# Render the labels\nprint(\"Writing annotated videos to '%s'\" % anno_dir)\nview.draw_labels(anno_dir, config=config)\nprint(\"Annotation complete\")\n</pre> import fiftyone.utils.annotations as foua  # Directory to write the output annotations anno_dir = \"/tmp/fiftyone/draw_labels/quickstart-video-anno\"  # Extract two random samples view = dataset.take(2)  # # You can customize the look-and-feel of the annotations # For more information, see: # https://voxel51.com/docs/fiftyone/user_guide/draw_labels.html#customizing-label-rendering # config = foua.DrawConfig({     \"per_object_label_colors\": True })  # Render the labels print(\"Writing annotated videos to '%s'\" % anno_dir) view.draw_labels(anno_dir, config=config) print(\"Annotation complete\") <pre>Writing annotated videos to '/tmp/fiftyone/draw_labels/quickstart-video-anno'\nRendering video 1/2: '/tmp/fiftyone/draw_labels/quickstart-video-anno/0587e1cfc2344523922652d8b227fba4-000014-video_052.mp4'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 120/120 [19.0s elapsed, 0s remaining, 6.7 frames/s]      \nRendering video 2/2: '/tmp/fiftyone/draw_labels/quickstart-video-anno/0587e1cfc2344523922652d8b227fba4-000014-video_164.mp4'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 120/120 [27.2s elapsed, 0s remaining, 4.3 frames/s]      \nAnnotation complete\n</pre> <p>Let's list the output directory to verify that the annotations have been generated:</p> In\u00a0[4]: Copied! <pre>!ls -lah /tmp/fiftyone/draw_labels/quickstart-video-anno\n</pre> !ls -lah /tmp/fiftyone/draw_labels/quickstart-video-anno <pre>total 34832\r\ndrwxr-xr-x  4 Brian  wheel   128B Oct  7 23:57 .\r\ndrwxr-xr-x  3 Brian  wheel    96B Oct  7 23:57 ..\r\n-rw-r--r--  1 Brian  wheel   7.5M Oct  7 23:57 0587e1cfc2344523922652d8b227fba4-000014-video_052.mp4\r\n-rw-r--r--  1 Brian  wheel   8.5M Oct  7 23:58 0587e1cfc2344523922652d8b227fba4-000014-video_164.mp4\r\n</pre> <p>Here's a snippet of an annotated video that was generated:</p> <p></p> In\u00a0[7]: Copied! <pre>!rm -rf /tmp/fiftyone\n</pre> !rm -rf /tmp/fiftyone"},{"location":"how_do_i/recipes/draw_labels/#drawing-labels-on-samples","title":"Drawing Labels on Samples\u00b6","text":"<p>This recipe demonstrates how to use FiftyOne to render annotated versions of image and video samples with their label field(s) overlaid.</p>"},{"location":"how_do_i/recipes/draw_labels/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"how_do_i/recipes/draw_labels/#drawing-coco-detections","title":"Drawing COCO detections\u00b6","text":"<p>You can download the validation split of the COCO-2017 dataset to <code>~/fiftyone/coco-2017/validation</code> by running the following command:</p>"},{"location":"how_do_i/recipes/draw_labels/#drawing-caltech-101-classifications","title":"Drawing Caltech 101 classifications\u00b6","text":"<p>You can download the test split of the Caltech 101 dataset to <code>~/fiftyone/caltech101/test</code> by running the following command:</p>"},{"location":"how_do_i/recipes/draw_labels/#drawing-labels-on-videos","title":"Drawing labels on videos\u00b6","text":"<p>FiftyOne can also render frame labels onto video samples.</p> <p>To demonstrate, let's work with the (small) video quickstart dataset from the zoo:</p>"},{"location":"how_do_i/recipes/draw_labels/#cleanup","title":"Cleanup\u00b6","text":"<p>You can cleanup the files generated by this recipe by running the command below:</p>"},{"location":"how_do_i/recipes/image_deduplication/","title":"Image Deduplication with FiftyOne","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>This notebook also requires the <code>tensorflow</code> package:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install tensorflow\n</pre> !pip install tensorflow In\u00a0[1]: Copied! <pre>from image_deduplication_helpers import download_dataset\n\ndownload_dataset()\n</pre> from image_deduplication_helpers import download_dataset  download_dataset() <pre>Downloading dataset of 1000 samples to:\n\t/tmp/fiftyone/cifar100_with_duplicates\nand corrupting the data (5% duplicates)\nDownload successful\n</pre> <p>The above script uses <code>tensorflow.keras.datasets</code> to download the dataset, so you must have TensorFlow installed.</p> <p>The dataset is organized on disk as follows:</p> <pre><code>/tmp/fiftyone/\n\u2514\u2500\u2500 cifar100_with_duplicates/\n    \u251c\u2500\u2500 &lt;classA&gt;/\n    \u2502   \u251c\u2500\u2500 &lt;image1&gt;.jpg\n    \u2502   \u251c\u2500\u2500 &lt;image2&gt;.jpg\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 &lt;classB&gt;/\n    \u2502   \u251c\u2500\u2500 &lt;image1&gt;.jpg\n    \u2502   \u251c\u2500\u2500 &lt;image2&gt;.jpg\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 ...\n</code></pre> <p>As we will soon come to discover, some of these samples are duplicates and we have no clue which they are!</p> In\u00a0[2]: Copied! <pre>import fiftyone as fo\n</pre> import fiftyone as fo <p>Let's use a utililty method provided by FiftyOne to load the image classification dataset from disk:</p> In\u00a0[3]: Copied! <pre>import os\n\ndataset_name = \"cifar100_with_duplicates\"\ndataset_dir = os.path.join(\"/tmp/fiftyone\", dataset_name)\n\ndataset = fo.Dataset.from_dir(\n    dataset_dir,\n    fo.types.ImageClassificationDirectoryTree,\n    name=dataset_name\n)\n</pre> import os  dataset_name = \"cifar100_with_duplicates\" dataset_dir = os.path.join(\"/tmp/fiftyone\", dataset_name)  dataset = fo.Dataset.from_dir(     dataset_dir,     fo.types.ImageClassificationDirectoryTree,     name=dataset_name ) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [1.2s elapsed, 0s remaining, 718.5 samples/s]         \n</pre> In\u00a0[4]: Copied! <pre># Print summary information about the dataset\nprint(dataset)\n</pre> # Print summary information about the dataset print(dataset) <pre>Name:           cifar100_with_duplicates\nMedia type:     image\nNum samples:    1000\nPersistent:     False\nInfo:           {'classes': ['apple', 'aquarium_fish', 'baby', ...]}\nTags:           []\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n</pre> In\u00a0[5]: Copied! <pre># Print a sample\nprint(dataset.first())\n</pre> # Print a sample print(dataset.first()) <pre>&lt;Sample: {\n    'id': '5ff8dc665b5b9368e094de5a',\n    'media_type': 'image',\n    'filepath': '/tmp/fiftyone/cifar100_with_duplicates/apple/113.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '5ff8dc665b5b9368e094de59',\n        'label': 'apple',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n}&gt;\n</pre> <p>Create a view that contains only samples whose ground truth label is <code>mountain</code>:</p> In\u00a0[6]: Copied! <pre># Used to write view expressions that involve sample fields\nfrom fiftyone import ViewField as F\n\nview = dataset.match(F(\"ground_truth.label\") == \"mountain\")\n\n# Print summary information about the view\nprint(view)\n</pre> # Used to write view expressions that involve sample fields from fiftyone import ViewField as F  view = dataset.match(F(\"ground_truth.label\") == \"mountain\")  # Print summary information about the view print(view) <pre>Dataset:        cifar100_with_duplicates\nMedia type:     image\nNum samples:    8\nTags:           []\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\nView stages:\n    1. Match(filter={'$expr': {'$eq': [...]}})\n</pre> In\u00a0[7]: Copied! <pre># Print the first sample in the view\nprint(view.first())\n</pre> # Print the first sample in the view print(view.first()) <pre>&lt;SampleView: {\n    'id': '5ff8dc675b5b9368e094e436',\n    'media_type': 'image',\n    'filepath': '/tmp/fiftyone/cifar100_with_duplicates/mountain/0.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '5ff8dc675b5b9368e094e435',\n        'label': 'mountain',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n}&gt;\n</pre> <p>Create a view with samples sorted by their ground truth labels in reverse alphabetical order:</p> In\u00a0[8]: Copied! <pre>view = dataset.sort_by(\"ground_truth\", reverse=True)\n\n# Print summary information about the view\nprint(view)\n</pre> view = dataset.sort_by(\"ground_truth\", reverse=True)  # Print summary information about the view print(view) <pre>Dataset:        cifar100_with_duplicates\nMedia type:     image\nNum samples:    1000\nTags:           []\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\nView stages:\n    1. SortBy(field_or_expr='ground_truth', reverse=True)\n</pre> In\u00a0[9]: Copied! <pre># Print the first sample in the view\nprint(view.first())\n</pre> # Print the first sample in the view print(view.first()) <pre>&lt;SampleView: {\n    'id': '5ff8dc685b5b9368e094ea0f',\n    'media_type': 'image',\n    'filepath': '/tmp/fiftyone/cifar100_with_duplicates/worm/905.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '5ff8dc685b5b9368e094ea0e',\n        'label': 'worm',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n}&gt;\n</pre> In\u00a0[10]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) <p>Narrow your scope to 10 random samples:</p> In\u00a0[11]: Copied! <pre>session.view = dataset.take(10)\n</pre> session.view = dataset.take(10) <p>Click on some some samples in the App to select them and access their IDs from code!</p> In\u00a0[12]: Copied! <pre># Get the IDs of the currently selected samples in the App\nsample_ids = session.selected\n</pre> # Get the IDs of the currently selected samples in the App sample_ids = session.selected <p>Create a view that contains your currently selected samples:</p> In\u00a0[13]: Copied! <pre>selected_view = dataset.select(session.selected)\n</pre> selected_view = dataset.select(session.selected) <p>Update the App to only show your selected samples:</p> In\u00a0[14]: Copied! <pre>session.view = selected_view\n</pre> session.view = selected_view In\u00a0[15]: Copied! <pre>import fiftyone.core.utils as fou\n\nfor sample in dataset:\n    sample[\"file_hash\"] = fou.compute_filehash(sample.filepath)\n    sample.save()\n\nprint(dataset)\n</pre> import fiftyone.core.utils as fou  for sample in dataset:     sample[\"file_hash\"] = fou.compute_filehash(sample.filepath)     sample.save()  print(dataset) <pre>Name:           cifar100_with_duplicates\nMedia type:     image\nNum samples:    1000\nPersistent:     False\nInfo:           {'classes': ['apple', 'aquarium_fish', 'baby', ...]}\nTags:           []\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n    file_hash:    fiftyone.core.fields.IntField\n</pre> <p>We have two ways to visualize this new information.</p> <p>First, you can view the sample from your Terminal:</p> In\u00a0[16]: Copied! <pre>sample = dataset.first()\nprint(sample)\n</pre> sample = dataset.first() print(sample) <pre>&lt;Sample: {\n    'id': '5ff8dc665b5b9368e094de5a',\n    'media_type': 'image',\n    'filepath': '/tmp/fiftyone/cifar100_with_duplicates/apple/113.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '5ff8dc665b5b9368e094de59',\n        'label': 'apple',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n    'file_hash': -6346202338154836458,\n}&gt;\n</pre> <p>Or you can refresh the App and toggle on the new <code>file_hash</code> field:</p> In\u00a0[17]: Copied! <pre>session.dataset = dataset\n</pre> session.dataset = dataset In\u00a0[18]: Copied! <pre>from collections import Counter\n\nfilehash_counts = Counter(sample.file_hash for sample in dataset)\ndup_filehashes = [k for k, v in filehash_counts.items() if v &gt; 1]\n\nprint(\"Number of duplicate file hashes: %d\" % len(dup_filehashes))\n</pre> from collections import Counter  filehash_counts = Counter(sample.file_hash for sample in dataset) dup_filehashes = [k for k, v in filehash_counts.items() if v &gt; 1]  print(\"Number of duplicate file hashes: %d\" % len(dup_filehashes)) <pre>Number of duplicate file hashes: 36\n</pre> <p>Now let's create a view that contains only the samples with these duplicate file hashes:</p> In\u00a0[19]: Copied! <pre>dup_view = (dataset\n    # Extract samples with duplicate file hashes\n    .match(F(\"file_hash\").is_in(dup_filehashes))\n    # Sort by file hash so duplicates will be adjacent\n    .sort_by(\"file_hash\")\n)\n\nprint(\"Number of images that have a duplicate: %d\" % len(dup_view))\nprint(\"Number of duplicates: %d\" % (len(dup_view) - len(dup_filehashes)))\n</pre> dup_view = (dataset     # Extract samples with duplicate file hashes     .match(F(\"file_hash\").is_in(dup_filehashes))     # Sort by file hash so duplicates will be adjacent     .sort_by(\"file_hash\") )  print(\"Number of images that have a duplicate: %d\" % len(dup_view)) print(\"Number of duplicates: %d\" % (len(dup_view) - len(dup_filehashes))) <pre>Number of images that have a duplicate: 72\nNumber of duplicates: 36\n</pre> <p>Of course, we can always use the App to visualize our work!</p> In\u00a0[20]: Copied! <pre>session.view = dup_view\n</pre> session.view = dup_view In\u00a0[21]: Copied! <pre>print(\"Length of dataset before: %d\" % len(dataset))\n\n_dup_filehashes = set()\nfor sample in dup_view:\n    if sample.file_hash not in _dup_filehashes:\n        _dup_filehashes.add(sample.file_hash)\n        continue\n\n    del dataset[sample.id]\n\nprint(\"Length of dataset after: %d\" % len(dataset))\n\n# Verify that the dataset no longer contains any duplicates\nprint(\"Number of unique file hashes: %d\" % len({s.file_hash for s in dataset}))\n</pre> print(\"Length of dataset before: %d\" % len(dataset))  _dup_filehashes = set() for sample in dup_view:     if sample.file_hash not in _dup_filehashes:         _dup_filehashes.add(sample.file_hash)         continue      del dataset[sample.id]  print(\"Length of dataset after: %d\" % len(dataset))  # Verify that the dataset no longer contains any duplicates print(\"Number of unique file hashes: %d\" % len({s.file_hash for s in dataset})) <pre>Length of dataset before: 1000\nLength of dataset after: 964\nNumber of unique file hashes: 964\n</pre> In\u00a0[22]: Copied! <pre>EXPORT_DIR = \"/tmp/fiftyone/image-deduplication\"\ndataset.export\n\ndataset.export(label_field=\"ground_truth\", export_dir=EXPORT_DIR, dataset_type=fo.types.FiftyOneImageClassificationDataset,)\n</pre> EXPORT_DIR = \"/tmp/fiftyone/image-deduplication\" dataset.export  dataset.export(label_field=\"ground_truth\", export_dir=EXPORT_DIR, dataset_type=fo.types.FiftyOneImageClassificationDataset,) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 964/964 [408.8ms elapsed, 0s remaining, 2.4K samples/s]      \n</pre> <p>Check out the contents of <code>/tmp/fiftyone/image-deduplication</code> on disk to see how the data is organized.</p> <p>You can load the deduplicated dataset that you exported back into FiftyOne at any time as follows:</p> In\u00a0[23]: Copied! <pre>no_dups_dataset = fo.Dataset.from_dir(\n    EXPORT_DIR,\n    fo.types.FiftyOneImageClassificationDataset,\n    name=\"no_duplicates\",\n)\n\nprint(no_dups_dataset)\n</pre> no_dups_dataset = fo.Dataset.from_dir(     EXPORT_DIR,     fo.types.FiftyOneImageClassificationDataset,     name=\"no_duplicates\", )  print(no_dups_dataset) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 964/964 [1.1s elapsed, 0s remaining, 838.4 samples/s]         \nName:           no_duplicates\nMedia type:     image\nNum samples:    964\nPersistent:     False\nInfo:           {'classes': ['apple', 'aquarium_fish', 'baby', ...]}\nTags:           []\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n</pre> In\u00a0[28]: Copied! <pre>!rm -rf /tmp/fiftyone\n</pre> !rm -rf /tmp/fiftyone In\u00a0[24]: Copied! <pre>session.freeze() # screenshot the active App for sharing\n</pre> session.freeze() # screenshot the active App for sharing"},{"location":"how_do_i/recipes/image_deduplication/#image-deduplication-with-fiftyone","title":"Image Deduplication with FiftyOne\u00b6","text":"<p>This recipe demonstrates a simple use case of using FiftyOne to detect and remove duplicate images from your dataset.</p>"},{"location":"how_do_i/recipes/image_deduplication/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"how_do_i/recipes/image_deduplication/#download-the-data","title":"Download the data\u00b6","text":"<p>First we download the dataset to disk. The dataset is a 1000 sample subset of CIFAR-100, a dataset of 32x32 pixel images with one of 100 different classification labels such as <code>apple</code>, <code>bicycle</code>, <code>porcupine</code>, etc. You can use this helper script.</p>"},{"location":"how_do_i/recipes/image_deduplication/#create-a-dataset","title":"Create a dataset\u00b6","text":"<p>Let's start by importing the FiftyOne library:</p>"},{"location":"how_do_i/recipes/image_deduplication/#explore-the-dataset","title":"Explore the dataset\u00b6","text":"<p>We can poke around in the dataset:</p>"},{"location":"how_do_i/recipes/image_deduplication/#visualize-the-dataset","title":"Visualize the dataset\u00b6","text":"<p>Start browsing the dataset:</p>"},{"location":"how_do_i/recipes/image_deduplication/#compute-file-hashes","title":"Compute file hashes\u00b6","text":"<p>Iterate over the samples and compute their file hashes:</p>"},{"location":"how_do_i/recipes/image_deduplication/#check-for-duplicates","title":"Check for duplicates\u00b6","text":"<p>Now let's use a simple Python statement to locate the duplicate files in the dataset, i.e., those with the same file hashses:</p>"},{"location":"how_do_i/recipes/image_deduplication/#delete-duplicates","title":"Delete duplicates\u00b6","text":"<p>Now let's delete the duplicate samples from the dataset using our <code>dup_view</code> to restrict our attention to known duplicates:</p>"},{"location":"how_do_i/recipes/image_deduplication/#export-the-deduplicated-dataset","title":"Export the deduplicated dataset\u00b6","text":"<p>Finally, let's export a fresh copy of our now-duplicate-free dataset:</p>"},{"location":"how_do_i/recipes/image_deduplication/#cleanup","title":"Cleanup\u00b6","text":"<p>You can cleanup the files generated by this recipe by running:</p>"},{"location":"how_do_i/recipes/image_deduplication_helpers/","title":"Image deduplication helpers","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nDownloads a subset of CIFAR-100 and stores it to disk as follows::\n\n    /tmp/fiftyone/\n    \u2514\u2500\u2500 cifar100_with_duplicates/\n        \u251c\u2500\u2500 &lt;classA&gt;/\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 &lt;image1&gt;.jpg\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 &lt;image2&gt;.jpg\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n        \u251c\u2500\u2500 &lt;classB&gt;/\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 &lt;image1&gt;.jpg\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 &lt;image2&gt;.jpg\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n        \u2514\u2500\u2500 ...\n\nA random 5% of the samples are duplicates, instead of the original samples.\n\n| Copyright 2017-2024, Voxel51, Inc.\n| `voxel51.com &lt;https://voxel51.com/&gt;`_\n|\n\"\"\"\nimport os\nimport random\n</pre> \"\"\" Downloads a subset of CIFAR-100 and stores it to disk as follows::      /tmp/fiftyone/     \u2514\u2500\u2500 cifar100_with_duplicates/         \u251c\u2500\u2500 /         \u2502\u00a0\u00a0 \u251c\u2500\u2500 .jpg         \u2502\u00a0\u00a0 \u251c\u2500\u2500 .jpg         \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...         \u251c\u2500\u2500 /         \u2502\u00a0\u00a0 \u251c\u2500\u2500 .jpg         \u2502\u00a0\u00a0 \u251c\u2500\u2500 .jpg         \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...         \u2514\u2500\u2500 ...  A random 5% of the samples are duplicates, instead of the original samples.  | Copyright 2017-2024, Voxel51, Inc. | `voxel51.com `_ | \"\"\" import os import random In\u00a0[\u00a0]: Copied! <pre>import eta.core.utils as etau\n</pre> import eta.core.utils as etau In\u00a0[\u00a0]: Copied! <pre>import fiftyone.core.utils as fou\nimport fiftyone.utils.image as foui\n</pre> import fiftyone.core.utils as fou import fiftyone.utils.image as foui In\u00a0[\u00a0]: Copied! <pre>fou.ensure_tf()\nfrom tensorflow.keras.datasets import cifar100  # pylint: disable=import-error\n</pre> fou.ensure_tf() from tensorflow.keras.datasets import cifar100  # pylint: disable=import-error In\u00a0[\u00a0]: Copied! <pre>DATASET_SIZE = 1000\nDATASET_DIR = os.path.join(\"/tmp/fiftyone/cifar100_with_duplicates\")\nCORRUPTION_RATE = 0.05\n</pre> DATASET_SIZE = 1000 DATASET_DIR = os.path.join(\"/tmp/fiftyone/cifar100_with_duplicates\") CORRUPTION_RATE = 0.05 In\u00a0[\u00a0]: Copied! <pre>FINE_CLASSES = [\n    \"apple\",\n    \"aquarium_fish\",\n    \"baby\",\n    \"bear\",\n    \"beaver\",\n    \"bed\",\n    \"bee\",\n    \"beetle\",\n    \"bicycle\",\n    \"bottle\",\n    \"bowl\",\n    \"boy\",\n    \"bridge\",\n    \"bus\",\n    \"butterfly\",\n    \"camel\",\n    \"can\",\n    \"castle\",\n    \"caterpillar\",\n    \"cattle\",\n    \"chair\",\n    \"chimpanzee\",\n    \"clock\",\n    \"cloud\",\n    \"cockroach\",\n    \"couch\",\n    \"crab\",\n    \"crocodile\",\n    \"cup\",\n    \"dinosaur\",\n    \"dolphin\",\n    \"elephant\",\n    \"flatfish\",\n    \"forest\",\n    \"fox\",\n    \"girl\",\n    \"hamster\",\n    \"house\",\n    \"kangaroo\",\n    \"computer_keyboard\",\n    \"lamp\",\n    \"lawn_mower\",\n    \"leopard\",\n    \"lion\",\n    \"lizard\",\n    \"lobster\",\n    \"man\",\n    \"maple_tree\",\n    \"motorcycle\",\n    \"mountain\",\n    \"mouse\",\n    \"mushroom\",\n    \"oak_tree\",\n    \"orange\",\n    \"orchid\",\n    \"otter\",\n    \"palm_tree\",\n    \"pear\",\n    \"pickup_truck\",\n    \"pine_tree\",\n    \"plain\",\n    \"plate\",\n    \"poppy\",\n    \"porcupine\",\n    \"possum\",\n    \"rabbit\",\n    \"raccoon\",\n    \"ray\",\n    \"road\",\n    \"rocket\",\n    \"rose\",\n    \"sea\",\n    \"seal\",\n    \"shark\",\n    \"shrew\",\n    \"skunk\",\n    \"skyscraper\",\n    \"snail\",\n    \"snake\",\n    \"spider\",\n    \"squirrel\",\n    \"streetcar\",\n    \"sunflower\",\n    \"sweet_pepper\",\n    \"table\",\n    \"tank\",\n    \"telephone\",\n    \"television\",\n    \"tiger\",\n    \"tractor\",\n    \"train\",\n    \"trout\",\n    \"tulip\",\n    \"turtle\",\n    \"wardrobe\",\n    \"whale\",\n    \"willow_tree\",\n    \"wolf\",\n    \"woman\",\n    \"worm\",\n]\n</pre> FINE_CLASSES = [     \"apple\",     \"aquarium_fish\",     \"baby\",     \"bear\",     \"beaver\",     \"bed\",     \"bee\",     \"beetle\",     \"bicycle\",     \"bottle\",     \"bowl\",     \"boy\",     \"bridge\",     \"bus\",     \"butterfly\",     \"camel\",     \"can\",     \"castle\",     \"caterpillar\",     \"cattle\",     \"chair\",     \"chimpanzee\",     \"clock\",     \"cloud\",     \"cockroach\",     \"couch\",     \"crab\",     \"crocodile\",     \"cup\",     \"dinosaur\",     \"dolphin\",     \"elephant\",     \"flatfish\",     \"forest\",     \"fox\",     \"girl\",     \"hamster\",     \"house\",     \"kangaroo\",     \"computer_keyboard\",     \"lamp\",     \"lawn_mower\",     \"leopard\",     \"lion\",     \"lizard\",     \"lobster\",     \"man\",     \"maple_tree\",     \"motorcycle\",     \"mountain\",     \"mouse\",     \"mushroom\",     \"oak_tree\",     \"orange\",     \"orchid\",     \"otter\",     \"palm_tree\",     \"pear\",     \"pickup_truck\",     \"pine_tree\",     \"plain\",     \"plate\",     \"poppy\",     \"porcupine\",     \"possum\",     \"rabbit\",     \"raccoon\",     \"ray\",     \"road\",     \"rocket\",     \"rose\",     \"sea\",     \"seal\",     \"shark\",     \"shrew\",     \"skunk\",     \"skyscraper\",     \"snail\",     \"snake\",     \"spider\",     \"squirrel\",     \"streetcar\",     \"sunflower\",     \"sweet_pepper\",     \"table\",     \"tank\",     \"telephone\",     \"television\",     \"tiger\",     \"tractor\",     \"train\",     \"trout\",     \"tulip\",     \"turtle\",     \"wardrobe\",     \"whale\",     \"willow_tree\",     \"wolf\",     \"woman\",     \"worm\", ] In\u00a0[\u00a0]: Copied! <pre>def download_dataset():\n    print(\n        \"Downloading dataset of %d samples to:\\n\\t%s\"\n        % (DATASET_SIZE, DATASET_DIR)\n    )\n    print(\n        \"and corrupting the data (%d%% duplicates)\" % (100 * CORRUPTION_RATE)\n    )\n\n    # if not empty, delete current contents\n    etau.ensure_empty_dir(DATASET_DIR, cleanup=True)\n\n    (_, _), (x_test, y_test) = cifar100.load_data(label_mode=\"fine\")\n\n    dataset_size = min(DATASET_SIZE, 10000)\n\n    x = x_test[:dataset_size, :]\n    y = y_test[:dataset_size, :]\n\n    for i in range(x.shape[0]):\n        if random.random() &gt; 0.95:\n            # pick a random sample 5% of the time\n            idx = random.randint(0, x.shape[0])\n        else:\n            idx = i\n\n        # get label\n        fine_label = FINE_CLASSES[y[idx, 0]]\n\n        # read image\n        img = x[idx, :]\n\n        rel_img_path = os.path.join(fine_label, \"%d.jpg\" % i)\n        abs_img_path = os.path.join(DATASET_DIR, rel_img_path)\n\n        foui.write(img, abs_img_path)\n\n    print(\"Download successful\")\n</pre> def download_dataset():     print(         \"Downloading dataset of %d samples to:\\n\\t%s\"         % (DATASET_SIZE, DATASET_DIR)     )     print(         \"and corrupting the data (%d%% duplicates)\" % (100 * CORRUPTION_RATE)     )      # if not empty, delete current contents     etau.ensure_empty_dir(DATASET_DIR, cleanup=True)      (_, _), (x_test, y_test) = cifar100.load_data(label_mode=\"fine\")      dataset_size = min(DATASET_SIZE, 10000)      x = x_test[:dataset_size, :]     y = y_test[:dataset_size, :]      for i in range(x.shape[0]):         if random.random() &gt; 0.95:             # pick a random sample 5% of the time             idx = random.randint(0, x.shape[0])         else:             idx = i          # get label         fine_label = FINE_CLASSES[y[idx, 0]]          # read image         img = x[idx, :]          rel_img_path = os.path.join(fine_label, \"%d.jpg\" % i)         abs_img_path = os.path.join(DATASET_DIR, rel_img_path)          foui.write(img, abs_img_path)      print(\"Download successful\") In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    download_dataset()\n</pre> if __name__ == \"__main__\":     download_dataset()"},{"location":"how_do_i/recipes/merge_datasets/","title":"Merging Datasets","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>In this recipe, we'll work with a dataset downloaded from the FiftyOne Dataset Zoo.</p> <p>To access the dataset, install <code>torch</code> and <code>torchvision</code>, if necessary:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install torch torchvision\n</pre> !pip install torch torchvision <p>Then download the test split of CIFAR-10:</p> In\u00a0[1]: Copied! <pre># Download the validation split of COCO-2017\n!fiftyone zoo datasets download cifar10 --splits test\n</pre> # Download the validation split of COCO-2017 !fiftyone zoo datasets download cifar10 --splits test <pre>Split 'test' already downloaded\r\n</pre> In\u00a0[1]: Copied! <pre>import random\nimport os\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Load test split of CIFAR-10\ndataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\", dataset_name=\"merge-example\")\nclasses = dataset.info[\"classes\"]\n\nprint(dataset)\n</pre> import random import os  import fiftyone as fo import fiftyone.zoo as foz  # Load test split of CIFAR-10 dataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\", dataset_name=\"merge-example\") classes = dataset.info[\"classes\"]  print(dataset) <pre>Split 'test' already downloaded\nLoading 'cifar10' split 'test'\n 100% |\u2588\u2588\u2588| 10000/10000 [14.1s elapsed, 0s remaining, 718.2 samples/s]      \nName:           merge-example\nMedia type:     image\nNum samples:    10000\nPersistent:     False\nInfo:           {'classes': ['airplane', 'automobile', 'bird', ...]}\nTags:           ['test']\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n</pre> <p>The dataset contains ground truth labels in its <code>ground_truth</code> field:</p> In\u00a0[2]: Copied! <pre># Print a sample from the dataset\nprint(dataset.first())\n</pre> # Print a sample from the dataset print(dataset.first()) <pre>&lt;Sample: {\n    'id': '5fee1a40f653ce52a9d077b1',\n    'media_type': 'image',\n    'filepath': '/Users/Brian/fiftyone/cifar10/test/data/000001.jpg',\n    'tags': BaseList(['test']),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '5fee1a40f653ce52a9d077b0',\n        'label': 'horse',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n}&gt;\n</pre> <p>Suppose you would like to add model predictions to some samples from the dataset.</p> <p>The usual way to do this is to just iterate over the dataset and add your predictions directly to the samples:</p> In\u00a0[3]: Copied! <pre>def run_inference(filepath):\n    # Run inference on `filepath` here.\n    # For simplicity, we'll just generate a random label\n    label = random.choice(classes)\n    \n    return fo.Classification(label=label)\n</pre> def run_inference(filepath):     # Run inference on `filepath` here.     # For simplicity, we'll just generate a random label     label = random.choice(classes)          return fo.Classification(label=label) In\u00a0[4]: Copied! <pre># Choose 100 samples at random\nrandom_samples = dataset.take(100)\n\n# Add model predictions to dataset\nfor sample in random_samples:\n    sample[\"predictions\"] = run_inference(sample.filepath)\n    sample.save()\n\nprint(dataset)\n</pre> # Choose 100 samples at random random_samples = dataset.take(100)  # Add model predictions to dataset for sample in random_samples:     sample[\"predictions\"] = run_inference(sample.filepath)     sample.save()  print(dataset) <pre>Name:           merge-example\nMedia type:     image\nNum samples:    10000\nPersistent:     False\nInfo:           {'classes': ['airplane', 'automobile', 'bird', ...]}\nTags:           ['test']\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n    predictions:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n</pre> <p>However, suppose you store the predictions in a separate dataset:</p> In\u00a0[5]: Copied! <pre># Filepaths of images to proces\nfilepaths = [s.filepath for s in dataset.take(100)]\n\n# Run inference\npredictions = fo.Dataset()\nfor filepath in filepaths:\n    sample = fo.Sample(filepath=filepath)\n\n    sample[\"predictions\"] = run_inference(filepath)\n\n    predictions.add_sample(sample)\n\nprint(predictions)\n</pre> # Filepaths of images to proces filepaths = [s.filepath for s in dataset.take(100)]  # Run inference predictions = fo.Dataset() for filepath in filepaths:     sample = fo.Sample(filepath=filepath)      sample[\"predictions\"] = run_inference(filepath)      predictions.add_sample(sample)  print(predictions) <pre>Name:           2020.12.31.12.37.09\nMedia type:     image\nNum samples:    100\nPersistent:     False\nInfo:           {}\nTags:           []\nSample fields:\n    filepath:    fiftyone.core.fields.StringField\n    tags:        fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    predictions: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n</pre> <p>You can easily merge the <code>predictions</code> dataset into the main dataset via Dataset.merge_samples().</p> <p>Let's start by creating a fresh copy of CIFAR-10 that doesn't have predictions:</p> In\u00a0[6]: Copied! <pre>dataset2 = dataset.exclude_fields(\"predictions\").clone(name=\"merge-example2\")\nprint(dataset2)\n</pre> dataset2 = dataset.exclude_fields(\"predictions\").clone(name=\"merge-example2\") print(dataset2) <pre>Name:           merge-example2\nMedia type:     image\nNum samples:    10000\nPersistent:     False\nInfo:           {'classes': ['airplane', 'automobile', 'bird', ...]}\nTags:           ['test']\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n</pre> <p>Now let's merge the predictions into the fresh dataset:</p> In\u00a0[7]: Copied! <pre># Merge predictions\ndataset2.merge_samples(predictions)\n\n# Verify that 100 samples in `dataset2` now have predictions\nprint(dataset2.exists(\"predictions\"))\n</pre> # Merge predictions dataset2.merge_samples(predictions)  # Verify that 100 samples in `dataset2` now have predictions print(dataset2.exists(\"predictions\")) <pre>Dataset:        merge-example2\nMedia type:     image\nNum samples:    100\nTags:           []\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n    predictions:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\nView stages:\n    1. Exists(field='predictions', bool=True)\n</pre> <p>Let's print a sample with predictions to verify that the merge happened as expected:</p> In\u00a0[8]: Copied! <pre># Print a sample with predictions\nprint(dataset2.exists(\"predictions\").first())\n</pre> # Print a sample with predictions print(dataset2.exists(\"predictions\").first()) <pre>&lt;SampleView: {\n    'id': '5fee1a40f653ce52a9d07883',\n    'media_type': 'image',\n    'filepath': '/Users/Brian/fiftyone/cifar10/test/data/000071.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '5fee1a40f653ce52a9d07882',\n        'label': 'frog',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n    'predictions': &lt;Classification: {\n        'id': '5fee1a56f653ce52a9d0ee71',\n        'label': 'horse',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n}&gt;\n</pre> In\u00a0[9]: Copied! <pre># Create another fresh dataset to work with\ndataset3 = dataset.exclude_fields(\"predictions\").clone(name=\"merge-example3\")\n\n# Merge predictions, using the base filename of the samples to decide which samples to merge\n# In this case, we've already performed the merge, so the existing data is overwritten\nkey_fcn = lambda sample: os.path.basename(sample.filepath)\n\ndataset3.merge_samples(predictions, key_fcn=key_fcn)\n</pre> # Create another fresh dataset to work with dataset3 = dataset.exclude_fields(\"predictions\").clone(name=\"merge-example3\")  # Merge predictions, using the base filename of the samples to decide which samples to merge # In this case, we've already performed the merge, so the existing data is overwritten key_fcn = lambda sample: os.path.basename(sample.filepath)  dataset3.merge_samples(predictions, key_fcn=key_fcn) <pre>Indexing dataset...\n 100% |\u2588\u2588\u2588| 10000/10000 [3.6s elapsed, 0s remaining, 2.8K samples/s]        \nMerging samples...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [348.5ms elapsed, 0s remaining, 287.0 samples/s]      \n</pre> <p>Let's print a sample with predictions to verify that the merge happened as expected:</p> In\u00a0[10]: Copied! <pre># Print a sample with predictions\nprint(dataset3.exists(\"predictions\").first())\n</pre> # Print a sample with predictions print(dataset3.exists(\"predictions\").first()) <pre>&lt;SampleView: {\n    'id': '5fee1a40f653ce52a9d07883',\n    'media_type': 'image',\n    'filepath': '/Users/Brian/fiftyone/cifar10/test/data/000071.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '5fee1a40f653ce52a9d07882',\n        'label': 'frog',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n    'predictions': &lt;Classification: {\n        'id': '5fee1a56f653ce52a9d0ee71',\n        'label': 'horse',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n}&gt;\n</pre>"},{"location":"how_do_i/recipes/merge_datasets/#merging-datasets","title":"Merging Datasets\u00b6","text":"<p>This recipe demonstrates a simple pattern for merging FiftyOne Datasets via Dataset.merge_samples().</p> <p>Merging datasets is an easy way to:</p> <ul> <li>Combine multiple datasets with information about the same underlying raw media (images and videos)</li> <li>Add model predictions to a FiftyOne dataset, to compare with ground truth annotations and/or other models</li> </ul>"},{"location":"how_do_i/recipes/merge_datasets/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"how_do_i/recipes/merge_datasets/#merging-model-predictions","title":"Merging model predictions\u00b6","text":"<p>Load the test split of CIFAR-10 into FiftyOne:</p>"},{"location":"how_do_i/recipes/merge_datasets/#customizing-the-merge-key","title":"Customizing the merge key\u00b6","text":"<p>By default, samples with the same absolute <code>filepath</code> are merged. However, you can customize this as desired via various keyword arguments of Dataset.merge_samples().</p> <p>For example, the command below will merge samples with the same base filename, ignoring the directory:</p>"},{"location":"how_do_i/recipes/remove_duplicate_annos/","title":"Removing Duplicate Objects","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone In\u00a0[1]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\", max_samples=1000)\n</pre> import fiftyone as fo import fiftyone.zoo as foz  dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\", max_samples=1000) <pre>Downloading split 'validation' to '/Users/Brian/fiftyone/coco-2017/validation' if necessary\nFound annotations at '/Users/Brian/fiftyone/coco-2017/raw/instances_val2017.json'\nSufficient images already downloaded\nExisting download of split 'validation' is sufficient\nLoading 'coco-2017' split 'validation'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [4.9s elapsed, 0s remaining, 216.7 samples/s]      \nDataset 'coco-2017-validation-1000' created\n</pre> <p>Let's print the dataset to see what we downloaded:</p> In\u00a0[2]: Copied! <pre>print(dataset)\n</pre> print(dataset) <pre>Name:        coco-2017-validation-1000\nMedia type:  image\nNum samples: 1000\nPersistent:  False\nTags:        ['validation']\nSample fields:\n    id:           fiftyone.core.fields.ObjectIdField\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n</pre> In\u00a0[3]: Copied! <pre>import fiftyone.utils.iou as foui\n\nfoui.compute_max_ious(dataset, \"ground_truth\", iou_attr=\"max_iou\", classwise=True)\nprint(\"Max IoU range: (%f, %f)\" % dataset.bounds(\"ground_truth.detections.max_iou\"))\n</pre> import fiftyone.utils.iou as foui  foui.compute_max_ious(dataset, \"ground_truth\", iou_attr=\"max_iou\", classwise=True) print(\"Max IoU range: (%f, %f)\" % dataset.bounds(\"ground_truth.detections.max_iou\")) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [3.2s elapsed, 0s remaining, 348.2 samples/s]      \nMax IoU range: (0.000000, 0.951640)\n</pre> <p>Note that compute_max_ious() provides an optional <code>other_field</code> parameter if you would like to compute IoUs between objects in different fields instead.</p> <p>In any case, let's create a view that contains only labels with a max IoU &gt; 0.75:</p> In\u00a0[4]: Copied! <pre>from fiftyone import ViewField as F\n\n# Retrieve detections that overlap above a chosen threshold\ndups_view = dataset.filter_labels(\"ground_truth\", F(\"max_iou\") &gt; 0.75)\nprint(dups_view)\n</pre> from fiftyone import ViewField as F  # Retrieve detections that overlap above a chosen threshold dups_view = dataset.filter_labels(\"ground_truth\", F(\"max_iou\") &gt; 0.75) print(dups_view) <pre>Dataset:     coco-2017-validation-1000\nMedia type:  image\nNum samples: 7\nTags:        ['validation']\nSample fields:\n    id:           fiftyone.core.fields.ObjectIdField\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\nView stages:\n    1. FilterLabels(field='ground_truth', filter={'$gt': ['$$this.max_iou', 0.75]}, only_matches=True, trajectories=False)\n</pre> <p>and load it in the App:</p> In\u00a0[8]: Copied! <pre>session = fo.launch_app(view=dups_view)\n</pre> session = fo.launch_app(view=dups_view) Activate In\u00a0[9]: Copied! <pre>session.show()\n</pre> session.show() Activate <p>Then we can simply delete the labels with the duplicate tag that we populated:</p> In\u00a0[16]: Copied! <pre>dataset.delete_labels(tags=\"duplicate\")\n\n# Verify that tagged labels were deleted\nprint(dataset.count_label_tags())\n</pre> dataset.delete_labels(tags=\"duplicate\")  # Verify that tagged labels were deleted print(dataset.count_label_tags()) <pre>{}\n</pre> In\u00a0[12]: Copied! <pre>anno_key = \"remove_dups\"\ndups_view.annotate(anno_key, label_field=\"ground_truth\", launch_editor=True)\n</pre> anno_key = \"remove_dups\" dups_view.annotate(anno_key, label_field=\"ground_truth\", launch_editor=True) <pre>Uploading samples to CVAT...\nUpload complete\nLaunching editor at 'http://localhost:8080/tasks/286/jobs/292'...\n</pre> Out[12]: <pre>&lt;fiftyone.utils.cvat.CVATAnnotationResults at 0x7fc07c97c7b8&gt;</pre> <p></p> <p>Once we're finished editing in CVAT, we simply load the results back into FiftyOne:</p> In\u00a0[\u00a0]: Copied! <pre>dataset.load_annotations(anno_key)\n</pre> dataset.load_annotations(anno_key) <p>and refresh the App to verify that the duplicates have been properly addressed:</p> In\u00a0[14]: Copied! <pre>session.show()\n</pre> session.show() Activate In\u00a0[6]: Copied! <pre>dup_ids = foui.find_duplicates(\n    dataset, \"ground_truth\", iou_thresh=0.75, classwise=True\n)\nprint(\"Found %d duplicates\" % len(dup_ids))\n</pre> dup_ids = foui.find_duplicates(     dataset, \"ground_truth\", iou_thresh=0.75, classwise=True ) print(\"Found %d duplicates\" % len(dup_ids)) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [3.5s elapsed, 0s remaining, 315.1 samples/s]      \nFound 7 duplicates\n</pre> <p>Of course, when dealing with duplicate labels, there is inherent ambiguity: which one is \"correct\" and which one(s) are \"duplicate\"?</p> <p>By default, find_duplicates() will simply iterate through the labels in each sample and flag any label whose IoU with a previous label exceeds the chosen threshold as a duplicate.</p> <p>Alternatively, you can pass the <code>method=\"greedy\"</code> option to instead use a greedy approach to mark the fewest number of labels as duplicate such that no non-duplicate labels have IoU greater than the specified threshold with each other.</p> <p>In either case, it is recommended to visualize the duplicates in the App before taking any action. One convenient way to do this is to first tag the duplicates:</p> In\u00a0[11]: Copied! <pre># Cleanup any previous tags (if necessary)\ndataset.untag_labels(\"duplicate\")\n</pre> # Cleanup any previous tags (if necessary) dataset.untag_labels(\"duplicate\") In\u00a0[12]: Copied! <pre># Tag the automatically selected duplicates\ndataset.select_labels(ids=dup_ids).tag_labels(\"duplicate\")\nprint(dataset.count_label_tags())\n</pre> # Tag the automatically selected duplicates dataset.select_labels(ids=dup_ids).tag_labels(\"duplicate\") print(dataset.count_label_tags()) <pre>{'duplicate': 7}\n</pre> <p>Then, use match_labels() to load the samples containing at least one duplicate label in the App and use the <code>duplicate</code> tag you added to conveniently isolate and evaluate the duplicates.</p> <p>If you see any erroneous duplicates, simply remove the <code>duplicate</code> tag in the App:</p> In\u00a0[16]: Copied! <pre>session.view = dataset.match_labels(ids=dup_ids)\n</pre> session.view = dataset.match_labels(ids=dup_ids) Activate <p>When you're ready to act, you can then easily delete the duplicate labels as follows:</p> In\u00a0[17]: Copied! <pre>dataset.delete_labels(tags=\"duplicate\")\n\n# If you want to delete every label flagged by `find_duplicates()`\n# dataset.delete_labels(ids=dup_ids)\n</pre> dataset.delete_labels(tags=\"duplicate\")  # If you want to delete every label flagged by `find_duplicates()` # dataset.delete_labels(ids=dup_ids) In\u00a0[18]: Copied! <pre>session.freeze() # screenshot the active App for sharing\n</pre> session.freeze() # screenshot the active App for sharing"},{"location":"how_do_i/recipes/remove_duplicate_annos/#removing-duplicate-objects","title":"Removing Duplicate Objects\u00b6","text":"<p>This recipe demonstrates a simple workflow for finding and removing duplicate objects in your FiftyOne datasets using intersection over union (IoU).</p> <p>Specificially, it covers:</p> <ul> <li>Using the compute_max_ious() utility to compute overlap between spatial objects</li> <li>Using the App's tagging UI to review and delete duplicate labels</li> <li>Using FiftyOne's CVAT integration to edit duplicate labels</li> <li>Using the find_duplicates() utility to automatically detect duplicate objects</li> </ul> <p>Also, check out our blog post for more information about using IoU to evaluate your object detection models.</p>"},{"location":"how_do_i/recipes/remove_duplicate_annos/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"how_do_i/recipes/remove_duplicate_annos/#load-a-dataset","title":"Load a dataset\u00b6","text":"<p>In this recipe, we'll work with the validation split of the COCO dataset, which is conveniently available for download via the FiftyOne Dataset Zoo.</p> <p>The snippet below downloads and loads a subset of the validation split into FiftyOne:</p>"},{"location":"how_do_i/recipes/remove_duplicate_annos/#finding-duplicate-objects","title":"Finding duplicate objects\u00b6","text":"<p>Now let's use the compute_max_ious() utility to compute the maximum IoU between each object in the <code>ground_truth</code> field with another object of the same class (<code>classwise=True</code>) within the same image.</p> <p>The max IOU will be stored in a <code>max_iou</code> attribute of each object, and the idea here is that duplicate objects will necessarily have high IoU with another object.</p>"},{"location":"how_do_i/recipes/remove_duplicate_annos/#removing-duplicates-in-the-app","title":"Removing duplicates in the App\u00b6","text":"<p>One simple approach to removing the duplicate labels is to review them in the App and assign label tags to the labels that we deem to be duplicates:</p>"},{"location":"how_do_i/recipes/remove_duplicate_annos/#removing-duplicates-in-cvat","title":"Removing duplicates in CVAT\u00b6","text":"<p>Another approach to resolve the duplicate labegls is to use FiftyOne's CVAT integration to upload the duplicate labels to CVAT for review, editing, and/or deletion:</p>"},{"location":"how_do_i/recipes/remove_duplicate_annos/#automatically-finding-duplicates","title":"Automatically finding duplicates\u00b6","text":"<p>A third approach is to use the find_duplicates() utility to automatically retrieve the IDs of duplicate labels:</p>"},{"location":"integrations/","title":"FiftyOne Integrations \u00b6","text":"<p>FiftyOne integrates naturally with other ML tools that you know and love. Click on the cards below to see how!</p>"},{"location":"integrations/#coco-dataset","title":"COCO Dataset","text":"<p>See how FiftyOne makes downloading, visualizing, and evaluating on the COCO dataset (or your own COCO-formatted data) a breeze.</p> <p>Datasets,Model-Evaluation</p> <p></p>"},{"location":"integrations/#open-images-dataset","title":"Open Images Dataset","text":"<p>See why FiftyOne is a recommended tool for downloading, visualizing, and evaluating on Google's Open Images Dataset.</p> <p>Datasets,Model-Evaluation</p> <p></p>"},{"location":"integrations/#activitynet-dataset","title":"ActivityNet Dataset","text":"<p>See how to use FiftyOne to download, visualize, and evaluate on the ActivityNet dataset with ease.</p> <p>Datasets,Model-Evaluation</p> <p></p>"},{"location":"integrations/#cvat","title":"CVAT","text":"<p>Use our CVAT integration to easily annotate and edit your FiftyOne datasets.</p> <p>Annotation</p> <p></p>"},{"location":"integrations/#label-studio","title":"Label Studio","text":"<p>Annotate and edit your FiftyOne datasets in Label Studio through our integration.</p> <p>Annotation</p> <p></p>"},{"location":"integrations/#v7","title":"V7","text":"<p>Use our V7 integration to easily annotate and edit your FiftyOne datasets.</p> <p>Annotation</p> <p></p>"},{"location":"integrations/#labelbox","title":"Labelbox","text":"<p>Use our Labelbox integration to get your FiftyOne datasets annotated.</p> <p>Annotation</p> <p></p>"},{"location":"integrations/#qdrant","title":"Qdrant","text":"<p>Use our Qdrant integration to enable vector search and query your FiftyOne datasets at scale.</p> <p>Brain,Embeddings,Vector-Search</p> <p></p>"},{"location":"integrations/#redis","title":"Redis","text":"<p>Use our Redis vector search integration to index your FiftyOne datasets and perform embeddings queries at scale.</p> <p>Brain,Embeddings,Vector-Search</p> <p></p>"},{"location":"integrations/#pinecone","title":"Pinecone","text":"<p>Use our Pinecone integration to index your FiftyOne datasets and perform embeddings queries at scale.</p> <p>Brain,Embeddings,Vector-Search</p> <p></p>"},{"location":"integrations/#mongodb","title":"MongoDB","text":"<p>Use our MongoDB vector search integration to index your FiftyOne datasets and perform embeddings queries at scale.</p> <p>Brain,Embeddings,Vector-Search</p> <p></p>"},{"location":"integrations/#elasticsearch","title":"Elasticsearch","text":"<p>Use our Elasticsearch integration to enable vector search and query your FiftyOne datasets at scale.</p> <p>Brain,Embeddings,Vector-Search</p> <p></p>"},{"location":"integrations/#milvus","title":"Milvus","text":"<p>Use our Milvus integration to index your FiftyOne datasets and perform embeddings queries at scale.</p> <p>Brain,Embeddings,Vector-Search</p> <p></p>"},{"location":"integrations/#lancedb","title":"LanceDB","text":"<p>Use our LancedDB integration to index your datasets and perform embeddings queries at scale without the need for a cloud service.</p> <p>Brain,Embeddings,Vector-Search</p> <p></p>"},{"location":"integrations/#hugging-face","title":"Hugging Face","text":"<p>Use our Hugging Face Transformers integration to run inference on your FiftyOne datasets with just a few lines of code.</p> <p>Model-Training,Model-Evaluation,Models,Brain,Embeddings</p> <p></p>"},{"location":"integrations/#ultralytics","title":"Ultralytics","text":"<p>Load, fine-tune, and run inference with Ultralytics models on your FiftyOne datasets with just a few lines of code.</p> <p>Model-Training,Model-Evaluation,Models</p> <p></p>"},{"location":"integrations/#albumentations","title":"Albumentations","text":"<p>Use our Albumentations integration to test out data augmentation transformations in real-time on your FiftyOne datasets.</p> <p>Datasets,Model-Training</p> <p></p>"},{"location":"integrations/#supergradients","title":"SuperGradients","text":"<p>Use our SuperGradients integration to run inference with YOLO-NAS models on your FiftyOne datasets with just a few lines of code.</p> <p>Model-Training,Model-Evaluation,Models</p> <p></p>"},{"location":"integrations/#openclip","title":"OpenCLIP","text":"<p>Use our OpenCLIP integration to run inference with CLIP models on your FiftyOne datasets with just a few lines of code.</p> <p>Brain,Embeddings,Model-Evaluation,Models</p> <p></p>"},{"location":"integrations/#pytorch-hub","title":"PyTorch Hub","text":"<p>Did you know? You can load any model from the PyTorch Hub and run inference on your FiftyOne datasets with just a few lines of code.</p> <p>Model-Training,Model-Evaluation,Models</p> <p></p>"},{"location":"integrations/activitynet/","title":"ActivityNet Integration \u00b6","text":"<p>With FiftyOne, you can easily download, visualize, and evaluate on the ActivityNet dataset!</p> <p></p>"},{"location":"integrations/activitynet/#loading-the-activitynet-dataset","title":"Loading the ActivityNet dataset \u00b6","text":"<p>The FiftyOne Dataset Zoo provides support for loading both the ActivityNet 100 and ActivityNet 200 datasets.</p> <p>Like all other zoo datasets, you can use <code>load_zoo_dataset()</code> to download and load an ActivityNet split into FiftyOne:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Download and load 10 samples from the validation split of ActivityNet 200\ndataset = foz.load_zoo_dataset(\n    \"activitynet-200\",\n    split=\"validation\",\n    max_samples=10,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>ActivityNet 200 is a superset of ActivityNet 100 so we have made sure to only store one copy of every video on disk. Videos in the ActivityNet 100 zoo directory are used directly by ActivityNet 200.</p>"},{"location":"integrations/activitynet/#partial-downloads","title":"Partial Downloads \u00b6","text":"<p>In addition, FiftyOne provides parameters that can be used to efficiently download specific subsets of the ActivityNet dataset, allowing you to quickly explore different slices of the dataset without downloading the entire split.</p> <p>When performing partial downloads, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from YouTube.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n#\n# Load 10 random samples from the validation split\n#\n# Only the required videos will be downloaded (if necessary).\n#\n\ndataset = foz.load_zoo_dataset(\n    \"activitynet-200\",\n    split=\"validation\",\n    max_samples=10,\n    shuffle=True,\n)\n\nsession = fo.launch_app(dataset)\n\n#\n# Load 10 samples from the validation split that\n# contain the actions \"Bathing dog\" and \"Walking the dog\"\n# with a maximum duration of 20 seconds\n#\n# Videos that contain all ``classes`` will be prioritized first, followed\n# by videos that contain at least one of the required ``classes``. If\n# there are not enough videos matching ``classes`` in the split to meet\n# ``max_samples``, only the available videos will be loaded.\n#\n# Videos will only be downloaded if necessary\n#\n\ndataset = foz.load_zoo_dataset(\n    \"activitynet-200\",\n    split=\"validation\",\n    classes=[\"Bathing dog\", \"Walking the dog\"],\n    max_samples=10,\n    max_duration=20,\n)\n\nsession.dataset = dataset\n</code></pre> <p>The following parameters are available to configure partial downloads of both ActivityNet 100 and ActivityNet 200 by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If none are provided, all available splits are loaded</p> </li> <li> <p>source_dir ( None): the directory containing the manually downloaded ActivityNet files used to avoid downloading videos from YouTube</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded</p> </li> <li> <p>max_duration ( None): only videos with a duration in seconds that is less than or equal to the <code>max_duration</code> will be downloaded. By default, all videos are downloaded</p> </li> <li> <p>copy_files ( True): whether to move (False) or create copies (True) of the source files when populating <code>dataset_dir</code>. This is only applicable when a <code>source_dir</code> is provided</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual videos. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>classes</code> are also specified, only up to the number of samples that contain at least one specified class will be loaded. By default, all matching samples are loaded</p> </li> </ul>"},{"location":"integrations/activitynet/#full-split-downloads","title":"Full Split Downloads \u00b6","text":"<p>Many videos have been removed from YouTube since the creation of ActivityNet. As a result, you must first download the official source files from the ActivityNet maintainers in order to load a full split into FiftyOne.</p> <p>To download the source files, you must fill out this form.</p> <p>After downloading the source files, they can be loaded into FiftyOne like so:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\nsource_dir = \"/path/to/dir-with-activitynet-files\"\n\n# Load the entire ActivityNet 200 dataset into FiftyOne\ndataset = foz.load_zoo_dataset(\"activitynet-200\", source_dir=source_dir)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>where <code>source_dir</code> contains the source files in the following format:</p> <pre><code>source_dir/\n    missing_files.zip\n    missing_files_v1-2_test.zip\n    missing_files_v1-3_test.zip\n    v1-2_test.tar.gz\n    v1-2_train.tar.gz\n    v1-2_val.tar.gz\n    v1-3_test.tar.gz\n    v1-3_train_val.tar.gz\n</code></pre> <p>If you have already decompressed the archives, that is okay too:</p> <pre><code>source_dir/\n    missing_files/\n        v_&lt;id&gt;.&lt;ext&gt;\n        ...\n    missing_files_v1-2_test/\n        v_&lt;id&gt;.&lt;ext&gt;\n        ...\n    missing_files_v_1-3_test/\n        v_&lt;id&gt;.&lt;ext&gt;\n        ...\n    v1-2/\n        train/\n            v_&lt;id&gt;.&lt;ext&gt;\n            ...\n        val/\n            ...\n        test/\n            ...\n    v1-3/\n        train_val/\n            v_&lt;id&gt;.&lt;ext&gt;\n            ...\n        test/\n            ...\n</code></pre> <p>If you are only interested in loading specific splits into FiftyOne, the files for the other splits do not need to be present.</p> <p>Note</p> <p>When <code>load_zoo_dataset()</code> is called with the <code>source_dir</code> parameter, the contents are copied (or moved, if <code>copy_files=False</code>) into the zoo dataset\u2019s backing directory.</p> <p>Therefore, future use of the loaded dataset or future calls to <code>load_zoo_dataset()</code> will not require the <code>source_dir</code> parameter.</p>"},{"location":"integrations/activitynet/#activitynet-style-evaluation","title":"ActivityNet-style evaluation \u00b6","text":"<p>The <code>evaluate_detections()</code> method provides builtin support for running ActivityNet-style evaluation.</p> <p>ActivityNet-style evaluation is the default method when evaluating <code>TemporalDetections</code> labels, but you can also explicitly request it by setting the <code>method</code> parameter to <code>\"actvitynet\"</code>.</p> <p>Note</p> <p>FiftyOne\u2019s implementation of ActivityNet-style evaluation matches the reference implementation available via the ActivityNet API.</p>"},{"location":"integrations/activitynet/#overview","title":"Overview \u00b6","text":"<p>When running ActivityNet-style evaluation using <code>evaluate_detections()</code>:</p> <ul> <li> <p>Predicted and ground truth segments are matched using a specified IoU threshold (default = 0.50). This threshold can be customized via the <code>iou</code> parameter</p> </li> <li> <p>By default, only segments with the same <code>label</code> will be matched. Classwise matching can be disabled by passing <code>classwise=False</code></p> </li> <li> <p>mAP is computed by averaging over the same range of IoU values used by COCO</p> </li> </ul> <p>When you specify an <code>eval_key</code> parameter, a number of helpful fields will be populated on each sample and its predicted/ground truth segments:</p> <ul> <li>True positive (TP), false positive (FP), and false negative (FN) counts for the each sample are saved in top-level fields of each sample:</li> </ul> <pre><code>TP: sample.&lt;eval_key&gt;_tp\nFP: sample.&lt;eval_key&gt;_fp\nFN: sample.&lt;eval_key&gt;_fn\n</code></pre> <ul> <li>The fields listed below are populated on each individual temporal detection segment; these fields tabulate the TP/FP/FN status of the segment, the ID of the matching segment (if any), and the matching IoU:</li> </ul> <pre><code>TP/FP/FN: segment.&lt;eval_key&gt;\n        ID: segment.&lt;eval_key&gt;_id\n       IoU: segment.&lt;eval_key&gt;_iou\n</code></pre> <p>Note</p> <p>See <code>ActivityNetEvaluationConfig</code> for complete descriptions of the optional keyword arguments that you can pass to <code>evaluate_detections()</code> when running ActivityNet-style evaluation.</p>"},{"location":"integrations/activitynet/#example-evaluation","title":"Example evaluation \u00b6","text":"<p>The example below demonstrates ActivityNet-style temporal detection evaluation on the ActivityNet 200 dataset:</p> <pre><code>import random\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\n# Load subset of ActivityNet 200\nclasses = [\"Bathing dog\", \"Walking the dog\"]\ndataset = foz.load_zoo_dataset(\n    \"activitynet-200\",\n    split=\"validation\",\n    classes=classes,\n    max_samples=10,\n)\nprint(dataset)\n\n# Generate some fake predictions for this example\nrandom.seed(51)\ndataset.clone_sample_field(\"ground_truth\", \"predictions\")\nfor sample in dataset:\n    for det in sample.predictions.detections:\n        det.support[0] += random.randint(-10,10)\n        det.support[1] += random.randint(-10,10)\n        det.support[0] = max(det.support[0], 1)\n        det.support[1] = max(det.support[1], det.support[0] + 1)\n        det.confidence = random.random()\n        det.label = random.choice(classes)\n\n    sample.save()\n\n# Evaluate the segments in the `predictions` field with respect to the\n# segments in the `ground_truth` field\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval\",\n)\n\n# Print a classification report for the classes\nresults.print_report(classes=classes)\n\n# Print some statistics about the total TP/FP/FN counts\nprint(\"TP: %d\" % dataset.sum(\"eval_tp\"))\nprint(\"FP: %d\" % dataset.sum(\"eval_fp\"))\nprint(\"FN: %d\" % dataset.sum(\"eval_fn\"))\n\n# Create a view that has samples with the most false positives first, and\n# only includes false positive segments in the `predictions` field\nview = (\n    dataset\n    .sort_by(\"eval_fp\", reverse=True)\n    .filter_labels(\"predictions\", F(\"eval\") == \"fp\")\n)\n\n# Visualize results in the App\nsession = fo.launch_app(view=view)\n</code></pre> <pre><code>                 precision    recall  f1-score   support\n\n    Bathing dog       0.50      0.40      0.44         5\nWalking the dog       0.50      0.60      0.55         5\n\n      micro avg       0.50      0.50      0.50        10\n      macro avg       0.50      0.50      0.49        10\n   weighted avg       0.50      0.50      0.49        10\n</code></pre> <p></p>"},{"location":"integrations/activitynet/#map-and-pr-curves","title":"mAP and PR curves \u00b6","text":"<p>You can compute mean average precision (mAP) and precision-recall (PR) curves for your segments by passing the <code>compute_mAP=True</code> flag to <code>evaluate_detections()</code>:</p> <p>Note</p> <p>All mAP calculations are performed according to the ActivityNet evaluation protocol.</p> <pre><code>import random\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Load subset of ActivityNet 200\nclasses = [\"Bathing dog\", \"Walking the dog\"]\ndataset = foz.load_zoo_dataset(\n    \"activitynet-200\",\n    split=\"validation\",\n    classes=classes,\n    max_samples=10,\n)\nprint(dataset)\n\n# Generate some fake predictions for this example\nrandom.seed(51)\ndataset.clone_sample_field(\"ground_truth\", \"predictions\")\nfor sample in dataset:\n    for det in sample.predictions.detections:\n        det.support[0] += random.randint(-10,10)\n        det.support[1] += random.randint(-10,10)\n        det.support[0] = max(det.support[0], 1)\n        det.support[1] = max(det.support[1], det.support[0] + 1)\n        det.confidence = random.random()\n        det.label = random.choice(classes)\n\n    sample.save()\n\n# Performs an IoU sweep so that mAP and PR curves can be computed\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval\",\n    compute_mAP=True,\n)\n\nprint(results.mAP())\n# 0.367\n\nplot = results.plot_pr_curves(classes=classes)\nplot.show()\n</code></pre> <p></p>"},{"location":"integrations/activitynet/#confusion-matrices","title":"Confusion matrices \u00b6","text":"<p>You can also easily generate confusion matrices for the results of ActivityNet-style evaluations.</p> <p>In order for the confusion matrix to capture anything other than false positive/negative counts, you will likely want to set the <code>classwise</code> parameter to <code>False</code> during evaluation so that predicted segments can be matched with ground truth segments of different classes.</p> <pre><code>import random\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Load subset of ActivityNet 200\nclasses = [\"Bathing dog\", \"Grooming dog\", \"Grooming horse\", \"Walking the dog\"]\ndataset = foz.load_zoo_dataset(\n    \"activitynet-200\",\n    split=\"validation\",\n    classes=classes,\n    max_samples=20,\n)\nprint(dataset)\n\n# Generate some fake predictions for this example\nrandom.seed(51)\ndataset.clone_sample_field(\"ground_truth\", \"predictions\")\nfor sample in dataset:\n    for det in sample.predictions.detections:\n        det.support[0] += random.randint(-10,10)\n        det.support[1] += random.randint(-10,10)\n        det.support[0] = max(det.support[0], 1)\n        det.support[1] = max(det.support[1], det.support[0] + 1)\n        det.confidence = random.random()\n        det.label = random.choice(classes)\n\n    sample.save()\n\n# Perform evaluation, allowing objects to be matched between classes\nresults = dataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", classwise=False\n)\n\n# Generate a confusion matrix for the specified classes\nplot = results.plot_confusion_matrix(classes=classes)\nplot.show()\n</code></pre> <p></p> <p>Note</p> <p>Did you know? Confusion matrices can be attached to your <code>Session</code> object and dynamically explored using FiftyOne\u2019s interactive plotting features!</p>"},{"location":"integrations/activitynet/#activitynet-challenge","title":"ActivityNet Challenge \u00b6","text":"<p>Since FiftyOne\u2019s implementation of ActivityNet-style evaluation matches the reference implementation from the ActivityNet API used in the ActivityNet challenges. you can use it to compute the official mAP for your model while also enjoying the benefits of working in the FiftyOne ecosystem, including using views to manipulate your dataset and visually exploring your model\u2019s predictions in the FiftyOne App!</p> <p>The example snippet below loads the ActivityNet 200 dataset and runs the official ActivityNet evaluation protocol on some mock model predictions:</p> <pre><code>import random\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Load subset of ActivityNet 200\nclasses = [\"Bathing dog\", \"Walking the dog\"]\ndataset = foz.load_zoo_dataset(\n    \"activitynet-200\",\n    split=\"validation\",\n    classes=classes,\n    max_samples=10,\n)\n\n# Generate some fake predictions for this example\nrandom.seed(51)\ndataset.clone_sample_field(\"ground_truth\", \"predictions\")\nfor sample in dataset:\n    for det in sample.predictions.detections:\n        det.support[0] += random.randint(-10,10)\n        det.support[1] += random.randint(-10,10)\n        det.support[0] = max(det.support[0], 1)\n        det.support[1] = max(det.support[1], det.support[0] + 1)\n        det.confidence = random.random()\n        det.label = random.choice(classes)\n\n    sample.save()\n\n# Evaluate predictions via the official ActivityNet protocol\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    compute_mAP=True,\n)\n\n# The official mAP for the results\nprint(results.mAP())\n</code></pre> <p>Note</p> <p>Check out this recipe to learn how to add your model\u2019s predictions to a FiftyOne Dataset.</p>"},{"location":"integrations/activitynet/#map-protocol","title":"mAP protocol \u00b6","text":"<p>The ActivityNet mAP protocol is similar to COCO-style mAP, with the primary difference being a different IoU computation using temporal segments, a lack of crowds, and the way interpolation of precision values is handled.</p> <p>The steps to compute ActivityNet-style mAP are detailed below.</p> <p>Preprocessing</p> <ul> <li> <p>Filter ground truth and predicted segments by class (unless <code>classwise=False</code>)</p> </li> <li> <p>Sort predicted segments by confidence score so high confidence segments are matched first</p> </li> <li> <p>Compute IoU between every ground truth and predicted segment within the same class (and between classes if <code>classwise=False</code>) in each video</p> </li> </ul> <p>Matching</p> <p>Once IoUs have been computed, predictions and ground truth segments are matched to compute true positives, false positives, and false negatives:</p> <ul> <li> <p>For each class, start with the highest confidence prediction, match it to the ground truth segment that it overlaps with the highest IoU. A prediction only matches if the IoU is above the specified <code>iou</code> threshold</p> </li> <li> <p>If a prediction maximally overlaps with a ground truth segment that has already been matched (by a higher confidence prediction), the prediction is matched with the next highest IoU ground truth segment</p> </li> </ul> <p>Computing mAP</p> <ul> <li> <p>Compute matches for 10 IoU thresholds from 0.5 to 0.95 in increments of 0.05</p> </li> <li> <p>The next 6 steps are computed separately for each class and IoU threshold:</p> </li> <li> <p>Construct a boolean array of true positives and false positives, sorted by confidence</p> </li> <li> <p>Compute the cumulative sum of the true positive and false positive array</p> </li> <li> <p>Compute precision by elementwise dividing the TP-FP-sum array by the total number of predictions up to that point</p> </li> <li> <p>Compute recall by elementwise dividing TP-FP-sum array by the number of ground truth segments for the class</p> </li> <li> <p>Ensure that precision is a non-increasing array</p> </li> <li> <p>(Unlike COCO) DO NOT interpolate precision values onto an 101 evenly spaced recall values. In FiftyOne, this step is performed anyway with the results stored separately for the purpose of plotting PR curves. It is not factored into mAP calculation</p> </li> <li> <p>For every class that contains at least one ground truth segment, compute the average precision (AP) by averaging the precision values over all 10 IoU thresholds. Then compute mAP by averaging the per-class AP values over all classes</p> </li> </ul>"},{"location":"integrations/albumentations/","title":"Albumentations Integration \u00b6","text":"<p>The Albumentations library is the leading open-source library for image augmentation in machine learning. It is widely used in the computer vision community and is known for its extensive collection of augmentations and its high performance.</p> <p>Now, we\u2019ve integrated Albumentations transformation pipelines directly with FiftyOne datasets, enabling you to visualize Albumentations augmentations and test their effects on your data directly within the FiftyOne App!</p> <p>This integration takes the form of a FiftyOne plugin, which is easy to install and can be used entirely via a convenient graphical interface.</p> <p>With the FiftyOne Albumentations plugin, you can transform any and all labels of type <code>Detections</code>, <code>Keypoints</code>, <code>Segmentation</code>, and <code>Heatmap</code>, or just the images themselves.</p> <p>This integration guide will focus on the setup process and the functionality of the plugin. For a tutorial on how to curate your augmentations, check out the Data Augmentation Tutorial.</p>"},{"location":"integrations/albumentations/#overview","title":"Overview \u00b6","text":"<p>Before we get started, let\u2019s take a look at the main features of the FiftyOne Albumentations integration.</p>"},{"location":"integrations/albumentations/#supported-transformations","title":"Supported transformations \u00b6","text":"<p>Albumentations supports 80+ transformations, spanning pixel-level transformations, geometric transformations, and more.</p> <p>The FiftyOne Albumentations plugin currently supports all but the following transformations:</p> <ul> <li> <p>AdvancedBlur</p> </li> <li> <p>GridDropout</p> </li> <li> <p>MaskDropout</p> </li> <li> <p>PiecewiseAffine</p> </li> <li> <p>RandomGravel</p> </li> <li> <p>RandomGridShuffle</p> </li> <li> <p>RandomShadow</p> </li> <li> <p>RandomSunFlare</p> </li> <li> <p>Rotate</p> </li> </ul>"},{"location":"integrations/albumentations/#functionality","title":"Functionality \u00b6","text":"<p>The FiftyOne Albumentations plugin provides the following functionality:</p> <ul> <li> <p>Apply Albumentations transformations to your dataset, your current view, or selected samples</p> </li> <li> <p>Visualize the effects of these transformations directly within the FiftyOne App</p> </li> <li> <p>View samples generated by the last applied transformation</p> </li> <li> <p>Save augmented samples to the dataset</p> </li> <li> <p>Get info about the last applied transformation</p> </li> <li> <p>Save transformation pipelines to the dataset for reproducibility</p> </li> </ul>"},{"location":"integrations/albumentations/#setup","title":"Setup \u00b6","text":"<p>To get started, first make sure you have FiftyOne and Albumentations installed:</p> <pre><code>$ pip install -U fiftyone albumentations\n</code></pre> <p>Next, install the FiftyOne Albumentations plugin:</p> <pre><code>$ fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin\n</code></pre> <p>Note</p> <p>If you have the FiftyOne Plugin Utils plugin installed, you can also install the Albumentations plugin via the <code>install_plugin</code> operator, selecting the Albumentations plugin from the community dropdown menu.</p> <p>You will also need to load (and download if necessary) a dataset to apply the augmentations to. For this guide, we\u2019ll use the quickstart dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n## only take 5 samples for quick demonstration\ndataset = foz.load_zoo_dataset(\"quickstart\", max_samples=5)\n\n# only keep the ground truth labels\ndataset.select_fields(\"ground_truth\").keep_fields()\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>The quickstart dataset only contains <code>Detections</code> labels. If you want to test Albumentations transformations on other label types, here are some quick examples to get you started, using FiftyOne\u2019s Hugging Face Transformers and Ultralytics integrations:</p> <pre><code>pip install -U transformers ultralytics\n</code></pre> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\nfrom ultralytics import YOLO\n\n# Keypoints\nmodel = YOLO(\"yolov8l-pose.pt\")\ndataset.apply_model(model, label_field=\"keypoints\")\n\n# Instance Segmentation\nmodel = YOLO(\"yolov8l-seg.pt\")\ndataset.apply_model(model, label_field=\"instances\")\n\n# Semantic Segmentation\nmodel = foz.load_zoo_model(\n    \"segmentation-transformer-torch\",\n    name_or_path=\"Intel/dpt-large-ade\",\n)\ndataset.apply_model(model, label_field=\"mask\")\n\n# Heatmap\nmodel = foz.load_zoo_model(\n    \"depth-estimation-transformer-torch\",\n    name_or_path=\"LiheYoung/depth-anything-small-hf\",\n)\ndataset.apply_model(model, label_field=\"depth_map\")\n</code></pre>"},{"location":"integrations/albumentations/#apply-transformations","title":"Apply transformations \u00b6","text":"<p>To apply Albumentations transformations to your dataset, you can use the augment_with_albumentations operator. Press the backtick key (\u2018`\u2019) to open the operator modal, and select the <code>augment_with_albumentations</code> operator from the dropdown menu.</p> <p>You can then configure the transformations to apply:</p> <ul> <li> <p>Number of augmentations per sample: The number of augmented samples to generate for each input sample. The default is 1, which is sufficient for deterministic transformations, but for probabilistic transformations, you may want to generate multiple samples to see the range of possible outputs.</p> </li> <li> <p>Number of transforms: The number of transformations to compose into the pipeline to be applied to each sample. The default is 1, but you can set this as high as you\u2019d like \u2014 the more transformations, the more complex the augmentations will be. You will be able to configure each transform separately.</p> </li> <li> <p>Target view: The view to which the transformations will be applied. The default is <code>dataset</code>, but you can also apply the transformations to the current view or to currently selected samples within the app.</p> </li> <li> <p>Execution mode: If you set <code>delegated=False</code>, the operation will be executed immediately. If you set <code>delegated=True</code>, the operation will be queued as a job, which you can then run in the background from your terminal with:</p> </li> </ul> <pre><code>$ fiftyone delegated launch\n</code></pre> <p>For each transformation, you can select either a \u201cprimitive\u201d transformation from the Albumentations library, or a \u201csaved\u201d transformation pipeline that you have previously saved to the dataset. These saved pipelines can consist of one or more transformations.</p> <p>When you apply a primitive transformation, you can configure the parameters of the transformation directly within the app. The available parameters, their default values, types, and docstrings are all integrated directly from the Albumentations library.</p> <p></p> <p>When you apply a saved pipeline, there will not be any parameters to configure.</p> <p></p>"},{"location":"integrations/albumentations/#visualize-transformations","title":"Visualize transformations \u00b6","text":"<p>Once you\u2019ve applied the transformations, you can visualize the effects of the transformations directly within the FiftyOne App. All augmented samples will be added to the dataset, and will be tagged as <code>augmented</code> so that you can easily filter for just augmented or non-augmented samples in the app.</p> <p></p> <p>You can also filter for augmented samples programmatically with the <code>match_tags()</code> method:</p> <pre><code># get just the augmented samples\naugmented_view = dataset.match_tags(\"augmented\")\n\n# get just the non-augmented samples\nnon_augmented_view = dataset.match_tags(\"augmented\", bool=False)\n</code></pre> <p>However, matching on these tags will return all samples that have been generated by an augmentation, not just the samples that were generated by the last applied transformation \u2014 as you will see shortly, we can save augmentations to the dataset. To get just the samples generated by the last applied transformation, you can use the view_last_albumentations_run operator:</p> <p></p> <p>Note</p> <p>For all samples added to the dataset by the FiftyOne Albumentations plugin, there will be a field <code>\"transform\"</code>, which contains the information not just about the pipeline that was applied, but also about the specific parameters that were used for this application of the pipeline. For example, if you had a <code>HorizontalFlip</code> transformation with an application probability of <code>p=0.5</code>, the contents of the <code>\"transform\"</code> field tell you whether or not this transformation was applied to the sample!</p>"},{"location":"integrations/albumentations/#save-augmentations","title":"Save augmentations \u00b6","text":"<p>By default all augmentations are temporary, as the FiftyOne Albumentations plugin is primarily designed for rapid prototyping and experimentation. This means that when you generated a new batch of augmented samples, the previous batch of augmented samples will be removed from the dataset, and the image files will be deleted from disk.</p> <p>However, if you want to save the augmented samples to the dataset, you can use the save_albumentations_augmentations operator, which will save the augmented samples to the dataset while keeping the <code>augmented</code> tag on the samples.</p> <p></p>"},{"location":"integrations/albumentations/#get-last-transformation-info","title":"Get last transformation info \u00b6","text":"<p>When you apply a transformation pipeline to samples in your dataset using the FiftyOne Albumentations plugin, this information is captured and stored using FiftyOne\u2019s custom runs. This means that you can easily access the information about the last applied transformation.</p> <p>In the FiftyOne App, you can use the get_last_albumentations_run_info operator to display a formatted summary of the relevant information:</p> <p></p> <p>Note</p> <p>You can also access this information programmatically by getting info about the custom run that the information is stored in. For the Albumentations plugin, this info is stored via the key <code>'_last_albumentations_run'</code>:</p> <pre><code>last_run_info = dataset.get_run_info(\"_last_albumentations_run\")\nprint(last_run_info)\n</code></pre>"},{"location":"integrations/albumentations/#save-transformations","title":"Save transformations \u00b6","text":"<p>If you are satisfied with the transformation pipeline you have created, you can save the entire composition of transformations to the dataset, hyperparameters and all. This means that after your rapid prototyping phase, you can easily move to a more reproducible workflow, and you can share your transformations or port them to other datasets.</p> <p>To save a transformation pipeline, you can use the save_albumentations_transform operator:</p> <p>After doing so, you will be able to view the information about this saved transformation pipeline using the get_albumentations_run_info operator:</p> <p></p> <p>Additionally, you will have access to this saved transformation pipeline under the \u201csaved\u201d tab for each transformation in the augment_with_albumentations operator modal.</p>"},{"location":"integrations/coco/","title":"COCO Integration \u00b6","text":"<p>With support from the team behind the COCO dataset, we\u2019ve made it easy to download, visualize, and evaluate on the COCO dataset natively in FiftyOne!</p> <p>Note</p> <p>Check out this tutorial to see how you can use FiftyOne to evaluate a model on COCO.</p> <p></p>"},{"location":"integrations/coco/#loading-the-coco-dataset","title":"Loading the COCO dataset \u00b6","text":"<p>The FiftyOne Dataset Zoo provides support for loading both the COCO-2014 and COCO-2017 datasets.</p> <p>Like all other zoo datasets, you can use <code>load_zoo_dataset()</code> to download and load a COCO split into FiftyOne:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Download and load the validation split of COCO-2017\ndataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>FiftyOne supports loading annotations for the detection task, including bounding boxes and segmentations.</p> <p>By default, only the bounding boxes are loaded, but you can customize which label types are loaded via the optional <code>label_types</code> argument (see below for details).</p> <p>Note</p> <p>We will soon support loading labels for the keypoints, captions, and panoptic segmentation tasks as well. Stay tuned!</p> <p>In addition, FiftyOne provides parameters that can be used to efficiently download specific subsets of the COCO dataset, allowing you to quickly explore different slices of the dataset without downloading the entire split.</p> <p>When performing partial downloads, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from the web.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n#\n# Load 50 random samples from the validation split\n#\n# Only the required images will be downloaded (if necessary).\n# By default, only detections are loaded\n#\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    max_samples=50,\n    shuffle=True,\n)\n\nsession = fo.launch_app(dataset)\n\n#\n# Load segmentations for 25 samples from the validation split that\n# contain cats and dogs\n#\n# Images that contain all `classes` will be prioritized first, followed\n# by images that contain at least one of the required `classes`. If\n# there are not enough images matching `classes` in the split to meet\n# `max_samples`, only the available images will be loaded.\n#\n# Images will only be downloaded if necessary\n#\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    label_types=[\"segmentations\"],\n    classes=[\"cat\", \"dog\"],\n    max_samples=25,\n)\n\nsession.dataset = dataset\n</code></pre> <p>The following parameters are available to configure partial downloads of both COCO-2014 and COCO-2017 by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If neither is provided, all available splits are loaded</p> </li> <li> <p>label_types ( None): a label type or list of label types to load. Supported values are <code>(\"detections\", \"segmentations\")</code>. By default, only detections are loaded</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded</p> </li> <li> <p>image_ids ( None): a list of specific image IDs to load. The IDs can be specified either as <code>&lt;split&gt;/&lt;image-id&gt;</code> strings or <code>&lt;image-id&gt;</code> ints of strings. Alternatively, you can provide the path to a TXT (newline-separated), JSON, or CSV file containing the list of image IDs to load in either of the first two formats</p> </li> <li> <p>include_id ( False): whether to include the COCO ID of each sample in the loaded labels</p> </li> <li> <p>include_license ( False): whether to include the COCO license of each sample in the loaded labels, if available. The supported values are:</p> </li> <li> <p><code>\"False\"</code> (default): don\u2019t load the license</p> </li> <li> <p><code>True</code>/ <code>\"name\"</code>: store the string license name</p> </li> <li> <p><code>\"id\"</code>: store the integer license ID</p> </li> <li> <p><code>\"url\"</code>: store the license URL</p> </li> <li> <p>only_matching ( False): whether to only load labels that match the <code>classes</code> or <code>attrs</code> requirements that you provide (True), or to load all labels for samples that match the requirements (False)</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual images. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>label_types</code> and/or <code>classes</code> are also specified, first priority will be given to samples that contain all of the specified label types and/or classes, followed by samples that contain at least one of the specified labels types or classes. The actual number of samples loaded may be less than this maximum value if the dataset does not contain sufficient samples matching your requirements</p> </li> </ul> <p>Note</p> <p>See <code>COCO2017Dataset</code> and <code>COCODetectionDatasetImporter</code> for complete descriptions of the optional keyword arguments that you can pass to <code>load_zoo_dataset()</code>.</p>"},{"location":"integrations/coco/#loading-coco-formatted-data","title":"Loading COCO-formatted data \u00b6","text":"<p>In addition to loading the COCO datasets themselves, FiftyOne also makes it easy to load your own datasets and model predictions stored in COCO format.</p> <p>The example code below demonstrates this workflow. First, we generate a JSON file containing COCO-formatted labels to work with:</p> <pre><code>import os\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# The directory in which the dataset's images are stored\nIMAGES_DIR = os.path.dirname(dataset.first().filepath)\n\n# Export some labels in COCO format\ndataset.take(5, seed=51).export(\n    dataset_type=fo.types.COCODetectionDataset,\n    label_field=\"ground_truth\",\n    labels_path=\"/tmp/coco.json\",\n)\n</code></pre> <p>Now we have a <code>/tmp/coco.json</code> file on disk containing COCO labels corresponding to the images in <code>IMAGES_DIR</code>:</p> <pre><code>python -m json.tool /tmp/coco.json\n</code></pre> <pre><code>{\n    \"info\": {...},\n    \"licenses\": [],\n    \"categories\": [\\\n        {\\\n            \"id\": 1,\\\n            \"name\": \"airplane\",\\\n            \"supercategory\": null\\\n        },\\\n        ...\\\n    ],\n    \"images\": [\\\n        {\\\n            \"id\": 1,\\\n            \"file_name\": \"003486.jpg\",\\\n            \"height\": 427,\\\n            \"width\": 640,\\\n            \"license\": null,\\\n            \"coco_url\": null\\\n        },\\\n        ...\\\n    ],\n    \"annotations\": [\\\n        {\\\n            \"id\": 1,\\\n            \"image_id\": 1,\\\n            \"category_id\": 1,\\\n            \"bbox\": [\\\n                34.34,\\\n                147.46,\\\n                492.69,\\\n                192.36\\\n            ],\\\n            \"area\": 94773.8484,\\\n            \"iscrowd\": 0\\\n        },\\\n        ...\\\n    ]\n}\n</code></pre> <p>We can now use <code>Dataset.from_dir()</code> to load the COCO-formatted labels into a new FiftyOne dataset:</p> <pre><code># Load COCO formatted dataset\ncoco_dataset = fo.Dataset.from_dir(\n    dataset_type=fo.types.COCODetectionDataset,\n    data_path=IMAGES_DIR,\n    labels_path=\"/tmp/coco.json\",\n    include_id=True,\n)\n\n# COCO categories are also imported\nprint(coco_dataset.info[\"categories\"])\n# [{'id': 1, 'name': 'airplane', 'supercategory': None}, ...]\n\nprint(coco_dataset)\n</code></pre> <pre><code>Name:        2021.06.28.15.14.38\nMedia type:  image\nNum samples: 5\nPersistent:  False\nTags:        []\nSample fields:\n    id:               fiftyone.core.fields.ObjectIdField\n    filepath:         fiftyone.core.fields.StringField\n    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    created_at:       fiftyone.core.fields.DateTimeField\n    last_modified_at: fiftyone.core.fields.DateTimeField\n    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    coco_id:          fiftyone.core.fields.IntField\n</code></pre> <p>In the above call to <code>Dataset.from_dir()</code>, we provide the <code>data_path</code> and <code>labels_path</code> parameters to specify the location of the source images and their COCO labels, respectively, and we set <code>include_id=True</code> so that the COCO ID for each image from our JSON labels will be added to each imported sample.</p> <p>Note</p> <p>See <code>COCODetectionDatasetImporter</code> for complete descriptions of the optional keyword arguments that you can pass to <code>Dataset.from_dir()</code>.</p> <p>If your workflow generates model predictions in COCO format, you can use the <code>add_coco_labels()</code> utility method to add them to your dataset as follows:</p> <pre><code>import fiftyone.utils.coco as fouc\n\n#\n# Mock COCO predictions, where:\n# - `image_id` corresponds to the `coco_id` field of `coco_dataset`\n# - `category_id` corresponds to `coco_dataset.info[\"categories\"]`\n#\npredictions = [\\\n    {\"image_id\": 1, \"category_id\": 2, \"bbox\": [258, 41, 348, 243], \"score\": 0.87},\\\n    {\"image_id\": 2, \"category_id\": 4, \"bbox\": [61, 22, 504, 609], \"score\": 0.95},\\\n]\ncategories = coco_dataset.info[\"categories\"]\n\n# Add COCO predictions to `predictions` field of dataset\nfouc.add_coco_labels(coco_dataset, \"predictions\", predictions, categories)\n\n# Verify that predictions were added to two images\nprint(coco_dataset.count(\"predictions\"))  # 2\n</code></pre>"},{"location":"integrations/coco/#coco-style-evaluation","title":"COCO-style evaluation \u00b6","text":"<p>By default, <code>evaluate_detections()</code> will use COCO-style evaluation to analyze predictions.</p> <p>You can also explicitly request that COCO-style evaluation be used by setting the <code>method</code> parameter to <code>\"coco\"</code>.</p> <p>See this page for more information about using FiftyOne to analyze object detection models.</p> <p>Note</p> <p>FiftyOne\u2019s implementation of COCO-style evaluation matches the reference implementation available via pycocotools.</p>"},{"location":"integrations/coco/#overview","title":"Overview \u00b6","text":"<p>When running COCO-style evaluation using <code>evaluate_detections()</code>:</p> <ul> <li> <p>Predicted and ground truth objects are matched using a specified IoU threshold (default = 0.50). This threshold can be customized via the <code>iou</code> parameter</p> </li> <li> <p>By default, only objects with the same <code>label</code> will be matched. Classwise matching can be disabled via the <code>classwise</code> parameter</p> </li> <li> <p>Ground truth objects can have an <code>iscrowd</code> attribute that indicates whether the annotation contains a crowd of objects. Multiple predictions can be matched to crowd ground truth objects. The name of this attribute can be customized by passing the optional <code>iscrowd</code> attribute of <code>COCOEvaluationConfig</code> to <code>evaluate_detections()</code></p> </li> </ul> <p>When you specify an <code>eval_key</code> parameter, a number of helpful fields will be populated on each sample and its predicted/ground truth objects:</p> <ul> <li>True positive (TP), false positive (FP), and false negative (FN) counts for the each sample are saved in top-level fields of each sample:</li> </ul> <pre><code>TP: sample.&lt;eval_key&gt;_tp\nFP: sample.&lt;eval_key&gt;_fp\nFN: sample.&lt;eval_key&gt;_fn\n</code></pre> <ul> <li>The fields listed below are populated on each individual object instance; these fields tabulate the TP/FP/FN status of the object, the ID of the matching object (if any), and the matching IoU:</li> </ul> <pre><code>TP/FP/FN: object.&lt;eval_key&gt;\n        ID: object.&lt;eval_key&gt;_id\n       IoU: object.&lt;eval_key&gt;_iou\n</code></pre> <p>Note</p> <p>See <code>COCOEvaluationConfig</code> for complete descriptions of the optional keyword arguments that you can pass to <code>evaluate_detections()</code> when running COCO-style evaluation.</p>"},{"location":"integrations/coco/#example-evaluation","title":"Example evaluation \u00b6","text":"<p>The example below demonstrates COCO-style detection evaluation on the quickstart dataset from the Dataset Zoo:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nprint(dataset)\n\n# Evaluate the objects in the `predictions` field with respect to the\n# objects in the `ground_truth` field\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    method=\"coco\",\n    eval_key=\"eval\",\n)\n\n# Get the 10 most common classes in the dataset\ncounts = dataset.count_values(\"ground_truth.detections.label\")\nclasses = sorted(counts, key=counts.get, reverse=True)[:10]\n\n# Print a classification report for the top-10 classes\nresults.print_report(classes=classes)\n\n# Print some statistics about the total TP/FP/FN counts\nprint(\"TP: %d\" % dataset.sum(\"eval_tp\"))\nprint(\"FP: %d\" % dataset.sum(\"eval_fp\"))\nprint(\"FN: %d\" % dataset.sum(\"eval_fn\"))\n\n# Create a view that has samples with the most false positives first, and\n# only includes false positive boxes in the `predictions` field\nview = (\n    dataset\n    .sort_by(\"eval_fp\", reverse=True)\n    .filter_labels(\"predictions\", F(\"eval\") == \"fp\")\n)\n\n# Visualize results in the App\nsession = fo.launch_app(view=view)\n</code></pre> <pre><code>               precision    recall  f1-score   support\n\n       person       0.45      0.74      0.56       783\n         kite       0.55      0.72      0.62       156\n          car       0.12      0.54      0.20        61\n         bird       0.63      0.67      0.65       126\n       carrot       0.06      0.49      0.11        47\n         boat       0.05      0.24      0.08        37\n    surfboard       0.10      0.43      0.17        30\n     airplane       0.29      0.67      0.40        24\ntraffic light       0.22      0.54      0.31        24\n        bench       0.10      0.30      0.15        23\n\n    micro avg       0.32      0.68      0.43      1311\n    macro avg       0.26      0.54      0.32      1311\n weighted avg       0.42      0.68      0.50      1311\n</code></pre> <p></p>"},{"location":"integrations/coco/#map-and-pr-curves","title":"mAP and PR curves \u00b6","text":"<p>You can compute mean average precision (mAP), mean average recall (mAR), and precision-recall (PR) curves for your predictions by passing the <code>compute_mAP=True</code> flag to <code>evaluate_detections()</code>:</p> <p>Note</p> <p>All mAP and mAR calculations are performed according to the COCO evaluation protocol.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nprint(dataset)\n\n# Performs an IoU sweep so that mAP, mAR, and PR curves can be computed\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    method=\"coco\",\n    compute_mAP=True,\n)\n\nprint(results.mAP())\n# 0.3957\n\nprint(results.mAR())\n# 0.5210\n\nplot = results.plot_pr_curves(classes=[\"person\", \"kite\", \"car\"])\nplot.show()\n</code></pre> <p></p>"},{"location":"integrations/coco/#confusion-matrices","title":"Confusion matrices \u00b6","text":"<p>You can also easily generate confusion matrices for the results of COCO-style evaluations.</p> <p>In order for the confusion matrix to capture anything other than false positive/negative counts, you will likely want to set the <code>classwise</code> parameter to <code>False</code> during evaluation so that predicted objects can be matched with ground truth objects of different classes.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Perform evaluation, allowing objects to be matched between classes\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    method=\"coco\",\n    classwise=False,\n)\n\n# Generate a confusion matrix for the specified classes\nplot = results.plot_confusion_matrix(classes=[\"car\", \"truck\", \"motorcycle\"])\nplot.show()\n</code></pre> <p></p> <p>Note</p> <p>Did you know? Confusion matrices can be attached to your <code>Session</code> object and dynamically explored using FiftyOne\u2019s interactive plotting features!</p>"},{"location":"integrations/coco/#map-protocol","title":"mAP protocol \u00b6","text":"<p>The COCO evaluation protocol is a popular evaluation protocol used by many works in the computer vision community.</p> <p>COCO-style mAP is derived from VOC-style evaluation with the addition of a crowd attribute and an IoU sweep.</p> <p>The steps to compute COCO-style mAP are detailed below.</p> <p>Preprocessing</p> <ul> <li> <p>Filter ground truth and predicted objects by class (unless <code>classwise=False</code>)</p> </li> <li> <p>Sort predicted objects by confidence score so high confidence objects are matched first. Only the top 100 predictions are factored into evaluation (configurable with <code>max_preds</code>)</p> </li> <li> <p>Sort ground truth objects so <code>iscrowd</code> objects are matched last</p> </li> <li> <p>Compute IoU between every ground truth and predicted object within the same class (and between classes if <code>classwise=False</code>) in each image</p> </li> <li> <p>IoU between predictions and crowd objects is calculated as the intersection of both boxes divided by the area of the prediction only. A prediction fully inside the crowd box has an IoU of 1</p> </li> </ul> <p>Matching</p> <p>Once IoUs have been computed, predictions and ground truth objects are matched to compute true positives, false positives, and false negatives:</p> <ul> <li> <p>For each class, start with the highest confidence prediction, match it to the ground truth object that it overlaps with the highest IoU. A prediction only matches if the IoU is above the specified <code>iou</code> threshold</p> </li> <li> <p>If a prediction matched to a non-crowd object, it will not match to a crowd even if the IoU is higher</p> </li> <li> <p>Multiple predictions can match to the same crowd ground truth object, each counting as a true positive</p> </li> <li> <p>If a prediction maximally overlaps with a ground truth object that has already been matched (by a higher confidence prediction), the prediction is matched with the next highest IoU ground truth object</p> </li> <li> <p>(Only relevant if <code>classwise=False</code>) predictions can only match to crowds if they are of the same class</p> </li> </ul> <p>Computing mAP</p> <ul> <li> <p>Compute matches for 10 IoU thresholds from 0.5 to 0.95 in increments of 0.05</p> </li> <li> <p>The next 6 steps are computed separately for each class and IoU threshold:</p> </li> <li> <p>Construct a boolean array of true positives and false positives, sorted ( via mergesort) by confidence</p> </li> <li> <p>Compute the cumulative sum of the true positive and false positive array</p> </li> <li> <p>Compute precision by elementwise dividing the TP-FP-sum array by the total number of predictions up to that point</p> </li> <li> <p>Compute recall by elementwise dividing TP-FP-sum array by the number of ground truth objects for the class</p> </li> <li> <p>Ensure that precision is a non-increasing array</p> </li> <li> <p>Interpolate precision values so that they can be plotted with an array of 101 evenly spaced recall values</p> </li> <li> <p>For every class that contains at least one ground truth object, compute the average precision (AP) by averaging the precision values over all 10 IoU thresholds. Then compute mAP by averaging the per-class AP values over all classes</p> </li> </ul>"},{"location":"integrations/cvat/","title":"CVAT Integration \u00b6","text":"<p>CVAT is one of the most popular open-source image and video annotation tools available, and we\u2019ve made it easy to upload your data directly from FiftyOne to CVAT to add or edit labels.</p> <p>You can use CVAT either through the hosted server at app.cvat.ai or through a self-hosted server. In either case, FiftyOne provides simple setup instructions that you can use to specify the necessary account credentials and server endpoint to use.</p> <p>Note</p> <p>Did you know? You can request, manage, and import annotations from within the FiftyOne App by installing the @voxel51/annotation plugin!</p> <p>CVAT provides three levels of abstraction for annotation workflows: projects, tasks, and jobs. A job contains one or more images and can be assigned to a specific annotator or reviewer. A task defines the label schema to use for annotation and contains one or more jobs. A project can optionally be created to group multiple tasks together under a shared label schema.</p> <p>FiftyOne provides an API to create tasks and jobs, upload data, define label schemas, and download annotations using CVAT, all programmatically in Python. All of the following label types are supported, for both image and video datasets:</p> <ul> <li> <p>Classifications</p> </li> <li> <p>Detections</p> </li> <li> <p>Instance segmentations</p> </li> <li> <p>Polygons and polylines</p> </li> <li> <p>Keypoints</p> </li> <li> <p>Scalar fields</p> </li> <li> <p>Semantic segmentation</p> </li> </ul> <p></p> <p>Note</p> <p>Check out this tutorial to see how you can use FiftyOne to upload your data to CVAT to create, delete, and fix annotations.</p>"},{"location":"integrations/cvat/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The basic workflow to use CVAT to add or edit labels on your FiftyOne datasets is as follows:</p> <ol> <li> <p>Load a labeled or unlabeled dataset into FiftyOne</p> </li> <li> <p>Explore the dataset using the App or dataset views to locate either unlabeled samples that you wish to annotate or labeled samples whose annotations you want to edit</p> </li> <li> <p>Use the <code>annotate()</code> method on your dataset or view to upload the samples and optionally their existing labels to CVAT</p> </li> <li> <p>In CVAT, perform the necessary annotation work</p> </li> <li> <p>Back in FiftyOne, load your dataset and use the <code>load_annotations()</code> method to merge the annotations back into your FiftyOne dataset</p> </li> <li> <p>If desired, delete the CVAT tasks and the record of the annotation run from your FiftyOne dataset</p> </li> </ol> <p>The example below demonstrates this workflow.</p> <p>Note</p> <p>You must create an account at app.cvat.ai in order to run this example.</p> <p>Note that you can store your credentials as described in this section to avoid entering them manually each time you interact with CVAT.</p> <p>First, we create the annotation tasks in CVAT:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\n# Step 1: Load your data into FiftyOne\n\ndataset = foz.load_zoo_dataset(\n    \"quickstart\", dataset_name=\"cvat-annotation-example\"\n)\ndataset.persistent = True\n\ndataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\"\n)\n\n# Step 2: Locate a subset of your data requiring annotation\n\n# Create a view that contains only high confidence false positive model\n# predictions, with samples containing the most false positives first\nmost_fp_view = (\n    dataset\n    .filter_labels(\"predictions\", (F(\"confidence\") &gt; 0.8) &amp; (F(\"eval\") == \"fp\"))\n    .sort_by(F(\"predictions.detections\").length(), reverse=True)\n)\n\n# Let's edit the ground truth annotations for the sample with the most\n# high confidence false positives\nsample_id = most_fp_view.first().id\nview = dataset.select(sample_id)\n\n# Step 3: Send samples to CVAT\n\n# A unique identifier for this run\nanno_key = \"cvat_basic_recipe\"\n\nview.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    attributes=[\"iscrowd\"],\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Step 4: Perform annotation in CVAT and save the tasks\n</code></pre> <p>Then, once the annotation work is complete, we merge the annotations back into FiftyOne:</p> <pre><code>import fiftyone as fo\n\nanno_key = \"cvat_basic_recipe\"\n\n# Step 5: Merge annotations back into FiftyOne dataset\n\ndataset = fo.load_dataset(\"cvat-annotation-example\")\ndataset.load_annotations(anno_key)\n\n# Load the view that was annotated in the App\nview = dataset.load_annotation_view(anno_key)\nsession = fo.launch_app(view=view)\n\n# Step 6: Cleanup\n\n# Delete tasks from CVAT\nresults = dataset.load_annotation_results(anno_key)\nresults.cleanup()\n\n# Delete run record (not the labels) from FiftyOne\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>Note</p> <p>Skip to this section to see a variety of common CVAT annotation patterns.</p>"},{"location":"integrations/cvat/#setup","title":"Setup \u00b6","text":"<p>FiftyOne supports both app.cvat.ai and self-hosted servers.</p> <p>The easiest way to get started is to use the default server app.cvat.ai, which simply requires creating an account and then providing your authentication credentials as shown below.</p> <p>Note</p> <p>CVAT is the default annotation backend used by FiftyOne. However, if you have changed your default backend, you can opt-in to using CVAT on a one-off basis by passing the optional <code>backend</code> parameter to <code>annotate()</code>:</p> <pre><code>view.annotate(anno_key, backend=\"cvat\", ...)\n</code></pre> <p>Refer to these instructions to see how to permanently change your default backend.</p>"},{"location":"integrations/cvat/#authentication","title":"Authentication \u00b6","text":"<p>In order to connect to a CVAT server, you must provide your login credentials, which can be done in a variety of ways.</p> <p>Environment variables (recommended)</p> <p>The recommended way to configure your CVAT login credentials is to store them in the <code>FIFTYONE_CVAT_USERNAME</code> and <code>FIFTYONE_CVAT_PASSWORD</code> environment variables. These are automatically accessed by FiftyOne whenever a connection to CVAT is made.</p> <pre><code>export FIFTYONE_CVAT_USERNAME=...\nexport FIFTYONE_CVAT_PASSWORD=...\nexport FIFTYONE_CVAT_EMAIL=...  # if applicable\n</code></pre> <p>FiftyOne annotation config</p> <p>You can also store your credentials in your annotation config located at <code>~/.fiftyone/annotation_config.json</code>:</p> <pre><code>{\n    \"backends\": {\n        \"cvat\": {\n            ...\n            \"username\": ...,\n            \"password\": ...,\n            \"email\": ...  # if applicable\n        }\n    }\n}\n</code></pre> <p>Note that this file will not exist until you create it.</p> <p>Warning</p> <p>Storing your username and password in plain text on disk is generally not recommended. Consider using environment variables instead.</p> <p>Keyword arguments</p> <p>You can manually provide your login credentials as keyword arguments each time you call methods like <code>annotate()</code> and <code>load_annotations()</code> that require connections to CVAT:</p> <pre><code>view.annotate(anno_key, ..., username=..., password=...)\n</code></pre> <p>Command line prompt</p> <p>If you have not stored your login credentials via another method, you will be prompted to enter them interactively in your shell each time you call a method that requires a connection to CVAT:</p> <pre><code>view.annotate(anno_key, label_field=\"ground_truth\", launch_editor=True)\n</code></pre> <pre><code>Please enter your login credentials.\nYou can avoid this in the future by setting your `FIFTYONE_CVAT_USERNAME` and `FIFTYONE_CVAT_PASSWORD` environment variables.\nUsername: ...\nPassword: ...\n</code></pre>"},{"location":"integrations/cvat/#self-hosted-servers","title":"Self-hosted servers \u00b6","text":"<p>If you wish to use a self-hosted server, you can configure the URL of your server in any of the following ways:</p> <ul> <li>Set the <code>FIFTYONE_CVAT_URL</code> environment variable:</li> </ul> <pre><code>export FIFTYONE_CVAT_URL=http://localhost:8080\n</code></pre> <ul> <li>Store the <code>url</code> of your server in your annotation config at <code>~/.fiftyone/annotation_config.json</code>:</li> </ul> <pre><code>{\n    \"backends\": {\n        \"cvat\": {\n            \"url\": \"http://localhost:8080\",\n            ...\n        }\n    }\n}\n</code></pre> <ul> <li>Pass the <code>url</code> parameter manually each time you call <code>annotate()</code>:</li> </ul> <pre><code>view.annotate(anno_key, ..., url=\"http://localhost:8080\")\n</code></pre> <p>If your self-hosted server requires additional headers in order to make HTTP requests, you can provide them in either of the following ways:</p> <ul> <li>Store your custom headers in a <code>headers</code> key of your annotation config at <code>~/.fiftyone/annotation_config.json</code>:</li> </ul> <pre><code>{\n    \"backends\": {\n        \"cvat\": {\n            ...\n            \"headers\": {\n                \"&lt;name&gt;\": \"&lt;value&gt;\",\n                ...\n            }\n        }\n    }\n}\n</code></pre> <ul> <li>Pass the <code>headers</code> parameter manually each time you call <code>annotate()</code> and <code>load_annotations()</code>:</li> </ul> <pre><code>view.annotate(anno_key, ... headers=...)\nview.load_annotations(anno_key, ... headers=...)\n</code></pre>"},{"location":"integrations/cvat/#requesting-annotations","title":"Requesting annotations \u00b6","text":"<p>Use the <code>annotate()</code> method to send the samples and optionally existing labels in a <code>Dataset</code> or <code>DatasetView</code> to CVAT for annotation.</p> <p>The basic syntax is:</p> <pre><code>anno_key = \"...\"\nview.annotate(anno_key, ...)\n</code></pre> <p>The <code>anno_key</code> argument defines a unique identifier for the annotation run, and you will provide it to methods like <code>load_annotations()</code>, <code>get_annotation_info()</code>, <code>load_annotation_results()</code>, <code>rename_annotation_run()</code>, and <code>delete_annotation_run()</code> to manage the run in the future.</p> <p>Warning</p> <p>FiftyOne assumes that all labels in an annotation run can fit in memory.</p> <p>If you are annotating very large scale video datasets with dense frame labels, you may violate this assumption. Instead, consider breaking the work into multiple smaller annotation runs that each contain limited subsets of the samples you wish to annotate.</p> <p>You can use <code>Dataset.stats()</code> to get a sense for the total size of the labels in a dataset as a rule of thumb to estimate the size of a candidate annotation run.</p> <p>In addition, <code>annotate()</code> provides various parameters that you can use to customize the annotation tasks that you wish to be performed.</p> <p>The following parameters are supported by all annotation backends:</p> <ul> <li> <p>backend ( None): the annotation backend to use. Use <code>\"cvat\"</code> for the CVAT backend. The supported values are <code>fiftyone.annotation_config.backends.keys()</code> and the default is <code>fiftyone.annotation_config.default_backend</code></p> </li> <li> <p>media_field ( \u201cfilepath\u201d): the sample field containing the path to the source media to upload</p> </li> <li> <p>launch_editor ( False): whether to launch the annotation backend\u2019s editor after uploading the samples</p> </li> </ul> <p>The following parameters allow you to configure the labeling schema to use for your annotation tasks. See this section for more details:</p> <ul> <li> <p>label_schema ( None): a dictionary defining the label schema to use. If this argument is provided, it takes precedence over <code>label_field</code> and <code>label_type</code></p> </li> <li> <p>label_field ( None): a string indicating a new or existing label field to annotate</p> </li> <li> <p>label_type ( None): a string indicating the type of labels to annotate. The possible label types are:</p> </li> <li> <p><code>\"classification\"</code>: a single classification stored in     <code>Classification</code> fields</p> </li> <li> <p><code>\"classifications\"</code>: multilabel classifications stored in     <code>Classifications</code> fields</p> </li> <li> <p><code>\"detections\"</code>: object detections stored in <code>Detections</code> fields</p> </li> <li> <p><code>\"instances\"</code>: instance segmentations stored in <code>Detections</code> fields     with their <code>mask</code>     attributes populated</p> </li> <li> <p><code>\"polylines\"</code>: polylines stored in <code>Polylines</code> fields with their     <code>filled</code> attributes set to     <code>False</code></p> </li> <li> <p><code>\"polygons\"</code>: polygons stored in <code>Polylines</code> fields with their     <code>filled</code> attributes set to     <code>True</code></p> </li> <li> <p><code>\"keypoints\"</code>: keypoints stored in <code>Keypoints</code> fields</p> </li> <li> <p><code>\"segmentation\"</code>: semantic segmentations stored in <code>Segmentation</code>     fields</p> </li> <li> <p><code>\"scalar\"</code>: scalar labels stored in <code>IntField</code>, <code>FloatField</code>,     <code>StringField</code>, or <code>BooleanField</code> fields</p> </li> </ul> <p>All new label fields must have their type specified via this argument or in <code>label_schema</code></p> <ul> <li> <p>classes ( None): a list of strings indicating the class options for <code>label_field</code> or all fields in <code>label_schema</code> without classes specified. All new label fields must have a class list provided via one of the supported methods. For existing label fields, if classes are not provided by this argument nor <code>label_schema</code>, the observed labels on your dataset are used</p> </li> <li> <p>attributes ( True): specifies the label attributes of each label field to include (other than their <code>label</code>, which is always included) in the annotation export. Can be any of the following:</p> </li> <li> <p><code>True</code>: export all label attributes</p> </li> <li> <p><code>False</code>: don\u2019t export any custom label attributes</p> </li> <li> <p>a list of label attributes to export</p> </li> <li> <p>a dict mapping attribute names to dicts specifying the <code>type</code>,     <code>values</code>, and <code>default</code> for each attribute</p> </li> </ul> <p>If a <code>label_schema</code> is also provided, this parameter determines which attributes are included for all fields that do not explicitly define their per-field attributes (in addition to any per-class attributes)</p> <ul> <li> <p>mask_targets ( None): a dict mapping pixel values to semantic label strings. Only applicable when annotating semantic segmentations</p> </li> <li> <p>allow_additions ( True): whether to allow new labels to be added. Only applicable when editing existing label fields</p> </li> <li> <p>allow_deletions ( True): whether to allow labels to be deleted. Only applicable when editing existing label fields</p> </li> <li> <p>allow_label_edits ( True): whether to allow the <code>label</code> attribute of existing labels to be modified. Only applicable when editing existing fields with <code>label</code> attributes</p> </li> <li> <p>allow_index_edits ( True): whether to allow the <code>index</code> attribute of existing video tracks to be modified. Only applicable when editing existing frame fields with <code>index</code> attributes</p> </li> <li> <p>allow_spatial_edits ( True): whether to allow edits to the spatial properties (bounding boxes, vertices, keypoints, masks, etc) of labels. Only applicable when editing existing spatial label fields</p> </li> </ul> <p>In addition, the following CVAT-specific parameters from <code>CVATBackendConfig</code> can also be provided:</p> <ul> <li> <p>task_size ( None): an optional maximum number of images to upload per task. Videos are always uploaded one per task</p> </li> <li> <p>segment_size ( None): the maximum number of images to upload per job. Not applicable to videos</p> </li> <li> <p>image_quality ( 75): an int in <code>[0, 100]</code> determining the image quality to upload to CVAT</p> </li> <li> <p>use_cache ( True): whether to use a cache when uploading data. Using a cache reduces task creation time as data will be processed on-the-fly and stored in the cache when requested</p> </li> <li> <p>use_zip_chunks ( True): when annotating videos, whether to upload video frames in smaller chunks. Setting this option to <code>False</code> may result in reduced video quality in CVAT due to size limitations on ZIP files that can be uploaded to CVAT</p> </li> <li> <p>chunk_size ( None): the number of frames to upload per ZIP chunk</p> </li> <li> <p>task_assignee ( None): the username to assign the generated tasks. This argument can be a list of usernames when annotating videos as each video is uploaded to a separate task</p> </li> <li> <p>job_assignees ( None): a list of usernames to assign jobs</p> </li> <li> <p>job_reviewers ( None): a list of usernames to assign job reviews. Only available in CVAT v1 servers</p> </li> <li> <p>project_name ( None): an optional project name to which to upload the created CVAT task. If a project with this name exists, it will be used, otherwise a new project is created. By default, no project is used</p> </li> <li> <p>project_id ( None): an optional ID of an existing CVAT project to which to upload the annotation tasks. By default, no project is used</p> </li> <li> <p>task_name (None): an optional task name to use for the created CVAT task</p> </li> <li> <p>occluded_attr ( None): an optional attribute name containing existing occluded values and/or in which to store downloaded occluded values for all objects in the annotation run</p> </li> <li> <p>group_id_attr ( None): an optional attribute name containing existing group ids and/or in which to store downloaded group ids for all objects in the annotation run</p> </li> <li> <p>issue_tracker ( None): URL(s) of an issue tracker to link to the created task(s). This argument can be a list of URLs when annotating videos or when using <code>task_size</code> and generating multiple tasks</p> </li> <li> <p>organization ( None): the name of the organization to use when sending requests to CVAT</p> </li> <li> <p>frame_start ( None): nonnegative integer(s) defining the first frame of videos to upload when creating video tasks. Supported values are:</p> </li> <li> <p><code>integer</code>: the first frame to upload for each video</p> </li> <li> <p><code>list</code>: a list of first frame integers corresponding to videos in the     given samples</p> </li> <li> <p><code>dict</code>: a dictionary mapping sample filepaths to first frame integers     to use for the corresponding videos</p> </li> <li> <p>frame_stop ( None): nonnegative integer(s) defining the last frame of videos to upload when creating video tasks. Supported values are:</p> </li> <li> <p><code>integer</code>: the last frame to upload for each video</p> </li> <li> <p><code>list</code>: a list of last frame integers corresponding to videos in the     given samples</p> </li> <li> <p><code>dict</code>: a dictionary mapping sample filepaths to last frame integers to     use for the corresponding videos</p> </li> <li> <p>frame_step ( None): positive integer(s) defining which frames to sample when creating video tasks. Supported values are:</p> </li> <li> <p><code>integer</code>: the frame step to apply to each video task</p> </li> <li> <p><code>list</code>: a list of frame step integers corresponding to videos in the     given samples</p> </li> <li> <p><code>dict</code>: a dictionary mapping sample filepaths to frame step integers to     use for the corresponding videos</p> </li> </ul> <p>Note that this argument cannot be provided when uploading existing tracks</p>"},{"location":"integrations/cvat/#label-schema","title":"Label schema \u00b6","text":"<p>The <code>label_schema</code>, <code>label_field</code>, <code>label_type</code>, <code>classes</code>, <code>attributes</code>, and <code>mask_targets</code> parameters to <code>annotate()</code> allow you to define the annotation schema that you wish to be used.</p> <p>The label schema may define new label field(s) that you wish to populate, and it may also include existing label field(s), in which case you can add, delete, or edit the existing labels on your FiftyOne dataset.</p> <p>The <code>label_schema</code> argument is the most flexible way to define how to construct tasks in CVAT. In its most verbose form, it is a dictionary that defines the label type, annotation type, possible classes, and possible attributes for each label field:</p> <pre><code>anno_key = \"...\"\n\nlabel_schema = {\n    \"new_field\": {\n        \"type\": \"classifications\",\n        \"classes\": [\"class1\", \"class2\"],\n        \"attributes\": {\n            \"attr1\": {\n                \"type\": \"select\",\n                \"values\": [\"val1\", \"val2\"],\n                \"default\": \"val1\",\n            },\n            \"attr2\": {\n                \"type\": \"radio\",\n                \"values\": [True, False],\n                \"default\": False,\n            }\n        },\n    },\n    \"existing_field\": {\n        \"classes\": [\"class3\", \"class4\"],\n        \"attributes\": {\n            \"attr3\": {\n                \"type\": \"text\",\n            }\n        }\n    },\n}\n\ndataset.annotate(anno_key, label_schema=label_schema)\n</code></pre> <p>You can also define class-specific attributes by setting elements of the <code>classes</code> list to dicts that specify groups of <code>classes</code> and their corresponding <code>attributes</code>. For example, in the configuration below, <code>attr1</code> only applies to <code>class1</code> and <code>class2</code> while <code>attr2</code> applies to all classes:</p> <pre><code>anno_key = \"...\"\n\nlabel_schema = {\n    \"new_field\": {\n        \"type\": \"detections\",\n        \"classes\": [\\\n            {\\\n                \"classes\": [\"class1\", \"class2\"],\\\n                \"attributes\": {\\\n                    \"attr1\": {\\\n                        \"type\": \"select\",\\\n                        \"values\": [\"val1\", \"val2\"],\\\n                        \"default\": \"val1\",\\\n                    }\\\n                 }\\\n            },\\\n            \"class3\",\\\n            \"class4\",\\\n        ],\n        \"attributes\": {\n            \"attr2\": {\n                \"type\": \"radio\",\n                \"values\": [True, False],\n                \"default\": False,\n            }\n        },\n    },\n}\n\ndataset.annotate(anno_key, label_schema=label_schema)\n</code></pre> <p>Alternatively, if you are only editing or creating a single label field, you can use the <code>label_field</code>, <code>label_type</code>, <code>classes</code>, <code>attributes</code>, and <code>mask_targets</code> parameters to specify the components of the label schema individually:</p> <pre><code>anno_key = \"...\"\n\nlabel_field = \"new_field\",\nlabel_type = \"classifications\"\nclasses = [\"class1\", \"class2\"]\n\n# These are optional\nattributes = {\n    \"attr1\": {\n        \"type\": \"select\",\n        \"values\": [\"val1\", \"val2\"],\n        \"default\": \"val1\",\n    },\n    \"attr2\": {\n        \"type\": \"radio\",\n        \"values\": [True, False],\n        \"default\": False,\n    }\n}\n\ndataset.annotate(\n    anno_key,\n    label_field=label_field,\n    label_type=label_type,\n    classes=classes,\n    attributes=attributes,\n)\n</code></pre> <p>When you are annotating existing label fields, you can omit some of these parameters from <code>annotate()</code>, as FiftyOne can infer the appropriate values to use:</p> <ul> <li> <p>label_type: if omitted, the <code>Label</code> type of the field will be used to infer the appropriate value for this parameter</p> </li> <li> <p>classes: if omitted for a non-semantic segmentation field, the observed labels on your dataset will be used to construct a classes list</p> </li> </ul>"},{"location":"integrations/cvat/#label-attributes","title":"Label attributes \u00b6","text":"<p>The <code>attributes</code> parameter allows you to configure whether custom attributes beyond the default <code>label</code> attribute are included in the annotation tasks.</p> <p>When adding new label fields for which you want to include attributes, you must use the dictionary syntax demonstrated below to define the schema of each attribute that you wish to label:</p> <pre><code>anno_key = \"...\"\n\nattributes = {\n    \"is_truncated\": {\n        \"type\": \"radio\",\n        \"values\": [True, False],\n        \"default\": False,\n    },\n    \"gender\": {\n        \"type\": \"select\",\n        \"values\": [\"male\", \"female\"],\n    },\n    \"caption\": {\n        \"type\": \"text\",\n    }\n}\n\nview.annotate(\n    anno_key,\n    label_field=\"new_field\",\n    label_type=\"detections\",\n    classes=[\"dog\", \"cat\", \"person\"],\n    attributes=attributes,\n)\n</code></pre> <p>You can always omit this parameter if you do not require attributes beyond the default <code>label</code>.</p> <p>For CVAT, the following <code>type</code> values are supported:</p> <ul> <li> <p><code>text</code>: a free-form text box. In this case, <code>default</code> is optional and <code>values</code> is unused</p> </li> <li> <p><code>select</code>: a selection dropdown. In this case, <code>values</code> is required and <code>default</code> is optional</p> </li> <li> <p><code>radio</code>: a radio button list UI. In this case, <code>values</code> is required and <code>default</code> is optional</p> </li> <li> <p><code>checkbox</code>: a boolean checkbox UI. In this case, <code>default</code> is optional and <code>values</code> is unused</p> </li> <li> <p><code>occluded</code>: CVAT\u2019s builtin occlusion toggle icon. This widget type can only be specified for at most one attribute, which must be a boolean</p> </li> <li> <p><code>group_id</code>: CVAT\u2019s grouping capabilities. This attribute type can only be specified for at most one attribute, which must be an integer</p> </li> </ul> <p>When you are annotating existing label fields, the <code>attributes</code> parameter can take additional values:</p> <ul> <li> <p><code>True</code> (default): export all custom attributes observed on the existing labels, using their observed values to determine the appropriate UI type and possible values, if applicable</p> </li> <li> <p><code>False</code>: do not include any custom attributes in the export</p> </li> <li> <p>a list of custom attributes to include in the export</p> </li> <li> <p>a full dictionary syntax described above</p> </li> </ul> <p>Note that only scalar-valued label attributes are supported. Other attribute types like lists, dictionaries, and arrays will be omitted.</p>"},{"location":"integrations/cvat/#restricting-additions-deletions-and-edits","title":"Restricting additions, deletions, and edits \u00b6","text":"<p>When you create annotation runs that involve editing existing label fields, you can optionally specify that certain changes are not allowed by passing the following flags to <code>annotate()</code>:</p> <ul> <li> <p>allow_additions ( True): whether to allow new labels to be added</p> </li> <li> <p>allow_deletions ( True): whether to allow labels to be deleted</p> </li> <li> <p>allow_label_edits ( True): whether to allow the <code>label</code> attribute to be modified</p> </li> <li> <p>allow_index_edits ( True): whether to allow the <code>index</code> attribute of video tracks to be modified</p> </li> <li> <p>allow_spatial_edits ( True): whether to allow edits to the spatial properties (bounding boxes, vertices, keypoints, etc) of labels</p> </li> </ul> <p>If you are using the <code>label_schema</code> parameter to provide a full annotation schema to <code>annotate()</code>, you can also directly include the above flags in the configuration dicts for any existing label field(s) you wish.</p> <p>For example, suppose you have an existing <code>ground_truth</code> field that contains objects of various types and you would like to add new <code>sex</code> and <code>age</code> attributes to all people in this field while also strictly enforcing that no objects can be added, deleted, or have their labels or bounding boxes modified. You can configure an annotation run for this as follows:</p> <pre><code>anno_key = \"...\"\n\nattributes = {\n    \"sex\": {\n        \"type\": \"select\",\n        \"values\": [\"male\", \"female\"],\n    },\n    \"age\": {\n        \"type\": \"text\",\n    },\n}\n\nview.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    classes=[\"person\"],\n    attributes=attributes,\n    allow_additions=False,\n    allow_deletions=False,\n    allow_label_edits=False,\n    allow_spatial_edits=False,\n)\n</code></pre> <p>You can also include a <code>read_only=True</code> parameter when uploading existing label attributes to specify that the attribute\u2019s value should be uploaded to the annotation backend for informational purposes, but any edits to the attribute\u2019s value should not be imported back into FiftyOne.</p> <p>For example, if you have vehicles with their <code>make</code> attribute populated and you want to populate a new <code>model</code> attribute based on this information without allowing changes to the vehicle\u2019s <code>make</code>, you can configure an annotation run for this as follows:</p> <pre><code>anno_key = \"...\"\n\nattributes = {\n    \"make\": {\n        \"type\": \"text\",\n        \"read_only\": True,\n    },\n    \"model\": {\n        \"type\": \"text\",\n    },\n}\n\nview.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    classes=[\"vehicle\"],\n    attributes=attributes,\n)\n</code></pre> <p>Note that, if you use CVAT projects to organize your annotation tasks, the above restrictions must be manually re-specified in your call to <code>annotate()</code> for each annotation task that you add to an existing project, since CVAT does not provide support for these settings natively.</p> <p>Warning</p> <p>The CVAT backend does not support restrictions to additions, deletions, spatial edits, and read-only attributes in its editing interface.</p> <p>However, any restrictions that you specify via the above parameters will still be enforced when you call <code>load_annotations()</code> to merge the annotations back into FiftyOne.</p> <p>IMPORTANT: When uploading existing labels to CVAT, the <code>id</code> of the labels in FiftyOne are stored in a <code>label_id</code> attribute of the CVAT shapes. If a <code>label_id</code> is modified in CVAT, then FiftyOne may not be able to merge the annotation with its existing <code>Label</code> instance; it must instead delete the existing label and create a new <code>Label</code> with the shape\u2019s contents. In such cases, if <code>allow_additions</code> and/or <code>allow_deletions</code> were set to <code>False</code> on the annotation schema, this can result in CVAT edits being rejected. See this section for details.</p>"},{"location":"integrations/cvat/#labeling-videos","title":"Labeling videos \u00b6","text":"<p>When annotating spatiotemporal objects in videos, you have a few additional options at your fingertips.</p> <p>First, each object attribute specification can include a <code>mutable</code> property that controls whether the attribute\u2019s value can change between frames for each object:</p> <pre><code>anno_key = \"...\"\n\nattributes = {\n    \"type\": {\n        \"type\": \"select\",\n        \"values\": [\"sedan\", \"suv\", \"truck\"],\n        \"mutable\": False,\n    },\n    \"visible_license_plate\": {\n        \"type\": \"radio\",\n        \"values\": [True, False],\n        \"default\": False,\n        \"mutable\": True,\n    },\n}\n\nview.annotate(\n    anno_key,\n    label_field=\"frames.new_field\",\n    label_type=\"detections\",\n    classes=[\"vehicle\"],\n    attributes=attributes,\n)\n</code></pre> <p>The meaning of the <code>mutable</code> attribute is defined as follows:</p> <ul> <li> <p><code>True</code> (default): the attribute is dynamic and can have a different value for every frame in which the object track appears</p> </li> <li> <p><code>False</code>: the attribute is static and is the same for every frame in which the object track appears</p> </li> </ul> <p>In addition, note that when you download annotation runs that include track annotations, the downloaded label corresponding to each keyframe of an object track will have its <code>keyframe=True</code> attribute set to denote that it was a keyframe.</p> <p>Similarly, when you create an annotation run on a video dataset that involves editing existing video tracks, if at least one existing label has its <code>keyframe=True</code> attribute populated, then the available keyframe information will be uploaded to CVAT.</p> <p>Note</p> <p>See this section for video annotation examples!</p> <p>Warning</p> <p>When uploading existing labels to CVAT, the <code>id</code> of the labels in FiftyOne are stored in a <code>label_id</code> attribute of the CVAT shapes.</p> <p>IMPORTANT: If a <code>label_id</code> is modified in CVAT, then FiftyOne may not be able to merge the annotation with its existing <code>Label</code> instance; in such cases, it must instead delete the existing label and create a new <code>Label</code> with the shape\u2019s contents. See this section for details.</p>"},{"location":"integrations/cvat/#cvat-limitations","title":"CVAT limitations \u00b6","text":"<p>When uploading existing labels to CVAT, FiftyOne uses two sources of provenance to associate <code>Label</code> instances in FiftyOne with their corresponding CVAT shapes:</p> <ul> <li> <p>The <code>id</code> of each <code>Label</code> is stored in a <code>label_id</code> attribute of the CVAT shape. When importing annotations from CVAT back into FiftyOne, if the <code>label_id</code> of a shape matches the ID of a label that was included in the annotation run, the shape will be merged into the existing <code>Label</code></p> </li> <li> <p>FiftyOne also maintains a mapping between <code>Label</code> IDs and the internal CVAT shape IDs that are created when the CVAT tasks are created. If, during download, a CVAT shape whose <code>label_id</code> has been deleted or otherwise modified and doesn\u2019t match an existing label ID but does have a recognized CVAT ID is encountered, this shape will be merged into the existing <code>Label</code></p> </li> </ul> <p>Unfortunately, CVAT does not guarantee that its internal IDs are immutable. Thus, if both the <code>label_id</code> attribute and (unknown to the user) the internal CVAT ID of a shape are both modified, merging the shape with its source <code>Label</code> is impossible.</p> <p>CVAT automatically clears/edits all attributes of a shape, including the <code>label_id</code> attribute, in the following cases:</p> <ul> <li> <p>When using a label schema with per-class attributes, all attributes of a shape are cleared whenever the class label of the shape is changed to a class whose attribute schema differs from the previous class. The recommended workaround in this case is to manually copy the <code>label_id</code> before changing the class and then pasting it back to ensure that the ID doesn\u2019t change.</p> </li> <li> <p>When splitting or merging video tracks, CVAT may clear or duplicate the shape\u2019s attributes during the process. If this results in missing or duplicate <code>label_id</code> values, then, although FiftyOne will gracefully proceed with the import, provenance has still been lost and thus existing <code>Label</code> instances whose IDs no longer exist must be deleted and replaced with newly created <code>Label</code> instances.</p> </li> </ul> <p>The primary issues that can arise due to modified/deleted <code>label_id</code> attributes are:</p> <ul> <li> <p>If the original <code>Label</code> in FiftyOne contained additional attributes that weren\u2019t included in the CVAT annotation run, then those attributes will be lost whenever loading annotations requires deleting the existing label and creating a new one.</p> </li> <li> <p>When working with annotation schemas that specify edit restrictions, CVAT edits that cause <code>label_id</code> changes may need to be rejected. For example, if <code>allow_additions</code> and <code>allow_deletions</code> are set to <code>False</code> and editing a CVAT shape\u2019s class label causes its attributes to be cleared, then this change will be rejected by FiftyOne because it would require both deleting an existing label and creating a new one.</p> </li> </ul> <p>Note</p> <p>Pro tip: If you are editing existing labels and only uploading a subset of their attributes to CVAT, restricting label deletions by setting <code>allow_deletions=False</code> provides a helpful guarantee that no labels will be deleted if label provenance snafus occur in CVAT.</p> <p>Note</p> <p>Pro tip: When working with annotation schemas that include per-class attributes, be sure that any class label changes that you would reasonably make all share the same attribute schemas so that unwanted <code>label_id</code> changes are not caused by CVAT.</p> <p>If a schema-altering class change must occur, remember to manually copy the <code>label_id</code> before making the change and then paste it back to ensure that the ID doesn\u2019t change.</p>"},{"location":"integrations/cvat/#loading-annotations","title":"Loading annotations \u00b6","text":"<p>After your annotations tasks in the annotation backend are complete, you can use the <code>load_annotations()</code> method to download them and merge them back into your FiftyOne dataset.</p> <pre><code>view.load_annotations(anno_key)\n</code></pre> <p>The <code>anno_key</code> parameter is the unique identifier for the annotation run that you provided when calling <code>annotate()</code>. You can use <code>list_annotation_runs()</code> to see the available keys on a dataset.</p> <p>Note</p> <p>By default, calling <code>load_annotations()</code> will not delete any information for the run from the annotation backend.</p> <p>However, you can pass <code>cleanup=True</code> to delete all information associated with the run from the backend after the annotations are downloaded.</p> <p>You can use the optional <code>dest_field</code> parameter to override the task\u2019s label schema and instead load annotations into different field name(s) of your dataset. This can be useful, for example, when editing existing annotations, if you would like to do a before/after comparison of the edits that you import. If the annotation run involves multiple fields, <code>dest_field</code> should be a dictionary mapping label schema field names to destination field names.</p> <p>Note that CVAT cannot explicitly prevent annotators from creating labels that don\u2019t obey the run\u2019s label schema. However, you can pass the optional <code>unexpected</code> parameter to <code>load_annotations()</code> to configure how to deal with any such unexpected labels that are found. The supported values are:</p> <ul> <li> <p><code>\"prompt\"</code> ( default): present an interactive prompt to direct/discard unexpected labels</p> </li> <li> <p><code>\"ignore\"</code>: automatically ignore any unexpected labels</p> </li> <li> <p><code>\"keep\"</code>: automatically keep all unexpected labels in a field whose name matches the the label type</p> </li> <li> <p><code>\"return\"</code>: return a dict containing all unexpected labels, if any</p> </li> </ul> <p>See this section for more details.</p>"},{"location":"integrations/cvat/#managing-annotation-runs","title":"Managing annotation runs \u00b6","text":"<p>FiftyOne provides a variety of methods that you can use to manage in-progress or completed annotation runs.</p> <p>For example, you can call <code>list_annotation_runs()</code> to see the available annotation keys on a dataset:</p> <pre><code>dataset.list_annotation_runs()\n</code></pre> <p>Or, you can use <code>get_annotation_info()</code> to retrieve information about the configuration of an annotation run:</p> <pre><code>info = dataset.get_annotation_info(anno_key)\nprint(info)\n</code></pre> <p>Use <code>load_annotation_results()</code> to load the <code>AnnotationResults</code> instance for an annotation run.</p> <p>All results objects provide a <code>cleanup()</code> method that you can use to delete all information associated with a run from the annotation backend.</p> <pre><code>results = dataset.load_annotation_results(anno_key)\nresults.cleanup()\n</code></pre> <p>In addition, the <code>AnnotationResults</code> subclasses for each backend may provide additional utilities such as support for programmatically monitoring the status of the annotation tasks in the run.</p> <p>You can use <code>rename_annotation_run()</code> to rename the annotation key associated with an existing annotation run:</p> <pre><code>dataset.rename_annotation_run(anno_key, new_anno_key)\n</code></pre> <p>Finally, you can use <code>delete_annotation_run()</code> to delete the record of an annotation run from your FiftyOne dataset:</p> <pre><code>dataset.delete_annotation_run(anno_key)\n</code></pre> <p>Note</p> <p>Calling <code>delete_annotation_run()</code> only deletes the record of the annotation run from your FiftyOne dataset; it will not delete any annotations loaded onto your dataset via <code>load_annotations()</code>, nor will it delete any associated information from the annotation backend.</p>"},{"location":"integrations/cvat/#examples","title":"Examples \u00b6","text":"<p>This section demonstrates how to perform some common annotation workflows on a FiftyOne dataset using the CVAT backend.</p> <p>Note</p> <p>All of the examples below assume you have configured your CVAT server and credentials as described in this section.</p>"},{"location":"integrations/cvat/#adding-new-label-fields","title":"Adding new label fields \u00b6","text":"<p>In order to annotate a new label field, you can provide the <code>label_field</code>, <code>label_type</code>, and <code>classes</code> parameters to <code>annotate()</code> to define the annotation schema for the field:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"cvat_new_field\"\n\nview.annotate(\n    anno_key,\n    label_field=\"new_classifications\",\n    label_type=\"classifications\",\n    classes=[\"dog\", \"cat\", \"person\"],\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Create annotations in CVAT\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>Alternatively, you can use the <code>label_schema</code> argument to define the same labeling task:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"cvat_new_field\"\n\nlabel_schema = {\n    \"new_classifications\": {\n        \"type\": \"classifications\",\n        \"classes\": [\"dog\", \"cat\", \"person\"],\n    }\n}\n\nview.annotate(anno_key, label_schema=label_schema, launch_editor=True)\nprint(dataset.get_annotation_info(anno_key))\n\n# Create annotations in CVAT\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p></p>"},{"location":"integrations/cvat/#editing-existing-labels","title":"Editing existing labels \u00b6","text":"<p>A common use case is to fix annotation mistakes that you discovered in your datasets through FiftyOne.</p> <p>You can easily edit the labels in an existing field of your FiftyOne dataset by simply passing the name of the field via the <code>label_field</code> parameter of <code>annotate()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"cvat_existing_field\"\n\nview.annotate(anno_key, label_field=\"ground_truth\", launch_editor=True)\nprint(dataset.get_annotation_info(anno_key))\n\n# Modify/add/delete bounding boxes and their attributes in CVAT\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p></p> <p>The above code snippet will infer the possible classes and label attributes from your FiftyOne dataset. However, the <code>classes</code> and <code>attributes</code> parameters can be used to annotate new classes and/or attributes:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"cvat_existing_field\"\n\n# The list of possible `label` values\nclasses = [\"person\", \"dog\", \"cat\", \"helicopter\"]\n\n# Details for the existing `iscrowd` attribute are automatically inferred\n# A new `attr2` attribute is also added\nattributes = {\n    \"iscrowd\": {},\n    \"attr2\": {\n        \"type\": \"select\",\n        \"values\": [\"val1\", \"val2\"],\n    }\n}\n\nview.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    classes=classes,\n    attributes=attributes,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Modify/add/delete bounding boxes and their attributes in CVAT\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p></p> <p>Warning</p> <p>When uploading existing labels to CVAT, the <code>id</code> of the labels in FiftyOne are stored in a <code>label_id</code> attribute of the CVAT shapes.</p> <p>IMPORTANT: If a <code>label_id</code> is modified in CVAT, then FiftyOne may not be able to merge the annotation with its existing <code>Label</code> instance; in such cases, it must instead delete the existing label and create a new <code>Label</code> with the shape\u2019s contents. See this section for details.</p>"},{"location":"integrations/cvat/#restricting-label-edits","title":"Restricting label edits \u00b6","text":"<p>You can use the <code>allow_additions</code>, <code>allow_deletions</code>, <code>allow_label_edits</code>, <code>allow_index_edits</code>, and <code>allow_spatial_edits</code> parameters to configure whether certain types of edits are allowed in your annotation run. See this section for more information about the available options.</p> <p>For example, suppose you have an existing <code>ground_truth</code> field that contains objects of various types and you would like to add new <code>sex</code> and <code>age</code> attributes to all people in this field while also strictly enforcing that no objects can be added, deleted, or have their labels or bounding boxes modified. You can configure an annotation run for this as follows:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Grab a sample that contains a person\nview = (\n    dataset\n    .match_labels(filter=F(\"label\") == \"person\", fields=\"ground_truth\")\n    .limit(1)\n)\n\nanno_key = \"cvat_edit_restrictions\"\n\n# The new attributes that we want to populate\nattributes = {\n    \"sex\": {\n        \"type\": \"select\",\n        \"values\": [\"male\", \"female\"],\n    },\n    \"age\": {\n        \"type\": \"text\",\n    },\n}\n\nview.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    classes=[\"person\"],\n    attributes=attributes,\n    allow_additions=False,\n    allow_deletions=False,\n    allow_label_edits=False,\n    allow_spatial_edits=False,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Populate attributes in CVAT\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>Similarly, you can include a <code>read_only=True</code> parameter when uploading existing label attributes to specify that the attribute\u2019s value should be uploaded to the annotation backend for informational purposes, but any edits to the attribute\u2019s value should not be imported back into FiftyOne.</p> <p>For example, the snippet below uploads the vehicle tracks in a video dataset along with their existing <code>type</code> attributes and requests that a new <code>make</code> attribute be populated without allowing edits to the vehicle\u2019s <code>type</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\nview = dataset.take(1)\n\nanno_key = \"cvat_read_only_attrs\"\n\n# Upload existing `type` attribute as read-only and add new `make` attribute\nattributes = {\n    \"type\": {\n        \"type\": \"text\",\n        \"read_only\": True,\n    },\n    \"make\": {\n        \"type\": \"text\",\n        \"mutable\": False,\n    },\n}\n\nview.annotate(\n    anno_key,\n    label_field=\"frames.detections\",\n    classes=[\"vehicle\"],\n    attributes=attributes,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Populate make attributes in CVAT\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>Warning</p> <p>The CVAT backend does not support restrictions to additions, deletions, spatial edits, and read-only attributes in its editing interface.</p> <p>However, any restrictions that you specify via the above parameters will still be enforced when you call <code>load_annotations()</code> to merge the annotations back into FiftyOne.</p> <p>IMPORTANT: When uploading existing labels to CVAT, the <code>id</code> of the labels in FiftyOne are stored in a <code>label_id</code> attribute of the CVAT shapes. If a <code>label_id</code> is modified in CVAT, then FiftyOne may not be able to merge the annotation with its existing <code>Label</code> instance; it must instead delete the existing label and create a new <code>Label</code> with the shape\u2019s contents. In such cases, if <code>allow_additions</code> and/or <code>allow_deletions</code> were set to <code>False</code> on the annotation schema, this can result in CVAT edits being rejected. See this section for details.</p>"},{"location":"integrations/cvat/#annotating-multiple-fields","title":"Annotating multiple fields \u00b6","text":"<p>The <code>label_schema</code> argument allows you to define an annotation task that involves multiple fields:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"cvat_multiple_fields\"\n\n# The details for existing `ground_truth` field are inferred\n# A new field `new_keypoints` is also added\nlabel_schema = {\n    \"ground_truth\": {},\n    \"new_keypoints\": {\n        \"type\": \"keypoints\",\n        \"classes\": [\"person\", \"cat\", \"dog\", \"food\"],\n        \"attributes\": {\n            \"is_truncated\": {\n                \"type\": \"select\",\n                \"values\": [True, False],\n            }\n        }\n    }\n}\n\nview.annotate(anno_key, label_schema=label_schema, launch_editor=True)\nprint(dataset.get_annotation_info(anno_key))\n\n# Add annotations in CVAT...\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>Note</p> <p>CVAT annotation schemas do not have a notion of label fields. Therefore, if you define an annotation schema that involves the same class label in multiple fields, the name of the label field will be appended to the class in CVAT in order to distinguish the class labels.</p> <p></p>"},{"location":"integrations/cvat/#unexpected-annotations","title":"Unexpected annotations \u00b6","text":"<p>The <code>annotate()</code> method allows you to define the annotation schema that should be followed in CVAT. However, CVAT does not explicitly allow for restricting the label types that can be created, so it is possible that your annotators may accidentally violate a task\u2019s intended schema.</p> <p>You can pass the optional <code>unexpected</code> parameter to <code>load_annotations()</code> to configure how to deal with any such unexpected labels that are found. The supported values are:</p> <ul> <li> <p><code>\"prompt\"</code> ( default): present an interactive prompt to direct/discard unexpected labels</p> </li> <li> <p><code>\"keep\"</code>: automatically keep all unexpected labels in a field whose name matches the the label type</p> </li> <li> <p><code>\"ignore\"</code>: automatically ignore any unexpected labels</p> </li> <li> <p><code>\"return\"</code>: return a dict containing all unexpected labels, if any</p> </li> </ul> <p>For example, suppose you upload a <code>Detections</code> field to CVAT for editing, but then polyline annotations are added instead. When <code>load_annotations()</code> is called, the default behavior is to present a command prompt asking you what field(s) (if any) to store these unexpected labels in:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"cvat_unexpected\"\n\nview.annotate(anno_key, label_field=\"ground_truth\", launch_editor=True)\nprint(dataset.get_annotation_info(anno_key))\n\n# Add some polyline annotations in CVAT (wrong type!)\n\n# You will be prompted for a field in which to store the polylines\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p></p>"},{"location":"integrations/cvat/#creating-projects","title":"Creating projects \u00b6","text":"<p>You can use the optional <code>project_name</code> parameter to specify the name of a CVAT project to which to upload the task(s) for an annotation run. If a project with the given name already exists, the task will be uploaded to the existing project and will automatically inherit its annotation schema. Otherwise, a new project with the schema you define will be created.</p> <p>A typical use case for this parameter is video annotation, since in CVAT every video must be annotated in a separate task. Creating a project allows all of the tasks to be organized together in one place.</p> <p>As with tasks, you can delete the project associated with an annotation run by passing the <code>cleanup=True</code> option to <code>load_annotations()</code>.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\nview = dataset.take(3)\n\nanno_key = \"cvat_create_project\"\n\nview.annotate(\n    anno_key,\n    label_field=\"frames.detections\",\n    project_name=\"fiftyone_project_example\",\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Annotate videos in CVAT...\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre>"},{"location":"integrations/cvat/#uploading-to-existing-projects","title":"Uploading to existing projects \u00b6","text":"<p>The <code>project_name</code> and <code>project_id</code> parameters can both be used to specify an existing CVAT project to which to upload the task(s) for an annotation run. In this case, the schema of the project is automatically applied to your annotation tasks.</p> <p>A typical use case for this workflow is when you use the same annotation schema for multiple datasets, since this allows you to organize the tasks under one CVAT project and avoid the need to re-specify the label schema in FiftyOne.</p> <p>Note</p> <p>When uploading to existing projects, because the annotation schema is inherited from the CVAT project definition, any class/attribute specifications that you attempt to provide via arguments such as <code>label_schema</code>, <code>classes</code>, and <code>attributes</code> to <code>annotate()</code> will be ignored.</p> <p>You can, however, use the <code>label_schema</code> and <code>label_field</code> arguments for the limited purpose of specifying the name of existing label field(s) to upload or the name and type of new field(s) in which you want to store the annotations that will be created. If no label fields are provided, then you will receive command line prompt(s) at import time to provide label field(s) in which to store the annotations.</p> <p>Warning</p> <p>Since the <code>label_schema</code> and <code>attribute</code> arguments are ignored, any occluded or group id attributes defined there will also be ignored. In order to connect occluded or group id attributes, use the <code>occluded_attr</code> and <code>group_id_attr</code> arguments directly.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\").clone()\nview = dataset.take(3)\n\nproject_name = \"fiftyone_project_example\"\n\n#\n# Upload existing `ground_truth` labels to a new CVAT project\n# The label schema is automatically inferred from the existing labels\n#\n\nview.annotate(\n    \"create_project\",\n    label_field=\"ground_truth\",\n    project_name=project_name,\n    launch_editor=True,\n)\n\n#\n# Now upload the `predictions` labels to the same CVAT project\n# Here the label schema of the existing CVAT project is automatically used\n#\n\nanno_key = \"cvat_existing_project\"\nview.annotate(\n    anno_key,\n    label_field=\"predictions\",\n    project_name=project_name,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Annotate in CVAT...\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n\n#\n# Now add a task with unspecified label fields to the same CVAT project\n# In this case you will be prompted for field names at download time\n#\n\nanno_key = \"cvat_new_fields\"\nview.annotate(\n    anno_key,\n    project_name=project_name,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Annotate in CVAT...\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre>"},{"location":"integrations/cvat/#assigning-users","title":"Assigning users \u00b6","text":"<p>When using the CVAT backend, you can provide the following optional parameters to <code>annotate()</code> to specify which users will be assigned to the created tasks:</p> <ul> <li> <p><code>segment_size</code>: the maximum number of images to include in a single job</p> </li> <li> <p><code>task_assignee</code>: a username to assign the generated tasks. This argument can be a list of usernames when annotating videos as each video is uploaded to a separate task</p> </li> <li> <p><code>job_assignees</code>: a list of usernames to assign jobs</p> </li> <li> <p><code>job_reviewers</code>: a list of usernames to assign job reviews. Only available in CVAT v1 servers</p> </li> </ul> <p>If the number of jobs exceeds the number of assignees or reviewers, the jobs will be assigned using a round-robin strategy.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(5)\n\nanno_key = \"cvat_assign_users\"\n\ntask_assignee = \"username1\"\njob_assignees = [\"username2\", \"username3\"]\n\n# If using a CVAT v1 server\n# job_reviewers = [\"username4\", \"username5\", \"username6\", \"username7\"]\n\n# Load \"ground_truth\" field into one task\n# Create another task for \"keypoints\" field\nlabel_schema = {\n    \"ground_truth\": {},\n    \"keypoints\": {\n        \"type\": \"keypoints\",\n        \"classes\": [\"person\"],\n    }\n}\n\nview.annotate(\n    anno_key,\n    label_schema=label_schema,\n    segment_size=2,\n    task_assignee=task_assignee,\n    job_assignees=job_assignees,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Cleanup\nresults = dataset.load_annotation_results(anno_key)\nresults.cleanup()\ndataset.delete_annotation_run(anno_key)\n</code></pre>"},{"location":"integrations/cvat/#large-annotation-runs","title":"Large annotation runs \u00b6","text":"<p>The CVAT API imposes a limit on the size of all requests. By default, all images are uploaded to a single CVAT task, which can result in errors when uploading annotation runs for large sample collections.</p> <p>Note</p> <p>The CVAT maintainers made an update to resolve this issue natively, but if you still encounter issues, try the following workflow to circumvent the issue.</p> <p>You can use the <code>task_size</code> parameter to break image annotation runs into multiple CVAT tasks, each with a specified maximum number of images. Note that we recommend providing a <code>project_name</code> whenever you use the <code>task_size</code> parameter so that the created tasks will be grouped together.</p> <p>The <code>task_size</code> parameter can also be used in conjunction with the <code>segment_size</code> parameter to configure both the number of images per task as well as the number of images per job within each task.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\", max_samples=20).clone()\n\nanno_key = \"batch_upload\"\n\nresults = dataset.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    task_size=6,  # 6 images per task\n    segment_size=2,  # 2 images per job\n    project_name=\"batch_example\",\n    launch_editor=True,\n)\n\n# Annotate in CVAT...\n\ndataset.load_annotations(anno_key, cleanup=True)\n</code></pre> <p>Note</p> <p>The <code>task_size</code> parameter only applies to image datasets, since videos are always uploaded one per task.</p>"},{"location":"integrations/cvat/#scalar-labels","title":"Scalar labels \u00b6","text":"<p><code>Label</code> fields are the preferred way to store information for common tasks such as classification and detection in your FiftyOne datasets. However, you can also store CVAT annotations in scalar fields of type <code>float</code>, <code>int</code>, <code>str</code>, or <code>bool</code> .</p> <p>When storing annotations in scalar fields, the <code>label_field</code> parameter is still used to define the name of the field, but the <code>classes</code> argument is now optional and the <code>attributes</code> argument is unused.</p> <p>If <code>classes</code> are provided, you will be able to select from these values in CVAT; otherwise, the CVAT tag will show the <code>label_field</code> name and you must enter the appropriate scalar in the <code>value</code> attribute of the tag.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"cvat_scalar_fields\"\n\n# Create two scalar fields, one with classes and one without\nlabel_schema = {\n    \"scalar1\": {\n        \"type\": \"scalar\",\n    },\n    \"scalar2\": {\n        \"type\": \"scalar\",\n        \"classes\": [\"class1\", \"class2\", \"class3\"],\n    }\n}\n\nview.annotate(anno_key, label_schema=label_schema, launch_editor=True)\nprint(dataset.get_annotation_info(anno_key))\n\n# Cleanup (without downloading results)\nresults = dataset.load_annotation_results(anno_key)\nresults.cleanup()\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p></p>"},{"location":"integrations/cvat/#uploading-alternate-media","title":"Uploading alternate media \u00b6","text":"<p>In some cases, you may want to upload media files other than those stored in the <code>filepath</code> field of your dataset\u2019s samples for annotation. For example, you may have a dataset with personal information like faces or license plates that must be anonymized before uploading for annotation.</p> <p>The recommended approach in this case is to store the alternative media files for each sample on disk and record these paths in a new field of your FiftyOne dataset. You can then specify this field via the <code>media_field</code> parameter of <code>annotate()</code>.</p> <p>For example, let\u2019s upload some blurred images to CVAT for annotation:</p> <pre><code>import os\nimport cv2\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nalt_dir = \"/tmp/blurred\"\nif not os.path.exists(alt_dir):\n    os.makedirs(alt_dir)\n\n# Blur images\nfor sample in view:\n    filepath = sample.filepath\n    alt_filepath = os.path.join(alt_dir, os.path.basename(filepath))\n\n    img = cv2.imread(filepath)\n    cv2.imwrite(alt_filepath, cv2.blur(img, (20, 20)))\n\n    sample[\"alt_filepath\"] = alt_filepath\n    sample.save()\n\nanno_key = \"cvat_alt_media\"\n\nview.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    media_field=\"alt_filepath\",\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Create annotations in CVAT\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p></p>"},{"location":"integrations/cvat/#using-cvats-occlusion-widget","title":"Using CVAT\u2019s occlusion widget \u00b6","text":"<p>The CVAT UI provides a variety of builtin widgets on each label you create that control properties like occluded, hidden, locked, and pinned.</p> <p>You can configure CVAT annotation runs so that the state of the occlusion widget is read/written to a FiftyOne label attribute of your choice by specifying the attribute\u2019s type as <code>occluded</code> in your label schema.</p> <p>In addition, if you are editing existing labels using the <code>attributes=True</code> syntax (the default) to infer the label schema for an existing field, if a boolean attribute with the name <code>\"occluded\"</code> is found, it will automatically be linked to the occlusion widget.</p> <p>Note</p> <p>You can only specify the <code>occluded</code> type for at most one attribute of each label field/class in your label schema, and, if you are editing existing labels, the attribute that you choose must contain boolean values.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\").clone()\nview = dataset.take(1)\n\nanno_key = \"cvat_occluded_widget\"\n\n# Populate a new `occluded` attribute on the existing `ground_truth` labels\n# using CVAT's occluded widget\nlabel_schema = {\n    \"ground_truth\": {\n        \"attributes\": {\n            \"occluded\": {\n                \"type\": \"occluded\",\n            }\n        }\n    }\n}\n\nview.annotate(anno_key, label_schema=label_schema, launch_editor=True)\nprint(dataset.get_annotation_info(anno_key))\n\n# Mark occlusions in CVAT...\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>You can also use the <code>occluded_attr</code> parameter to sync the state of CVAT\u2019s occlusion widget with a specified attribute of all spatial fields that are being annotated that did not explicitly have an occluded attribute defined in the label schema.</p> <p>This parameter is especially useful when working with existing CVAT projects, since CVAT project schemas are not able to retain information about occluded attributes between annotation runs.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\").clone()\nview = dataset.take(1)\n\nanno_key = \"cvat_occluded_widget_project\"\nproject_name = \"example_occluded_widget\"\nlabel_field = \"ground_truth\"\n\n# Create project\nview.annotate(\"new_proj\", label_field=label_field, project_name=project_name)\n\n# Upload to existing project\nview.annotate(\n    anno_key,\n    label_field=label_field,\n    occluded_attr=\"is_occluded\",\n    project_name=project_name,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Mark occlusions in CVAT...\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p></p>"},{"location":"integrations/cvat/#using-cvat-groups","title":"Using CVAT groups \u00b6","text":"<p>The CVAT UI provides a way to group objects together both visually and though a group id in the API.</p> <p>You can configure CVAT annotation runs so that the state of the group id is read/written to a FiftyOne label attribute of your choice by specifying the attribute\u2019s type as <code>group_id</code> in your label schema.</p> <p>In addition, if you are editing existing labels using the <code>attributes=True</code> syntax (the default) to infer the label schema for an existing field, if a boolean attribute with the name <code>\"group_id\"</code> is found, it will automatically be linked to CVAT groups.</p> <p>Note</p> <p>You can only specify the <code>group_id</code> type for at most one attribute of each label field/class in your label schema, and, if you are editing existing labels, the attribute that you choose must contain integer values.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\").clone()\nview = dataset.take(1)\n\nanno_key = \"cvat_group_id\"\n\n# Populate a new `group_id` attribute on the existing `ground_truth` labels\nlabel_schema = {\n    \"ground_truth\": {\n        \"attributes\": {\n            \"group_id\": {\n                \"type\": \"group_id\",\n            }\n        }\n    }\n}\n\nview.annotate(anno_key, label_schema=label_schema, launch_editor=True)\nprint(dataset.get_annotation_info(anno_key))\n\n# Mark groups in CVAT...\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>You can also use the <code>group_id_attr</code> parameter to sync the state of CVAT\u2019s group ids with a specified attribute of all spatial fields that are being annotated that did not explicitly have a group id attribute defined in the label schema.</p> <p>This parameter is especially useful when working with existing CVAT projects, since CVAT project schemas are not able to retain information about group id attributes between annotation runs.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\").clone()\nview = dataset.take(1)\n\nanno_key = \"cvat_group_id_project\"\nproject_name = \"example_group_id\"\nlabel_field = \"ground_truth\"\n\n# Create project\nview.annotate(\"new_proj\", label_field=label_field, project_name=project_name)\n\n# Upload to existing project\nview.annotate(\n    anno_key,\n    label_field=label_field,\n    group_id_attr=\"group_id_value\",\n    project_name=project_name,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Mark groups in CVAT...\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre>"},{"location":"integrations/cvat/#changing-destination-field","title":"Changing destination field \u00b6","text":"<p>When annotating an existing label field, it can be useful to load the annotations into a different field than the one used to upload annotations. The <code>dest_field</code> parameter can be used for this purpose when calling <code>load_annotations()</code>.</p> <p>If your annotation run involves a single label field, set <code>dest_field</code> to the name of the (new or existing) field you wish to load annotations into.</p> <p>If your annotation run involves multiple fields, <code>dest_field</code> should be a dictionary mapping existing field names in your run\u2019s label schema to updated destination fields.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\").clone()\nview = dataset.take(1)\n\nanno_key = \"dest_field\"\nlabel_field = \"ground_truth\"\n\n# Upload from `ground_truth` field\nview.annotate(\n    anno_key,\n    label_field=label_field,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Load into `test_field`\ndest_field = \"test_field\"\n\n# If your run involves multiple fields, use this syntax instead\n# dest_field = {\"ground_truth\": \"test_field\", ...}\n\ndataset.load_annotations(\n    anno_key,\n    cleanup=True,\n    dest_field=dest_field,\n)\ndataset.delete_annotation_run(anno_key)\n</code></pre>"},{"location":"integrations/cvat/#using-frame-start-stop-step","title":"Using frame start, stop, step \u00b6","text":"<p>When annotating videos, you can use the arguments <code>frame_start</code>, <code>frame_stop</code>, and <code>frame_step</code> to annotate subsampled clips of your videos rather than loading every frame into CVAT. These arguments are only supported for video tasks and accept either integer values to use for each video task that is created, a list of values that will be applied to video tasks in a round-robin strategy, or a dictionary of values mapping the video filepath to the corresponding integer value.</p> <p>Note: Uploading existing annotation tracks while using the <code>frame_step</code> argument is not currently supported.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=2).clone()\nsample_fps = dataset.values(\"filepath\")\n\n# Start video 1 at frame 10 and video 2 at frame 5\nframe_start = {sample_fps[0]: 10, sample_fps[1]: 5}\n\n# For video 1, load every frame after the start\n# For video 2, load every 10th frame\nframe_step = [1, 10]\n\n# Stop all videos at frame 100\nframe_stop = 100\n\nanno_key = \"frame_args\"\nlabel_field = \"frames.new_detections\"\nlabel_type = \"detections\"\nclasses = [\"person\", \"vehicle\"]\n\n# Annotate a new detections field\ndataset.annotate(\n    anno_key,\n    label_field=label_field,\n    label_type=label_type,\n    classes=classes,\n    frame_start=frame_start,\n    frame_stop=frame_stop,\n    frame_step=frame_step,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Annotate in CVAT\n\ndataset.load_annotations(\n    anno_key,\n    cleanup=True,\n)\ndataset.delete_annotation_run(anno_key)\n</code></pre>"},{"location":"integrations/cvat/#annotating-videos","title":"Annotating videos \u00b6","text":"<p>You can add or edit annotations for video datasets using the CVAT backend through the <code>annotate()</code> method.</p> <p>All CVAT label types except <code>tags</code> provide an option to annotate tracks in videos, which captures the identity of a single object as it moves through the video. When you import video tracks into FiftyOne, the <code>index</code> attribute of each label will contain the integer number of its track, and any labels that are keyframes will have their <code>keyframe=True</code> attribute set.</p> <p>Note that CVAT does not provide a straightforward way to annotate sample-level classification labels for videos. Instead, we recommend that you use frame-level fields to record classifications for your video datasets.</p> <p>Note</p> <p>CVAT only allows one video per task, so calling <code>annotate()</code> on a video dataset will result multiple tasks per label field.</p>"},{"location":"integrations/cvat/#adding-new-frame-labels","title":"Adding new frame labels \u00b6","text":"<p>The example below demonstrates how to configure a video annotation task that populates a new frame-level field of a video dataset with vehicle detection tracks with an immutable <code>type</code> attribute that denotes the type of each vehicle:</p> <p>Note</p> <p>Prepend <code>\"frames.\"</code> to reference frame-level fields when calling <code>annotate()</code>.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\").clone()\ndataset.delete_frame_field(\"detections\")  # delete existing labels\n\nview = dataset.limit(1)\n\nanno_key = \"video\"\n\n# Create annotation task\nview.annotate(\n    anno_key,\n    label_field=\"frames.detections\",\n    label_type=\"detections\",\n    classes=[\"vehicle\"],\n    attributes={\n        \"type\": {\n            \"type\": \"select\",\n            \"values\": [\"sedan\", \"suv\", \"truck\", \"other\"],\n            \"mutable\": False,\n        }\n    },\n    launch_editor=True,\n)\n\n# Add annotations in CVAT...\n\n# Download annotations\ndataset.load_annotations(anno_key)\n\n# Load the view that was annotated in the App\nview = dataset.load_annotation_view(anno_key)\nsession = fo.launch_app(view=view)\n\n# Cleanup\nresults = dataset.load_annotation_results(anno_key)\nresults.cleanup()\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p></p>"},{"location":"integrations/cvat/#editing-frame-level-label-tracks","title":"Editing frame-level label tracks \u00b6","text":"<p>You can also edit existing frame-level labels of video datasets in CVAT.</p> <p>Note</p> <p>If at least one existing label has its <code>keyframe=True</code> attribute set, only the keyframe labels will be uploaded to CVAT, which provides a better editing experience when performing spatial or time-varying attribute edits.</p> <p>If no keyframe information is available, every existing label must be marked as a keyframe in CVAT.</p> <p>The example below edits the existing detections of a video dataset. Note that, since the dataset\u2019s labels do not have keyframe markings, we artificially tag every 10th frame as a keyframe to provide a better editing experience in CVAT:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\").clone()\n\nview = dataset.take(1)\n\n# Mark some keyframes\nsample = view.first()\nnum_frames = len(sample.frames)\nkeyframes = set(range(1, num_frames, 10)).union({1, num_frames})\nfor frame_number in keyframes:\n    frame = sample.frames[frame_number]\n    for det in frame.detections.detections:\n        det.keyframe = True\n\nsample.save()\n\nanno_key = \"cvat_video\"\n\n# Send frame-level detections to CVAT\nview.annotate(\n    anno_key,\n    label_field=\"frames.detections\",\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Edit annotations in CVAT...\n\n# Merge edits back in\ndataset.load_annotations(anno_key)\n\n# Load the view that was annotated in the App\nview = dataset.load_annotation_view(anno_key)\nsession = fo.launch_app(view=view)\n\n# Cleanup\nresults = dataset.load_annotation_results(anno_key)\nresults.cleanup()\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>Warning</p> <p>When uploading existing labels to CVAT, the <code>id</code> of the labels in FiftyOne are stored in a <code>label_id</code> attribute of the CVAT shapes.</p> <p>IMPORTANT: If a <code>label_id</code> is modified in CVAT, then FiftyOne may not be able to merge the annotation with its existing <code>Label</code> instance; in such cases, it must instead delete the existing label and create a new <code>Label</code> with the shape\u2019s contents. See this section for details.</p>"},{"location":"integrations/cvat/#importing-existing-tasks","title":"Importing existing tasks \u00b6","text":"<p>FiftyOne\u2019s CVAT integration is designed to manage the full annotation workflow, from task creation to annotation import.</p> <p>However, if you have created CVAT tasks outside of FiftyOne, you can use the <code>import_annotations()</code> utility to import individual task(s) or an entire project into a FiftyOne dataset.</p> <pre><code>import os\n\nimport fiftyone as fo\nimport fiftyone.utils.cvat as fouc\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\", max_samples=3).clone()\n\n# Create a pre-existing CVAT project\nresults = dataset.annotate(\n    \"example_import\",\n    label_field=\"ground_truth\",\n    project_name=\"example_import\",\n)\n\n#\n# In the simplest case, you can download both the annotations and the media\n# from CVAT\n#\n\ndataset = fo.Dataset()\nfouc.import_annotations(\n    dataset,\n    project_name=project_name,\n    data_path=\"/tmp/cvat_import\",\n    download_media=True,\n)\n\nsession = fo.launch_app(dataset)\n\n#\n# If you already have the media stored locally, you can instead provide a\n# mapping between filenames in the pre-existing CVAT project and the\n# locations of the media locally on disk for the FiftyOne dataset\n#\n# Since we're using a CVAT task uploaded via FiftyOne, the mapping is a bit\n# weird\n#\n\ndata_map = {\n    \"%06d_%s\" % (idx, os.path.basename(p)): p\n    for idx, p in enumerate(dataset.values(\"filepath\"))\n}\n\ndataset = fo.Dataset()\nfouc.import_annotations(\n    dataset,\n    project_name=project_name,\n    data_path=data_map,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>Another strategy for importing existing CVAT annotations into FiftyOne is to simply export the annotations from the CVAT UI and then import them via the CVATImageDataset or CVATVideoDataset types.</p>"},{"location":"integrations/cvat/#additional-utilities","title":"Additional utilities \u00b6","text":"<p>You can perform additional CVAT-specific operations to monitor the progress of an annotation task initiated by <code>annotate()</code> via the returned <code>CVATAnnotationResults</code> instance.</p> <p>The sections below highlight some common actions that you may want to perform.</p>"},{"location":"integrations/cvat/#using-the-cvat-api","title":"Using the CVAT API \u00b6","text":"<p>You can use the <code>connect_to_api()</code> to retrieve a <code>CVATAnnotationAPI</code> instance, which is a wrapper around the CVAT REST API that provides convenient methods for performing common actions on your CVAT tasks:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.utils.annotations as foua\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"cvat_api\"\n\nview.annotate(anno_key, label_field=\"ground_truth\")\n\napi = foua.connect_to_api()\n\n# The context manager is optional and simply ensures that TCP connections\n# are always closed\nwith api:\n    # Launch CVAT in your browser\n    api.launch_editor(api.base_url)\n\n    # Get info about all tasks currently on the CVAT server\n    response = api.get(api.tasks_url).json()\n</code></pre>"},{"location":"integrations/cvat/#viewing-task-statuses","title":"Viewing task statuses \u00b6","text":"<p>You can use the <code>get_status()</code> and <code>print_status()</code> methods to get information about the current status of the task(s) and job(s) for that annotation run:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(3)\n\nanno_key = \"cvat_status\"\n\nview.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    segment_size=2,\n    task_assignee=\"user1\",\n    job_assignees=[\"user1\"],\n    job_reviewers=[\"user2\", \"user3\"],\n)\n\nresults = dataset.load_annotation_results(anno_key)\nresults.print_status()\n\nresults.cleanup()\ndataset.delete_annotation_run(anno_key)\n</code></pre> <pre><code>Status for label field 'ground_truth':\n\n    Task 331 (FiftyOne_quickstart_ground_truth):\n        Status: annotation\n        Assignee: user1\n        Last updated: 2021-08-11T15:09:02.680181Z\n        URL: http://localhost:8080/tasks/331\n\n        Job 369:\n            Status: annotation\n            Assignee: user1\n            Reviewer: user2\n\n        Job 370:\n            Status: annotation\n            Assignee: user1\n            Reviewer: user3\n</code></pre> <p>Note</p> <p>Pro tip: If you are iterating over many annotation runs, you can use <code>connect_to_api()</code> and <code>use_api()</code> as shown below to reuse a single <code>CVATAnnotationAPI</code> instance and avoid reauthenticating with CVAT for each run:</p> <pre><code>import fiftyone.utils.annotations as foua\n\napi = foua.connect_to_api()\n\nfor anno_key in dataset.list_annotation_runs():\n    results = dataset.load_annotation_results(anno_key)\n    results.use_api(api)\n    results.print_status()\n</code></pre>"},{"location":"integrations/cvat/#deleting-tasks","title":"Deleting tasks \u00b6","text":"<p>You can use the <code>delete_task()</code> method to delete specific CVAT tasks associated with an annotation run:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"cvat_delete_tasks\"\n\nview.annotate(anno_key, label_field=\"ground_truth\")\n\nresults = dataset.load_annotation_results(anno_key)\napi = results.connect_to_api()\n\nprint(results.task_ids)\n# [372]\n\napi.delete_task(372)\n</code></pre>"},{"location":"integrations/elasticsearch/","title":"Elasticsearch Vector Search Integration \u00b6","text":"<p>Elasticsearch is one of the most popular search platforms available, and we\u2019ve made it easy to use Elasticsearch\u2019s vector search capabilities on your computer vision data directly from FiftyOne!</p> <p>Follow these simple instructions to get started using Elasticsearch + FiftyOne.</p> <p>FiftyOne provides an API to create Elasticsearch indexes, upload vectors, and run similarity queries, both programmatically in Python and via point-and-click in the App.</p> <p>Note</p> <p>Did you know? You can search by natural language using Elasticsearch similarity indexes!</p> <p></p>"},{"location":"integrations/elasticsearch/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The basic workflow to use Elasticsearch to create a similarity index on your FiftyOne datasets and use this to query your data is as follows:</p> <ol> <li> <p>Connect to or start an Elasticsearch server</p> </li> <li> <p>Load a dataset into FiftyOne</p> </li> <li> <p>Compute embedding vectors for samples or patches in your dataset, or select a model to use to generate embeddings</p> </li> <li> <p>Use the <code>compute_similarity()</code> method to generate a Elasticsearch similarity index for the samples or object patches in a dataset by setting the parameter <code>backend=\"elasticsearch\"</code> and specifying a <code>brain_key</code> of your choice</p> </li> <li> <p>Use this Elasticsearch similarity index to query your data with <code>sort_by_similarity()</code></p> </li> <li> <p>If desired, delete the index</p> </li> </ol> <p>The example below demonstrates this workflow.</p> <p>Note</p> <p>You must have access to an Elasticsearch server and install the Elasticsearch Python client to run this example:</p> <pre><code>pip install elasticsearch\n</code></pre> <p>Note that, if you are using a custom Elasticsearch server, you can store your credentials as described in this section to avoid entering them manually each time you interact with your Elasticsearch index.</p> <p>First let\u2019s load a dataset into FiftyOne and compute embeddings for the samples:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\n# Step 1: Load your data into FiftyOne\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Steps 2 and 3: Compute embeddings and create a similarity index\nelasticsearch_index = fob.compute_similarity(\n    dataset,\n    brain_key=\"elasticsearch_index\",\n    backend=\"elasticsearch\",\n)\n</code></pre> <p>Once the similarity index has been generated, we can query our data in FiftyOne by specifying the <code>brain_key</code>:</p> <pre><code># Step 4: Query your data\nquery = dataset.first().id  # query by sample ID\nview = dataset.sort_by_similarity(\n    query,\n    brain_key=\"elasticsearch_index\",\n    k=10,  # limit to 10 most similar samples\n)\n\n# Step 5 (optional): Cleanup\n\n# Delete the Elasticsearch index\nelasticsearch_index.cleanup()\n\n# Delete run record from FiftyOne\ndataset.delete_brain_run(\"elasticsearch_index\")\n</code></pre> <p>Note</p> <p>Skip to this section for a variety of common Elasticsearch query patterns.</p>"},{"location":"integrations/elasticsearch/#setup","title":"Setup \u00b6","text":"<p>The easiest way to get started with Elasticsearch is to install locally via Docker.</p>"},{"location":"integrations/elasticsearch/#installing-the-elasticsearch-client","title":"Installing the Elasticsearch client \u00b6","text":"<p>In order to use the Elasticsearch backend, you must also install the Elasticsearch Python client:</p> <pre><code>pip install elasticsearch\n</code></pre>"},{"location":"integrations/elasticsearch/#using-the-elasticsearch-backend","title":"Using the Elasticsearch backend \u00b6","text":"<p>By default, calling <code>compute_similarity()</code> or <code>sort_by_similarity()</code> will use an sklearn backend.</p> <p>To use the Elasticsearch backend, simply set the optional <code>backend</code> parameter of <code>compute_similarity()</code> to <code>\"elasticsearch\"</code>:</p> <pre><code>import fiftyone.brain as fob\n\nfob.compute_similarity(..., backend=\"elasticsearch\", ...)\n</code></pre> <p>Alternatively, you can permanently configure FiftyOne to use the Elasticsearch backend by setting the following environment variable:</p> <pre><code>export FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND=elasticsearch\n</code></pre> <p>or by setting the <code>default_similarity_backend</code> parameter of your brain config located at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"default_similarity_backend\": \"elasticsearch\"\n}\n</code></pre>"},{"location":"integrations/elasticsearch/#authentication","title":"Authentication \u00b6","text":"<p>If you are using a custom Elasticsearch server, you can provide your credentials in a variety of ways.</p> <p>Environment variables (recommended)</p> <p>The recommended way to configure your Elasticsearch credentials is to store them in the environment variables shown below, which are automatically accessed by FiftyOne whenever a connection to Elasticsearch is made.</p> <pre><code>export FIFTYONE_BRAIN_SIMILARITY_ELASTICSEARCH_HOSTS=http://localhost:9200\nexport FIFTYONE_BRAIN_SIMILARITY_ELASTICSEARCH_USERNAME=XXXXXXXX\nexport FIFTYONE_BRAIN_SIMILARITY_ELASTICSEARCH_PASSWORD=XXXXXXXX\n</code></pre> <p>This is only one example of variables that can be used to authenticate an Elasticsearch client. Find more information here.</p> <p>FiftyOne Brain config</p> <p>You can also store your credentials in your brain config located at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"similarity_backends\": {\n        \"elasticsearch\": {\n            \"hosts\": \"http://localhost:9200\",\n            \"username\": \"XXXXXXXX\",\n            \"password\": \"XXXXXXXX\"\n        }\n    }\n}\n</code></pre> <p>Note that this file will not exist until you create it.</p> <p>Keyword arguments</p> <p>You can manually provide credentials as keyword arguments each time you call methods like <code>compute_similarity()</code> that require connections to Elasticsearch:</p> <pre><code>import fiftyone.brain as fob\n\nelasticsearch_index = fob.compute_similarity(\n    ...\n    backend=\"elasticsearch\",\n    brain_key=\"elasticsearch_index\",\n    hosts=\"http://localhost:9200\",\n    username=\"XXXXXXXX\",\n    password=\"XXXXXXXX\",\n)\n</code></pre> <p>Note that, when using this strategy, you must manually provide the credentials when loading an index later via <code>load_brain_results()</code>:</p> <pre><code>elasticsearch_index = dataset.load_brain_results(\n    \"elasticsearch_index\",\n    hosts=\"http://localhost:9200\",\n    username=\"XXXXXXXX\",\n    password=\"XXXXXXXX\",\n)\n</code></pre>"},{"location":"integrations/elasticsearch/#elasticsearch-config-parameters","title":"Elasticsearch config parameters \u00b6","text":"<p>The Elasticsearch backend supports a variety of query parameters that can be used to customize your similarity queries. These parameters include:</p> <ul> <li> <p>index_name ( None): the name of the Elasticsearch vector search index to use or create. If not specified, a new unique name is generated automatically</p> </li> <li> <p>metric ( \u201ccosine\u201d): the distance/similarity metric to use when creating a new index. The supported values are <code>(\"cosine\", \"dotproduct\", \"euclidean\", \"innerproduct\")</code></p> </li> </ul> <p>For detailed information on these parameters, see the Elasticsearch documentation.</p> <p>You can specify these parameters via any of the strategies described in the previous section. Here\u2019s an example of a brain config that includes all of the available parameters:</p> <pre><code>{\n    \"similarity_backends\": {\n        \"elasticsearch\": {\n            \"index_name\": \"your-index\",\n            \"metric\": \"cosine\"\n        }\n    }\n}\n</code></pre> <p>However, typically these parameters are directly passed to <code>compute_similarity()</code> to configure a specific new index:</p> <pre><code>elasticsearch_index = fob.compute_similarity(\n    ...\n    backend=\"elasticsearch\",\n    brain_key=\"elasticsearch_index\",\n    index_name=\"your-index\",\n    metric=\"cosine\",\n)\n</code></pre>"},{"location":"integrations/elasticsearch/#managing-brain-runs","title":"Managing brain runs \u00b6","text":"<p>FiftyOne provides a variety of methods that you can use to manage brain runs.</p> <p>For example, you can call <code>list_brain_runs()</code> to see the available brain keys on a dataset:</p> <pre><code>import fiftyone.brain as fob\n\n# List all brain runs\ndataset.list_brain_runs()\n\n# Only list similarity runs\ndataset.list_brain_runs(type=fob.Similarity)\n\n# Only list specific similarity runs\ndataset.list_brain_runs(\n    type=fob.Similarity,\n    patches_field=\"ground_truth\",\n    supports_prompts=True,\n)\n</code></pre> <p>Or, you can use <code>get_brain_info()</code> to retrieve information about the configuration of a brain run:</p> <pre><code>info = dataset.get_brain_info(brain_key)\nprint(info)\n</code></pre> <p>Use <code>load_brain_results()</code> to load the <code>SimilarityIndex</code> instance for a brain run.</p> <p>You can use <code>rename_brain_run()</code> to rename the brain key associated with an existing similarity results run:</p> <pre><code>dataset.rename_brain_run(brain_key, new_brain_key)\n</code></pre> <p>Finally, you can use <code>delete_brain_run()</code> to delete the record of a similarity index computation from your FiftyOne dataset:</p> <pre><code>dataset.delete_brain_run(brain_key)\n</code></pre> <p>Note</p> <p>Calling <code>delete_brain_run()</code> only deletes the record of the brain run from your FiftyOne dataset; it will not delete any associated Elasticsearch index, which you can do as follows:</p> <pre><code># Delete the Elasticsearch index\nelasticsearch_index = dataset.load_brain_results(brain_key)\nelasticsearch_index.cleanup()\n</code></pre>"},{"location":"integrations/elasticsearch/#examples","title":"Examples \u00b6","text":"<p>This section demonstrates how to perform some common vector search workflows on a FiftyOne dataset using the Elasticsearch backend.</p> <p>Note</p> <p>All of the examples below assume you have configured your Elasticsearch server as described in this section.</p>"},{"location":"integrations/elasticsearch/#create-a-similarity-index","title":"Create a similarity index \u00b6","text":"<p>In order to create a new Elasticsearch similarity index, you need to specify either the <code>embeddings</code> or <code>model</code> argument to <code>compute_similarity()</code>. Here\u2019s a few possibilities:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nmodel_name = \"clip-vit-base32-torch\"\nmodel = foz.load_zoo_model(model_name)\nbrain_key = \"elasticsearch_index\"\n\n# Option 1: Compute embeddings on the fly from model name\nfob.compute_similarity(\n    dataset,\n    model=model_name,\n    backend=\"elasticsearch\",\n    brain_key=brain_key,\n)\n\n# Option 2: Compute embeddings on the fly from model instance\nfob.compute_similarity(\n    dataset,\n    model=model,\n    backend=\"elasticsearch\",\n    brain_key=brain_key,\n)\n\n# Option 3: Pass precomputed embeddings as a numpy array\nembeddings = dataset.compute_embeddings(model)\nfob.compute_similarity(\n    dataset,\n    embeddings=embeddings,\n    backend=\"elasticsearch\",\n    brain_key=brain_key,\n)\n\n# Option 4: Pass precomputed embeddings by field name\ndataset.compute_embeddings(model, embeddings_field=\"embeddings\")\nfob.compute_similarity(\n    dataset,\n    embeddings=\"embeddings\",\n    backend=\"elasticsearch\",\n    brain_key=brain_key,\n)\n</code></pre>"},{"location":"integrations/elasticsearch/#create-a-patch-similarity-index","title":"Create a patch similarity index \u00b6","text":"<p>You can also create a similarity index for object patches within your dataset by including the <code>patches_field</code> argument to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    patches_field=\"ground_truth\",\n    model=\"clip-vit-base32-torch\",\n    backend=\"elasticsearch\",\n    brain_key=\"elasticsearch_patches\",\n)\n</code></pre>"},{"location":"integrations/elasticsearch/#connect-to-an-existing-index","title":"Connect to an existing index \u00b6","text":"<p>If you have already created a Elasticsearch index storing the embedding vectors for the samples or patches in your dataset, you can connect to it by passing the <code>index_name</code> to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",      # zoo model used (if applicable)\n    embeddings=False,                   # don't compute embeddings\n    index_name=\"your-index\",            # the existing Elasticsearch index\n    brain_key=\"elasticsearch_index\",\n    backend=\"elasticsearch\",\n)\n</code></pre>"},{"location":"integrations/elasticsearch/#addremove-embeddings-from-an-index","title":"Add/remove embeddings from an index \u00b6","text":"<p>You can use <code>add_to_index()</code> and <code>remove_from_index()</code> to add and remove embeddings from an existing Elasticsearch index.</p> <p>These methods can come in handy if you modify your FiftyOne dataset and need to update the Elasticsearch index to reflect these changes:</p> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nelasticsearch_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"elasticsearch_index\",\n    backend=\"elasticsearch\",\n)\nprint(elasticsearch_index.total_index_size)  # 200\n\nview = dataset.take(10)\nids = view.values(\"id\")\n\n# Delete 10 samples from a dataset\ndataset.delete_samples(view)\n\n# Delete the corresponding vectors from the index\nelasticsearch_index.remove_from_index(sample_ids=ids)\n\n# Add 20 samples to a dataset\nsamples = [fo.Sample(filepath=\"tmp%d.jpg\" % i) for i in range(20)]\nsample_ids = dataset.add_samples(samples)\n\n# Add corresponding embeddings to the index\nembeddings = np.random.rand(20, 512)\nelasticsearch_index.add_to_index(embeddings, sample_ids)\n\nprint(elasticsearch_index.total_index_size)  # 210\n</code></pre>"},{"location":"integrations/elasticsearch/#retrieve-embeddings-from-an-index","title":"Retrieve embeddings from an index \u00b6","text":"<p>You can use <code>get_embeddings()</code> to retrieve embeddings from a Elasticsearch index by ID:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nelasticsearch_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"elasticsearch_index\",\n    backend=\"elasticsearch\",\n)\n\n# Retrieve embeddings for the entire dataset\nids = dataset.values(\"id\")\nembeddings, sample_ids, _ = elasticsearch_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (200, 512)\nprint(sample_ids.shape)  # (200,)\n\n# Retrieve embeddings for a view\nids = dataset.take(10).values(\"id\")\nembeddings, sample_ids, _ = elasticsearch_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (10, 512)\nprint(sample_ids.shape)  # (10,)\n</code></pre>"},{"location":"integrations/elasticsearch/#querying-a-elasticsearch-index","title":"Querying a Elasticsearch index \u00b6","text":"<p>You can query a Elasticsearch index by appending a <code>sort_by_similarity()</code> stage to any dataset or view. The query can be any of the following:</p> <ul> <li> <p>An ID (sample or patch)</p> </li> <li> <p>A query vector of same dimension as the index</p> </li> <li> <p>A list of IDs (samples or patches)</p> </li> <li> <p>A text prompt (if supported by the model)</p> </li> </ul> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"elasticsearch_index\",\n    backend=\"elasticsearch\",\n)\n\n# Query by vector\nquery = np.random.rand(512)  # matches the dimension of CLIP embeddings\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"elasticsearch_index\")\n\n# Query by sample ID\nquery = dataset.first().id\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"elasticsearch_index\")\n\n# Query by a list of IDs\nquery = [dataset.first().id, dataset.last().id]\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"elasticsearch_index\")\n\n# Query by text prompt\nquery = \"a photo of a dog\"\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"elasticsearch_index\")\n</code></pre> <p>Note</p> <p>Performing a similarity search on a <code>DatasetView</code> will only return results from the view; if the view contains samples that were not included in the index, they will never be included in the result.</p> <p>This means that you can index an entire <code>Dataset</code> once and then perform searches on subsets of the dataset by constructing views that contain the images of interest.</p>"},{"location":"integrations/elasticsearch/#accessing-the-elasticsearch-client","title":"Accessing the Elasticsearch client \u00b6","text":"<p>You can use the <code>client</code> property of a Elasticsearch index to directly access the underlying Elasticsearch client instance and use its methods as desired:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nelasticsearch_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"elasticsearch_index\",\n    backend=\"elasticsearch\",\n)\n\nelasticsearch_client = elasticsearch_index.client\nprint(elasticsearch_client)\n</code></pre>"},{"location":"integrations/elasticsearch/#advanced-usage","title":"Advanced usage \u00b6","text":"<p>As previously mentioned, you can customize your Elasticsearch indexes by providing optional parameters to <code>compute_similarity()</code>.</p> <p>Here\u2019s an example of creating a similarity index backed by a customized Elasticsearch index. Just for fun, we\u2019ll specify a custom index name, use dot product similarity, and populate the index for only a subset of our dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Create a custom Elasticsearch index\nelasticsearch_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    embeddings=False,  # we'll add embeddings below\n    metric=\"dotproduct\",\n    brain_key=\"elasticsearch_index\",\n    backend=\"elasticsearch\",\n    index_name=\"custom-quickstart-index\",\n)\n\n# Add embeddings for a subset of the dataset\nview = dataset.take(10)\nembeddings, sample_ids, _ = elasticsearch_index.compute_embeddings(view)\nelasticsearch_index.add_to_index(embeddings, sample_ids)\n</code></pre>"},{"location":"integrations/huggingface/","title":"Hugging Face Integration \u00b6","text":"<p>FiftyOne integrates natively with Hugging Face\u2019s Transformers library, so you can load, fine-tune, and run inference with your favorite Transformers models on your FiftyOne datasets with just a few lines of code!</p> <p>FiftyOne also integrates with the Hugging Face Hub, so you can push datasets to and load datasets from the Hub with ease.</p>"},{"location":"integrations/huggingface/#transformers-library","title":"Transformers Library \u00b6","text":""},{"location":"integrations/huggingface/#setup","title":"Setup \u00b6","text":"<p>To get started with Transformers, just install the <code>transformers</code> package:</p> <pre><code>pip install -U transformers\n</code></pre>"},{"location":"integrations/huggingface/#inference","title":"Inference \u00b6","text":"<p>All Transformers models that support image classification, object detection, semantic segmentation, or monocular depth estimation tasks can be passed directly to your FiftyOne dataset\u2019s <code>apply_model()</code> method.</p> <p>The examples below show how to run inference with various Transformers models on the following sample dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\", max_samples=25)\ndataset.select_fields().keep_fields()\n</code></pre>"},{"location":"integrations/huggingface/#image-classification","title":"Image classification \u00b6","text":"<p>You can pass <code>transformers</code> classification models directly to FiftyOne dataset\u2019s <code>apply_model()</code> method:</p> <pre><code># BeiT\nfrom transformers import BeitForImageClassification\nmodel = BeitForImageClassification.from_pretrained(\n    \"microsoft/beit-base-patch16-224\"\n)\n\n# DeiT\nfrom transformers import DeiTForImageClassification\nmodel = DeiTForImageClassification.from_pretrained(\n    \"facebook/deit-base-distilled-patch16-224\"\n)\n\n# DINOv2\nfrom transformers import Dinov2ForImageClassification\nmodel = Dinov2ForImageClassification.from_pretrained(\n    \"facebook/dinov2-small-imagenet1k-1-layer\"\n)\n\n# MobileNetV2\nfrom transformers import MobileNetV2ForImageClassification\nmodel = MobileNetV2ForImageClassification.from_pretrained(\n    \"google/mobilenet_v2_1.0_224\"\n)\n\n# Swin Transformer\nfrom transformers import SwinForImageClassification\nmodel = SwinForImageClassification.from_pretrained(\n    \"microsoft/swin-tiny-patch4-window7-224\"\n)\n\n# ViT\nfrom transformers import ViTForImageClassification\nmodel = ViTForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224\"\n)\n\n# ViT-Hybrid\nfrom transformers import ViTHybridForImageClassification\nmodel = ViTHybridForImageClassification.from_pretrained(\n    \"google/vit-hybrid-base-bit-384\"\n)\n\n# Any auto model\nfrom transformers import AutoModelForImageClassification\nmodel = AutoModelForImageClassification.from_pretrained(\n    \"facebook/levit-128S\"\n)\n</code></pre> <pre><code>dataset.apply_model(model, label_field=\"classif_predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Alternatively, you can manually run inference with the <code>transformers</code> model and then use the <code>to_classification()</code> utility to convert the predictions to FiftyOne format:</p> <pre><code>from PIL import Image\nimport torch\nimport fiftyone.utils.transformers as fout\n\nfrom transformers import ViTHybridForImageClassification, AutoProcessor\ntransformers_model = ViTHybridForImageClassification.from_pretrained(\n    \"google/vit-hybrid-base-bit-384\"\n)\nprocessor = AutoProcessor.from_pretrained(\"google/vit-hybrid-base-bit-384\")\nid2label = transformers_model.config.id2label\n\nfor sample in dataset.iter_samples(progress=True):\n    image = Image.open(sample.filepath)\n    inputs = processor(image, return_tensors=\"pt\")\n    with torch.no_grad():\n        result = transformers_model(**inputs)\n\n    sample[\"classif_predictions\"] = fout.to_classification(result, id2label)\n    sample.save()\n</code></pre> <p>Finally, you can load <code>transformers</code> models directly from the FiftyOne Model Zoo!</p> <p>To load a <code>transformers</code> classification model from the zoo, specify <code>\"classification-transformer-torch\"</code> as the first argument, and pass in the model\u2019s name or path as a keyword argument:</p> <pre><code>import fiftyone.zoo as foz\n\nmodel = foz.load_zoo_model(\n    \"classification-transformer-torch\",\n    name_or_path=\"facebook/levit-128S\",  # HF model name or path\n)\n\ndataset.apply_model(model, label_field=\"levit\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"integrations/huggingface/#object-detection","title":"Object detection \u00b6","text":"<p>You can pass <code>transformers</code> detection models directly to your FiftyOne dataset\u2019s <code>apply_model()</code> method:</p> <pre><code># DETA\nfrom transformers import DetaForObjectDetection\nmodel = DetaForObjectDetection.from_pretrained(\n    \"jozhang97/deta-swin-large\"\n)\n\n# DETR\nfrom transformers import DetrForObjectDetection\nmodel = DetrForObjectDetection.from_pretrained(\n    \"facebook/detr-resnet-50\"\n)\n\n# DeformableDETR\nfrom transformers import DeformableDetrForObjectDetection\nmodel = DeformableDetrForObjectDetection.from_pretrained(\n    \"SenseTime/deformable-detr\"\n)\n\n# Table Transformer\nfrom transformers import TableTransformerForObjectDetection\nmodel = TableTransformerForObjectDetection.from_pretrained(\n    \"microsoft/table-transformer-detection\"\n)\n\n# YOLOS\nfrom transformers import YolosForObjectDetection\nmodel = YolosForObjectDetection.from_pretrained(\n    \"hustvl/yolos-tiny\"\n)\n\n# Any auto model\nfrom transformers import AutoModelForObjectDetection\nmodel = AutoModelForObjectDetection.from_pretrained(\n    \"microsoft/conditional-detr-resnet-50\"\n)\n</code></pre> <pre><code>dataset.apply_model(model, label_field=\"det_predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Alternatively, you can manually run inference with the <code>transformers</code> model and then use the <code>to_detections()</code> utility to convert the predictions to FiftyOne format:</p> <pre><code>from PIL import Image\nimport torch\n\nimport fiftyone.utils.transformers as fout\n\nfrom transformers import AutoModelForObjectDetection, AutoProcessor\ntransformers_model = AutoModelForObjectDetection.from_pretrained(\n    \"microsoft/conditional-detr-resnet-50\"\n)\nprocessor = AutoProcessor.from_pretrained(\n    \"microsoft/conditional-detr-resnet-50\"\n)\nid2label = transformers_model.config.id2label\n\nfor sample in dataset.iter_samples(progress=True):\n    image = Image.open(sample.filepath)\n    inputs = processor(image, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = transformers_model(**inputs)\n\n    target_sizes = torch.tensor([image.size[::-1]])\n    result = processor.post_process_object_detection(\n        outputs, target_sizes=target_sizes\n    )\n    sample[\"det_predictions\"] = fout.to_detections(\n        result, id2label, [image.size]\n    )\n    sample.save()\n</code></pre> <p>Finally, you can load <code>transformers</code> models directly from the FiftyOne Model Zoo!</p> <p>To load a <code>transformers</code> detection model from the zoo, specify <code>\"detection-transformer-torch\"</code> as the first argument, and pass in the model\u2019s name or path as a keyword argument:</p> <pre><code>import fiftyone.zoo as foz\n\nmodel = foz.load_zoo_model(\n    \"detection-transformer-torch\",\n    name_or_path=\"facebook/detr-resnet-50\",  # HF model name or path\n)\n\ndataset.apply_model(model, label_field=\"detr\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"integrations/huggingface/#semantic-segmentation","title":"Semantic segmentation \u00b6","text":"<p>You can pass a <code>transformers</code> semantic segmentation model directly to your FiftyOne dataset\u2019s <code>apply_model()</code> method:</p> <pre><code># Mask2Former\nfrom transformers import Mask2FormerForUniversalSegmentation\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(\n    \"facebook/mask2former-swin-small-coco-instance\"\n)\n\n# Mask2Former\nfrom transformers import MaskFormerForInstanceSegmentation\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(\n    \"facebook/maskformer-swin-base-ade\"\n)\n\n# Segformer\nfrom transformers import SegformerForSemanticSegmentation\nmodel = SegformerForSemanticSegmentation.from_pretrained(\n    \"nvidia/segformer-b0-finetuned-ade-512-512\"\n)\n\n# Any auto model\nfrom transformers import AutoModelForSemanticSegmentation\nmodel = AutoModelForSemanticSegmentation.from_pretrained(\n    \"Intel/dpt-large-ade\"\n)\n</code></pre> <pre><code>dataset.apply_model(model, label_field=\"seg_predictions\")\ndataset.default_mask_targets = model.config.id2label\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Alternatively, you can manually run inference with the <code>transformers</code> model and then use the <code>to_segmentation()</code> utility to convert the predictions to FiftyOne format:</p> <pre><code>from PIL import Image\nimport fiftyone.utils.transformers as fout\n\nfrom transformers import AutoModelForSemanticSegmentation, AutoProcessor\ntransformers_model = AutoModelForSemanticSegmentation.from_pretrained(\n    \"Intel/dpt-large-ade\"\n)\nprocessor = AutoProcessor.from_pretrained(\"Intel/dpt-large-ade\")\n\nfor sample in dataset.iter_samples(progress=True):\n    image = Image.open(sample.filepath)\n    inputs = processor(image, return_tensors=\"pt\")\n    target_size = [image.size[::-1]]\n    with torch.no_grad():\n        output = transformers_model(**inputs)\n\n    result = processor.post_process_semantic_segmentation(\n        output, target_sizes=target_size\n    )\n    sample[\"seg_predictions\"] = fout.to_segmentation(result)\n    sample.save()\n</code></pre> <p>Finally, you can load <code>transformers</code> models directly from the FiftyOne Model Zoo!</p> <p>To load a <code>transformers</code> semantic segmentation model from the zoo, specify <code>\"segmentation-transformer-torch\"</code> as the first argument, and pass in the model\u2019s name or path as a keyword argument:</p> <pre><code>import fiftyone.zoo as foz\n\nmodel = foz.load_zoo_model(\n    \"segmentation-transformer-torch\",\n    name_or_path=\"nvidia/segformer-b0-finetuned-ade-512-512\",\n)\n\ndataset.apply_model(model, label_field=\"segformer\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"integrations/huggingface/#monocular-depth-estimation","title":"Monocular depth estimation \u00b6","text":"<p>You can pass a <code>transformers</code> monocular depth estimation model directly to your FiftyOne dataset\u2019s <code>apply_model()</code> method:</p> <pre><code># DPT\nfrom transformers import DPTForDepthEstimation\nmodel = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n\n# GLPN\nfrom transformers import GLPNForDepthEstimation\nmodel = GLPNForDepthEstimation.from_pretrained(\"vinvino02/glpn-kitti\")\n\n# Depth Anything\nfrom transformers import AutoModelForDepthEstimation\nmodel = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n\n# Depth Anything-V2\nfrom transformers import AutoModelForDepthEstimation\nmodel = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/Depth-Anything-V2-Small-hf\")\n</code></pre> <pre><code>dataset.apply_model(model, label_field=\"depth_predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Alternatively, you can load <code>transformers</code> depth estimation models directly from the FiftyOne Model Zoo!</p> <p>To load a <code>transformers</code> depth estimation model from the zoo, specify <code>\"depth-estimation-transformer-torch\"</code> as the first argument, and pass in the model\u2019s name or path as a keyword argument:</p> <pre><code>import fiftyone.zoo as foz\n\nmodel = foz.load_zoo_model(\n    \"depth-estimation-transformer-torch\",\n    name_or_path=\"Intel/dpt-hybrid-midas\",\n)\n\ndataset.apply_model(model, label_field=\"dpt_hybrid_midas\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"integrations/huggingface/#zero-shot-classification","title":"Zero-shot classification \u00b6","text":"<p>Zero-shot image classification models from <code>transformers</code> can be loaded directly from the FiftyOne Model Zoo!</p> <p>To load a <code>transformers</code> zero-shot classification model from the zoo, specify <code>\"zero-shot-classification-transformer-torch\"</code> as the first argument, and pass in the model\u2019s name or path as a keyword argument:</p> <pre><code>import fiftyone.zoo as foz\n\nmodel = foz.load_zoo_model(\n    \"zero-shot-classification-transformer-torch\",\n    name_or_path=\"BAAI/AltCLIP\",  # HF model name or path\n    classes=[\"cat\", \"dog\", \"bird\", \"fish\", \"turtle\"],  # optional\n)\n</code></pre> <p>Once loaded, you can pass the model directly to your FiftyOne dataset\u2019s <code>apply_model()</code> method:</p> <pre><code>dataset.apply_model(model, label_field=\"altclip\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>You can also generate embeddings for the samples in your dataset with zero shot models as follows:</p> <pre><code>import fiftyone.zoo as foz\n\nmodel = foz.load_zoo_model(\n    \"zero-shot-classification-transformer-torch\",\n    name_or_path=\"BAAI/AltCLIP\",  # HF model name or path\n)\n\ndataset.compute_embeddings(model, embeddings_field=\"altclip_embeddings\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>You can also change the label classes of zero shot models any time by setting the <code>classes</code> attribute of the model:</p> <pre><code>model.classes = [\"cat\", \"dog\", \"bird\", \"fish\", \"turtle\"]\n\ndataset.apply_model(model, label_field=\"altclip\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>The <code>convert_transformers_model()</code> utility also allows you to manually convert a zero-shot <code>transformers</code> model to FiftyOne format:</p> <pre><code>import fiftyone.utils.transformers as fout\n\nfrom transformers import CLIPSegModel\ntransformers_model = CLIPSegModel.from_pretrained(\n    \"CIDAS/clipseg-rd64-refined\"\n)\n\nmodel = fout.convert_transformers_model(\n    transformers_model,\n    task=\"image-classification\",  # or \"semantic-segmentation\"\n)\n</code></pre> <p>Note</p> <p>Some zero-shot models are compatible with multiple tasks, so it is recommended that you specify the task type when converting the model.</p>"},{"location":"integrations/huggingface/#zero-shot-object-detection","title":"Zero-shot object detection \u00b6","text":"<p>Zero-shot object detection models from <code>transformers</code> can be loaded directly from the FiftyOne Model Zoo!</p> <p>To load a <code>transformers</code> zero-shot object detection model from the zoo, specify <code>\"zero-shot-detection-transformer-torch\"</code> as the first argument, and pass in the model\u2019s name or path as a keyword argument. You can optionally pass in a list of label classes as a keyword argument <code>classes</code>:</p> <pre><code>import fiftyone.zoo as foz\n\nmodel = foz.load_zoo_model(\n    \"zero-shot-detection-transformer-torch\",\n    name_or_path=\"google/owlvit-base-patch32\",  # HF model name or path\n    classes=[\"cat\", \"dog\", \"bird\", \"fish\", \"turtle\"],  # optional\n)\n</code></pre> <p>The <code>convert_transformers_model()</code> utility also allows you to manually convert a zero-shot <code>transformers</code> model to FiftyOne format:</p> <pre><code>import fiftyone.utils.transformers as fout\n\nfrom transformers import OwlViTForObjectDetection\ntransformers_model = OwlViTForObjectDetection.from_pretrained(\n    \"google/owlvit-base-patch32\"\n)\n\nmodel = fout.convert_transformers_model(\n    transformers_model,\n    task=\"object-detection\",\n)\n</code></pre> <p>Note</p> <p>Some zero-shot models are compatible with multiple tasks, so it is recommended that you specify the task type when converting the model.</p> <p>As of <code>transformers&gt;=4.40.0</code> and <code>fiftyone&gt;=0.24.0</code>, you can also use Grounding DINO models for zero-shot object detection:</p> <pre><code>import fiftyone.zoo as foz\n\nmodel = foz.load_zoo_model(\n    \"zero-shot-detection-transformer-torch\",\n    name_or_path=\"IDEA-Research/grounding-dino-tiny\",\n    classes=[\"cat\"],\n)\n\ndataset.apply_model(model, label_field=\"cats\", confidence_thresh=0.2)\n</code></pre> <p>Note</p> <p>The <code>confidence_thresh</code> parameter is optional and can be used to filter out predictions with confidence scores below the specified threshold. You may need to adjust this value based on the model and dataset you are working. Also note that whereas OwlViT models accept multiple classes, Grounding DINO models only accept a single class.</p>"},{"location":"integrations/huggingface/#batch-inference","title":"Batch inference \u00b6","text":"<p>When using <code>apply_model()</code>, you can request batch inference by passing the optional <code>batch_size</code> parameter:</p> <pre><code>dataset.apply_model(model, label_field=\"det_predictions\", batch_size=16)\n</code></pre> <p>The manual inference loops can be also executed using batch inference via the pattern below:</p> <pre><code>from fiftyone.core.utils import iter_batches\nimport fiftyone.utils.transformers as fout\n\n# Load a detection model and its corresponding processor\nfrom transformers import YolosForObjectDetection, AutoProcessor\ntransformers_model = YolosForObjectDetection.from_pretrained(\n    \"hustvl/yolos-tiny\"\n)\nprocessor = AutoProcessor.from_pretrained(\"hustvl/yolos-tiny\")\nid2label = transformers_model.config.id2label\n\nfilepaths = dataset.values(\"filepath\")\nbatch_size = 16\n\npredictions = []\nfor paths in iter_batches(filepaths, batch_size):\n    images = [Image.open(p) for p in paths]\n    image_sizes = [i.size for i in images]\n    target_sizes = torch.tensor([image.size[::-1] for image in images])\n    inputs = processor(images, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = transformers_model(**inputs)\n\n    results = processor.post_process_object_detection(\n        outputs, target_sizes=target_sizes\n    )\n    predictions.extend(fout.to_detections(results, id2label, image_sizes))\n\ndataset.set_values(\"det_predictions\", predictions)\n</code></pre> <p>Note</p> <p>See this section for more information about performing batch updates to your FiftyOne datasets.</p>"},{"location":"integrations/huggingface/#embeddings","title":"Embeddings \u00b6","text":"<p>Any <code>transformers</code> model that supports image classification or object detection tasks \u2014 zero-shot or otherwise \u2014 can be used to compute embeddings for your samples.</p> <p>Note</p> <p>For zero-shot models, FiftyOne will use the <code>transformers</code> model\u2019s <code>get_image_features()</code> method to extract embeddings.</p> <p>For non-zero-shot models, regardless of whether you use a classification, detection, or base model, FiftyOne will extract embeddings from the <code>last_hidden_state</code> of the model\u2019s base encoder.</p>"},{"location":"integrations/huggingface/#image-embeddings","title":"Image embeddings \u00b6","text":"<p>To compute embeddings for images, you can pass the <code>transformers</code> model directly to your FiftyOne dataset\u2019s <code>compute_embeddings()</code> method:</p> <pre><code># Embeddings from base model\nfrom transformers import BeitModel\nmodel = BeitModel.from_pretrained(\n    \"microsoft/beit-base-patch16-224-pt22k\"\n)\n\n# Embeddings from classification model\nfrom transformers import BeitForImageClassification\nmodel = BeitForImageClassification.from_pretrained(\n    \"microsoft/beit-base-patch16-224\"\n)\n\n# Embeddings from detection model\nfrom transformers import DetaForObjectDetection\nmodel = DetaForObjectDetection.from_pretrained(\n    \"jozhang97/deta-swin-large-o365\"\n)\n\n# Embeddings from zero-shot classification model\nfrom transformers import AltCLIPModel\nmodel = AltCLIPModel.from_pretrained(\n    \"BAAI/AltCLIP\"\n)\n\n# Embeddings from zero-shot detection model\nfrom transformers import OwlViTForObjectDetection\nmodel = OwlViTForObjectDetection.from_pretrained(\n    \"google/owlvit-base-patch32\"\n)\n</code></pre> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\", max_samples=25)\ndataset.select_fields().keep_fields()\n\ndataset.compute_embeddings(model, embeddings_field=\"embeddings\")\n</code></pre> <p>Alternatively, you can use the <code>convert_transformers_model()</code> utility to convert a <code>transformers</code> model to FiftyOne format, which allows you to check the model\u2019s <code>has_embeddings</code> property to see if the model can be used to generate embeddings:</p> <pre><code>import numpy as np\nfrom PIL import Image\nimport fiftyone.utils.transformers as fout\n\nfrom transformers import BeitModel\ntransformers_model = BeitModel.from_pretrained(\n    \"microsoft/beit-base-patch16-224-pt22k\"\n)\n\nmodel = fout.convert_transformers_model(transformers_model)\nprint(model.has_embeddings)  # True\n\n# Embed an image directly\nimage = Image.open(dataset.first().filepath)\nembedding = model.embed(np.array(image))\n</code></pre>"},{"location":"integrations/huggingface/#text-embeddings","title":"Text embeddings \u00b6","text":"<p>Zero-shot image classification and object detection models from <code>transformers</code> can also be used to compute embeddings for text:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\", max_samples=25)\ndataset.select_fields().keep_fields()\n\nmodel = foz.load_zoo_model(\n    \"zero-shot-classification-transformer-torch\",\n    name_or_path=\"BAAI/AltCLIP\",\n)\n\nembedding = model.embed_prompt(\"a photo of a dog\")\n</code></pre> <p>You can check whether a model supports text embeddings by checking the <code>can_embed_prompts</code> property:</p> <pre><code>import fiftyone.zoo as foz\n\n# A zero-shot model that supports text embeddings\nmodel = foz.load_zoo_model(\n    \"zero-shot-classification-transformer-torch\",\n    name_or_path=\"BAAI/AltCLIP\",\n)\nprint(model.can_embed_prompts)  # True\n\n# A classification model that does not support text embeddings\nmodel = foz.load_zoo_model(\n    \"classification-transformer-torch\",\n    name_or_path=\"microsoft/beit-base-patch16-224\",\n)\nprint(model.can_embed_prompts)  # False\n</code></pre>"},{"location":"integrations/huggingface/#batch-embeddings","title":"Batch embeddings \u00b6","text":"<p>You can request batch inference by passing the optional <code>batch_size</code> parameter to <code>compute_embeddings()</code>:</p> <pre><code>dataset.compute_embeddings(model, embeddings_field=\"embeddings\", batch_size=16)\n</code></pre>"},{"location":"integrations/huggingface/#patch-embeddings","title":"Patch embeddings \u00b6","text":"<p>You can compute embeddings for image patches by passing <code>transformers</code> models directly to your FiftyOne dataset\u2019s <code>compute_patch_embeddings()</code> method:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.utils.transformers as fout\n\ndataset = foz.load_zoo_dataset(\"quickstart\", max_samples=25)\n\nfrom transformers import BeitModel\nmodel = BeitModel.from_pretrained(\n    \"microsoft/beit-base-patch16-224-pt22k\"\n)\n\ndataset.compute_patch_embeddings(\n    model,\n    patches_field=\"ground_truth\",\n    embeddings_field=\"embeddings\",\n)\n</code></pre>"},{"location":"integrations/huggingface/#brain-methods","title":"Brain methods \u00b6","text":"<p>Because <code>transformers</code> models can be used to compute embeddings, they can be passed to Brain methods like <code>compute_similarity()</code> and <code>compute_visualization()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\", max_samples=25)\n\n# Classification model\nfrom transformers import BeitModel\ntransformers_model = BeitModel.from_pretrained(\n    \"microsoft/beit-base-patch16-224-pt22k\"\n)\n\n# Detection model\nfrom transformers import DetaForObjectDetection\ntransformers_model = DetaForObjectDetection.from_pretrained(\n    \"jozhang97/deta-swin-large\"\n)\n\n# Zero-shot classification model\nfrom transformers import AutoModelForImageClassification\ntransformers_model = AutoModelForImageClassification.from_pretrained(\n    \"BAAI/AltCLIP\"\n)\n\n# Zero-shot detection model\nfrom transformers import OwlViTForObjectDetection\ntransformers_model = OwlViTForObjectDetection.from_pretrained(\n    \"google/owlvit-base-patch32\"\n)\n</code></pre> <pre><code># Option 1: directly pass `transformers` model\nfob.compute_similarity(dataset, model=transformers_model, brain_key=\"sim1\")\nfob.compute_visualization(dataset, model=transformers_model, brain_key=\"vis1\")\n</code></pre> <pre><code># Option 2: pass pre-computed embeddings\ndataset.compute_embeddings(transformers_model, embeddings_field=\"embeddings\")\n\nfob.compute_similarity(dataset, embeddings=\"embeddings\", brain_key=\"sim2\")\nfob.compute_visualization(dataset, embeddings=\"embeddings\", brain_key=\"vis2\")\n</code></pre> <p>Because <code>transformers</code> zero-shot models can be used to embed text, they can also be used to construct similarity indexes on your datasets which support natural language queries.</p> <p>To use this functionality, you must pass the model by name into the brain method, along with any necessary keyword arguments that must be passed to <code>load_zoo_model()</code> to load the correct model:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\", max_samples=25)\n\nfob.compute_similarity(\n    dataset,\n    brain_key=\"zero_shot_sim\",\n    model=\"zero-shot-classification-transformer-torch\",\n    name_or_path=\"BAAI/AltCLIP\",\n)\n\nview = dataset.sort_by_similarity(\"A photo of a dog\", k=25)\n\nsession = fo.launch_app(view)\n</code></pre>"},{"location":"integrations/huggingface/#hugging-face-hub","title":"Hugging Face Hub \u00b6","text":"<p>FiftyOne integrates with the Hugging Face Hub to allow you to push datasets to and load datasets from the Hub with ease. This integration simplifies the process of sharing datasets with the machine learning and computer vision community, and allows you to easily access and work with many of the most popular vision and multimodal datasets available!</p>"},{"location":"integrations/huggingface/#setup_1","title":"Setup \u00b6","text":"<p>To push datasets to and load datasets from the Hugging Face Hub, you will need the Hugging Face Hub Python client, which you can install via PyPI:</p> <pre><code>pip install \"huggingface_hub&gt;=0.20.0\"\n</code></pre> <p>To push a dataset to the Hub, and in some cases, to access a dataset on the hub, you will need to have a Hugging Face Hub account.</p> <p>Hugging Face handles authentication via tokens, which you can obtain by logging into your account and navigating to the Access Tokens section of your profile. At the bottom of this page, you can create a new token with write or read access to the Hub. Once you have your token, you can set it as an environment variable:</p> <pre><code>export HF_TOKEN=\"&lt;your-token-here&gt;\"\n</code></pre>"},{"location":"integrations/huggingface/#pushing-datasets-to-the-hub","title":"Pushing datasets to the Hub \u00b6","text":"<p>If you are working with a dataset in FiftyOne and you want to quickly share it with others, you can do so via the <code>push_to_hub()</code> function, which takes two positional arguments:</p> <ul> <li> <p>the FiftyOne sample collection (a <code>Dataset</code> or <code>DatasetView</code>)</p> </li> <li> <p>the <code>repo_name</code>, which will be combined with your Hugging Face username or organization name to construct the <code>repo_id</code> where the sample collection will be uploaded.</p> </li> </ul> <p>As you will see, this simple function allows you to push datasets and filtered views containing images, videos, point clouds, and other multimodal data to the Hugging Face Hub, providing you with incredible flexibility in the process.</p>"},{"location":"integrations/huggingface/#basic-usage","title":"Basic usage \u00b6","text":"<p>The basic recipe for pushing a FiftyOne dataset to the Hub is just two lines of code. As a starting point, let\u2019s use the example Quickstart dataset dataset from the FiftyOne Dataset Zoo:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n</code></pre> <p>To push the dataset to the Hugging Face Hub, all you need to do is call <code>push_to_hub()</code> with the dataset and the desired <code>repo_name</code>:</p> <pre><code>from fiftyone.utils.huggingface import push_to_hub\n\npush_to_hub(dataset, \"my-quickstart-dataset\")\n</code></pre> <p>When you run this code, a few things happen:</p> <ul> <li> <p>The dataset and its media files are exported to a temporary directory and uploaded to the specified Hugging Face repo.</p> </li> <li> <p>A <code>fiftyone.yml</code> config file for the dataset is generated and uploaded to the repo, which contains all of the necessary information so that the dataset can be loaded with <code>load_from_hub()</code>.</p> </li> <li> <p>A Hugging Face Dataset Card for the dataset is auto-generated, providing tags, metadata, license info, and a code snippet illustrating how to load the dataset from the hub.</p> </li> </ul> <p>Your dataset will be available on the Hub at the following URL:</p> <pre><code>https://huggingface.co/datasets/&lt;your-username-or-org-name&gt;/my-quickstart-dataset\n</code></pre> <p>Pushing a <code>DatasetView</code> to the Hub works in exactly the same way. For example, if you want to push a filtered view of the <code>quickstart</code> dataset containing only predictions with high confidence, you can do so by creating the view as usual, and then passing that in to <code>push_to_hub()</code>:</p> <pre><code>from fiftyone.utils.huggingface import push_to_hub\n\n# Create view with high confidence predictions\nview = dataset.filter_labels(\"predictions\", F(\"confidence\") &gt; 0.95)\n\n# Push view to the Hub as a new dataset\npush_to_hub(view, \"my-quickstart-high-conf\")\n</code></pre> <p>When you do so, note that the view is exported as a new dataset, and other details from the original dataset are not included.</p> <p>FiftyOne is a visual toolkit, so when you push a dataset to the Hub, you can optionally include a preview (image, gif, or video) of the dataset, that will be displayed on the dataset page. To do this, you can pass the <code>preview_path</code> argument to <code>push_to_hub()</code>, with either a relative or absolute path to the preview file on your local machine:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\nfrom fiftyone.utils.huggingface import push_to_hub\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nsession = fo.launch_app(dataset)\n\n# Screenshot and save the preview image to a file...\n\npush_to_hub(\n    dataset,\n    \"my-quickstart-with-preview\",\n    preview_path=\"/path/to/preview.jpg\"\n)\n</code></pre> <p>The preview file will be uploaded to the Hub along with the dataset, and will be displayed on the dataset card!</p> <p></p>"},{"location":"integrations/huggingface/#pushing-large-datasets","title":"Pushing large datasets \u00b6","text":"<p>Large datasets with many samples require a bit more care when pushing to the Hub. Hugging Face limits the number of files that can be uploaded in a single directory to 10000, so if your dataset contains more than 10000 samples, the data will need to be split into multiple directories. FiftyOne handles this automatically when pushing large datasets to the Hub, but you can manually configure the number of samples per directory by passing the <code>chunk_size</code> argument to <code>push_to_hub()</code>:</p> <pre><code>from fiftyone.utils.huggingface import push_to_hub\n\n# Limit to 100 images per directory\npush_to_hub(dataset, \"my-large-dataset\", chunk_size=100)\n</code></pre> <p>Note</p> <p>The <code>chunk_size</code> argument is currently only supported when exporting in FiftyOneDataset format (the default).</p>"},{"location":"integrations/huggingface/#advanced-usage","title":"Advanced usage \u00b6","text":"<p>The <code>push_to_hub()</code> function provides a number of optional arguments that allow you to customize how your dataset is pushed to the Hub, including whether the dataset is public or private, what license it is released under, and more.</p> <p>FiftyOne\u2019s <code>push_to_hub()</code> function supports the Hugging Face Hub API arguments <code>private</code> and <code>exist_ok</code>.</p> <ul> <li> <p>private (bool): Whether the dataset should be private. If <code>True</code>, the dataset will be private and only accessible to you. If <code>False</code>, the dataset will be public and accessible to anyone with the link. Defaults to <code>False</code>.</p> </li> <li> <p>exist_ok (bool): Whether to overwrite an existing dataset with the same</p> </li> </ul> <p><code>repo_name</code>. If <code>True</code>, the existing dataset will be overwritten. If <code>False</code>, an error will be raised if a dataset with the same <code>repo_name</code> already exists. Defaults to <code>False</code>.</p> <p>For example, to push a dataset to the Hub as private, you can do the following:</p> <pre><code>from fiftyone.utils.huggingface import push_to_hub\n\npush_to_hub(dataset, \"my-private-dataset\", private=True)\n</code></pre> <p>You can also specify the <code>tags</code>, <code>license</code>, and <code>description</code> of the dataset, all of which will propagate to the <code>fiftyone.yml</code> config file and the Hugging Face Dataset Card. For example, to push a video action recognition dataset with an MIT license and a description, you can do the following:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone.utils.huggingface import push_to_hub\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\n\npush_to_hub(\n    dataset,\n    \"my-action-recognition-dataset\",\n    tags=[\"video\", \"action-recognition\"],\n    license=\"mit\",\n    description=\"A dataset of videos for action recognition tasks\",\n)\n</code></pre> <p>The pushed dataset will be available on the Hub and the dataset page will look like this:</p> <p></p> <p>Note</p> <p>The <code>tags</code> argument can be a string or a list of strings. The tag <code>fiftyone</code> is automatically added to all datasets pushed with FiftyOne, communicating that the dataset was created with FiftyOne and can be loaded with the <code>load_from_hub()</code> function.</p> <p>The license is specified as a string. For a list of supported licenses, see the Hugging Face Hub documentation.</p> <p>The <code>description</code> argument can be used for whatever you like. When the dataset is loaded from the Hub, this description will be accessible via the dataset\u2019s <code>description</code> property.</p> <p>Additionally, you can specify the \u201cformat\u201d of the uploaded dataset. By default, the format is the standard FiftyOneDataset format, but you can also specify the data is uploaded in any of these common formats. For example, to push the quickstart dataset in COCO format, with a Creative Commons Attribution 4.0 license, you can do the following:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone.utils.huggingface import push_to_hub\nimport fiftyone.types as fot\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\ndataset_type = fot.dataset_types.COCODetectionDataset\n\npush_to_hub(\n    dataset,\n    \"quickstart-coco\",\n    dataset_type=dataset_type,\n    license=\"cc-by-4.0\",\n    label_fields=\"*\",  # convert all label fields, not just ground truth\n)\n</code></pre> <p>Note</p> <p>The <code>label_fields</code> argument is used to specify which label fields to convert to the specified dataset type. By default when using some dataset formats, only the <code>ground_truth</code> label field is converted. If you want to convert all label fields, you can set <code>label_fields=\"*\"</code>. If you want to convert specific label fields, you can pass a list of field names.</p> <p>Additionally, you can specify the minimum version of FiftyOne required to load the dataset by passing the <code>min_fiftyone_version</code> argument. This is useful when the dataset utilizes features that are only available in versions above a certain release. For example, to specify that the dataset requires <code>fiftyone&gt;=0.23.0</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone.utils.huggingface import push_to_hub\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\npush_to_hub(\n    dataset,\n    \"quickstart-min-version\",\n    min_fiftyone_version=\"0.23.0\",\n)\n</code></pre>"},{"location":"integrations/huggingface/#loading-datasets-from-the-hub","title":"Loading datasets from the Hub \u00b6","text":"<p>To load a dataset from the Hugging Face Hub, you can use the <code>load_from_hub()</code> function. This function supports loading datasets in any of the common formats supported by FiftyOne, as well as image-based datasets stored via Parquet files, as is common with datasets from the datasets library which have been uploaded to the Hugging Face Hub. Below, we will walk through all of the ways you can load datasets from the Hub.</p> <p>In its simplest usage, the <code>load_from_hub()</code> function only requires the <code>repo_id</code> of the dataset you want to load. For example, to load the private dataset that we pushed to the Hub earlier, you can do the following:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\"&lt;username-or-org&gt;/my-private-dataset\")\n</code></pre> <p>Note</p> <p>As long as you have an environment variable <code>HF_TOKEN</code> set with your Hugging Face token (with read access), you can load private or gated datasets that you have access to from the Hub.</p>"},{"location":"integrations/huggingface/#loading-datasets-from-repo-configs","title":"Loading datasets from repo configs \u00b6","text":"<p>When you push a dataset to the Hub using <code>push_to_hub()</code>, a <code>fiftyone.yml</code> config file is generated and uploaded to the repo. This file contains all of the information necessary to load the dataset from the Hugging Face Hub. More generally, any repo on the Hugging Face Hub that contains a <code>fiftyone.yml</code> or <code>fiftyone.yaml</code> file (assuming the file is correctly formatted) can be loaded using the <code>load_from_hub()</code> function by passing the <code>repo_id</code> of the dataset, without needing to specify any additional arguments.</p> <p>For example, to load the <code>quickstart</code> dataset that we pushed to the Hub earlier,</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\"&lt;username&gt;/my-quickstart-dataset\")\n</code></pre> <p>where <code>&lt;username&gt;</code> is your Hugging Face username or organization name.</p>"},{"location":"integrations/huggingface/#loading-datasets-from-local-configs","title":"Loading datasets from local configs \u00b6","text":"<p>If the repo was uploaded to the Hugging Face Hub via FiftyOne\u2019s <code>push_to_hub()</code> function, then the <code>fiftyone.yml</code> config file will be generated and uploaded to the repo. However, some common datasets like mnist were uploaded to the Hub using the <code>datasets</code> library and do not contain a <code>fiftyone.yml</code> or <code>fiftyone.yaml</code> file. If you know how the dataset is structured, you can load the dataset by passing the path to a local yaml config file that describes the dataset via the <code>config_file</code> keyword argument.</p> <p>For example, to load the <code>mnist</code> dataset from the Hub, you might have a local yaml config file like this:</p> <pre><code>format: ParquetFilesDataset\nclassification_fields: label\n</code></pre> <p>To load the dataset from the Hub, you can pass the <code>repo_id</code> of the dataset and the path to the local yaml config file:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"ylecun/mnist\",\n    config_file=\"/path/to/mnist.yml\",\n)\n</code></pre> <p>For a comprehensive list of the supported fields in the yaml config file, see Supported config fields.</p>"},{"location":"integrations/huggingface/#loading-datasets-with-config-kwargs","title":"Loading datasets with config kwargs \u00b6","text":"<p>In addition to loading datasets from repo configs and local configs, you can also load datasets from the Hub by passing the necessary config arguments directly to <code>load_from_hub()</code>. This is useful when you want to load a dataset from the Hub that does not have a <code>fiftyone.yml</code> or <code>fiftyone.yaml</code> file, and the structure of the dataset is simple enough that you can specify the necessary arguments directly.</p> <p>For example, to load the <code>mnist</code> dataset from the Hub, you can pass the <code>format</code> and <code>classification_fields</code> arguments directly:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"ylecun/mnist\",\n    format=\"ParquetFilesDataset\",\n    classification_fields=\"label\",\n)\n</code></pre> <p>This will tell FiftyOne that the data is stored in Parquet files, and that the <code>label</code> field should be treated as a classification field, to be converted into a <code>Classification</code> label field in the dataset.</p>"},{"location":"integrations/huggingface/#supported-config-fields","title":"Supported config fields \u00b6","text":"<p>Whether you are loading a dataset from a repo config, a local config file, or passing the config arguments directly, you can specify a number of fields.</p> <p>Broadly speaking, these fields fall into three categories: format specification, media field specification, and label field specification.</p> <p>Let\u2019s look at these categories in more detail:</p> <p>Format specification:</p> <ul> <li> <p>format (str): The format of the dataset. This can be any of the common formats supported by FiftyOne \u2014 just pass the name of the format as a string. For example, to load a dataset in the COCO format, you can pass <code>format=\"COCODetectionDataset\"</code>. To specify that the dataset is stored in Parquet files, you can pass <code>format=\"ParquetFilesDataset\"</code> (or simply <code>format=\"parquet\"</code> for short). This is the only required field.</p> </li> <li> <p>name (str): The name of the FiftyOne <code>Dataset</code> to be created. If the <code>repo_id</code> is cumbersome, this can be used to specify a simpler default name. For example, for this sheep dataset rather than using the <code>repo_id</code> <code>keremberke/aerial-sheep-object-detection</code>, you can specify <code>name=\"sheep-detection\"</code>.</p> </li> <li> <p>subsets (str or list): The subset or subsets of the Hugging Face dataset that are compatible with this config, and are available to be loaded. In Hugging Face, the \u201cdataset\u201d in a repo can contain multiple \u201csubsets\u201d, which may or may not have the same schema. Take the Street View House Numbers dataset for example. This dataset has two subsets: <code>\"cropped_digits\"</code> and <code>\"full_numbers\"</code>. The <code>cropped_digits</code> subset contains classification labels, while the <code>full_numbers</code> subset contains detection labels. A single config would not be able to specify the schema for both subsets, so you can specify the subset you want to load (or if you are the dataset author, which subset you want to allow people to load in this way) with the <code>subsets</code> field. For example, to load the <code>cropped_digits</code> subset of the SVHN dataset, you can pass <code>subsets=\"cropped_digits\"</code>. Note that this is not a required field, and by default all subsets are loaded. Also note that subsets are distinct from splits in the dataset, which are handled by the <code>splits</code> field (see below).</p> </li> <li> <p>splits (str or list): The split or splits of the Hugging Face dataset that are compatible with this config, and are available to be loaded. As is standard for machine learning, many datasets are split into training, validation, and test sets. The specific names of these splits may vary from dataset to dataset, but <code>load_from_hub()</code> identifies the names of all splits and by default, will assume that all of these splits are to be loaded. If you only want to load a specific split or splits, you can specify them with the <code>splits</code> field. For example, to load the training split of the CIFAR10 dataset, you can pass <code>splits=\"train\"</code>. If you want to load multiple splits, you can pass them as a list, e.g., <code>splits=[\"train\", \"test\"]</code>. Note that this is not a required field, and by default all splits are loaded.</p> </li> </ul> <p>Media field specification:</p> <p>While not all Parquet datasets contain media fields, all FiftyOne <code>Sample</code> objects must be connected to at least one media file. The following fields can be used to configure the media fields in the Hugging Face dataset that should be converted to FiftyOne media fields:</p> <ul> <li> <p>filepath (str): In FiftyOne, <code>filepath</code> is a default field that is used to store the path to the primary media file for each sample in the dataset. For Hugging Face parquet datasets, primary media fields for image datasets are typically stored in the <code>image</code> columns, so this is where FiftyOne\u2019s <code>load_from_hub()</code> looks by default. If the primary media field is stored in a different column, you can specify the column name with the key <code>filepath</code>. For example, the COYO-700M dataset has the primary media field referenced in the <code>url</code> column. Specifying <code>filepath=\"url\"</code> will tell FiftyOne to look in the <code>url</code> column for the primary media file path. Images will be downloaded from the corresponding URLs and saved to disk.</p> </li> <li> <p>thumbnail_path (str): The field containing the path to a thumbnail image for each sample in the dataset, if such a field exists. If a <code>thumbnail_path</code> is specified, this media file will be shown in the sample grid in the FiftyOne App. This can be useful for quickly visualizing the dataset when the primary media field contains large (e.g., high-resolution) images. For more information on thumbnail images, see this section.</p> </li> <li> <p>additional_media_fields (dict): If each sample has multiple associated media files that you may want to visualize in the FiftyOne App, you can specify these non-default media fields in the <code>additional_media_fields</code> dictionary, where the keys are the column names in the Hugging Face dataset and the values are the names of the fields in the FiftyOne <code>Dataset</code> that will store the paths. Note that this is not the same as grouped datasets.</p> </li> </ul> <p>Label field specification:</p> <p>FiftyOne\u2019s Hugging Face Hub integration currently supports converting labels of type <code>Classification</code>, <code>Detections</code>, and <code>Segmentation</code> from Hugging Face Parquet datasets to FiftyOne label fields. The following fields can be used to specify the label fields in the Hugging Face dataset that should be converted to FiftyOne label fields:</p> <ul> <li> <p>classification_fields (str or list): The column or columns in the Hugging Face dataset that should be converted to FiftyOne <code>Classification</code> label fields. contain classification labels. For example, if the dataset contains a <code>label</code> field that contains classification labels, you can specify <code>classification_fields=\"label\"</code>. If the dataset contains multiple classification fields, you can specify them as a list, e.g., <code>classification_fields=[\"label1\", \"label2\"]</code>. This is not a required field, and if the dataset does not contain classification labels, you can omit it.</p> </li> <li> <p>detection_fields (str or list): The column or columns in the Hugging Face dataset that should be converted to FiftyOne <code>Detections</code> label fields. If the dataset contains detection labels, you can specify the column name or names here. For example, if the dataset contains a <code>detections</code> field that contains detection labels, you can specify <code>detection_fields=\"detections\"</code>. If the dataset contains multiple detection fields, you can specify them as a list, e.g., <code>detection_fields=[\"detections1\", \"detections2\"]</code>. This is not a required field, and if the dataset does not contain detection labels, you can omit it.</p> </li> <li> <p>mask_fields (str or list): The column or columns in the Hugging Face dataset that should be converted to FiftyOne <code>Segmentation</code> label fields. The column in the Hugging Face dataset must contain an image or the URL for an image that can be used as a segmentation mask. If necessary, the images will be downloaded and saved to disk. If the dataset contains mask labels, you can specify the column name or names here. For example, if the dataset contains a <code>masks</code> field that contains mask labels, you can specify <code>mask_fields=\"masks\"</code>. This is not a required field, and if the dataset does not contain mask labels, you can omit it.</p> </li> </ul>"},{"location":"integrations/huggingface/#configuring-the-download-process","title":"Configuring the download process \u00b6","text":"<p>When loading datasets from the Hugging Face Hub, FiftyOne will download the all of the data specified by the <code>repo_id</code> and the config. If no splits or subsets are listed in the config, this means that all samples across all splits and subsets will be downloaded. This can be a time-consuming process, especially for large datasets, and sometimes you may only want to download a fixed number of samples to get started exploring the dataset.</p> <p>FiftyOne\u2019s <code>load_from_hub()</code> function supports a variety of arguments that allow you to control the download process, from the maximum number of samples to be downloaded to the batch size to use when making requests to the Datasets Server. Here are the supported arguments:</p> <ul> <li> <p>max_samples (int): The number of samples to download from the dataset. If not specified, all samples will be downloaded.</p> </li> <li> <p>batch_size (int): The batch size to use when making requests to the Datasets Server. Defaults to 100, which is the max batch size allowed by the Datasets Server.</p> </li> <li> <p>num_workers (int): The number of worker to use when downloading media files. If not specified, the number of workers will be resolved by looking at your FiftyOne Config.</p> </li> <li> <p>splits (str or list): The split or splits of the Hugging Face dataset that you want to download. This overrides the <code>splits</code> field in the config.</p> </li> <li> <p>subsets (str or list): The subset or subsets of the Hugging Face dataset that you want to download. This overrides the <code>subsets</code> field in the config.</p> </li> <li> <p>overwrite (bool): Whether to overwrite existing an existing dataset with the same name. If <code>True</code>, the existing dataset will be overwritten. If <code>False</code>, an error will be raised if a dataset with the same name already exists. Defaults to <code>False</code>.</p> </li> <li> <p>persistent (bool): Whether to persist the dataset to the underlying database after it is loaded. If <code>True</code>, the dataset will be available for loading in future FiftyOne sessions by passing the dataset\u2019s name into FiftyOne\u2019s <code>load_dataset()</code> function. Defaults to <code>False</code>.</p> </li> <li> <p>revision (str): The revision (specified by a commit hash to the Hugging Face repo) of the dataset to load. If not specified, the latest revision will be loaded.</p> </li> </ul>"},{"location":"integrations/huggingface/#basic-examples","title":"Basic examples \u00b6","text":"<p>Okay, so <code>load_from_hub()</code> is very powerful, and can be used in a ton of ways. All of this flexibility can be a bit overwhelming, so let\u2019s walk through a few examples to show you how easy it is in practice to load datasets from the Hugging Face Hub.</p> <p>Note</p> <p>To make these downloads as fast as possible, we recommend setting the <code>max_samples</code> argument to a reasonable number, like 1000, to get a feel for the dataset. If you like what you see, you can always download more samples!</p> <p>Classification Datasets</p> <p>Let\u2019s start by loading the MNIST dataset into FiftyOne. All you need to do is pass the <code>repo_id</code> of the dataset \u2014\u00a0in this case <code>\"ylecun/mnist\"</code> \u2014 to <code>load_from_hub()</code>, specify the format as <code>\"parquet\"</code>, and specify the <code>classification_fields</code> as <code>\"label\"</code>:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"ylecun/mnist\",\n    format=\"parquet\",\n    classification_fields=\"label\",\n    max_samples=1000,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>The same exact syntax works for the CIFAR-10 and FashionMNIST datasets, which are also available on the Hub. In fact, you can load any of the following classification datasets from the Hub using the same syntax, just by changing the <code>repo_id</code>:</p> <ul> <li> <p>CIFAR-10 (use <code>\"uoft-cs/cifar10\"</code>)</p> </li> <li> <p>ImageNet (use <code>\"ILSVRC/imagenet-1k\"</code>)</p> </li> <li> <p>FashionMNIST (use <code>\"zalando-datasets/fashion_mnist\"</code>)</p> </li> <li> <p>Tiny ImageNet (use <code>\"zh-plus/tiny-imagenet\"</code>)</p> </li> <li> <p>Food-101 (use <code>\"ethz/food101\"</code>)</p> </li> <li> <p>Dog Food (use <code>\"sasha/dog-food\"</code>)</p> </li> <li> <p>ImageNet-Sketch (use <code>\"songweig/imagenet_sketch\"</code>)</p> </li> <li> <p>Oxford Flowers (use <code>\"nelorth/oxford-flowers\"</code>)</p> </li> <li> <p>Cats vs. Dogs (use <code>\"microsoft/cats_vs_dogs\"</code>)</p> </li> <li> <p>ObjectNet-1.0 (use <code>\"timm/objectnet\"</code>)</p> </li> </ul> <p>A very similar syntax can be used to load classification datasets that contain multiple classification fields, such as CIFAR-100 and the WikiArt dataset. For example, to load the CIFAR-100 dataset, you can specify the <code>classification_fields</code> as <code>[\"coarse_label\", \"fine_label\"]</code>:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"uoft-cs/cifar100\",\n    format=\"parquet\",\n    classification_fields=[\"coarse_label\", \"fine_label\"],\n    max_samples=1000,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>To load the WikiArt dataset, you can specify the <code>classification_fields</code> as <code>[\"artist\", \"genre\", \"style\"]</code>:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"huggan/wikiart\",\n    format=\"parquet\",\n    classification_fields=[\"artist\", \"genre\", \"style\"],\n    max_samples=1000,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>As touched upon earlier, you can also load a classification subset of a dataset. For example, to load the <code>cropped_digits</code> subset of the Street View House Numbers dataset:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"ufldl-stanford/svhn\",\n    format=\"parquet\",\n    classification_fields=\"label\",\n    subsets=\"cropped_digits\",\n    max_samples=1000,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Detection Datasets</p> <p>Loading detection datasets from the Hub is just as easy. For example, to load the MS COCO dataset, you can specify the <code>detection_fields</code> as <code>\"objects\"</code>, which is the standard column name for detection features in Hugging Face datasets:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"detection-datasets/coco\",\n    format=\"parquet\",\n    detection_fields=\"objects\",\n    max_samples=1000,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>The same syntax works for many other popular detection datasets on the Hub, including:</p> <ul> <li> <p>CPPE - 5 (use <code>\"rishitdagli/cppe-5\"</code>)</p> </li> <li> <p>WIDER FACE (use <code>\"CUHK-CSE/wider_face\"</code>)</p> </li> <li> <p>License Plate Object Detection (use <code>\"keremberke/license-plate-object-detection\"</code>)</p> </li> <li> <p>Aerial Sheep Object Detection (use <code>\"keremberke/aerial-sheep-object-detection\"</code>)</p> </li> </ul> <p>Some detection datasets have their detections stored under a column with a different name. For example, the <code>full_numbers</code> subset of the Street View House Numbers dataset stores its detections under the column <code>digits</code>. To load this subset, you can specify the <code>detection_fields</code> as <code>\"digits\"</code>:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"ufldl-stanford/svhn\",\n    format=\"parquet\",\n    detection_fields=\"digits\",\n    subsets=\"full_numbers\",\n    max_samples=1000,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>Not all detection datasets on the Hub are stored in a format that is currently supported by FiftyOne. For instance, the Fashionpedia dataset has detections stored in Pascal VOC format, which is not the standard\\ Hugging Face format.</p> <p>Segmentation Datasets</p> <p>Loading segmentation datasets from the Hub is also a breeze. For example, to load the \u201cinstance_segmentation\u201d subset from SceneParse150, all you need to do is specify the <code>mask_fields</code> as <code>\"annotation\"</code>:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"zhoubolei/scene_parse150\",\n    format=\"parquet\",\n    subsets=\"instance_segmentation\",\n    mask_fields=\"annotation\",\n    max_samples=1000,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Many other segmentation datasets on the Hub can be loaded in the same way, such as ADE 20K Tiny:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"nateraw/ade20k-tiny\",\n    format=\"parquet\",\n    mask_fields=\"label\",\n)\n\n# only 20 samples in the dataset\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>In other cases, because there are now multiple image columns \u2014 one for the sample image and one for the mask \u2014 the naming convention for the dataset might be different, and you may need to explicitly specify the <code>filepath</code>. For example, to load the Sidewalk Semantic dataset:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\n# Note: you need access to the dataset to load it!\n\ndataset = load_from_hub(\n    \"segments/sidewalk-semantic\",\n    format=\"parquet\",\n    filepath=\"pixel_values\",\n    mask_fields=\"label\",\n    max_samples=1000,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>Once you have the dataset loaded into FiftyOne, you may want to set the dataset\u2019s mask targets to specify the names of the classes represented in the segmentation masks.</p> <p>Unlabelled Image Datasets</p> <p>Some datasets on the Hub contain images and metadata in the form of features, but do not explicitly contain classification, detection, or segmentation labels. This is common for text-to-image tasks, as well as captioning and visual question answering tasks. These datasets can also be converted and loaded into FiftyOne! Once the dataset is loaded into FiftyOne, you can process the data and generate labels for whatever tasks you are interested in.</p> <p>Let\u2019s look at a few examples:</p> <p>For DiffusionDB, you can load the dataset as follows:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"poloclub/diffusiondb\",\n    format=\"parquet\",\n    max_samples=1000,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Here are some other popular datasets on the Hub that can be loaded following the same syntax:</p> <ul> <li> <p>Nouns: (use <code>\"m1guelpf/nouns\"</code>)</p> </li> <li> <p>New Yorker Caption Contest: (use <code>\"jmhessel/newyorker_caption_contest\"</code>)</p> </li> <li> <p>Captcha Dataset: (use <code>\"project-sloth/captcha-images\"</code>)</p> </li> <li> <p>MathVista: (use <code>\"AI4Math/MathVista\"</code>)</p> </li> <li> <p>TextVQA: (use <code>\"textvqa\"</code>)</p> </li> <li> <p>VQA-RAD: (use <code>\"flaviagiammarino/vqa-rad\"</code>)</p> </li> <li> <p>ScienceQA: (use <code>\"derek-thomas/ScienceQA\"</code>)</p> </li> <li> <p>PathVQA: (use <code>\"flaviagiammarino/path-vqa\"</code>)</p> </li> </ul> <p>Many other popular datasets on the Hub can be loaded in the same way, with slight modifications to <code>filepath</code> or other arguments as needed. Here are a few examples:</p> <p>For COYO-700M, we just need to specify the <code>filepath</code> as <code>\"url\"</code>:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"kakaobrain/coyo-700m\",\n    format=\"parquet\",\n    filepath=\"url\",\n    max_samples=1000,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>For RedCaps, we instead use <code>\"image_url\"</code> as the <code>filepath</code>:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"kdexd/red_caps\",\n    format=\"parquet\",\n    filepath=\"image_url\",\n    max_samples=1000,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>For MMMU (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI), we use <code>\"image_1\"</code> as the <code>filepath</code>:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"MMMU/MMMU\",\n    format=\"parquet\",\n    filepath=\"image_1\",\n    max_samples=1000,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"integrations/huggingface/#advanced-examples","title":"Advanced examples \u00b6","text":"<p>The <code>load_from_hub()</code> function also allows us to load datasets in much more complex formats, as well as with more advanced configurations. Let\u2019s walk through a few examples to show you how to leverage the full power of FiftyOne\u2019s Hugging Face Hub integration.</p> <p>Loading Datasets from Revisions</p> <p>When you load a dataset from the Hugging Face Hub, you are loading the latest revision of the dataset. However, you can also load a specific revision of the dataset by specifying the <code>revision</code> argument. For example, to load the last revision of DiffusionDB before NSFW scores were added, you can specify this via:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"poloclub/diffusiondb\",\n    format=\"parquet\",\n    subset=\"2m_random_1k\", ## just one of the subsets\n    max_samples=1000,\n    revision=\"5fa48ba66a44822d82d024d195fbe918e6c42ca6\",\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Loading Datasets with Multiple Media Fields</p> <p>Some datasets on the Hub contain multiple media fields for each sample. Take MagicBrush for example, which contains a <code>\"source_img\"</code> and a <code>\"target_img\"</code> for each sample, in addition to a segmentation mask denoting the area of the source image to be modified. To load this dataset, you can specify the <code>filepath</code> as <code>\"source_img\"</code> and the target image via <code>additional_media_fields</code>. Because this is getting a bit more complex, we\u2019ll create a local yaml config file to specify the dataset format:</p> <pre><code>format: ParquetFilesDataset\nname: magicbrush\nfilepath: source_img\nadditional_media_fields:\n    target_img: target_img\nmask_fields: mask_img\n</code></pre> <p>Now, you can load the dataset using the local yaml config file:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"osunlp/MagicBrush\",\n    config_file=\"/path/to/magicbrush.yml\",\n    max_samples=1000,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Customizing the Download Process</p> <p>When loading datasets from the Hub, you can customize the download process by specifying the <code>batch_size</code>, <code>num_workers</code>, and <code>overwrite</code> arguments. For example, to download the <code>full_numbers</code> subset of the Street View House Numbers dataset with a batch size of 50 and 4 workers, you can do the following:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"ufldl-stanford/svhn\",\n    format=\"parquet\",\n    detection_fields=\"digits\",\n    subsets=\"full_numbers\",\n    max_samples=1000,\n    batch_size=50,\n    num_workers=4,\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Loading Private or Gated Datasets</p> <p>Like public datasets, you can also load private or gated datasets from the Hub, as long as you have the necessary permissions. If your Hugging Face token is set as an environment variable <code>HF_TOKEN</code>, this is as simple as specifying the <code>repo_id</code> of the dataset. If you don\u2019t have your token set, or you need to use a specific token for a specific dataset, you can specify the <code>token</code> argument. You can do so following this recipe:</p> <pre><code>from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"my-private-dataset-repo-id\",\n    token=\"&lt;my-secret-token&gt;\",\n    ...\n)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"integrations/labelbox/","title":"Labelbox Integration \u00b6","text":"<p>Labelbox is one of the most popular cloud-based image and video annotation tools available, and we\u2019ve made it easy to upload your data directly from FiftyOne to Labelbox for labeling.</p> <p>You can create a free Labelbox account to upload and annotate raw data in the user-friendly Labelbox editor. FiftyOne provides simple setup instructions that you can use to specify the necessary API key and server endpoint to use.</p> <p>Note</p> <p>Did you know? You can request, manage, and import annotations from within the FiftyOne App by installing the @voxel51/annotation plugin!</p> <p>FiftyOne provides an API to create projects, upload data, define label schemas, and download annotations using Labelbox, all programmatically in Python. All of the following label types are supported, for both image and video datasets:</p> <ul> <li> <p>Classifications</p> </li> <li> <p>Detections</p> </li> <li> <p>Instance segmentations</p> </li> <li> <p>Polygons and polylines</p> </li> <li> <p>Keypoints</p> </li> <li> <p>Scalar fields</p> </li> <li> <p>Semantic segmentation</p> </li> </ul> <p></p>"},{"location":"integrations/labelbox/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The basic workflow to use Labelbox to add or edit labels on your FiftyOne datasets is as follows:</p> <ol> <li> <p>Load a labeled or unlabeled dataset into FiftyOne</p> </li> <li> <p>Explore the dataset using the App or dataset views to locate either unlabeled samples that you wish to annotate or labeled samples whose annotations you want to edit</p> </li> <li> <p>Use the <code>annotate()</code> method on your dataset or view to upload the samples and optionally their existing labels to Labelbox by setting the parameter <code>backend=\"labelbox\"</code></p> </li> <li> <p>In Labelbox, perform the necessary annotation work</p> </li> <li> <p>Back in FiftyOne, load your dataset and use the <code>load_annotations()</code> method to merge the annotations back into your FiftyOne dataset</p> </li> <li> <p>If desired, delete the Labelbox tasks and the record of the annotation run from your FiftyOne dataset</p> </li> </ol> <p>The example below demonstrates this workflow.</p> <p>Note</p> <p>You must create an account at labelbox.com in order to run this example.</p> <p>Note that you can store your credentials as described in this section to avoid entering them manually each time you interact with Labelbox.</p> <p>You\u2019ll also need to install the Labelbox Python client:</p> <pre><code>pip install labelbox\n</code></pre> <p>First, we create the annotation tasks in Labelbox:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\n# Step 1: Load your data into FiftyOne\n\ndataset = foz.load_zoo_dataset(\n    \"quickstart\", dataset_name=\"lb-annotation-example\"\n)\ndataset.persistent = True\n\ndataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\"\n)\n\n# Step 2: Locate a subset of your data requiring annotation\n\n# Create a view that contains only high confidence false positive model\n# predictions, with samples containing the most false positives first\nmost_fp_view = (\n    dataset\n    .filter_labels(\"predictions\", (F(\"confidence\") &gt; 0.8) &amp; (F(\"eval\") == \"fp\"))\n    .sort_by(F(\"predictions.detections\").length(), reverse=True)\n)\n\n# Retrieve the sample with the most high confidence false positives\nsample_id = most_fp_view.first().id\nview = dataset.select(sample_id)\n\n# Step 3: Send samples to Labelbox\n\n# A unique identifier for this run\nanno_key = \"labelbox_basic_recipe\"\n\nlabel_schema = {\n    \"new_ground_truth\": {\n        \"type\": \"detections\",\n        \"classes\": dataset.distinct(\"ground_truth.detections.label\"),\n        \"attributes\": {\n            \"iscrowd\": {\n                \"type\": \"radio\",\n                \"values\": [True, False],\n            },\n        },\n    },\n}\n\nview.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_schema=label_schema,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Step 4: Perform annotation in Labelbox and save the tasks\n</code></pre> <p>Then, once the annotation work is complete, we merge the annotations back into FiftyOne:</p> <pre><code>import fiftyone as fo\n\nanno_key = \"labelbox_basic_recipe\"\n\n# Step 5: Merge annotations back into FiftyOne dataset\n\ndataset = fo.load_dataset(\"lb-annotation-example\")\ndataset.load_annotations(anno_key)\n\n# Load the view that was annotated in the App\nview = dataset.load_annotation_view(anno_key)\nsession = fo.launch_app(view=view)\n\n# Step 6: Cleanup\n\n# Delete tasks from Labelbox\nresults = dataset.load_annotation_results(anno_key)\nresults.cleanup()\n\n# Delete run record (not the labels) from FiftyOne\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>Note</p> <p>See this section to see a variety of common Labelbox annotation patterns.</p>"},{"location":"integrations/labelbox/#setup","title":"Setup \u00b6","text":"<p>FiftyOne supports both standard Labelbox cloud accounts and on-premise Labelbox deployments.</p> <p>The easiest way to get started is to use app.labelbox.com, which simply requires creating an account and then providing your API key as shown below.</p>"},{"location":"integrations/labelbox/#installing-the-labelbox-client","title":"Installing the Labelbox client \u00b6","text":"<p>In order to use the Labelbox backend, you must install the Labelbox Python client:</p> <pre><code>pip install labelbox\n</code></pre>"},{"location":"integrations/labelbox/#using-the-labelbox-backend","title":"Using the Labelbox backend \u00b6","text":"<p>By default, calling <code>annotate()</code> will use the CVAT backend.</p> <p>To use the Labelbox backend, simply set the optional <code>backend</code> parameter of <code>annotate()</code> to <code>\"labelbox\"</code>:</p> <pre><code>view.annotate(anno_key, backend=\"labelbox\", ...)\n</code></pre> <p>Alternatively, you can permanently configure FiftyOne to use the Labelbox backend by setting the <code>FIFTYONE_ANNOTATION_DEFAULT_BACKEND</code> environment variable:</p> <pre><code>export FIFTYONE_ANNOTATION_DEFAULT_BACKEND=labelbox\n</code></pre> <p>or by setting the <code>default_backend</code> parameter of your annotation config located at <code>~/.fiftyone/annotation_config.json</code>:</p> <pre><code>{\n    \"default_backend\": \"labelbox\"\n}\n</code></pre>"},{"location":"integrations/labelbox/#authentication","title":"Authentication \u00b6","text":"<p>In order to connect to a Labelbox server, you must provide your API key, which can be done in a variety of ways.</p> <p>Environment variables (recommended)</p> <p>The recommended way to configure your Labelbox API key is to store it in the <code>FIFTYONE_LABELBOX_API_KEY</code> environment variable. This is automatically accessed by FiftyOne whenever a connection to Labelbox is made.</p> <pre><code>export FIFTYONE_LABELBOX_API_KEY=...\n</code></pre> <p>FiftyOne annotation config</p> <p>You can also store your credentials in your annotation config located at <code>~/.fiftyone/annotation_config.json</code>:</p> <pre><code>{\n    \"backends\": {\n        \"labelbox\": {\n            \"api_key\": ...,\n        }\n    }\n}\n</code></pre> <p>Note that this file will not exist until you create it.</p> <p>Keyword arguments</p> <p>You can manually provide your API key as a keyword argument each time you call methods like <code>annotate()</code> and <code>load_annotations()</code> that require connections to Labelbox:</p> <pre><code>view.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=\"ground_truth\",\n    api_key=...,\n)\n</code></pre> <p>Command line prompt</p> <p>If you have not stored your API key via another method, you will be prompted to enter it interactively in your shell each time you call a method that requires a connection to Labelbox:</p> <pre><code>view.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=\"ground_truth\",\n    launch_editor=True,\n)\n</code></pre> <pre><code>Please enter your API key.\nYou can avoid this in the future by setting your `FIFTYONE_LABELBOX_API_KEY` environment variable.\nAPI key: ...\n</code></pre>"},{"location":"integrations/labelbox/#on-premises-servers","title":"On-premises servers \u00b6","text":"<p>If you have an on-premises Labelbox server, you can configure the URL of your server in any of the following ways:</p> <ul> <li>Set the <code>FIFTYONE_LABELBOX_URL</code> environment variable:</li> </ul> <pre><code>export FIFTYONE_LABELBOX_URL=http://localhost:8080\n</code></pre> <ul> <li>Store the <code>url</code> of your server in your annotation config at <code>~/.fiftyone/annotation_config.json</code>:</li> </ul> <pre><code>{\n    \"backends\": {\n        \"labelbox\": {\n            \"url\": \"http://localhost:8080\"\n        }\n    }\n}\n</code></pre> <ul> <li>Pass the <code>url</code> parameter manually each time you call <code>annotate()</code>:</li> </ul> <pre><code>view.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=\"ground_truth\",\n    url=\"http://localhost:8080\",\n    api_key=...,\n)\n</code></pre>"},{"location":"integrations/labelbox/#requesting-annotations","title":"Requesting annotations \u00b6","text":"<p>Use the <code>annotate()</code> method to send the samples and optionally existing labels in a <code>Dataset</code> or <code>DatasetView</code> to Labelbox for annotation.</p> <p>The basic syntax is:</p> <pre><code>anno_key = \"...\"\nview.annotate(anno_key, backend=\"labelbox\", ...)\n</code></pre> <p>The <code>anno_key</code> argument defines a unique identifier for the annotation run, and you will provide it to methods like <code>load_annotations()</code>, <code>get_annotation_info()</code>, <code>load_annotation_results()</code>, <code>rename_annotation_run()</code>, and <code>delete_annotation_run()</code> to manage the run in the future.</p> <p>Note</p> <p>Calling <code>annotate()</code> will upload the source media files to the Labelbox server.</p> <p>In addition, <code>annotate()</code> provides various parameters that you can use to customize the annotation tasks that you wish to be performed.</p> <p>The following parameters are supported by all annotation backends:</p> <ul> <li> <p>backend ( None): the annotation backend to use. Use <code>\"labelbox\"</code> for the Labelbox backend. The supported values are <code>fiftyone.annotation_config.backends.keys()</code> and the default is <code>fiftyone.annotation_config.default_backend</code></p> </li> <li> <p>media_field ( \u201cfilepath\u201d): the sample field containing the path to the source media to upload</p> </li> <li> <p>launch_editor ( False): whether to launch the annotation backend\u2019s editor after uploading the samples</p> </li> </ul> <p>The following parameters allow you to configure the labeling schema to use for your annotation tasks. See this section for more details:</p> <ul> <li> <p>label_schema ( None): a dictionary defining the label schema to use. If this argument is provided, it takes precedence over <code>label_field</code> and <code>label_type</code></p> </li> <li> <p>label_field ( None): a string indicating a new or existing label field to annotate</p> </li> <li> <p>label_type ( None): a string indicating the type of labels to annotate. The possible label types are:</p> </li> <li> <p><code>\"classification\"</code>: a single classification stored in     <code>Classification</code> fields</p> </li> <li> <p><code>\"classifications\"</code>: multilabel classifications stored in     <code>Classifications</code> fields</p> </li> <li> <p><code>\"detections\"</code>: object detections stored in <code>Detections</code> fields</p> </li> <li> <p><code>\"instances\"</code>: instance segmentations stored in <code>Detections</code> fields     with their <code>mask</code>     attributes populated</p> </li> <li> <p><code>\"polylines\"</code>: polylines stored in <code>Polylines</code> fields with their     <code>filled</code> attributes set to     <code>False</code></p> </li> <li> <p><code>\"polygons\"</code>: polygons stored in <code>Polylines</code> fields with their     <code>filled</code> attributes set to     <code>True</code></p> </li> <li> <p><code>\"keypoints\"</code>: keypoints stored in <code>Keypoints</code> fields</p> </li> <li> <p><code>\"segmentation\"</code>: semantic segmentations stored in <code>Segmentation</code>     fields</p> </li> <li> <p><code>\"scalar\"</code>: scalar labels stored in <code>IntField</code>, <code>FloatField</code>,     <code>StringField</code>, or <code>BooleanField</code> fields</p> </li> </ul> <p>All new label fields must have their type specified via this argument or in <code>label_schema</code></p> <ul> <li> <p>classes ( None): a list of strings indicating the class options for <code>label_field</code> or all fields in <code>label_schema</code> without classes specified. All new label fields must have a class list provided via one of the supported methods. For existing label fields, if classes are not provided by this argument nor <code>label_schema</code>, the observed labels on your dataset are used</p> </li> <li> <p>attributes ( True): specifies the label attributes of each label field to include (other than their <code>label</code>, which is always included) in the annotation export. Can be any of the following:</p> </li> <li> <p><code>True</code>: export all label attributes</p> </li> <li> <p><code>False</code>: don\u2019t export any custom label attributes</p> </li> <li> <p>a list of label attributes to export</p> </li> <li> <p>a dict mapping attribute names to dicts specifying the <code>type</code>,     <code>values</code>, and <code>default</code> for each attribute</p> </li> </ul> <p>If a <code>label_schema</code> is also provided, this parameter determines which attributes are included for all fields that do not explicitly define their per-field attributes (in addition to any per-class attributes)</p> <ul> <li> <p>mask_targets ( None): a dict mapping pixel values to semantic label strings. Only applicable when annotating semantic segmentations</p> </li> <li> <p>allow_additions ( True): whether to allow new labels to be added. Only applicable when editing existing label fields</p> </li> <li> <p>allow_deletions ( True): whether to allow labels to be deleted. Only applicable when editing existing label fields</p> </li> <li> <p>allow_label_edits ( True): whether to allow the <code>label</code> attribute of existing labels to be modified. Only applicable when editing existing fields with <code>label</code> attributes</p> </li> <li> <p>allow_index_edits ( True): whether to allow the <code>index</code> attribute of existing video tracks to be modified. Only applicable when editing existing frame fields with <code>index</code> attributes</p> </li> <li> <p>allow_spatial_edits ( True): whether to allow edits to the spatial properties (bounding boxes, vertices, keypoints, masks, etc) of labels. Only applicable when editing existing spatial label fields</p> </li> </ul> <p>In addition, the following Labelbox-specific parameters from <code>LabelboxBackendConfig</code> can also be provided:</p> <ul> <li> <p>project_name ( None): a name for the Labelbox project that will be created. The default is <code>\"FiftyOne_&lt;dataset_name&gt;\"</code></p> </li> <li> <p>members (None): an optional list of <code>(email, role)</code> tuples specifying the email addresses and roles of users to add to the project. If a user is not a member of the project\u2019s organization, an email invitation will be sent to them. The supported roles are <code>[\"LABELER\", \"REVIEWER\", \"TEAM_MANAGER\", \"ADMIN\"]</code></p> </li> <li> <p>classes_as_attrs ( True): whether to show every object class at the top level of the editor (False) or whether to show the label field at the top level and annotate the class as a required attribute of each object (True)</p> </li> <li> <p>export_version ( \u201cv2\u201d): the Labelbox export format and API version to use. Supported values are <code>(\"v1\", \"v2\")</code></p> </li> </ul> <p>Note</p> <p>See this section for details about editing existing labels.</p>"},{"location":"integrations/labelbox/#label-schema","title":"Label schema \u00b6","text":"<p>The <code>label_schema</code>, <code>label_field</code>, <code>label_type</code>, <code>classes</code>, <code>attributes</code>, and <code>mask_targets</code> parameters to <code>annotate()</code> allow you to define the annotation schema that you wish to be used.</p> <p>The label schema may define new label field(s) that you wish to populate, and it may also include existing label field(s), in which case you can add, delete, or edit the existing labels on your FiftyOne dataset.</p> <p>The <code>label_schema</code> argument is the most flexible way to define how to construct tasks in Labelbox. In its most verbose form, it is a dictionary that defines the label type, annotation type, possible classes, and possible attributes for each label field:</p> <pre><code>anno_key = \"...\"\n\nlabel_schema = {\n    \"new_field\": {\n        \"type\": \"classifications\",\n        \"classes\": [\"class1\", \"class2\"],\n        \"attributes\": {\n            \"attr1\": {\n                \"type\": \"checkbox\",\n                \"values\": [\"val1\", \"val2\"],\n            },\n            \"attr2\": {\n                \"type\": \"radio\",\n                \"values\": [True, False],\n            }\n        },\n    },\n    \"existing_field\": {\n        \"classes\": [\"class3\", \"class4\"],\n        \"attributes\": {\n            \"attr3\": {\n                \"type\": \"text\",\n            }\n        }\n    },\n}\n\ndataset.annotate(anno_key, backend=\"labelbox\", label_schema=label_schema)\n</code></pre> <p>You can also define class-specific attributes by setting elements of the <code>classes</code> list to dicts that specify groups of <code>classes</code> and their corresponding <code>attributes</code>. For example, in the configuration below, <code>attr1</code> only applies to <code>class1</code> and <code>class2</code> while <code>attr2</code> applies to all classes:</p> <pre><code>anno_key = \"...\"\n\nlabel_schema = {\n    \"new_field\": {\n        \"type\": \"detections\",\n        \"classes\": [\\\n            {\\\n                \"classes\": [\"class1\", \"class2\"],\\\n                \"attributes\": {\\\n                    \"attr1\": {\\\n                        \"type\": \"radio\",\\\n                        \"values\": [\"val1\", \"val2\"],\\\n                    }\\\n                 }\\\n            },\\\n            \"class3\",\\\n            \"class4\",\\\n        ],\n        \"attributes\": {\n            \"attr2\": {\n                \"type\": \"radio\",\n                \"values\": [True, False],\n            }\n        },\n    },\n}\n\ndataset.annotate(anno_key, backend=\"labelbox\", label_schema=label_schema)\n</code></pre> <p>Alternatively, if you are only editing or creating a single label field, you can use the <code>label_field</code>, <code>label_type</code>, <code>classes</code>, <code>attributes</code>, and <code>mask_targets</code> parameters to specify the components of the label schema individually:</p> <pre><code>anno_key = \"...\"\n\nlabel_field = \"new_field\",\nlabel_type = \"classifications\"\nclasses = [\"class1\", \"class2\"]\n\n# These are optional\nattributes = {\n    \"attr1\": {\n        \"type\": \"radio\",\n        \"values\": [\"val1\", \"val2\"],\n    },\n    \"attr2\": {\n        \"type\": \"radio\",\n        \"values\": [True, False],\n    }\n}\n\ndataset.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=label_field,\n    label_type=label_type,\n    classes=classes,\n    attributes=attributes,\n)\n</code></pre> <p>When you are annotating existing label fields, you can omit some of these parameters from <code>annotate()</code>, as FiftyOne can infer the appropriate values to use:</p> <ul> <li> <p>label_type: if omitted, the <code>Label</code> type of the field will be used to infer the appropriate value for this parameter</p> </li> <li> <p>classes: if omitted for a non-semantic segmentation field, the observed labels on your dataset will be used to construct a classes list</p> </li> </ul> <p>Note</p> <p>See this section for details about editing existing labels.</p>"},{"location":"integrations/labelbox/#label-attributes","title":"Label attributes \u00b6","text":"<p>The <code>attributes</code> parameter allows you to configure whether custom attributes beyond the default <code>label</code> attribute are included in the annotation tasks.</p> <p>When adding new label fields for which you want to include attributes, you must use the dictionary syntax demonstrated below to define the schema of each attribute that you wish to label:</p> <pre><code>anno_key = \"...\"\n\nattributes = {\n    \"occluded\": {\n        \"type\": \"radio\",\n        \"values\": [True, False],\n    },\n    \"weather\": {\n        \"type\": \"checkbox\",\n        \"values\": [\"cloudy\", \"sunny\", \"overcast\"],\n    },\n    \"caption\": {\n        \"type\": \"text\",\n    }\n}\n\nview.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=\"new_field\",\n    label_type=\"detections\",\n    classes=[\"dog\", \"cat\", \"person\"],\n    attributes=attributes,\n)\n</code></pre> <p>You can always omit this parameter if you do not require attributes beyond the default <code>label</code>.</p> <p>For Labelbox, the following <code>type</code> values are supported:</p> <ul> <li> <p><code>text</code>: a free-form text box. In this case, <code>values</code> is unused</p> </li> <li> <p><code>radio</code>: a radio button list UI. In this case, <code>values</code> is required</p> </li> <li> <p><code>checkbox</code>: a list of checkboxes. In this case, <code>values</code> is required</p> </li> </ul> <p>When you are annotating existing label fields, the <code>attributes</code> parameter can take additional values:</p> <ul> <li> <p><code>True</code> (default): export all custom attributes observed on the existing labels, using their observed values to determine the appropriate UI type and possible values, if applicable</p> </li> <li> <p><code>False</code>: do not include any custom attributes in the export</p> </li> <li> <p>a list of custom attributes to include in the export</p> </li> <li> <p>a full dictionary syntax described above</p> </li> </ul> <p>Note that only scalar-valued label attributes are supported. Other attribute types like lists, dictionaries, and arrays will be omitted.</p> <p>Note</p> <p>Labelbox does not support default values for attributes, so the <code>default</code> key described here will be ignored if included in label schemas provided when annotating with Labelbox.</p>"},{"location":"integrations/labelbox/#video-label-attributes","title":"Video label attributes \u00b6","text":"<p>When annotating spatiotemporal objects in videos, each object attribute specification can include a <code>mutable</code> property that controls whether the attribute\u2019s value can change between frames for each object:</p> <pre><code>anno_key = \"...\"\n\nattributes = {\n    \"type\": {\n        \"type\": \"checkbox\",\n        \"values\": [\"sedan\", \"suv\", \"truck\"],\n        \"mutable\": False,\n    },\n    \"occluded\": {\n        \"type\": \"radio\",\n        \"values\": [True, False],\n        \"mutable\": True,\n    },\n}\n\nview.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=\"frames.new_field\",\n    label_type=\"detections\",\n    classes=[\"vehicle\"],\n    attributes=attributes,\n)\n</code></pre> <p>The meaning of the <code>mutable</code> attribute is defined as follows:</p> <ul> <li> <p><code>True</code> (default): the attribute is dynamic and can have a different value for every frame in which the object track appears</p> </li> <li> <p><code>False</code>: the attribute is static and is the same for every frame in which the object track appears ( Not yet supported)</p> </li> </ul>"},{"location":"integrations/labelbox/#loading-annotations","title":"Loading annotations \u00b6","text":"<p>After your annotations tasks in the annotation backend are complete, you can use the <code>load_annotations()</code> method to download them and merge them back into your FiftyOne dataset.</p> <pre><code>view.load_annotations(anno_key)\n</code></pre> <p>The <code>anno_key</code> parameter is the unique identifier for the annotation run that you provided when calling <code>annotate()</code>. You can use <code>list_annotation_runs()</code> to see the available keys on a dataset.</p> <p>Note</p> <p>By default, calling <code>load_annotations()</code> will not delete any information for the run from the annotation backend.</p> <p>However, you can pass <code>cleanup=True</code> to delete all information associated with the run from the backend after the annotations are downloaded. Specifically, it will delete the project and ontology associated with this annotation run. Data rows are not deleted since they can be reused by other annotation runs.</p> <p>You can use the optional <code>dest_field</code> parameter to override the task\u2019s label schema and instead load annotations into different field name(s) of your dataset. This can be useful, for example, when editing existing annotations, if you would like to do a before/after comparison of the edits that you import. If the annotation run involves multiple fields, <code>dest_field</code> should be a dictionary mapping label schema field names to destination field names.</p>"},{"location":"integrations/labelbox/#managing-annotation-runs","title":"Managing annotation runs \u00b6","text":"<p>FiftyOne provides a variety of methods that you can use to manage in-progress or completed annotation runs.</p> <p>For example, you can call <code>list_annotation_runs()</code> to see the available annotation keys on a dataset:</p> <pre><code>dataset.list_annotation_runs()\n</code></pre> <p>Or, you can use <code>get_annotation_info()</code> to retrieve information about the configuration of an annotation run:</p> <pre><code>info = dataset.get_annotation_info(anno_key)\nprint(info)\n</code></pre> <p>Use <code>load_annotation_results()</code> to load the <code>AnnotationResults</code> instance for an annotation run.</p> <p>All results objects provide a <code>cleanup()</code> method that you can use to delete all information associated with a run from the annotation backend.</p> <pre><code>results = dataset.load_annotation_results(anno_key)\nresults.cleanup()\n</code></pre> <p>In addition, the <code>AnnotationResults</code> subclasses for each backend may provide additional utilities such as support for programmatically monitoring the status of the annotation tasks in the run.</p> <p>You can use <code>rename_annotation_run()</code> to rename the annotation key associated with an existing annotation run:</p> <pre><code>dataset.rename_annotation_run(anno_key, new_anno_key)\n</code></pre> <p>Finally, you can use <code>delete_annotation_run()</code> to delete the record of an annotation run from your FiftyOne dataset:</p> <pre><code>dataset.delete_annotation_run(anno_key)\n</code></pre> <p>Note</p> <p>Calling <code>delete_annotation_run()</code> only deletes the record of the annotation run from your FiftyOne dataset; it will not delete any annotations loaded onto your dataset via <code>load_annotations()</code>, nor will it delete any associated information from the annotation backend.</p>"},{"location":"integrations/labelbox/#examples","title":"Examples \u00b6","text":"<p>This section demonstrates how to perform some common annotation workflows on a FiftyOne dataset using the Labelbox backend.</p> <p>Note</p> <p>All of the examples below assume you have configured your Labelbox server and API key as described in this section.</p>"},{"location":"integrations/labelbox/#adding-new-label-fields","title":"Adding new label fields \u00b6","text":"<p>In order to annotate a new label field, you can provide the <code>label_field</code>, <code>label_type</code>, and <code>classes</code> parameters to <code>annotate()</code> to define the annotation schema for the field:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"labelbox_new_field\"\n\nview.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=\"new_classifications\",\n    label_type=\"classifications\",\n    classes=[\"dog\", \"cat\", \"person\"],\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Create annotations in Labelbox\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>Alternatively, you can use the <code>label_schema</code> argument to define the same labeling task:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"labelbox_new_field\"\n\nlabel_schema = {\n    \"new_classifications\": {\n        \"type\": \"classifications\",\n        \"classes\": [\"dog\", \"cat\", \"person\"],\n    }\n}\n\nview.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_schema=label_schema,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Create annotations in Labelbox\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p></p>"},{"location":"integrations/labelbox/#editing-labels-with-a-free-labelbox-account","title":"Editing labels with a free Labelbox account \u00b6","text":"<p>A common use case is to fix annotation mistakes that you discovered in your datasets through FiftyOne.</p> <p>If you have a paid Labelbox account with access to Labelbox\u2019s Model Assisted Labeling feature, see this section for the recommended workflow for editing existing labels.</p> <p>For free Labelbox users, one possible workflow for editing existing labels is the following:</p> <ul> <li> <p>Tag the labels that need editing in FiftyOne</p> </li> <li> <p>Use FiftyOne to construct the label schema for the existing label field</p> </li> <li> <p>Upload the samples containing the tagged labels to Labelbox using <code>annotate()</code> using a new (temporary) label field to hold the edited labels</p> </li> <li> <p>Perform the annotation work in Labelbox, and download the results</p> </li> <li> <p>Use the FiftyOne App to compare the newly loaded labels with the previously tagged labels to make sure you\u2019re happy with the edits</p> </li> <li> <p>Use <code>merge_labels()</code> to merge edits into the original label field and then delete the tagged labels that you edited</p> </li> </ul> <p>The example snippet below demonstrates this workflow:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nsession = fo.launch_app(view=view)\n\n# In the App, tag some ground truth labels with the \"edit\" tag...\n\n# Create view that only contains samples having labels with the \"edit\" tag\nedit_view = view.match_labels(tags=\"edit\")\n\n#\n# Create an annotation run to reannotate the chosen samples in a new\n# `ground_truth_edits` field\n#\n\nanno_key = \"labelbox_edit_labels\"\n\nlabel_schema = {\n    \"ground_truth_edits\": {\n        \"type\": \"detections\",\n        \"classes\": dataset.distinct(\"ground_truth.detections.label\"),\n        \"attributes\": {\n            \"iscrowd\": {\n                \"type\": \"radio\",\n                \"values\": [True, False],\n            }\n        }\n    }\n}\n\nedit_view.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_schema=label_schema,\n    launch_editor=True,\n)\n\nprint(dataset.get_annotation_info(anno_key))\n\n# In Labelbox, re-annotate the relevant objects...\n\n# Download the results\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n\n# In the App, compare the tagged and re-annotated labels\nsession.view = edit_view\n\n# If the edits look good, merge them into the `ground_truth` field\n# and delete the previously tagged labels\ndataset.merge_labels(\"ground_truth_edits\", \"ground_truth\")\ndataset.delete_labels(tags=\"edit\")\n</code></pre> <p></p>"},{"location":"integrations/labelbox/#editing-existing-labels","title":"Editing existing labels \u00b6","text":"<p>Warning</p> <p>Uploading existing labels is not yet implemented for the Labelbox backend.</p> <p>See this section for one possible workflow for editing existing labels with Labelbox.</p>"},{"location":"integrations/labelbox/#annotating-multiple-fields","title":"Annotating multiple fields \u00b6","text":"<p>The <code>label_schema</code> argument allows you to define annotation tasks for multiple fields at once:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"labelbox_multiple_fields\"\n\nlabel_schema = {\n    \"people\": {\n        \"type\": \"detections\",\n        \"classes\": [\"person\"],\n    },\n    \"keypoints\": {\n        \"type\": \"keypoints\",\n        \"classes\": [\"person\", \"cat\", \"dog\", \"food\"],\n        \"attributes\": {\n            \"occluded\": {\n                \"type\": \"radio\",\n                \"values\": [True, False],\n            }\n        }\n    }\n}\n\nview.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_schema=label_schema,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Add annotations in Labelbox...\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p></p>"},{"location":"integrations/labelbox/#configuring-labelbox-projects","title":"Configuring Labelbox projects \u00b6","text":"<p>When using the Labelbox backend, you can provide the optional <code>project_name</code> and <code>members</code> parameters to <code>annotate()</code> to configure the Labelbox project that is created.</p> <p>The <code>members</code> parameter can contain a list of <code>(email, role)</code> tuples defining the email addresses and project-level roles of members to add to the Labelbox project. The supported roles are:</p> <ul> <li> <p><code>\"LABELER\"</code></p> </li> <li> <p><code>\"REVIEWER\"</code></p> </li> <li> <p><code>\"TEAM_MANAGER\"</code></p> </li> <li> <p><code>\"ADMIN\"</code></p> </li> </ul> <p>If any email addresses do not correspond to users already in your organization, an email invitation will be sent to them.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(5)\n\nanno_key = \"labelbox_assign_users\"\n\nproject_name = \"your_project_name\"\nmembers = [\\\n    (\"user1@domain.com\", \"LABELER\"),\\\n    (\"user2@domain.com\", \"REVIEWER\"),\\\n    (\"user3@domain.com\", \"TEAM_MANAGER\"),\\\n]\n\nview.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=\"people\",\n    label_type=\"detections\",\n    classes=[\"person\"],\n    project_name=project_name,\n    members=members,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Cleanup (without downloading results)\nresults = dataset.load_annotation_results(anno_key)\nresults.cleanup()\ndataset.delete_annotation_run(anno_key)\n</code></pre>"},{"location":"integrations/labelbox/#scalar-labels","title":"Scalar labels \u00b6","text":"<p><code>Label</code> fields are the preferred way to store information for common tasks such as classification and detection in your FiftyOne datasets. However, you can also store Labelbox annotations in scalar fields of type <code>float</code>, <code>int</code>, <code>str</code>, or <code>bool</code>.</p> <p>When storing annotations in scalar fields, the <code>label_field</code> parameter is still used to define the name of the field, but the <code>classes</code> argument is now optional and the <code>attributes</code> argument is unused.</p> <p>If <code>classes</code> are provided, you will be able to select from these values in Labelbox; otherwise, the Labelbox tag will show the <code>label_field</code> name and you must enter the appropriate scalar in the <code>value</code> attribute of the tag.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"labelbox_scalar_fields\"\n\n# Create two scalar fields, one with classes and one without\nlabel_schema = {\n    \"scalar1\": {\n        \"type\": \"scalar\",\n    },\n    \"scalar2\": {\n        \"type\": \"scalar\",\n        \"classes\": [\"class1\", \"class2\", \"class3\"],\n    }\n}\n\nview.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_schema=label_schema,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Cleanup (without downloading results)\nresults = dataset.load_annotation_results(anno_key)\nresults.cleanup()\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p></p>"},{"location":"integrations/labelbox/#uploading-alternate-media","title":"Uploading alternate media \u00b6","text":"<p>In some cases, you may want to upload media files other than those stored in the <code>filepath</code> field of your dataset\u2019s samples for annotation. For example, you may have a dataset with personal information like faces or license plates that must be anonymized before uploading for annotation.</p> <p>The recommended approach in this case is to store the alternative media files for each sample on disk and record these paths in a new field of your FiftyOne dataset. You can then specify this field via the <code>media_field</code> parameter of <code>annotate()</code>.</p> <p>For example, let\u2019s upload some blurred images to Labelbox for annotation:</p> <pre><code>import os\nimport cv2\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nalt_dir = \"/tmp/blurred\"\nif not os.path.exists(alt_dir):\n    os.makedirs(alt_dir)\n\n# Blur images\nfor sample in view:\n    filepath = sample.filepath\n    alt_filepath = os.path.join(alt_dir, os.path.basename(filepath))\n\n    img = cv2.imread(filepath)\n    cv2.imwrite(alt_filepath, cv2.blur(img, (20, 20)))\n\n    sample[\"alt_filepath\"] = alt_filepath\n    sample.save()\n\nanno_key = \"labelbox_alt_media\"\n\nview.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=\"objects\",\n    label_type=\"detections\",\n    classes=[\"person\", \"car\"],\n    media_field=\"alt_filepath\",\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Create annotations in Labelbox\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p></p>"},{"location":"integrations/labelbox/#annotating-classes-directly","title":"Annotating classes directly \u00b6","text":"<p>By default, the Labelbox editor is constructed so that all label fields being annotated are shown on the left sidebar at the top-level. When an object is annotated, the class name is then selected as an attribute.</p> <p>However, it can be useful to directly show the object classes at the top-level of the sidebar to avoid additional clicks. The <code>classes_as_attrs</code> argument can be set to <code>False</code> to provide this functionality.</p> <p>Note</p> <p>When <code>classes_as_attrs=False</code>, only one label field of each type of spatial label is allowed. For example, only one \u201cdetections\u201d label field can be annotated. Annotating multiple \u201cscalar\u201d, \u201cclassification\u201d, an \u201cclassifications\u201d fields is still allowed.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"labelbox_classes_as_attrs\"\n\nview.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=\"new_detections\",\n    label_type=\"detections\",\n    classes=[\"dog\", \"cat\", \"person\"],\n    classes_as_attrs=False,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Create annotations in Labelbox...\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p></p>"},{"location":"integrations/labelbox/#annotating-videos","title":"Annotating videos \u00b6","text":"<p>You can annotate for video datasets using the Labelbox backend through the <code>annotate()</code> method.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\")\nview = dataset.take(1)\n\nanno_key = \"labelbox_video\"\n\nview.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=\"frames.new_detections\",\n    label_type=\"detections\",\n    classes=[\"person\"],\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Create annotations in Labelbox...\n\n# Download annotations\ndataset.load_annotations(anno_key)\n\n# Load the view that was annotated in the App\nview = dataset.load_annotation_view(anno_key)\nsession = fo.launch_app(view=view)\n\n# Cleanup\nresults = dataset.load_annotation_results(anno_key)\nresults.cleanup()\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>Note</p> <p>Prepend <code>\"frames.\"</code> to reference frame-level fields when calling <code>annotate()</code>.</p> <p></p>"},{"location":"integrations/labelbox/#additional-utilities","title":"Additional utilities \u00b6","text":"<p>You can perform additional Labelbox-specific operations to monitor the progress of an annotation project initiated by <code>annotate()</code> via the returned <code>LabelboxAnnotationResults</code> instance.</p> <p>The sections below highlight some common actions that you may want to perform.</p>"},{"location":"integrations/labelbox/#viewing-project-status","title":"Viewing project status \u00b6","text":"<p>You can use the <code>get_status()</code> and <code>print_status()</code> methods to get information about the current status of the project for that annotation run:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(3)\n\nanno_key = \"labelbox_status\"\n\nview.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=\"people\",\n    label_type=\"detections\",\n    classes=[\"person\"],\n)\n\n# Print the project's status\nresults = dataset.load_annotation_results(anno_key)\nresults.print_status()\n\nresults.cleanup()\ndataset.delete_annotation_run(anno_key)\n</code></pre> <pre><code>Project: FiftyOne_quickstart\nID: cktixtv70e8zm0yba501v0ltz\nCreated at: 2021-09-13 17:46:21+00:00\nUpdated at: 2021-09-13 17:46:24+00:00\n\nMembers:\n    User: user1\n    Name: user1\n    Role: Admin\n    Email: USER1_EMAIL@email.com\n    ID: ckl137jfiss1c07320dacd81l\n\n    User: user2\n    Name: FIRSTNAME LASTNAME\n    Role: Labeler\n    Email: USER2_EMAIL@email.com\n    ID: ckl137jfiss1c07320dacd82y\n\nReviews:\n    Positive: 2\n    Zero: 0\n    Negative: 1\n</code></pre>"},{"location":"integrations/labelbox/#deleting-projects","title":"Deleting projects \u00b6","text":"<p>You can use <code>delete_project()</code> or <code>delete_projects()</code> methods to delete specific Labelbox project(s) associated with an annotation run.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"labelbox_delete_tasks\"\n\nview.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=\"people\",\n    label_type=\"detections\",\n    classes=[\"person\"],\n)\n\nresults = dataset.load_annotation_results(anno_key)\napi = results.connect_to_api()\n\napi.delete_project(\n    results.project_id,\n    delete_batches=True,\n    delete_ontologies=False,\n)\n\n# OR\n\n# List all projects or datasets associated with your Labelbox account\nproject_ids = api.list_projects()\ndataset_ids = api.list_datasets()\n\n# Delete all projects and datasets from your Labelbox account\napi.delete_projects(project_ids)\napi.delete_datasets(dataset_ids)\n</code></pre> <p>Note</p> <p>Note that passing <code>delete_batches=True</code> when deleting projects will not delete the corresponding data rows from Labelbox when using the V2 export API (the default).</p>"},{"location":"integrations/labelstudio/","title":"Label Studio Integration \u00b6","text":"<p>Label Studio is a popular open-source data labeling tool with a friendly UI. The integration between FiftyOne and Label Studio allows you to easily upload your data directly from FiftyOne to Label Studio for labeling.</p> <p>You can get started with Label Studio through a simple pip install to get a local server up and running. FiftyOne provides simple setup instructions that you can use to specify the necessary account credentials and server endpoint to use.</p> <p>Note</p> <p>Did you know? You can request, manage, and import annotations from within the FiftyOne App by installing the @voxel51/annotation plugin!</p> <p>FiftyOne provides an API to create projects, upload data, define label schemas, and download annotations using Label Studio, all programmatically in Python. All of the following label types are supported for image datasets:</p> <ul> <li> <p>Classification</p> </li> <li> <p>Multilabel classification</p> </li> <li> <p>Detections</p> </li> <li> <p>Instance segmentations</p> </li> <li> <p>Polygons and polylines</p> </li> <li> <p>Keypoints</p> </li> <li> <p>Scalar fields</p> </li> <li> <p>Semantic segmentation</p> </li> </ul>"},{"location":"integrations/labelstudio/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The basic workflow to use Label Studio to add or edit labels on your FiftyOne datasets is as follows:</p> <ol> <li> <p>Load a labeled or unlabeled dataset into FiftyOne</p> </li> <li> <p>Explore the dataset using the App or dataset views to locate either unlabeled samples that you wish to annotate or labeled samples whose annotations you want to edit</p> </li> <li> <p>Use the <code>annotate()</code> method on your dataset or view to upload the samples and optionally their existing labels to Label Studio by setting the parameter <code>backend=\"labelstudio\"</code></p> </li> <li> <p>In Label Studio, perform the necessary annotation work</p> </li> <li> <p>Back in FiftyOne, load your dataset and use the <code>load_annotations()</code> method to merge the annotations back into your FiftyOne dataset</p> </li> <li> <p>If desired, delete the Label Studio tasks and the record of the annotation run from your FiftyOne dataset</p> </li> </ol> <p>The example below demonstrates this workflow.</p> <p>Note</p> <p>You must start by installing and setting up Label Studio as described in this section.</p> <p>Note that you can also store your credentials to avoid entering them manually each time you interact with Label Studio.</p> <p>First, we create the annotation tasks in Label Studio:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\n# Step 1: Load your data into FiftyOne\n\ndataset = foz.load_zoo_dataset(\n    \"quickstart\", dataset_name=\"ls-annotation-example\"\n)\ndataset.persistent = True\n\ndataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\"\n)\n\n# Step 2: Locate a subset of your data requiring annotation\n\n# Create a view that contains only high confidence false positive model\n# predictions, with samples containing the most false positives first\nmost_fp_view = (\n    dataset\n    .filter_labels(\"predictions\", (F(\"confidence\") &gt; 0.8) &amp; (F(\"eval\") == \"fp\"))\n    .sort_by(F(\"predictions.detections\").length(), reverse=True)\n)\n\n# Retrieve the sample with the most high confidence false positives\nsample_id = most_fp_view.first().id\nview = dataset.select(sample_id)\n\n# Step 3: Send samples to Label Studio\n\n# A unique identifier for this run\nanno_key = \"labelstudio_basic_recipe\"\n\nlabel_schema = {\n    \"new_ground_truth\": {\n        \"type\": \"detections\",\n        \"classes\": dataset.distinct(\"ground_truth.detections.label\"),\n    },\n}\n\nview.annotate(\n    anno_key,\n    backend=\"labelstudio\",\n    label_schema=label_schema,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Step 4: Perform annotation in Label Studio and save the tasks\n</code></pre> <p>Then, once the annotation work is complete, we merge the annotations back into FiftyOne:</p> <pre><code>import fiftyone as fo\n\nanno_key = \"labelstudio_basic_recipe\"\n\n# Step 5: Merge annotations back into FiftyOne dataset\n\ndataset = fo.load_dataset(\"ls-annotation-example\")\ndataset.load_annotations(anno_key)\n\n# Load the view that was annotated in the App\nview = dataset.load_annotation_view(anno_key)\nsession = fo.launch_app(view=view)\n\n# Step 6: Cleanup\n\n# Delete tasks from Label Studio\nresults = dataset.load_annotation_results(anno_key)\nresults.cleanup()\n\n# Delete run record (not the labels) from FiftyOne\ndataset.delete_annotation_run(anno_key)\n</code></pre>"},{"location":"integrations/labelstudio/#setup","title":"Setup \u00b6","text":"<p>The easiest way to get started with Label Studio is to install it locally and create an account.</p> <pre><code>pip install label-studio\n\n# Launch it!\nlabel-studio\n</code></pre>"},{"location":"integrations/labelstudio/#installing-the-label-studio-client","title":"Installing the Label Studio client \u00b6","text":"<p>In order to use the Label Studio backend, you must install the Label Studio Python SDK:</p> <pre><code>pip install label-studio-sdk\n</code></pre>"},{"location":"integrations/labelstudio/#using-the-label-studio-backend","title":"Using the Label Studio backend \u00b6","text":"<p>By default, calling <code>annotate()</code> will use the CVAT backend.</p> <p>To use the Label Studio backend, simply set the optional <code>backend</code> parameter of <code>annotate()</code> to <code>\"labelstudio\"</code>:</p> <pre><code>view.annotate(anno_key, backend=\"labelstudio\", ...)\n</code></pre> <p>Alternatively, you can permanently configure FiftyOne to use the Label Studio backend by setting the <code>FIFTYONE_ANNOTATION_DEFAULT_BACKEND</code> environment variable:</p> <pre><code>export FIFTYONE_ANNOTATION_DEFAULT_BACKEND=labelstudio\n</code></pre> <p>or by setting the <code>default_backend</code> parameter of your annotation config located at <code>~/.fiftyone/annotation_config.json</code>:</p> <pre><code>{\n    \"default_backend\": \"labelstudio\"\n}\n</code></pre>"},{"location":"integrations/labelstudio/#authentication","title":"Authentication \u00b6","text":"<p>In order to connect to a Label Studio server, you must provide your API key, which can be done in a variety of ways.</p> <p>Environment variables (recommended)</p> <p>The recommended way to configure your Label Studio API key is to store it in the <code>FIFTYONE_LABELSTUDIO_API_KEY</code> environment variable. This is automatically accessed by FiftyOne whenever a connection to Label Studio is made.</p> <pre><code>export FIFTYONE_LABELSTUDIO_API_KEY=...\n</code></pre> <p>FiftyOne annotation config</p> <p>You can also store your credentials in your annotation config located at <code>~/.fiftyone/annotation_config.json</code>:</p> <pre><code>{\n    \"backends\": {\n        \"labelstudio\": {\n            \"api_key\": ...,\n        }\n    }\n}\n</code></pre> <p>Note that this file will not exist until you create it.</p> <p>Keyword arguments</p> <p>You can manually provide your API key as a keyword argument each time you call methods like <code>annotate()</code> and <code>load_annotations()</code> that require connections to Label Studio:</p> <pre><code>view.annotate(\n    anno_key,\n    backend=\"labelstudio\",\n    label_field=\"ground_truth\",\n    api_key=...,\n)\n</code></pre> <p>Command line prompt</p> <p>If you have not stored your API key via another method, you will be prompted to enter it interactively in your shell each time you call a method that requires a connection to Label Studio:</p> <pre><code>view.annotate(\n    anno_key,\n    backend=\"labelstudio\",\n    label_field=\"ground_truth\",\n    launch_editor=True,\n)\n</code></pre> <pre><code>Please enter your API key.\nYou can avoid this in the future by setting your `FIFTYONE_LABELSTUDIO_API_KEY` environment variable.\nAPI key: ...\n</code></pre>"},{"location":"integrations/labelstudio/#server-url","title":"Server URL \u00b6","text":"<p>You can configure the URL to the desired Label Studio server in any of the following ways:</p> <ul> <li>Set the <code>FIFTYONE_LABELSTUDIO_URL</code> environment variable:</li> </ul> <pre><code>export FIFTYONE_LABELSTUDIO_URL=http://localhost:8080\n</code></pre> <ul> <li>Store the <code>url</code> of your server in your annotation config at <code>~/.fiftyone/annotation_config.json</code>:</li> </ul> <pre><code>{\n    \"backends\": {\n        \"labelstudio\": {\n            \"url\": \"http://localhost:8080\"\n        }\n    }\n}\n</code></pre> <ul> <li>Pass the <code>url</code> parameter manually each time you call <code>annotate()</code>:</li> </ul> <pre><code>view.annotate(\n    anno_key,\n    backend=\"labelstudio\",\n    label_field=\"ground_truth\",\n    url=\"http://localhost:8080\",\n    api_key=...,\n)\n</code></pre>"},{"location":"integrations/labelstudio/#configuring-local-file-storage","title":"Configuring local file storage \u00b6","text":"<p>If you are using FiftyOne on the same machine that is hosting Label Studio, then you can make use of the local storage feature of Label Studio to avoid needing to copy your media.</p> <p>To enable this, you just need to configure the <code>LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT</code> and <code>LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED</code> environment variables as defined in the documentation.</p> <p>Then when you request annotations, if all of the samples in your <code>Dataset</code> or <code>DatasetView</code> reside in a subdirectory of the <code>LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT</code>, the media will not be copied over and only filepaths for you media will be used to create the Label Studio project.</p>"},{"location":"integrations/labelstudio/#requesting-annotations","title":"Requesting annotations \u00b6","text":"<p>Use the <code>annotate()</code> method to send the samples and optionally existing labels in a <code>Dataset</code> or <code>DatasetView</code> to Label Studio for annotation.</p> <p>The basic syntax is:</p> <pre><code>anno_key = \"...\"\nview.annotate(anno_key, backend=\"labelstudio\", ...)\n</code></pre> <p>The <code>anno_key</code> argument defines a unique identifier for the annotation run, and you will provide it to methods like <code>load_annotations()</code>, <code>get_annotation_info()</code>, <code>load_annotation_results()</code>, <code>rename_annotation_run()</code>, and <code>delete_annotation_run()</code> to manage the run in the future.</p> <p>Note</p> <p>Calling <code>annotate()</code> will upload the source media files to the Label Studio server.</p> <p>In addition, <code>annotate()</code> provides various parameters that you can use to customize the annotation tasks that you wish to be performed.</p> <p>The following parameters are supported by all annotation backends:</p> <ul> <li> <p>backend ( None): the annotation backend to use. Use <code>\"labelstudio\"</code> for the Label Studio backend. The supported values are <code>fiftyone.annotation_config.backends.keys()</code> and the default is <code>fiftyone.annotation_config.default_backend</code></p> </li> <li> <p>media_field ( \u201cfilepath\u201d): the sample field containing the path to the source media to upload</p> </li> <li> <p>launch_editor ( False): whether to launch the annotation backend\u2019s editor after uploading the samples</p> </li> </ul> <p>The following parameters allow you to configure the labeling schema to use for your annotation tasks. See this section for more details:</p> <ul> <li> <p>label_schema ( None): a dictionary defining the label schema to use. If this argument is provided, it takes precedence over <code>label_field</code> and <code>label_type</code></p> </li> <li> <p>label_field ( None): a string indicating a new or existing label field to annotate</p> </li> <li> <p>label_type ( None): a string indicating the type of labels to annotate. The possible label types are:</p> </li> <li> <p><code>\"classification\"</code>: a single classification stored in     <code>Classification</code> fields</p> </li> <li> <p><code>\"classifications\"</code>: multilabel classifications stored in     <code>Classifications</code> fields</p> </li> <li> <p><code>\"detections\"</code>: object detections stored in <code>Detections</code> fields</p> </li> <li> <p><code>\"instances\"</code>: instance segmentations stored in <code>Detections</code> fields     with their <code>mask</code>     attributes populated</p> </li> <li> <p><code>\"polylines\"</code>: polylines stored in <code>Polylines</code> fields with their     <code>filled</code> attributes set to     <code>False</code></p> </li> <li> <p><code>\"polygons\"</code>: polygons stored in <code>Polylines</code> fields with their     <code>filled</code> attributes set to     <code>True</code></p> </li> <li> <p><code>\"keypoints\"</code>: keypoints stored in <code>Keypoints</code> fields</p> </li> <li> <p><code>\"segmentation\"</code>: semantic segmentations stored in <code>Segmentation</code>     fields</p> </li> </ul> <p>All new label fields must have their type specified via this argument or in <code>label_schema</code></p> <ul> <li> <p>classes ( None): a list of strings indicating the class options for <code>label_field</code> or all fields in <code>label_schema</code> without classes specified. All new label fields must have a class list provided via one of the supported methods. For existing label fields, if classes are not provided by this argument nor <code>label_schema</code>, they are parsed from <code>Dataset.classes</code> or <code>Dataset.default_classes</code></p> </li> <li> <p>mask_targets ( None): a dict mapping pixel values to semantic label strings. Only applicable when annotating semantic segmentations</p> </li> </ul> <p>In addition, the following Label Studio-specific parameters from <code>LabelStudioBackendConfig</code> can also be provided:</p> <ul> <li>project_name ( None): a name for the Label Studio project that will be created. The default is <code>\"FiftyOne_&lt;dataset_name&gt;\"</code></li> </ul>"},{"location":"integrations/labelstudio/#label-schema","title":"Label schema \u00b6","text":"<p>The <code>label_schema</code>, <code>label_field</code>, <code>label_type</code>, <code>classes</code>, and <code>mask_targets</code> parameters to <code>annotate()</code> allow you to define the annotation schema that you wish to be used.</p> <p>The label schema may define new label field(s) that you wish to populate, and it may also include existing label field(s), in which case you can add, delete, or edit the existing labels on your FiftyOne dataset.</p> <p>The <code>label_schema</code> argument is the most flexible way to define how to construct tasks in Label Studio. In its most verbose form, it is a dictionary that defines the label type, annotation type, and possible classes for each label field:</p> <pre><code>anno_key = \"...\"\n\nlabel_schema = {\n    \"new_field\": {\n        \"type\": \"detections\",\n        \"classes\": [\"class1\", \"class2\"],\n    },\n    \"existing_field\": {\n        \"classes\": [\"class3\", \"class4\"],\n    },\n}\n\ndataset.annotate(anno_key, backend=\"labelstudio\", label_schema=label_schema)\n</code></pre> <p>Alternatively, if you are only editing or creating a single label field, you can use the <code>label_field</code>, <code>label_type</code>, <code>classes</code>, and <code>mask_targets</code> parameters to specify the components of the label schema individually:</p> <pre><code>anno_key = \"...\"\n\nlabel_field = \"new_field\",\nlabel_type = \"detections\"\nclasses = [\"class1\", \"class2\"]\n\ndataset.annotate(\n    anno_key,\n    backend=\"labelstudio\",\n    label_field=label_field,\n    label_type=label_type,\n    classes=classes,\n)\n</code></pre> <p>When you are annotating existing label fields, you can omit some of these parameters from <code>annotate()</code>, as FiftyOne can infer the appropriate values to use:</p> <ul> <li> <p>label_type: if omitted, the <code>Label</code> type of the field will be used to infer the appropriate value for this parameter</p> </li> <li> <p>classes: if omitted for a non-semantic segmentation field, the class lists from the <code>classes</code> or <code>default_classes</code> properties of your dataset will be used, if available. Otherwise, the observed labels on your dataset will be used to construct a classes list</p> </li> <li> <p>mask_targets: if omitted for a semantic segmentation field, the mask targets from the <code>mask_targets</code> or <code>default_mask_targets</code> properties of your dataset will be used, if available</p> </li> </ul>"},{"location":"integrations/labelstudio/#label-attributes","title":"Label attributes \u00b6","text":"<p>Warning</p> <p>The Label Studio integration does not yet support annotating label attributes.</p>"},{"location":"integrations/labelstudio/#loading-annotations","title":"Loading annotations \u00b6","text":"<p>After your annotations tasks in the annotation backend are complete, you can use the <code>load_annotations()</code> method to download them and merge them back into your FiftyOne dataset.</p> <pre><code>view.load_annotations(anno_key)\n</code></pre> <p>The <code>anno_key</code> parameter is the unique identifier for the annotation run that you provided when calling <code>annotate()</code>. You can use <code>list_annotation_runs()</code> to see the available keys on a dataset.</p> <p>Note</p> <p>By default, calling <code>load_annotations()</code> will not delete any information for the run from the annotation backend.</p> <p>However, you can pass <code>cleanup=True</code> to delete all information associated with the run from the backend after the annotations are downloaded.</p> <p>You can use the optional <code>dest_field</code> parameter to override the task\u2019s label schema and instead load annotations into different field name(s) of your dataset. This can be useful, for example, when editing existing annotations, if you would like to do a before/after comparison of the edits that you import. If the annotation run involves multiple fields, <code>dest_field</code> should be a dictionary mapping label schema field names to destination field names.</p>"},{"location":"integrations/labelstudio/#managing-annotation-runs","title":"Managing annotation runs \u00b6","text":"<p>FiftyOne provides a variety of methods that you can use to manage in-progress or completed annotation runs.</p> <p>For example, you can call <code>list_annotation_runs()</code> to see the available annotation keys on a dataset:</p> <pre><code>dataset.list_annotation_runs()\n</code></pre> <p>Or, you can use <code>get_annotation_info()</code> to retrieve information about the configuration of an annotation run:</p> <pre><code>info = dataset.get_annotation_info(anno_key)\nprint(info)\n</code></pre> <p>Use <code>load_annotation_results()</code> to load the <code>AnnotationResults</code> instance for an annotation run.</p> <p>All results objects provide a <code>cleanup()</code> method that you can use to delete all information associated with a run from the annotation backend.</p> <pre><code>results = dataset.load_annotation_results(anno_key)\nresults.cleanup()\n</code></pre> <p>In addition, the <code>AnnotationResults</code> subclasses for each backend may provide additional utilities such as support for programmatically monitoring the status of the annotation tasks in the run.</p> <p>You can use <code>rename_annotation_run()</code> to rename the annotation key associated with an existing annotation run:</p> <pre><code>dataset.rename_annotation_run(anno_key, new_anno_key)\n</code></pre> <p>Finally, you can use <code>delete_annotation_run()</code> to delete the record of an annotation run from your FiftyOne dataset:</p> <pre><code>dataset.delete_annotation_run(anno_key)\n</code></pre> <p>Note</p> <p>Calling <code>delete_annotation_run()</code> only deletes the record of the annotation run from your FiftyOne dataset; it will not delete any annotations loaded onto your dataset via <code>load_annotations()</code>, nor will it delete any associated information from the annotation backend.</p>"},{"location":"integrations/labelstudio/#annotating-videos","title":"Annotating videos \u00b6","text":"<p>Warning</p> <p>The Label Studio integration does not currently support annotating videos.</p>"},{"location":"integrations/labelstudio/#acknowledgements","title":"Acknowledgements \u00b6","text":"<p>Note</p> <p>Special thanks to Rustem Galiullin, Ganesh Tata, and Emil Zakirov for building this integration!</p>"},{"location":"integrations/lancedb/","title":"LanceDB Integration \u00b6","text":"<p>LanceDB is a serverless vector database with deep integrations with the Python ecosystem. It requires no setup and is free to use.</p> <p>FiftyOne provides an API to create LanceDB tables and run similarity queries, both programmatically in Python and via point-and-click in the App.</p>"},{"location":"integrations/lancedb/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The basic workflow to use LanceDB to create a similarity index on your FiftyOne datasets and use this to query your data is as follows:</p> <ol> <li> <p>Load a dataset into FiftyOne</p> </li> <li> <p>Compute embedding vectors for samples or patches in your dataset, or select a model to use to generate embeddings</p> </li> <li> <p>Use the <code>compute_similarity()</code> method to generate a LanceDB table for the samples or object patches embeddings in a dataset by setting the parameter <code>backend=\"lancedb\"</code> and specifying a <code>brain_key</code> of your choice</p> </li> <li> <p>Use this LanceDB table to query your data with <code>sort_by_similarity()</code></p> </li> <li> <p>If desired, delete the table</p> </li> </ol> <p>The example below demonstrates this workflow.</p> <p>Note</p> <p>You must install the LanceDB Python client to run this example:</p> <pre><code>pip install lancedb\n</code></pre> <p>Note that, if you are using a custom LanceDB URI, you can store your credentials as described in this section to avoid entering them manually each time you interact with your LanceDB index.</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\n# Step 1: Load your data into FiftyOne\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Steps 2 and 3: Compute embeddings and create a similarity index\nlancedb_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"lancedb_index\",\n    backend=\"lancedb\",\n)\n</code></pre> <p>Once the similarity index has been generated, we can query our data in FiftyOne by specifying the <code>brain_key</code>:</p> <pre><code># Step 4: Query your data\nquery = dataset.first().id  # query by sample ID\nview = dataset.sort_by_similarity(\n    query,\n    brain_key=\"lancedb_index\",\n    k=10,  # limit to 10 most similar samples\n)\n\n# Step 5 (optional): Cleanup\n\n# Delete the LanceDB table\nlancedb_index.cleanup()\n\n# Delete run record from FiftyOne\ndataset.delete_brain_run(\"lancedb_index\")\n</code></pre>"},{"location":"integrations/lancedb/#setup","title":"Setup \u00b6","text":"<p>You can get started using LanceDB by simply installing the LanceDB Python client:</p> <pre><code>pip install lancedb\n</code></pre>"},{"location":"integrations/lancedb/#using-the-lancedb-backend","title":"Using the LanceDB backend \u00b6","text":"<p>By default, calling <code>compute_similarity()</code> or <code>sort_by_similarity()</code> will use an sklearn backend.</p> <p>To use the LanceDB backend, simply set the optional <code>backend</code> parameter of <code>compute_similarity()</code> to <code>\"lancedb\"</code>:</p> <pre><code>import fiftyone.brain as fob\n\nfob.compute_similarity(..., backend=\"lancedb\", ...)\n</code></pre> <p>Alternatively, you can permanently configure FiftyOne to use the LanceDB backend by setting the following environment variable:</p> <pre><code>export FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND=lancedb\n</code></pre> <p>or by setting the <code>default_similarity_backend</code> parameter of your brain config located at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"default_similarity_backend\": \"lancedb\"\n}\n</code></pre>"},{"location":"integrations/lancedb/#lancedb-config-parameters","title":"LanceDB config parameters \u00b6","text":"<p>The LanceDB backend supports query parameters that can be used to customize your similarity queries. These parameters include:</p> <ul> <li> <p>table_name ( None): the name of the LanceDB table to use. If none is provided, a new table will be created</p> </li> <li> <p>metric ( \u201ccosine\u201d): the embedding distance metric to use when creating a new table. The supported values are <code>(\"cosine\", \"euclidean\")</code></p> </li> <li> <p>uri ( \u201c/tmp/lancedb\u201d): the database URI to use</p> </li> </ul> <p>You can specify these parameters via any of the strategies described in the previous section. Here\u2019s an example of a brain config that includes all of the available parameters:</p> <pre><code>{\n    \"similarity_backends\": {\n        \"lancedb\": {\n            \"table_name\": \"your-table\",\n            \"metric\": \"euclidean\",\n            \"uri\": \"/tmp/lancedb\"\n        }\n    }\n}\n</code></pre> <p>However, typically these parameters are directly passed to <code>compute_similarity()</code> to configure a specific new index:</p> <pre><code>lancedb_index = fob.compute_similarity(\n    ...\n    backend=\"lancedb\",\n    brain_key=\"lancedb_index\",\n    table_name=\"your-table\",\n    metric=\"euclidean\",\n    uri=\"/tmp/lancedb\",\n)\n</code></pre>"},{"location":"integrations/lancedb/#managing-brain-runs","title":"Managing brain runs \u00b6","text":"<p>FiftyOne provides a variety of methods that you can use to manage brain runs.</p> <p>For example, you can call <code>list_brain_runs()</code> to see the available brain keys on a dataset:</p> <pre><code>import fiftyone.brain as fob\n\n# List all brain runs\ndataset.list_brain_runs()\n\n# Only list similarity runs\ndataset.list_brain_runs(type=fob.Similarity)\n\n# Only list specific similarity runs\ndataset.list_brain_runs(\n    type=fob.Similarity,\n    patches_field=\"ground_truth\",\n    supports_prompts=True,\n)\n</code></pre> <p>Or, you can use <code>get_brain_info()</code> to retrieve information about the configuration of a brain run:</p> <pre><code>info = dataset.get_brain_info(brain_key)\nprint(info)\n</code></pre> <p>Use <code>load_brain_results()</code> to load the <code>SimilarityIndex</code> instance for a brain run.</p> <p>You can use <code>rename_brain_run()</code> to rename the brain key associated with an existing similarity results run:</p> <pre><code>dataset.rename_brain_run(brain_key, new_brain_key)\n</code></pre> <p>Finally, you can use <code>delete_brain_run()</code> to delete the record of a similarity index computation from your FiftyOne dataset:</p> <pre><code>dataset.delete_brain_run(brain_key)\n</code></pre> <p>Note</p> <p>Calling <code>delete_brain_run()</code> only deletes the record of the brain run from your FiftyOne dataset; it will not delete any associated LanceDB table, which you can do as follows:</p> <pre><code># Delete the LanceDB table\nlancedb_index = dataset.load_brain_results(brain_key)\nlancedb_index.cleanup()\n</code></pre>"},{"location":"integrations/lancedb/#examples","title":"Examples \u00b6","text":"<p>This section demonstrates how to perform some common vector search workflows on a FiftyOne dataset using the LanceDB backend.</p>"},{"location":"integrations/lancedb/#create-a-similarity-index","title":"Create a similarity index \u00b6","text":"<p>In order to create a new LanceDB similarity index, you need to specify either the <code>embeddings</code> or <code>model</code> argument to <code>compute_similarity()</code>. Here\u2019s a few possibilities:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nmodel_name = \"clip-vit-base32-torch\"\nmodel = foz.load_zoo_model(model_name)\nbrain_key = \"lancedb_index\"\n\n# Option 1: Compute embeddings on the fly from model name\nfob.compute_similarity(\n    dataset,\n    model=model_name,\n    backend=\"lancedb\",\n    brain_key=brain_key,\n)\n\n# Option 2: Compute embeddings on the fly from model instance\nfob.compute_similarity(\n    dataset,\n    model=model,\n    backend=\"lancedb\",\n    brain_key=brain_key,\n)\n\n# Option 3: Pass precomputed embeddings as a numpy array\nembeddings = dataset.compute_embeddings(model)\nfob.compute_similarity(\n    dataset,\n    embeddings=embeddings,\n    backend=\"lancedb\",\n    brain_key=brain_key,\n)\n\n# Option 4: Pass precomputed embeddings by field name\ndataset.compute_embeddings(model, embeddings_field=\"embeddings\")\nfob.compute_similarity(\n    dataset,\n    embeddings=\"embeddings\",\n    backend=\"lancedb\",\n    brain_key=brain_key,\n)\n</code></pre> <p>Note</p> <p>You can customize the LanceDB index by passing any supported parameters as extra kwargs.</p>"},{"location":"integrations/lancedb/#create-a-patch-similarity-index","title":"Create a patch similarity index \u00b6","text":"<p>You can also create a similarity index for object patches within your dataset by specifying a <code>patches_field</code> argument to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    patches_field=\"ground_truth\",\n    model=\"clip-vit-base32-torch\",\n    backend=\"lancedb\",\n    brain_key=\"lancedb_index\",\n)\n</code></pre> <p>Note</p> <p>You can customize the LanceDB index by passing any supported parameters as extra kwargs.</p>"},{"location":"integrations/lancedb/#connect-to-an-existing-index","title":"Connect to an existing index \u00b6","text":"<p>If you have already created a LanceDB table storing the embedding vectors for the samples or patches in your dataset, you can connect to it by passing the <code>table_name</code> to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",      # zoo model used (if applicable)\n    embeddings=False,                   # don't compute embeddings\n    table_name=\"your-table\",            # the existing LanceDB table\n    brain_key=\"lancedb_index\",\n    backend=\"lancedb\",\n)\n</code></pre>"},{"location":"integrations/lancedb/#addremove-embeddings-from-an-index","title":"Add/remove embeddings from an index \u00b6","text":"<p>You can use <code>add_to_index()</code> and <code>remove_from_index()</code> to add and remove embeddings from an existing Lancedb index.</p> <p>These methods can come in handy if you modify your FiftyOne dataset and need to update the LanceDB index to reflect these changes:</p> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nlancedb_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"lancedb_index\",\n    backend=\"lancedb\",\n)\nprint(lancedb_index.total_index_size)  # 200\n\nview = dataset.take(10)\nids = view.values(\"id\")\n\n# Delete 10 samples from a dataset\ndataset.delete_samples(view)\n\n# Delete the corresponding vectors from the index\nlancedb_index.remove_from_index(sample_ids=ids)\n\n# Add 20 samples to a dataset\nsamples = [fo.Sample(filepath=\"tmp%d.jpg\" % i) for i in range(20)]\nsample_ids = dataset.add_samples(samples)\n\n# Add corresponding embeddings to the index\nembeddings = np.random.rand(20, 512)\nlancedb_index.add_to_index(embeddings, sample_ids)\n\nprint(lancedb_index.total_index_size)  # 210\n</code></pre>"},{"location":"integrations/lancedb/#retrieve-embeddings-from-an-index","title":"Retrieve embeddings from an index \u00b6","text":"<p>You can use <code>get_embeddings()</code> to retrieve embeddings from a LanceDB index by ID:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nlancedb_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"lancedb_index\",\n    backend=\"lancedb\",\n)\n\n# Retrieve embeddings for the entire dataset\nids = dataset.values(\"id\")\nembeddings, sample_ids, _ = lancedb_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (200, 512)\nprint(sample_ids.shape)  # (200,)\n\n# Retrieve embeddings for a view\nids = dataset.take(10).values(\"id\")\nembeddings, sample_ids, _ = lancedb_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (10, 512)\nprint(sample_ids.shape)  # (10,)\n</code></pre>"},{"location":"integrations/lancedb/#querying-a-lancedb-index","title":"Querying a LanceDB index \u00b6","text":"<p>You can query a LanceDB index by appending a <code>sort_by_similarity()</code> stage to any dataset or view. The query can be any of the following:</p> <ul> <li> <p>An ID (sample or patch)</p> </li> <li> <p>A query vector of same dimension as the index</p> </li> <li> <p>A list of IDs (samples or patches)</p> </li> <li> <p>A text prompt (if supported by the model)</p> </li> </ul> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"lancedb_index\",\n    backend=\"lancedb\",\n)\n\n# Query by vector\nquery = np.random.rand(512)  # matches the dimension of CLIP embeddings\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"lancedb_index\")\n\n# Query by sample ID\nquery = dataset.first().id\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"lancedb_index\")\n\n# Query by a list of IDs\nquery = [dataset.first().id, dataset.last().id]\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"lancedb_index\")\n\n# Query by text prompt\nquery = \"a photo of a dog\"\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"lancedb_index\")\n</code></pre> <p>Note</p> <p>Performing a similarity search on a <code>DatasetView</code> will only return results from the view; if the view contains samples that were not included in the index, they will never be included in the result.</p> <p>This means that you can index an entire <code>Dataset</code> once and then perform searches on subsets of the dataset by constructing views that contain the images of interest.</p>"},{"location":"integrations/lancedb/#advanced-usage","title":"Advanced usage \u00b6","text":"<p>LanceDB is compatible with the Python ecosystem and can be used with pandas, numpy, and arrow:</p> <pre><code>lancedb_index = fob.compute_similarity(..., backend=\"lancedb\", ...)\n\n# Retrieve the raw LanceDB table\ntable = lancedb_index.table\n\ndf = table.to_pandas()  # get the table as a pandas dataframe\npa = table.to_arrow()   # get the table as an arrow table\n</code></pre>"},{"location":"integrations/milvus/","title":"Milvus Integration \u00b6","text":"<p>Milvus is one of the most popular vector databases available, and we\u2019ve made it easy to use Milvus\u2019s vector search capabilities on your computer vision data directly from FiftyOne!</p> <p>Follow these simple instructions to get started using Milvus + FiftyOne.</p> <p>FiftyOne provides an API to create Milvus collections, upload vectors, and run similarity queries, both programmatically in Python and via point-and-click in the App.</p> <p>Note</p> <p>Did you know? You can search by natural language using Milvus similarity indexes!</p> <p></p>"},{"location":"integrations/milvus/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The basic workflow to use Milvus to create a similarity index on your FiftyOne datasets and use this to query your data is as follows:</p> <ol> <li> <p>Load a dataset into FiftyOne</p> </li> <li> <p>Compute embedding vectors for samples or patches in your dataset, or select a model to use to generate embeddings</p> </li> <li> <p>Use the <code>compute_similarity()</code> methodto generate a Milvus similarity index for the samples or object patches in a dataset by setting the parameter <code>backend=\"milvus\"</code> and specifying a <code>brain_key</code> of your choice</p> </li> <li> <p>Use this Milvus similarity index to query your data with <code>sort_by_similarity()</code></p> </li> <li> <p>If desired, delete the index</p> </li> </ol> <p>The example below demonstrates this workflow.</p> <p>Note</p> <p>You must connect to a Milvus server and install the Milvus Python client to run this example:</p> <pre><code>wget https://github.com/milvus-io/milvus/releases/download/v2.2.11/milvus-standalone-docker-compose.yml -O docker-compose.yml\nsudo docker compose up -d\n\npip install pymilvus\n</code></pre> <p>Note that, if you are using a custom Milvus server, you can store your credentials as described in this section to avoid entering them manually each time you interact with your Milvus index.</p> <p>First let\u2019s load a dataset into FiftyOne and compute embeddings for the samples:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\n# Step 1: Load your data into FiftyOne\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Steps 2 and 3: Compute embeddings and create a similarity index\nmilvus_index = fob.compute_similarity(\n    dataset,\n    brain_key=\"milvus_index\",\n    backend=\"milvus\",\n)\n</code></pre> <p>Once the similarity index has been generated, we can query our data in FiftyOne by specifying the <code>brain_key</code>:</p> <pre><code># Step 4: Query your data\nquery = dataset.first().id  # query by sample ID\nview = dataset.sort_by_similarity(\n    query,\n    brain_key=\"milvus_index\",\n    k=10,  # limit to 10 most similar samples\n)\n\n# Step 5 (optional): Cleanup\n\n# Delete the Milvus collection\nmilvus_index.cleanup()\n\n# Delete run record from FiftyOne\ndataset.delete_brain_run(\"milvus_index\")\n</code></pre> <p>Note</p> <p>Skip to this section to see a variety of common Milvus query patterns.</p>"},{"location":"integrations/milvus/#setup","title":"Setup \u00b6","text":"<p>The easiest way to get started is to install Milvus standalone via Docker Compose:</p> <pre><code>wget https://github.com/milvus-io/milvus/releases/download/v2.2.11/milvus-standalone-docker-compose.yml -O docker-compose.yml\nsudo docker compose up -d\n</code></pre>"},{"location":"integrations/milvus/#installing-the-milvus-client","title":"Installing the Milvus client \u00b6","text":"<p>In order to use the Milvus backend, you must also install the Milvus Python client:</p> <pre><code>pip install pymilvus\n</code></pre>"},{"location":"integrations/milvus/#using-the-milvus-backend","title":"Using the Milvus backend \u00b6","text":"<p>By default, calling <code>compute_similarity()</code> or <code>sort_by_similarity()</code> will use an sklearn backend.</p> <p>To use the Milvus backend, simply set the optional <code>backend</code> parameter of <code>compute_similarity()</code> to <code>\"milvus\"</code>:</p> <pre><code>import fiftyone.brain as fob\n\nfob.compute_similarity(..., backend=\"milvus\", ...)\n</code></pre> <p>Alternatively, you can permanently configure FiftyOne to use the Milvus backend by setting the following environment variable:</p> <pre><code>export FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND=milvus\n</code></pre> <p>or by setting the <code>default_similarity_backend</code> parameter of your brain config located at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"default_similarity_backend\": \"milvus\"\n}\n</code></pre>"},{"location":"integrations/milvus/#authentication","title":"Authentication \u00b6","text":"<p>If you are using a custom Milvus server, you can provide your credentials in a variety of ways.</p> <p>Environment variables (recommended)</p> <p>The recommended way to configure your Milvus credentials is to store them in the environment variables shown below, which are automatically accessed by FiftyOne whenever a connection to Milvus is made.</p> <pre><code>export FIFTYONE_BRAIN_SIMILARITY_MILVUS_URI=XXXXXX\nexport FIFTYONE_BRAIN_SIMILARITY_MILVUS_USER=XXXXXX\nexport FIFTYONE_BRAIN_SIMILARITY_MILVUS_PASSWORD=XXXXXX\n\n# also available if necessary\nexport FIFTYONE_BRAIN_SIMILARITY_MILVUS_SECURE=true\nexport FIFTYONE_BRAIN_SIMILARITY_MILVUS_TOKEN=XXXXXX\nexport FIFTYONE_BRAIN_SIMILARITY_MILVUS_DB_NAME=XXXXXX\nexport FIFTYONE_BRAIN_SIMILARITY_MILVUS_CLIENT_KEY_PATH=XXXXXX\nexport FIFTYONE_BRAIN_SIMILARITY_MILVUS_CLIENT_PEM_PATH=XXXXXX\nexport FIFTYONE_BRAIN_SIMILARITY_MILVUS_CA_PEM_PATH=XXXXXX\nexport FIFTYONE_BRAIN_SIMILARITY_MILVUS_SERVER_PEM_PATH=XXXXXX\nexport FIFTYONE_BRAIN_SIMILARITY_MILVUS_SERVER_NAME=XXXXXX\n</code></pre> <p>FiftyOne Brain config</p> <p>You can also store your credentials in your brain config located at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"similarity_backends\": {\n        \"milvus\": {\n            \"uri\": \"XXXXXX\",\n            \"user\": \"XXXXXX\",\n            \"password\": \"XXXXXX\",\n\n            # also available if necessary\n            \"secure\": true,\n            \"token\": \"XXXXXX\",\n            \"db_name\": \"XXXXXX\",\n            \"client_key_path\": \"XXXXXX\",\n            \"client_pem_path\": \"XXXXXX\",\n            \"ca_pem_path\": \"XXXXXX\",\n            \"server_pem_path\": \"XXXXXX\",\n            \"server_name\": \"XXXXXX\"\n        }\n    }\n}\n</code></pre> <p>Note that this file will not exist until you create it.</p> <p>Keyword arguments</p> <p>You can manually provide your Milvus credentials as keyword arguments each time you call methods like <code>compute_similarity()</code> that require connections to Milvus:</p> <pre><code>import fiftyone.brain as fob\n\nmilvus_index = fob.compute_similarity(\n    ...\n    backend=\"milvus\",\n    brain_key=\"milvus_index\",\n    uri=\"XXXXXX\",\n    user=\"XXXXXX\",\n    password=\"XXXXXX\",\n\n    # also available if necessary\n    secure=True,\n    token=\"XXXXXX\",\n    db_name=\"XXXXXX\",\n    client_key_path=\"XXXXXX\",\n    client_pem_path=\"XXXXXX\",\n    ca_pem_path=\"XXXXXX\",\n    server_pem_path=\"XXXXXX\",\n    server_name=\"XXXXXX\",\n)\n</code></pre> <p>Note that, when using this strategy, you must manually provide the credentials when loading an index later via <code>load_brain_results()</code>:</p> <pre><code>milvus_index = dataset.load_brain_results(\n    \"milvus_index\",\n    uri=\"XXXXXX\",\n    user=\"XXXXXX\",\n    password=\"XXXXXX\",\n\n    # also available if necessary\n    secure=True,\n    token=\"XXXXXX\",\n    db_name=\"XXXXXX\",\n    client_key_path=\"XXXXXX\",\n    client_pem_path=\"XXXXXX\",\n    ca_pem_path=\"XXXXXX\",\n    server_pem_path=\"XXXXXX\",\n    server_name=\"XXXXXX\",\n)\n</code></pre>"},{"location":"integrations/milvus/#milvus-config-parameters","title":"Milvus config parameters \u00b6","text":"<p>The Milvus backend supports a variety of query parameters that can be used to customize your similarity queries. These parameters include:</p> <ul> <li> <p>collection_name ( None): the name of the Milvus collection to use or create. If none is provided, a new collection will be created</p> </li> <li> <p>metric ( \u201cdotproduct\u201d): the embedding distance metric to use when creating a new index. The supported values are <code>(\"cosine\", \"dotproduct\", \"euclidean\")</code></p> </li> <li> <p>consistency_level ( \u201cSession\u201d): the consistency level to use. Supported values are <code>(\"Strong\", \"Session\", \"Bounded\", \"Eventually\")</code></p> </li> </ul> <p>For detailed information on these parameters, see the Milvus authentication documentation and Milvus consistency levels documentation.</p> <p>You can specify these parameters via any of the strategies described in the previous section. Here\u2019s an example of a brain config that includes all of the available parameters:</p> <pre><code>{\n    \"similarity_backends\": {\n        \"milvus\": {\n            \"collection_name\": \"your_collection\",\n            \"metric\": \"dotproduct\",\n            \"consistency_level\": \"Strong\"\n        }\n    }\n}\n</code></pre> <p>However, typically these parameters are directly passed to <code>compute_similarity()</code> to configure a specific new index:</p> <pre><code>milvus_index = fob.compute_similarity(\n    ...\n    backend=\"milvus\",\n    brain_key=\"milvus_index\",\n    collection_name=\"your_collection\",\n    metric=\"dotproduct\",\n    consistency_level=\"Strong\",\n)\n</code></pre>"},{"location":"integrations/milvus/#managing-brain-runs","title":"Managing brain runs \u00b6","text":"<p>FiftyOne provides a variety of methods that you can use to manage brain runs.</p> <p>For example, you can call <code>list_brain_runs()</code> to see the available brain keys on a dataset:</p> <pre><code>import fiftyone.brain as fob\n\n# List all brain runs\ndataset.list_brain_runs()\n\n# Only list similarity runs\ndataset.list_brain_runs(type=fob.Similarity)\n\n# Only list specific similarity runs\ndataset.list_brain_runs(\n    type=fob.Similarity,\n    patches_field=\"ground_truth\",\n    supports_prompts=True,\n)\n</code></pre> <p>Or, you can use <code>get_brain_info()</code> to retrieve information about the configuration of a brain run:</p> <pre><code>info = dataset.get_brain_info(brain_key)\nprint(info)\n</code></pre> <p>Use <code>load_brain_results()</code> to load the <code>SimilarityIndex</code> instance for a brain run.</p> <p>You can use <code>rename_brain_run()</code> to rename the brain key associated with an existing similarity results run:</p> <pre><code>dataset.rename_brain_run(brain_key, new_brain_key)\n</code></pre> <p>Finally, you can use <code>delete_brain_run()</code> to delete the record of a similarity index computation from your FiftyOne dataset:</p> <pre><code>dataset.delete_brain_run(brain_key)\n</code></pre> <p>Note</p> <p>Calling <code>delete_brain_run()</code> only deletes the record of the brain run from your FiftyOne dataset; it will not delete any associated Milvus collection, which you can do as follows:</p> <pre><code># Delete the Milvus collection\nmilvus_index = dataset.load_brain_results(brain_key)\nmilvus_index.cleanup()\n</code></pre>"},{"location":"integrations/milvus/#examples","title":"Examples \u00b6","text":"<p>This section demonstrates how to perform some common vector search workflows on a FiftyOne dataset using the Milvus backend.</p> <p>Note</p> <p>All of the examples below assume you have configured your Milvus server and connection as described in this section.</p>"},{"location":"integrations/milvus/#create-a-similarity-index","title":"Create a similarity index \u00b6","text":"<p>In order to create a new Milvus similarity index, you need to specify either the <code>embeddings</code> or <code>model</code> argument to <code>compute_similarity()</code>. Here\u2019s a few possibilities:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nmodel_name = \"clip-vit-base32-torch\"\nmodel = foz.load_zoo_model(model_name)\nbrain_key = \"milvus_index\"\n\n# Option 1: Compute embeddings on the fly from model name\nfob.compute_similarity(\n    dataset,\n    model=model_name,\n    backend=\"milvus\",\n    brain_key=brain_key,\n)\n\n# Option 2: Compute embeddings on the fly from model instance\nfob.compute_similarity(\n    dataset,\n    model=model,\n    backend=\"milvus\",\n    brain_key=brain_key,\n)\n\n# Option 3: Pass precomputed embeddings as a numpy array\nembeddings = dataset.compute_embeddings(model)\nfob.compute_similarity(\n    dataset,\n    embeddings=embeddings,\n    backend=\"milvus\",\n    brain_key=brain_key,\n)\n\n# Option 4: Pass precomputed embeddings by field name\ndataset.compute_embeddings(model, embeddings_field=\"embeddings\")\nfob.compute_similarity(\n    dataset,\n    embeddings=\"embeddings\",\n    backend=\"milvus\",\n    brain_key=brain_key,\n)\n</code></pre> <p>Note</p> <p>You can customize the Milvus similarity index by passing any supported parameters as extra kwargs.</p>"},{"location":"integrations/milvus/#create-a-patch-similarity-index","title":"Create a patch similarity index \u00b6","text":"<p>You can also create a similarity index for object patches within your dataset by specifying a <code>patches_field</code> argument to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    patches_field=\"ground_truth\",\n    model=\"clip-vit-base32-torch\",\n    backend=\"milvus\",\n    brain_key=\"milvus_patches\",\n)\n</code></pre> <p>Note</p> <p>You can customize the Milvus index by passing any supported parameters as extra kwargs.</p>"},{"location":"integrations/milvus/#connect-to-an-existing-collection","title":"Connect to an existing collection \u00b6","text":"<p>If you have already created a Milvus collection storing the embedding vectors for the samples or patches in your dataset, you can connect to it by passing the <code>collection_name</code> to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",      # zoo model used (if applicable)\n    embeddings=False,                   # don't compute embeddings\n    collection_name=\"your_collection\",  # the existing Milvus collection\n    brain_key=\"milvus_index\",\n    backend=\"milvus\",\n)\n</code></pre>"},{"location":"integrations/milvus/#addremove-embeddings-from-an-index","title":"Add/remove embeddings from an index \u00b6","text":"<p>You can use <code>add_to_index()</code> and <code>remove_from_index()</code> to add and remove embeddings from an existing Milvus similarity index.</p> <p>These methods can come in handy if you modify your FiftyOne dataset and need to update the Milvus similarity index to reflect these changes:</p> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nmilvus_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"milvus_index\",\n    backend=\"milvus\",\n)\nprint(milvus_index.total_index_size)  # 200\n\nview = dataset.take(10)\nids = view.values(\"id\")\n\n# Delete 10 samples from a dataset\ndataset.delete_samples(view)\n\n# Delete the corresponding vectors from the index\nmilvus_index.remove_from_index(sample_ids=ids)\n\n# Add 20 samples to a dataset\nsamples = [fo.Sample(filepath=\"tmp%d.jpg\" % i) for i in range(20)]\nsample_ids = dataset.add_samples(samples)\n\n# Add corresponding embeddings to the index\nembeddings = np.random.rand(20, 512)\nmilvus_index.add_to_index(embeddings, sample_ids)\n\nprint(milvus_index.total_index_size)  # 210\n</code></pre>"},{"location":"integrations/milvus/#retrieve-embeddings-from-an-index","title":"Retrieve embeddings from an index \u00b6","text":"<p>You can use <code>get_embeddings()</code> to retrieve embeddings from a Milvus index by ID:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nmilvus_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"milvus_index\",\n    backend=\"milvus\",\n)\n\n# Retrieve embeddings for the entire dataset\nids = dataset.values(\"id\")\nembeddings, sample_ids, _ = milvus_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (200, 512)\nprint(sample_ids.shape)  # (200,)\n\n# Retrieve embeddings for a view\nids = dataset.take(10).values(\"id\")\nembeddings, sample_ids, _ = milvus_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (10, 512)\nprint(sample_ids.shape)  # (10,)\n</code></pre>"},{"location":"integrations/milvus/#querying-a-milvus-index","title":"Querying a Milvus index \u00b6","text":"<p>You can query a Milvus index by appending a <code>sort_by_similarity()</code> stage to any dataset or view. The query can be any of the following:</p> <ul> <li> <p>An ID (sample or patch)</p> </li> <li> <p>A query vector of same dimension as the index</p> </li> <li> <p>A list of IDs (samples or patches)</p> </li> <li> <p>A text prompt (if supported by the model)</p> </li> </ul> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"milvus_index\",\n    backend=\"milvus\",\n)\n\n# Query by vector\nquery = np.random.rand(512)  # matches the dimension of CLIP embeddings\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"milvus_index\")\n\n# Query by sample ID\nquery = dataset.first().id\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"milvus_index\")\n\n# Query by a list of IDs\nquery = [dataset.first().id, dataset.last().id]\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"milvus_index\")\n\n# Query by text prompt\nquery = \"a photo of a dog\"\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"milvus_index\")\n</code></pre> <p>Note</p> <p>Performing a similarity search on a <code>DatasetView</code> will only return results from the view; if the view contains samples that were not included in the index, they will never be included in the result.</p> <p>This means that you can index an entire <code>Dataset</code> once and then perform searches on subsets of the dataset by constructing views that contain the images of interest.</p>"},{"location":"integrations/milvus/#accessing-the-milvus-client","title":"Accessing the Milvus client \u00b6","text":"<p>You can use the <code>collection</code> property of a Milvus index to directly access the underlying Milvus collection and use its methods as desired:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nmilvus_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"milvus_index\",\n    backend=\"milvus\",\n)\n\nprint(milvus_index.collection)\n\n# The Milvus SDK is already initialized for you as well\nimport pymilvus\nprint(pymilvus.utility.list_collections())\n</code></pre>"},{"location":"integrations/milvus/#advanced-usage","title":"Advanced usage \u00b6","text":"<p>As previously mentioned, you can customize your Milvus indexes by providing optional parameters to <code>compute_similarity()</code>.</p> <p>Here\u2019s an example of creating a similarity index backed by a customized Milvus similarity index. Just for fun, we\u2019ll specify a custom collection name, use euclidean distance, and populate the index for only a subset of our dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Create a custom Milvus index\nmilvus_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    embeddings=False,  # we'll add embeddings below\n    metric=\"euclidean\",\n    brain_key=\"milvus_index\",\n    backend=\"milvus\",\n    collection_name=\"custom_milvus_collection\",\n)\n\n# Add embeddings for a subset of the dataset\nview = dataset.take(10)\nembeddings, sample_ids, _ = milvus_index.compute_embeddings(view)\nmilvus_index.add_to_index(embeddings, sample_ids)\n\nprint(milvus_index.collection)\n\n# The Milvus SDK is already initialized for you as well\nimport pymilvus\nprint(pymilvus.utility.list_collections())\n</code></pre>"},{"location":"integrations/mongodb/","title":"MongoDB Vector Search Integration \u00b6","text":"<p>MongoDB is the leading open source database for unstructured data, and we\u2019ve made it easy to use MongoDB Atlas\u2019 vector search capabilities on your computer vision data directly from FiftyOne!</p> <p>Follow these simple instructions to configure a MongoDB Atlas cluster and get started using MongoDB Atlas + FiftyOne.</p> <p>FiftyOne provides an API to create MongoDB Atlas vector search indexes, upload vectors, and run similarity queries, both programmatically in Python and via point-and-click in the App.</p> <p>Note</p> <p>Did you know? You can search by natural language using MongoDB similarity indexes!</p> <p></p>"},{"location":"integrations/mongodb/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The basic workflow to use MongoDB Atlas to create a similarity index on your FiftyOne datasets and use this to query your data is as follows:</p> <ol> <li> <p>Configure a MongoDB Atlas cluster</p> </li> <li> <p>Load a dataset into FiftyOne</p> </li> <li> <p>Compute embedding vectors for samples or patches in your dataset, or select a model to use to generate embeddings</p> </li> <li> <p>Use the <code>compute_similarity()</code> method to generate a MongoDB similarity index for the samples or object patches in a dataset by setting the parameter <code>backend=\"mongodb\"</code> and specifying a <code>brain_key</code> of your choice</p> </li> <li> <p>Use this MongoDB similarity index to query your data with <code>sort_by_similarity()</code></p> </li> <li> <p>If desired, delete the index</p> </li> </ol> <p>The example below demonstrates this workflow.</p> <p>Note</p> <p>You must configure a MongoDB Atlas 7.0 or later cluster and provide its connection string to run this example:</p> <pre><code>export FIFTYONE_DATABASE_NAME=fiftyone\nexport FIFTYONE_DATABASE_URI='mongodb+srv://$USERNAME:$PASSWORD@fiftyone.XXXXXX.mongodb.net/?retryWrites=true&amp;w=majority'\n</code></pre> <p>First let\u2019s load a dataset into FiftyOne and compute embeddings for the samples:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\n# Step 1: Load your data into FiftyOne\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Steps 2 and 3: Compute embeddings and create a similarity index\nmongodb_index = fob.compute_similarity(\n    dataset,\n    embeddings=\"embeddings\",  # the field in which to store the embeddings\n    brain_key=\"mongodb_index\",\n    backend=\"mongodb\",\n)\n</code></pre> <p>Once the similarity index has been generated, we can query our data in FiftyOne by specifying the <code>brain_key</code>:</p> <pre><code># Wait for the index to be ready for querying...\nassert mongodb_index.ready\n\n# Step 4: Query your data\nquery = dataset.first().id  # query by sample ID\nview = dataset.sort_by_similarity(\n    query,\n    brain_key=\"mongodb_index\",\n    k=10,  # limit to 10 most similar samples\n)\n\n# Step 5 (optional): Cleanup\n\n# Delete the MongoDB vector search index\nmongodb_index.cleanup()\n\n# Delete run record from FiftyOne\ndataset.delete_brain_run(\"mongodb_index\")\n</code></pre> <p>Note</p> <p>Skip to this section for a variety of common MongoDB query patterns.</p>"},{"location":"integrations/mongodb/#setup","title":"Setup \u00b6","text":"<p>In order to use MongoDB vector search, you must connect your FiftyOne installation to MongoDB Atlas, which you can do by navigating to https://cloud.mongodb.com, creating an account, and following the instructions there to configure your cluster.</p> <p>Note</p> <p>You must be running MongoDB Atlas 7.0 or later in order to programmatically create vector search indexes ( source).</p> <p>As of this writing, Atlas\u2019 shared tier (M0, M2, M5) is running MongoDB 6. In order to use MongoDB 7, you must upgrade to an M10 cluster, which starts at $0.08/hour.</p>"},{"location":"integrations/mongodb/#configuring-your-connection-string","title":"Configuring your connection string \u00b6","text":"<p>You can connect FiftyOne to your MongoDB Atlas cluster by simply providing its connection string:</p> <pre><code>export FIFTYONE_DATABASE_NAME=fiftyone\nexport FIFTYONE_DATABASE_URI='mongodb+srv://$USERNAME:$PASSWORD@fiftyone.XXXXXX.mongodb.net/?retryWrites=true&amp;w=majority'\n</code></pre>"},{"location":"integrations/mongodb/#using-the-mongodb-backend","title":"Using the MongoDB backend \u00b6","text":"<p>By default, calling <code>compute_similarity()</code> or <code>sort_by_similarity()</code> will use an sklearn backend.</p> <p>To use the MongoDB backend, simply set the optional <code>backend</code> parameter of <code>compute_similarity()</code> to <code>\"mongodb\"</code>:</p> <pre><code>import fiftyone.brain as fob\n\nfob.compute_similarity(..., backend=\"mongodb\", ...)\n</code></pre> <p>Alternatively, you can permanently configure FiftyOne to use the MonogDB backend by setting the following environment variable:</p> <pre><code>export FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND=mongodb\n</code></pre> <p>or by setting the <code>default_similarity_backend</code> parameter of your brain config located at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"default_similarity_backend\": \"mongodb\"\n}\n</code></pre>"},{"location":"integrations/mongodb/#mongodb-config-parameters","title":"MongoDB config parameters \u00b6","text":"<p>The MongoDB backend supports a variety of query parameters that can be used to customize your similarity queries. These parameters include:</p> <ul> <li> <p>index_name ( None): the name of the MongoDB vector search index to use or create. If not specified, a new unique name is generated automatically</p> </li> <li> <p>metric ( \u201ccosine\u201d): the distance/similarity metric to use when creating a new index. The supported values are <code>(\"cosine\", \"dotproduct\", \"euclidean\")</code></p> </li> </ul> <p>For detailed information on these parameters, see the MongoDB documentation.</p> <p>You can specify these parameters via any of the strategies described in the previous section. Here\u2019s an example of a brain config that includes all of the available parameters:</p> <pre><code>{\n    \"similarity_backends\": {\n        \"mongodb\": {\n            \"index_name\": \"your-index\",\n            \"metric\": \"cosine\"\n        }\n    }\n}\n</code></pre> <p>However, typically these parameters are directly passed to <code>compute_similarity()</code> to configure a specific new index:</p> <pre><code>mongodb_index = fob.compute_similarity(\n    ...\n    backend=\"mongodb\",\n    brain_key=\"mongodb_index\",\n    index_name=\"your-index\",\n    metric=\"cosine\",\n)\n</code></pre>"},{"location":"integrations/mongodb/#managing-brain-runs","title":"Managing brain runs \u00b6","text":"<p>FiftyOne provides a variety of methods that you can use to manage brain runs.</p> <p>For example, you can call <code>list_brain_runs()</code> to see the available brain keys on a dataset:</p> <pre><code>import fiftyone.brain as fob\n\n# List all brain runs\ndataset.list_brain_runs()\n\n# Only list similarity runs\ndataset.list_brain_runs(type=fob.Similarity)\n\n# Only list specific similarity runs\ndataset.list_brain_runs(\n    type=fob.Similarity,\n    patches_field=\"ground_truth\",\n    supports_prompts=True,\n)\n</code></pre> <p>Or, you can use <code>get_brain_info()</code> to retrieve information about the configuration of a brain run:</p> <pre><code>info = dataset.get_brain_info(brain_key)\nprint(info)\n</code></pre> <p>Use <code>load_brain_results()</code> to load the <code>SimilarityIndex</code> instance for a brain run.</p> <p>You can use <code>rename_brain_run()</code> to rename the brain key associated with an existing similarity results run:</p> <pre><code>dataset.rename_brain_run(brain_key, new_brain_key)\n</code></pre> <p>Finally, you can use <code>delete_brain_run()</code> to delete the record of a similarity index computation from your FiftyOne dataset:</p> <pre><code>dataset.delete_brain_run(brain_key)\n</code></pre> <p>Note</p> <p>Calling <code>delete_brain_run()</code> only deletes the record of the brain run from your FiftyOne dataset; it will not delete any associated MongoDB vector search index, which you can do as follows:</p> <pre><code># Delete the MongoDB vector search index\nmongodb_index = dataset.load_brain_results(brain_key)\nmongodb_index.cleanup()\n</code></pre>"},{"location":"integrations/mongodb/#examples","title":"Examples \u00b6","text":"<p>This section demonstrates how to perform some common vector search workflows on a FiftyOne dataset using the MongoDB backend.</p> <p>Note</p> <p>All of the examples below assume you have configured your MongoDB Atlas cluster as described in this section.</p>"},{"location":"integrations/mongodb/#create-a-similarity-index","title":"Create a similarity index \u00b6","text":"<p>In order to create a new MongoDB similarity index, you need to specify either the <code>embeddings</code> or <code>model</code> argument to <code>compute_similarity()</code>. Here\u2019s a few possibilities:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nmodel_name = \"clip-vit-base32-torch\"\nmodel = foz.load_zoo_model(model_name)\nbrain_key = \"mongodb_index\"\n\n# Option 1: Compute embeddings on the fly from model name\nfob.compute_similarity(\n    dataset,\n    model=model_name,\n    embeddings=\"embeddings\",  # the field in which to store the embeddings\n    backend=\"mongodb\",\n    brain_key=brain_key,\n)\n\n# Option 2: Compute embeddings on the fly from model instance\nfob.compute_similarity(\n    dataset,\n    model=model,\n    embeddings=\"embeddings\",  # the field in which to store the embeddings\n    backend=\"mongodb\",\n    brain_key=brain_key,\n)\n\n# Option 3: Pass precomputed embeddings as a numpy array\nembeddings = dataset.compute_embeddings(model)\nfob.compute_similarity(\n    dataset,\n    embeddings=embeddings,\n    embeddings_field=\"embeddings\",  # the field in which to store the embeddings\n    backend=\"mongodb\",\n    brain_key=brain_key,\n)\n\n# Option 4: Pass precomputed embeddings by field name\n# Note that MongoDB vector indexes require list fields\nembeddings = dataset.compute_embeddings(model)\ndataset.set_values(\"embeddings\", embeddings.tolist())\nfob.compute_similarity(\n    dataset,\n    embeddings=\"embeddings\",  # the field that contains the embeddings\n    backend=\"mongodb\",\n    brain_key=brain_key,\n)\n</code></pre> <p>Note</p> <p>You can customize the MongoDB index by passing any supported parameters as extra kwargs.</p>"},{"location":"integrations/mongodb/#create-a-patch-similarity-index","title":"Create a patch similarity index \u00b6","text":"<p>Warning</p> <p>The MongoDB backend does not yet support indexing object patches, so the code below will not yet run. Check back soon!</p> <p>You can also create a similarity index for object patches within your dataset by including the <code>patches_field</code> argument to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    patches_field=\"ground_truth\",\n    model=\"clip-vit-base32-torch\",\n    embeddings=\"embeddings\",  # the attribute in which to store the embeddings\n    backend=\"mongodb\",\n    brain_key=\"mongodb_patches\",\n)\n</code></pre> <p>Note</p> <p>You can customize the MongoDB index by passing any supported parameters as extra kwargs.</p>"},{"location":"integrations/mongodb/#connect-to-an-existing-index","title":"Connect to an existing index \u00b6","text":"<p>If you have already created a MongoDB index storing the embedding vectors for the samples or patches in your dataset, you can connect to it by passing the <code>index_name</code> to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",      # zoo model used (if applicable)\n    embeddings=False,                   # don't compute embeddings\n    index_name=\"your-index\",            # the existing MongoDB index\n    brain_key=\"mongodb_index\",\n    backend=\"mongodb\",\n)\n</code></pre>"},{"location":"integrations/mongodb/#addremove-embeddings-from-an-index","title":"Add/remove embeddings from an index \u00b6","text":"<p>You can use <code>add_to_index()</code> and <code>remove_from_index()</code> to add and remove embeddings from an existing Mongodb index.</p> <p>These methods can come in handy if you modify your FiftyOne dataset and need to update the Mongodb index to reflect these changes:</p> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nmongodb_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    embeddings=\"embeddings\",  # the field in which to store the embeddings\n    brain_key=\"mongodb_index\",\n    backend=\"mongodb\",\n)\nprint(mongodb_index.total_index_size)  # 200\n\nview = dataset.take(10)\nids = view.values(\"id\")\n\n# Delete 10 samples from a dataset\ndataset.delete_samples(view)\n\n# Delete the corresponding vectors from the index\nmongodb_index.remove_from_index(sample_ids=ids)\n\n# Add 20 samples to a dataset\nsamples = [fo.Sample(filepath=\"tmp%d.jpg\" % i) for i in range(20)]\nsample_ids = dataset.add_samples(samples)\n\n# Add corresponding embeddings to the index\nembeddings = np.random.rand(20, 512)\nmongodb_index.add_to_index(embeddings, sample_ids)\n\nprint(mongodb_index.total_index_size)  # 210\n</code></pre>"},{"location":"integrations/mongodb/#retrieve-embeddings-from-an-index","title":"Retrieve embeddings from an index \u00b6","text":"<p>You can use <code>get_embeddings()</code> to retrieve embeddings from a Mongodb index by ID:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nmongodb_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    embeddings=\"embeddings\",  # the field in which to store the embeddings\n    brain_key=\"mongodb_index\",\n    backend=\"mongodb\",\n)\n\n# Retrieve embeddings for the entire dataset\nids = dataset.values(\"id\")\nembeddings, sample_ids, _ = mongodb_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (200, 512)\nprint(sample_ids.shape)  # (200,)\n\n# Retrieve embeddings for a view\nids = dataset.take(10).values(\"id\")\nembeddings, sample_ids, _ = mongodb_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (10, 512)\nprint(sample_ids.shape)  # (10,)\n</code></pre>"},{"location":"integrations/mongodb/#querying-a-mongodb-index","title":"Querying a MongoDB index \u00b6","text":"<p>You can query a MongoDB index by appending a <code>sort_by_similarity()</code> stage to any dataset or view. The query can be any of the following:</p> <ul> <li> <p>An ID (sample or patch)</p> </li> <li> <p>A query vector of same dimension as the index</p> </li> <li> <p>A list of IDs (samples or patches)</p> </li> <li> <p>A text prompt (if supported by the model)</p> </li> </ul> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nmongodb_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    embeddings=\"embeddings\",  # the field in which to store the embeddings\n    brain_key=\"mongodb_index\",\n    backend=\"mongodb\",\n)\n\n# Wait for the index to be ready for querying...\nassert mongodb_index.ready\n\n# Query by vector\nquery = np.random.rand(512)  # matches the dimension of CLIP embeddings\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"mongodb_index\")\n\n# Query by sample ID\nquery = dataset.first().id\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"mongodb_index\")\n\n# Query by a list of IDs\nquery = [dataset.first().id, dataset.last().id]\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"mongodb_index\")\n\n# Query by text prompt\nquery = \"a photo of a dog\"\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"mongodb_index\")\n</code></pre> <p>Note</p> <p>Performing a similarity search on a <code>DatasetView</code> will only return results from the view; if the view contains samples that were not included in the index, they will never be included in the result.</p> <p>This means that you can index an entire <code>Dataset</code> once and then perform searches on subsets of the dataset by constructing views that contain the images of interest.</p> <p>Note</p> <p>Currently, when performing a similarity search on a view with the MongoDB backend, the full index is queried and the resulting samples are restricted to the desired view. This may result in fewer samples than requested being returned by the search.</p>"},{"location":"integrations/mongodb/#checking-if-an-index-is-ready","title":"Checking if an index is ready \u00b6","text":"<p>You can use the <code>ready</code> property of a MongoDB index to check whether a newly created vector search index is ready for querying:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nmongodb_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    embeddings=\"embeddings\",  # the field in which to store the embeddings\n    brain_key=\"mongodb_index\",\n    backend=\"mongodb\",\n)\n\n# Wait for the index to be ready for querying...\nassert mongodb_index.ready\n</code></pre>"},{"location":"integrations/mongodb/#advanced-usage","title":"Advanced usage \u00b6","text":"<p>As previously mentioned, you can customize your MongoDB index by providing optional parameters to <code>compute_similarity()</code>.</p> <p>Here\u2019s an example of creating a similarity index backed by a customized MongoDB index. Just for fun, we\u2019ll specify a custom index name, use dot product similarity, and populate the index for only a subset of our dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Create a custom MongoDB index\nmongodb_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    embeddings_field=\"embeddings\",  # the field in which to store the embeddings\n    embeddings=False,               # add embeddings later\n    brain_key=\"mongodb_index\",\n    backend=\"mongodb\",\n    index_name=\"custom-quickstart-index\",\n    metric=\"dotproduct\",\n)\n\n# Add embeddings for a subset of the dataset\nview = dataset[:20]\nembeddings, sample_ids, _ = mongodb_index.compute_embeddings(view)\nmongodb_index.add_to_index(embeddings, sample_ids)\n\nprint(mongodb_index.total_index_size)  # 20\nprint(mongodb_index.config.index_name)  # custom-quickstart-index\nprint(mongodb_index.config.metric)  # dotproduct\n</code></pre>"},{"location":"integrations/open_images/","title":"Open Images Integration \u00b6","text":"<p>We\u2019ve collaborated with the team behind the Open Images Dataset to make it easy to download, visualize, and evaluate on the Open Images dataset natively in FiftyOne!</p> <p>Note</p> <p>Check out this tutorial to see how you can use FiftyOne to download and evaluate models on Open Images.</p> <p></p>"},{"location":"integrations/open_images/#loading-open-images","title":"Loading Open Images \u00b6","text":"<p>The FiftyOne Dataset Zoo provides support for loading the Open Images V6 and Open Images V7 datasets.</p> <p>Like all other zoo datasets, you can use <code>load_zoo_dataset()</code> to download and load an Open Images V7 split into FiftyOne:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Download and load the validation split of Open Images V7\ndataset = foz.load_zoo_dataset(\"open-images-v7\", split=\"validation\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>FiftyOne supports loading annotations for classification, detection, segmentation, and visual relationship tasks for the 600 boxable classes ( cf. dataset overview).</p> <p>By default, all label types are loaded, but you can customize this via the optional <code>label_types</code> argument (see below for details).</p> <p>In addition, FiftyOne provides parameters that can be used to efficiently download specific subsets of the Open Images dataset, allowing you to quickly explore different slices of the dataset without downloading the entire split.</p> <p>When performing partial downloads, FiftyOne will use existing downloaded data first if possible before resorting to downloading additional data from the web.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n#\n# Load 50 random samples from the validation split of Open Images V7.\n#\n# Only the required images will be downloaded (if necessary).\n# By default, all label types are loaded\n#\n\ndataset = foz.load_zoo_dataset(\n    \"open-images-v7\",\n    split=\"validation\",\n    max_samples=50,\n    shuffle=True,\n)\n\nsession = fo.launch_app(dataset)\n\n#\n# Load detections and classifications for 25 samples from the\n# validation split of Open Images V6 that contain fedoras and pianos\n#\n# Images that contain all `label_types` and `classes` will be\n# prioritized first, followed by images that contain at least one of\n# the required `classes`. If there are not enough images matching\n# `classes` in the split to meet `max_samples`, only the available\n# images will be loaded.\n#\n# Images will only be downloaded if necessary\n#\n\ndataset = foz.load_zoo_dataset(\n    \"open-images-v6\",\n    split=\"validation\",\n    label_types=[\"detections\", \"classifications\"],\n    classes=[\"Fedora\", \"Piano\"],\n    max_samples=25,\n)\n\nsession.dataset = dataset\n\n#\n# Load classifications and point labels for all samples from the\n# validation split of Open Images V7 with class \"Turtle\" or \"Tortoise\".\n#\n# If there are not enough images matching classes` in the split to\n# meet `max_samples`, only the available images will be loaded.\n#\n# Images will only be downloaded if necessary\n#\n\ndataset = foz.load_zoo_dataset(\n    \"open-images-v7\",\n    split=\"validation\",\n    label_types=[\"points\", \"classifications\"],\n    classes=[\"Turtle\", \"Tortoise\"],\n)\n\nsession.dataset = dataset\n</code></pre> <p>The following parameters are available to configure a partial download of Open Images V6 or Open Images V7 by passing them to <code>load_zoo_dataset()</code>:</p> <ul> <li> <p>split ( None) and splits ( None): a string or list of strings, respectively, specifying the splits to load. Supported values are <code>(\"train\", \"test\", \"validation\")</code>. If neither is provided, all available splits are loaded</p> </li> <li> <p>label_types ( None): a label type or list of label types to load. Supported values for Open Images V6 are <code>(\"detections\", \"classifications\", \"relationships\", \"segmentations\")</code>. Open Images V7 also supports <code>\"points\"</code> labels. By default, all labels types are loaded</p> </li> <li> <p>classes ( None): a string or list of strings specifying required classes to load. If provided, only samples containing at least one instance of a specified class will be loaded. You can use <code>get_classes()</code> and <code>get_segmentation_classes()</code> to see the available classes and segmentation classes, respectively</p> </li> <li> <p>attrs ( None): a string or list of strings specifying required relationship attributes to load. This parameter is only applicable if <code>label_types</code> contains <code>\"relationships\"</code>. If provided, only samples containing at least one instance of a specified attribute will be loaded. You can use <code>get_attributes()</code> to see the available attributes</p> </li> <li> <p>image_ids ( None): a list of specific image IDs to load. The IDs can be specified either as <code>&lt;split&gt;/&lt;image-id&gt;</code> or <code>&lt;image-id&gt;</code> strings. Alternatively, you can provide the path to a TXT (newline-separated), JSON, or CSV file containing the list of image IDs to load in either of the first two formats</p> </li> <li> <p>include_id ( True): whether to include the Open Images ID of each sample in the loaded labels</p> </li> <li> <p>only_matching ( False): whether to only load labels that match the <code>classes</code> or <code>attrs</code> requirements that you provide (True), or to load all labels for samples that match the requirements (False)</p> </li> <li> <p>num_workers ( None): the number of processes to use when downloading individual images. By default, <code>multiprocessing.cpu_count()</code> is used</p> </li> <li> <p>shuffle ( False): whether to randomly shuffle the order in which samples are chosen for partial downloads</p> </li> <li> <p>seed ( None): a random seed to use when shuffling</p> </li> <li> <p>max_samples ( None): a maximum number of samples to load per split. If <code>label_types</code>, <code>classes</code>, and/or <code>attrs</code> are also specified, first priority will be given to samples that contain all of the specified label types, classes, and/or attributes, followed by samples that contain at least one of the specified labels types or classes. The actual number of samples loaded may be less than this maximum value if the dataset does not contain sufficient samples matching your requirements</p> </li> </ul> <p>Note</p> <p>See <code>OpenImagesV7Dataset</code> , <code>OpenImagesV7Dataset</code> and <code>OpenImagesDatasetImporter</code> for complete descriptions of the optional keyword arguments that you can pass to <code>load_zoo_dataset()</code>.</p>"},{"location":"integrations/open_images/#open-images-style-evaluation","title":"Open Images-style evaluation \u00b6","text":"<p>The <code>evaluate_detections()</code> method provides builtin support for running Open Images-style evaluation.</p> <p>In order to run Open Images-style evaluation, simply set the <code>method</code> parameter to <code>\"open-images\"</code>.</p> <p>Note</p> <p>FiftyOne\u2019s implementation of Open Images-style evaluation matches the reference implementation available via the TF Object Detection API.</p>"},{"location":"integrations/open_images/#overview","title":"Overview \u00b6","text":"<p>Open Images-style evaluation provides additional features not found in COCO-style evaluation that you may find useful when evaluating your custom datasets.</p> <p>The two primary differences are:</p> <ul> <li> <p>Non-exhaustive image labeling: positive and negative sample-level <code>Classifications</code> fields can be provided to indicate which object classes were considered when annotating the image. Predicted objects whose classes are not included in the sample-level labels for a sample are ignored. The names of these fields can be specified via the <code>pos_label_field</code> and <code>neg_label_field</code> parameters</p> </li> <li> <p>Class hierarchies: If your dataset includes a class hierarchy, you can configure this evaluation protocol to automatically expand ground truth and/or predicted leaf classes so that all levels of the hierarchy can be correctly evaluated. You can provide a label hierarchy via the <code>hierarchy</code> parameter. By default, if you provide a hierarchy, then image-level label fields and ground truth detections will be expanded to incorporate parent classes (child classes for negative image-level labels). You can disable this feature by setting the <code>expand_gt_hierarchy</code> parameter to <code>False</code>. Alternatively, you can expand predictions by setting the <code>expand_pred_hierarchy</code> parameter to <code>True</code></p> </li> </ul> <p>In addition, note that:</p> <ul> <li> <p>Like VOC-style evaluation, only one IoU (default = 0.5) is used to calculate mAP. You can customize this value via the <code>iou</code> parameter</p> </li> <li> <p>When dealing with crowd objects, Open Images-style evaluation dictates that if a crowd is matched with multiple predictions, each counts as one true positive when computing mAP</p> </li> </ul> <p>When you specify an <code>eval_key</code> parameter, a number of helpful fields will be populated on each sample and its predicted/ground truth objects:</p> <ul> <li>True positive (TP), false positive (FP), and false negative (FN) counts for the each sample are saved in top-level fields of each sample:</li> </ul> <pre><code>TP: sample.&lt;eval_key&gt;_tp\nFP: sample.&lt;eval_key&gt;_fp\nFN: sample.&lt;eval_key&gt;_fn\n</code></pre> <ul> <li>The fields listed below are populated on each individual object instance; these fields tabulate the TP/FP/FN status of the object, the ID of the matching object (if any), and the matching IoU:</li> </ul> <pre><code>TP/FP/FN: object.&lt;eval_key&gt;\n        ID: object.&lt;eval_key&gt;_id\n       IoU: object.&lt;eval_key&gt;_iou\n</code></pre> <p>Note</p> <p>See <code>OpenImagesEvaluationConfig</code> for complete descriptions of the optional keyword arguments that you can pass to <code>evaluate_detections()</code> when running Open Images-style evaluation.</p>"},{"location":"integrations/open_images/#example-evaluation","title":"Example evaluation \u00b6","text":"<p>The example below demonstrates Open Images-style detection evaluation on the quickstart dataset from the Dataset Zoo:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nprint(dataset)\n\n# Evaluate the objects in the `predictions` field with respect to the\n# objects in the `ground_truth` field\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    method=\"open-images\",\n    eval_key=\"eval\",\n)\n\n# Get the 10 most common classes in the dataset\ncounts = dataset.count_values(\"ground_truth.detections.label\")\nclasses = sorted(counts, key=counts.get, reverse=True)[:10]\n\n# Print a classification report for the top-10 classes\nresults.print_report(classes=classes)\n\n# Print some statistics about the total TP/FP/FN counts\nprint(\"TP: %d\" % dataset.sum(\"eval_tp\"))\nprint(\"FP: %d\" % dataset.sum(\"eval_fp\"))\nprint(\"FN: %d\" % dataset.sum(\"eval_fn\"))\n\n# Create a view that has samples with the most false positives first, and\n# only includes false positive boxes in the `predictions` field\nview = (\n    dataset\n    .sort_by(\"eval_fp\", reverse=True)\n    .filter_labels(\"predictions\", F(\"eval\") == \"fp\")\n)\n\n# Visualize results in the App\nsession = fo.launch_app(view=view)\n</code></pre> <pre><code>               precision    recall  f1-score   support\n\n       person       0.25      0.86      0.39       378\n         kite       0.27      0.75      0.40        75\n          car       0.18      0.80      0.29        61\n         bird       0.20      0.51      0.28        51\n       carrot       0.09      0.74      0.16        47\n         boat       0.09      0.46      0.16        37\n    surfboard       0.17      0.73      0.28        30\n     airplane       0.36      0.83      0.50        24\ntraffic light       0.32      0.79      0.45        24\n      giraffe       0.36      0.91      0.52        23\n\n    micro avg       0.21      0.79      0.34       750\n    macro avg       0.23      0.74      0.34       750\n weighted avg       0.23      0.79      0.36       750\n</code></pre> <p></p>"},{"location":"integrations/open_images/#map-and-pr-curves","title":"mAP and PR curves \u00b6","text":"<p>You can easily compute mean average precision (mAP) and precision-recall (PR) curves using the results object returned by <code>evaluate_detections()</code>:</p> <p>Note</p> <p>FiftyOne\u2019s implementation of Open Images-style evaluation matches the reference implementation available via the TF Object Detection API.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nprint(dataset)\n\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    method=\"open-images\",\n)\n\nprint(results.mAP())\n# 0.599\n\nplot = results.plot_pr_curves(classes=[\"person\", \"dog\", \"car\"])\nplot.show()\n</code></pre> <p></p>"},{"location":"integrations/open_images/#confusion-matrices","title":"Confusion matrices \u00b6","text":"<p>You can also easily generate confusion matrices for the results of Open Images-style evaluations.</p> <p>In order for the confusion matrix to capture anything other than false positive/negative counts, you will likely want to set the <code>classwise</code> parameter to <code>False</code> during evaluation so that predicted objects can be matched with ground truth objects of different classes.</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Perform evaluation, allowing objects to be matched between classes\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    method=\"open-images\",\n    classwise=False,\n)\n\n# Generate a confusion matrix for the specified classes\nplot = results.plot_confusion_matrix(classes=[\"car\", \"truck\", \"motorcycle\"])\nplot.show()\n</code></pre> <p></p> <p>Note</p> <p>Did you know? Confusion matrices can be attached to your <code>Session</code> object and dynamically explored using FiftyOne\u2019s interactive plotting features!</p>"},{"location":"integrations/open_images/#open-images-challenge","title":"Open Images Challenge \u00b6","text":"<p>Since FiftyOne\u2019s implementation of Open Images-style evaluation matches the reference implementation from the TF Object Detection API used in the Open Images detection challenges. you can use it to compute the official mAP for your model while also enjoying the benefits of working in the FiftyOne ecosystem, including using views to manipulate your dataset and visually exploring your model\u2019s predictions in the FiftyOne App!</p> <p>In order to compute the official Open Images mAP for a model, your dataset must include the appropriate positive and negative sample-level labels, and you must provide the class hierarchy. Fortunately, when you load the Open Images dataset from the FiftyOne Dataset Zoo, all of the necessary information is automatically loaded for you!</p> <p>The example snippet below loads the Open Images V6 dataset and runs the official Open Images evaluation protocol on some mock model predictions:</p> <pre><code>import random\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Load some samples from the Open Images V6 dataset from the zoo\ndataset = foz.load_zoo_dataset(\n    \"open-images-v6\",\n    \"validation\",\n    max_samples=100,\n    label_types=[\"detections\", \"classifications\"],\n)\n\n# Generate some fake predictions\nfor sample in dataset:\n    predictions = sample[\"detections\"].copy()\n    for detection in predictions.detections:\n        detection.confidence = random.random()\n\n    sample[\"predictions\"] = predictions\n    sample.save()\n\n# Evaluate your predictions via the official Open Images protocol\nresults = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"detections\",\n    method=\"open-images\",\n    pos_label_field=\"positive_labels\",\n    neg_label_field=\"negative_labels\",\n    hierarchy=dataset.info[\"hierarchy\"],\n\n)\n\n# The official mAP for the results\nprint(results.mAP())\n</code></pre> <p>Most models trained on Open Images return the predictions for every class in the hierarchy. However, if your model does not, then you can set the <code>expand_pred_hierarchy</code> parameter to <code>False</code> to automatically generate predictions for parent classes in the hierarchy for evaluation purposes.</p> <p>Note</p> <p>Check out this recipe to learn how to add your model\u2019s predictions to a FiftyOne Dataset.</p>"},{"location":"integrations/open_images/#map-protocol","title":"mAP protocol \u00b6","text":"<p>The Open Images mAP protocol is similar to COCO-style mAP, with the primary differences being support for image-level labels, class hierarchies, and differences in the way that objects are matched to crowds.</p> <p>The steps to compute Open Images-style mAP are detailed below.</p> <p>Preprocessing</p> <ul> <li> <p>Filter ground truth and predicted objects by class (unless <code>classwise=False</code>)</p> </li> <li> <p>Expand the ground truth predictions by duplicating every object and positive image-level label and modifying the class to include all parent classes in the class hierarchy. Negative image-level labels are expanded to include all child classes in the hierarchy for every label in the image</p> </li> <li> <p>Sort predicted objects by confidence so that high confidence objects are matched first</p> </li> <li> <p>Sort ground truth objects so that objects with <code>IsGroupOf=True</code> (the name of this attribute can be customized via the <code>iscrowd</code> parameter) are matched last</p> </li> <li> <p>Compute IoU between every ground truth and predicted object within the same class (and between classes if <code>classwise=False</code>) in each image</p> </li> <li> <p>Compute IoU between predictions and crowd objects as the intersection of both boxes divided by the area of the prediction only. A prediction fully inside the crowd box has an IoU of 1</p> </li> </ul> <p>Matching</p> <p>Once IoUs have been computed, predictions and ground truth objects are matched to compute true positives, false positives, and false negatives:</p> <ul> <li> <p>For each class, start with the highest confidence prediction, match it to the ground truth object that it overlaps with the highest IoU. A prediction only matches if the IoU is above the specified <code>iou</code> threshold (default = 0.5)</p> </li> <li> <p>If a prediction matched to a non-crowd gt object, it will not match to a crowd even if the IoU is higher</p> </li> <li> <p>Multiple predictions can match to the same crowd ground truth object, but only one counts as a true positive, the others are ignored (unlike COCO). If the crowd is not matched by any prediction, it is a false negative</p> </li> <li> <p>(Unlike COCO) If a prediction maximally overlaps with a non-crowd ground truth object that has already been matched with a higher confidence prediction, the prediction is marked as a false positive</p> </li> <li> <p>If <code>classwise=False</code>, predictions can only match to crowds if they are of the same class</p> </li> </ul> <p>Computing mAP</p> <ul> <li> <p>(Unlike COCO) Only one IoU threshold (default = 0.5) is used to compute mAP</p> </li> <li> <p>The next 6 steps are computed separately for each class:</p> </li> <li> <p>Construct an array of true positives and false positives, sorted by confidence</p> </li> <li> <p>Compute the cumulative sum of this TP FP array</p> </li> <li> <p>Compute precision array by elementwise dividing the TP-FP-sum array by the total number of predictions up to that point</p> </li> <li> <p>Compute recall array by elementwise dividing the TP-FP-sum array with the total number of ground truth objects for the class</p> </li> <li> <p>Ensure that precision is a non-increasing array</p> </li> <li> <p>Add values <code>0</code> and <code>1</code> to precision and recall arrays</p> </li> <li> <p>(Unlike COCO) Precision values are not interpolated and all recall values are used to compute AP. This means that every class will produce a different number of precision and recall values depending on the number of true and false positives existing for that class</p> </li> <li> <p>For every class that contains at least one ground truth object, compute the AP by averaging the precision values. Then compute mAP by averaging the AP values for each class</p> </li> </ul>"},{"location":"integrations/openclip/","title":"OpenCLIP Integration \u00b6","text":"<p>FiftyOne integrates natively with the OpenCLIP library, an open source implementation of OpenAI\u2019s CLIP (Contrastive Language-Image Pre-training) model that you can use to run inference on your FiftyOne datasets with a few lines of code!</p>"},{"location":"integrations/openclip/#setup","title":"Setup \u00b6","text":"<p>To get started with OpenCLIP, install the <code>open_clip_torch</code> package:</p> <pre><code>pip install open_clip_torch\n\n# May also be needed\npip install timm --upgrade\n</code></pre>"},{"location":"integrations/openclip/#model-zoo","title":"Model zoo \u00b6","text":"<p>You can load the original ViT-B-32 OpenAI pretrained model from the FiftyOne Model Zoo as follows:</p> <pre><code>import fiftyone.zoo as foz\n\nmodel = foz.load_zoo_model(\"open-clip-torch\")\n</code></pre> <p>You can also specify different model architectures and pretrained weights by passing in optional parameters. Pretrained models can be loaded directly from OpenCLIP or from Hugging Face\u2019s Model Hub:</p> <pre><code>rn50 = foz.load_zoo_model(\n    \"open-clip-torch\",\n    clip_model=\"RN50\",\n    pretrained=\"cc12m\",\n)\n\nmeta_clip = foz.load_zoo_model(\n    \"open-clip-torch\",\n    clip_model=\"ViT-B-32-quickgelu\",\n    pretrained=\"metaclip_400m\",\n)\n\neva_clip = foz.load_zoo_model(\n    \"open-clip-torch\",\n    clip_model=\"EVA02-B-16\",\n    pretrained=\"merged2b_s8b_b131k\",\n)\n\nclipa = foz.load_zoo_model(\n    \"open-clip-torch\",\n    clip_model=\"hf-hub:UCSC-VLAA/ViT-L-14-CLIPA-datacomp1B\",\n    pretrained=\"\",\n)\n\nsiglip = foz.load_zoo_model(\n    \"open-clip-torch\",\n    clip_model=\"hf-hub:timm/ViT-B-16-SigLIP\",\n    pretrained=\"\",\n)\n</code></pre>"},{"location":"integrations/openclip/#inference","title":"Inference \u00b6","text":"<p>When running inference with OpenCLIP, you can specify a text prompt to help guide the model towards a solution as well as only specify a certain number of classes to output during zero shot classification.</p> <p>Note</p> <p>While OpenCLIP models are typically set to train mode by default, the FiftyOne integration sets the model to eval mode before running inference.</p> <p>For example we can run inference as such:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nmodel = foz.load_zoo_model(\n    \"open-clip-torch\",\n    text_prompt=\"A photo of a\",\n    classes=[\"person\", \"dog\", \"cat\", \"bird\", \"car\", \"tree\", \"chair\"],\n)\n\ndataset.apply_model(model, label_field=\"clip_predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p></p>"},{"location":"integrations/openclip/#embeddings","title":"Embeddings \u00b6","text":"<p>Another application of OpenCLIP is embeddings visualization.</p> <p>For example, let\u2019s compare the embeddings of the original OpenAI CLIP model to MetaCLIP. We\u2019ll also perform a quick zero shot classification to color the embeddings:</p> <pre><code>import fiftyone.brain as fob\n\nmeta_clip = foz.load_zoo_model(\n    \"open-clip-torch\",\n    clip_model=\"ViT-B-32-quickgelu\",\n    pretrained=\"metaclip_400m\",\n    text_prompt=\"A photo of a\",\n)\n\ndataset.apply_model(meta_clip, label_field=\"meta_clip_classification\")\n\nfob.compute_visualization(\n    dataset,\n    model=meta_clip,\n    brain_key=\"meta_clip\",\n)\n\nopenai_clip = foz.load_zoo_model(\n    \"open-clip-torch\",\n    text_prompt=\"A photo of a\",\n)\n\ndataset.apply_model(openai_clip, label_field=\"openai_clip_classifications\")\n\nfob.compute_visualization(\n    dataset,\n    model=openai_clip,\n    brain_key=\"openai_clip\",\n)\n</code></pre> <p>Here is the final result!</p> <p></p>"},{"location":"integrations/openclip/#text-similarity-search","title":"Text similarity search \u00b6","text":"<p>OpenCLIP can also be used for text similarity search.</p> <p>To use a specific pretrained-checkpoint pair for text similarity search, pass these in as a dictionary via the <code>model_kwargs</code> argument to <code>compute_similarity()</code>.</p> <p>For example, for MetaCLIP, we can do the following:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.brain as fob\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nmodel_kwargs = {\n    \"clip_model\": \"ViT-B-32-quickgelu\",\n    \"pretrained\": \"metaclip_400m\",\n    \"text_prompt\": \"A photo of a\",\n}\n\nfob.compute_similarity(\n    dataset,\n    model=\"open-clip-torch\",\n    model_kwargs=model_kwargs,\n    brain_key=\"sim_metaclip\",\n)\n</code></pre> <p>You can then search by text similarity in Python via the <code>sort_by_similarity()</code> stage as follows:</p> <pre><code>query = \"kites flying in the sky\"\n\nview = dataset.sort_by_similarity(query, k=25, brain_key=\"sim_metaclip\")\n</code></pre> <p>Note</p> <p>Did you know? You can also perform text similarity queries directly in the App!</p>"},{"location":"integrations/pinecone/","title":"Pinecone Integration \u00b6","text":"<p>Pinecone is one of the most popular vector search engines available, and we\u2019ve made it easy to use Pinecone\u2019s vector search capabilities on your computer vision data directly from FiftyOne!</p> <p>Follow these simple instructions to configure your credentials and get started using Pinecone + FiftyOne.</p> <p>FiftyOne provides an API to create Pinecone indexes, upload vectors, and run similarity queries, both programmatically in Python and via point-and-click in the App.</p> <p>Note</p> <p>Did you know? You can search by natural language using Pinecone similarity indexes!</p> <p></p>"},{"location":"integrations/pinecone/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The basic workflow to use Pinecone to create a similarity index on your FiftyOne datasets and use this to query your data is as follows:</p> <ol> <li> <p>Load a dataset into FiftyOne</p> </li> <li> <p>Compute embedding vectors for samples or patches in your dataset, or select a model to use to generate embeddings</p> </li> <li> <p>Use the <code>compute_similarity()</code> methodto generate a Pinecone similarity index for the samples or object patches in a dataset by setting the parameter <code>backend=\"pinecone\"</code> and specifying a <code>brain_key</code> of your choice</p> </li> <li> <p>Use this Pinecone similarity index to query your data with <code>sort_by_similarity()</code></p> </li> <li> <p>If desired, delete the index</p> </li> </ol> <p>The example below demonstrates this workflow.</p> <p>Note</p> <p>You must create a Pinecone account, download a Pinecone API key, and install the Pinecone Python client to run this example:</p> <pre><code>pip install -U pinecone-client\n</code></pre> <p>Note that you can store your Pinecone credentials as described in this section to avoid entering them manually each time you interact with your Pinecone index.</p> <p>First let\u2019s load a dataset into FiftyOne and compute embeddings for the samples:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\n# Step 1: Load your data into FiftyOne\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Steps 2 and 3: Compute embeddings and create a similarity index\npinecone_index = fob.compute_similarity(\n    dataset,\n    brain_key=\"pinecone_index\",\n    backend=\"pinecone\",\n)\n</code></pre> <p>Once the similarity index has been generated, we can query our data in FiftyOne by specifying the <code>brain_key</code>:</p> <pre><code># Step 4: Query your data\nquery = dataset.first().id  # query by sample ID\nview = dataset.sort_by_similarity(\n    query,\n    brain_key=brain_key,\n    k=10,  # limit to 10 most similar samples\n)\n\n# Step 5 (optional): Cleanup\n\n# Delete the Pinecone index\npinecone_index = dataset.load_brain_results(brain_key)\npinecone_index.cleanup()\n\n# Delete run record from FiftyOne\ndataset.delete_brain_run(\"pinecone_index\")\n</code></pre> <p>Note</p> <p>Skip to this section to see a variety of common Pinecone query patterns.</p>"},{"location":"integrations/pinecone/#setup","title":"Setup \u00b6","text":"<p>The easiest way to get started with Pinecone is to create a free Pinecone account and copy your Pinecone API key.</p>"},{"location":"integrations/pinecone/#installing-the-pinecone-client","title":"Installing the Pinecone client \u00b6","text":"<p>In order to use the Pinecone backend, you must install the Pinecone Python client:</p> <pre><code>pip install pinecone-client\n</code></pre>"},{"location":"integrations/pinecone/#using-the-pinecone-backend","title":"Using the Pinecone backend \u00b6","text":"<p>By default, calling <code>compute_similarity()</code> or <code>sort_by_similarity()</code> will use an sklearn backend.</p> <p>To use the Pinecone backend, simply set the optional <code>backend</code> parameter of <code>compute_similarity()</code> to <code>\"pinecone\"</code>:</p> <pre><code>import fiftyone.brain as fob\n\nfob.compute_similarity(..., backend=\"pinecone\", ...)\n</code></pre> <p>Alternatively, you can permanently configure FiftyOne to use the Pinecone backend by setting the following environment variable:</p> <pre><code>export FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND=pinecone\n</code></pre> <p>or by setting the <code>default_similarity_backend</code> parameter of your brain config located at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"default_similarity_backend\": \"pinecone\"\n}\n</code></pre>"},{"location":"integrations/pinecone/#authentication","title":"Authentication \u00b6","text":"<p>In order to connect to a Pinecone server, you must provide your credentials, which can be done in a variety of ways.</p> <p>Environment variables (recommended)</p> <p>The recommended way to configure your Pinecone credentials is to store them in the environment variables shown below, which are automatically accessed by FiftyOne whenever a connection to Pinecone is made:</p> <pre><code>export FIFTYONE_BRAIN_SIMILARITY_PINECONE_API_KEY=XXXXXX\n\n# Serverless indexes\nexport FIFTYONE_BRAIN_SIMILARITY_PINECONE_CLOUD=\"aws\"\nexport FIFTYONE_BRAIN_SIMILARITY_PINECONE_REGION=\"us-east-1\"\n\n# Pod-based indexes\nexport FIFTYONE_BRAIN_SIMILARITY_PINECONE_ENVIRONMENT=\"us-east-1-aws\"\n</code></pre> <p>FiftyOne Brain config</p> <p>You can also store your credentials in your brain config located at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"similarity_backends\": {\n        \"pinecone\": {\n            \"api_key\": \"XXXXXXXXXXXX\",\n            \"cloud\": \"aws\",                 # serverless indexes\n            \"region\": \"us-east-1\",          # serverless indexes\n            \"environment\": \"us-east-1-aws\"  # pod-based indexes\n        }\n    }\n}\n</code></pre> <p>Note that this file will not exist until you create it.</p> <p>Keyword arguments</p> <p>You can manually provide your Pinecone credentials as keyword arguments each time you call methods like <code>compute_similarity()</code> that require connections to Pinecone:</p> <pre><code>import fiftyone.brain as fob\n\npinecone_index = fob.compute_similarity(\n    ...\n    backend=\"pinecone\",\n    brain_key=\"pinecone_index\",\n    api_key=\"XXXXXX\",\n    cloud=\"aws\",\n    region=\"us-east-1\",\n)\n</code></pre> <p>Note that, when using this strategy, you must manually provide the credentials when loading an index later via <code>load_brain_results()</code>:</p> <pre><code>pinecone_index = dataset.load_brain_results(\n    \"pinecone_index\",\n    api_key=\"XXXXXX\",\n    cloud=\"aws\",\n    region=\"us-east-1\",\n)\n</code></pre>"},{"location":"integrations/pinecone/#pinecone-config-parameters","title":"Pinecone config parameters \u00b6","text":"<p>The Pinecone backend supports a variety of query parameters that can be used to customize your similarity queries. These parameters include:</p> <ul> <li> <p>index_name ( None): the name of the Pinecone index to use or create. If not specified, a new unique name is generated automatically</p> </li> <li> <p>index_type ( None): the index type to use when creating a new index. The supported values are <code>[\"serverless\", \"pod\"]</code>, and the default is <code>\"serverless\"</code></p> </li> <li> <p>namespace ( None): a namespace under which to store vectors added to the index</p> </li> <li> <p>metric ( \u201ccosine\u201d): the distance/similarity metric to use for the index. Supported values are <code>(\"cosine\", \"dotproduct\", \"euclidean\")</code></p> </li> <li> <p>replicas ( None): an optional number of replicas to use when creating a new pod-based index</p> </li> <li> <p>shards ( None): an optional number of shards to use when creating a new pod-based index</p> </li> <li> <p>pods ( None): an optional number of pods to use when creating a new pod-based index</p> </li> <li> <p>pod_type ( None): an optional pod type to use when creating a new pod-based index</p> </li> </ul> <p>For detailed information on these parameters, see the Pinecone documentation.</p> <p>You can specify these parameters via any of the strategies described in the previous section. Here\u2019s an example of a brain config that configures a serverless index:</p> <pre><code>{\n    \"similarity_backends\": {\n        \"pinecone\": {\n            \"index_name\": \"your-index\",\n            \"index_type\": \"serverless\",\n            \"metric\": \"cosine\",\n        }\n    }\n}\n</code></pre> <p>However, typically these parameters are directly passed to <code>compute_similarity()</code> to configure a specific new index:</p> <pre><code>pinecone_index = fob.compute_similarity(\n    ...\n    backend=\"pinecone\",\n    brain_key=\"pinecone_index\",\n    index_name=\"your-index\",\n    index_type=\"serverless\",\n    metric=\"cosine\",\n)\n</code></pre>"},{"location":"integrations/pinecone/#managing-brain-runs","title":"Managing brain runs \u00b6","text":"<p>FiftyOne provides a variety of methods that you can use to manage brain runs.</p> <p>For example, you can call <code>list_brain_runs()</code> to see the available brain keys on a dataset:</p> <pre><code>import fiftyone.brain as fob\n\n# List all brain runs\ndataset.list_brain_runs()\n\n# Only list similarity runs\ndataset.list_brain_runs(type=fob.Similarity)\n\n# Only list specific similarity runs\ndataset.list_brain_runs(\n    type=fob.Similarity,\n    patches_field=\"ground_truth\",\n    supports_prompts=True,\n)\n</code></pre> <p>Or, you can use <code>get_brain_info()</code> to retrieve information about the configuration of a brain run:</p> <pre><code>info = dataset.get_brain_info(brain_key)\nprint(info)\n</code></pre> <p>Use <code>load_brain_results()</code> to load the <code>SimilarityIndex</code> instance for a brain run.</p> <p>You can use <code>rename_brain_run()</code> to rename the brain key associated with an existing similarity results run:</p> <pre><code>dataset.rename_brain_run(brain_key, new_brain_key)\n</code></pre> <p>Finally, you can use <code>delete_brain_run()</code> to delete the record of a similarity index computation from your FiftyOne dataset:</p> <pre><code>dataset.delete_brain_run(brain_key)\n</code></pre> <p>Note</p> <p>Calling <code>delete_brain_run()</code> only deletes the record of the brain run from your FiftyOne dataset; it will not delete any associated Pinecone index, which you can do as follows:</p> <pre><code># Delete the Pinecone index\npinecone_index = dataset.load_brain_results(brain_key)\npinecone_index.cleanup()\n</code></pre>"},{"location":"integrations/pinecone/#examples","title":"Examples \u00b6","text":"<p>This section demonstrates how to perform some common vector search workflows on a FiftyOne dataset using the Pinecone backend.</p> <p>Note</p> <p>All of the examples below assume you have configured your Pinecone API key as described in this section.</p>"},{"location":"integrations/pinecone/#create-a-similarity-index","title":"Create a similarity index \u00b6","text":"<p>In order to create a new Pinecone similarity index, you need to specify either the <code>embeddings</code> or <code>model</code> argument to <code>compute_similarity()</code>. Here\u2019s a few possibilities:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nmodel_name = \"clip-vit-base32-torch\"\nmodel = foz.load_zoo_model(model_name)\nbrain_key = \"pinecone_index\"\n\n# Option 1: Compute embeddings on the fly from model name\nfob.compute_similarity(\n    dataset,\n    model=model_name,\n    backend=\"pinecone\",\n    brain_key=brain_key,\n)\n\n# Option 2: Compute embeddings on the fly from model instance\nfob.compute_similarity(\n    dataset,\n    model=model,\n    backend=\"pinecone\",\n    brain_key=brain_key,\n)\n\n# Option 3: Pass precomputed embeddings as a numpy array\nembeddings = dataset.compute_embeddings(model)\nfob.compute_similarity(\n    dataset,\n    embeddings=embeddings,\n    backend=\"pinecone\",\n    brain_key=brain_key,\n)\n\n# Option 4: Pass precomputed embeddings by field name\ndataset.compute_embeddings(model, embeddings_field=\"embeddings\")\nfob.compute_similarity(\n    dataset,\n    embeddings=\"embeddings\",\n    backend=\"pinecone\",\n    brain_key=brain_key,\n)\n</code></pre> <p>Note</p> <p>You can customize the Pinecone index by passing any supported parameters as extra kwargs.</p>"},{"location":"integrations/pinecone/#create-a-patch-similarity-index","title":"Create a patch similarity index \u00b6","text":"<p>You can also create a similarity index for object patches within your dataset by specifying a <code>patches_field</code> argument to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    patches_field=\"ground_truth\",\n    model=\"clip-vit-base32-torch\",\n    backend=\"pinecone\",\n    brain_key=\"pinecone_patches\",\n)\n</code></pre> <p>Note</p> <p>You can customize the Pinecone index by passing any supported parameters as extra kwargs.</p>"},{"location":"integrations/pinecone/#connect-to-an-existing-index","title":"Connect to an existing index \u00b6","text":"<p>If you have already created a Pinecone index storing the embedding vectors for the samples or patches in your dataset, you can connect to it by passing the <code>index_name</code> to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",      # zoo model used (if applicable)\n    embeddings=False,                   # don't compute embeddings\n    index_name=\"your-index\",            # the existing Pinecone index\n    brain_key=\"pinecone_index\",\n    backend=\"pinecone\",\n)\n</code></pre>"},{"location":"integrations/pinecone/#addremove-embeddings-from-an-index","title":"Add/remove embeddings from an index \u00b6","text":"<p>You can use <code>add_to_index()</code> and <code>remove_from_index()</code> to add and remove embeddings from an existing Pinecone index.</p> <p>These methods can come in handy if you modify your FiftyOne dataset and need to update the Pinecone index to reflect these changes:</p> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\npinecone_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"pinecone_index\",\n    backend=\"pinecone\",\n)\nprint(pinecone_index.total_index_size)  # 200\n\nview = dataset.take(10)\nids = view.values(\"id\")\n\n# Delete 10 samples from a dataset\ndataset.delete_samples(view)\n\n# Delete the corresponding vectors from the index\npinecone_index.remove_from_index(sample_ids=ids)\n\n# Add 20 samples to a dataset\nsamples = [fo.Sample(filepath=\"tmp%d.jpg\" % i) for i in range(20)]\nsample_ids = dataset.add_samples(samples)\n\n# Add corresponding embeddings to the index\nembeddings = np.random.rand(20, 512)\npinecone_index.add_to_index(embeddings, sample_ids)\n\nprint(pinecone_index.total_index_size)  # 210\n</code></pre>"},{"location":"integrations/pinecone/#retrieve-embeddings-from-an-index","title":"Retrieve embeddings from an index \u00b6","text":"<p>You can use <code>get_embeddings()</code> to retrieve embeddings from a Pinecone index by ID:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\npinecone_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"pinecone_index\",\n    backend=\"pinecone\",\n)\n\n# Retrieve embeddings for the entire dataset\nids = dataset.values(\"id\")\nembeddings, sample_ids, _ = pinecone_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (200, 512)\nprint(sample_ids.shape)  # (200,)\n\n# Retrieve embeddings for a view\nids = dataset.take(10).values(\"id\")\nembeddings, sample_ids, _ = pinecone_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (10, 512)\nprint(sample_ids.shape)  # (10,)\n</code></pre>"},{"location":"integrations/pinecone/#querying-a-pinecone-index","title":"Querying a Pinecone index \u00b6","text":"<p>You can query a Pinecone index by appending a <code>sort_by_similarity()</code> stage to any dataset or view. The query can be any of the following:</p> <ul> <li> <p>An ID (sample or patch)</p> </li> <li> <p>A query vector of same dimension as the index</p> </li> <li> <p>A list of IDs (samples or patches)</p> </li> <li> <p>A text prompt (if supported by the model)</p> </li> </ul> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"pinecone_index\",\n    backend=\"pinecone\",\n)\n\n# Query by vector\nquery = np.random.rand(512)  # matches the dimension of CLIP embeddings\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"pinecone_index\")\n\n# Query by sample ID\nquery = dataset.first().id\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"pinecone_index\")\n\n# Query by a list of IDs\nquery = [dataset.first().id, dataset.last().id]\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"pinecone_index\")\n\n# Query by text prompt\nquery = \"a photo of a dog\"\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"pinecone_index\")\n</code></pre> <p>Note</p> <p>Performing a similarity search on a <code>DatasetView</code> will only return results from the view; if the view contains samples that were not included in the index, they will never be included in the result.</p> <p>This means that you can index an entire <code>Dataset</code> once and then perform searches on subsets of the dataset by constructing views that contain the images of interest.</p>"},{"location":"integrations/pinecone/#accessing-the-pinecone-client","title":"Accessing the Pinecone client \u00b6","text":"<p>You can use the <code>index</code> property of a Pinecone index to directly access the underlying Pinecone client instance and use its methods as desired:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\npinecone_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"pinecone_index\",\n    backend=\"pinecone\",\n)\n\nprint(pinecone_index.index)\n</code></pre>"},{"location":"integrations/pinecone/#advanced-usage","title":"Advanced usage \u00b6","text":"<p>As previously mentioned, you can customize your Pinecone indexes by providing optional parameters to <code>compute_similarity()</code>.</p> <p>Here\u2019s an example of creating a similarity index backed by a customized Pinecone index. Just for fun, we\u2019ll specify a custom index name, use dot product similarity, and populate the index for only a subset of our dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Create a custom Pinecone index\npinecone_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    embeddings=False,  # we'll add embeddings below\n    metric=\"dotproduct\",\n    brain_key=\"pinecone_index\",\n    backend=\"pinecone\",\n    index_name=\"custom-pinecone-index\",\n)\n\n# Add embeddings for a subset of the dataset\nview = dataset.take(10)\nembeddings, sample_ids, _ = pinecone_index.compute_embeddings(view)\npinecone_index.add_to_index(embeddings, sample_ids)\n\nprint(pinecone_index.index)\n</code></pre>"},{"location":"integrations/pytorch_hub/","title":"PyTorch Hub Integration \u00b6","text":"<p>FiftyOne integrates natively with PyTorch Hub, so you can load any Hub model and run inference on your FiftyOne datasets with just a few lines of code!</p>"},{"location":"integrations/pytorch_hub/#loading-a-model","title":"Loading a model \u00b6","text":""},{"location":"integrations/pytorch_hub/#image-models","title":"Image models \u00b6","text":"<p>You can use the builtin <code>load_torch_hub_image_model()</code> utility to load models from the PyTorch Hub:</p> <pre><code>import fiftyone.utils.torch as fout\n\nmodel = fout.load_torch_hub_image_model(\n    \"pytorch/vision\",\n    \"resnet18\",\n    hub_kwargs=dict(weights=\"ResNet18_Weights.DEFAULT\"),\n)\n</code></pre> <p>The function returns a <code>TorchImageModel</code> instance that wraps the raw Torch model in FiftyOne\u2019s Model interface, which means that you can directly pass the model to builtin methods like <code>apply_model()</code>, <code>compute_embeddings()</code>, <code>compute_patch_embeddings()</code>, <code>compute_visualization()</code>, and <code>compute_similarity()</code>.</p> <pre><code>import fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\ndataset.limit(10).apply_model(model, label_field=\"resnet18\")\n\n# Logits\nprint(dataset.first().resnet18.shape)  # (1000,)\n</code></pre> <p>Note</p> <p>In the above example, the <code>resnet18</code> field is populated with raw logits. Refer to this page to see how to configure output processors to automatically parse model outputs into FiftyOne label types.</p>"},{"location":"integrations/pytorch_hub/#utilities","title":"Utilities \u00b6","text":"<p>FiftyOne also provides lower-level utilities for direct access to information about PyTorch Hub models:</p> <pre><code>import fiftyone.utils.torch as fout\n\n# Load a raw Hub model\nmodel = fout.load_torch_hub_raw_model(\n    \"facebookresearch/dinov2\",\n    \"dinov2_vits14\",\n)\nprint(type(model))\n# &lt;class 'dinov2.models.vision_transformer.DinoVisionTransformer'&gt;\n\n# Locate the `requirements.txt` for the model on disk\nreq_path = fout.find_torch_hub_requirements(\"facebookresearch/dinov2\")\nprint(req_path)\n# '~/.cache/torch/hub/facebookresearch_dinov2_main/requirements.txt'\n\n# Load the package requirements for the model\nrequirements = fout.load_torch_hub_requirements(\"facebookresearch/dinov2\")\nprint(requirements)\n# ['torch==2.0.0', 'torchvision==0.15.0', ...]\n</code></pre>"},{"location":"integrations/pytorch_hub/#example-yolov5","title":"Example: YOLOv5 \u00b6","text":"<p>Here\u2019s how to load Ultralytics YOLOv5 and use it to generate object detections:</p> <pre><code>from PIL import Image\nimport numpy as np\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.utils.torch as fout\n\nclass YOLOv5OutputProcessor(fout.OutputProcessor):\n    \"\"\"Transforms ``ultralytics/yolov5`` outputs to FiftyOne format.\"\"\"\n\n    def __call__(self, result, frame_size, confidence_thresh=None):\n        batch = []\n        for df in result.pandas().xywhn:\n            if confidence_thresh is not None:\n                df = df[df[\"confidence\"] &gt;= confidence_thresh]\n\n            batch.append(self._to_detections(df))\n\n        return batch\n\n    def _to_detections(self, df):\n        return fo.Detections(\n            detections=[\\\n                fo.Detection(\\\n                    label=row.name,\\\n                    bounding_box=[\\\n                        row.xcenter - 0.5 * row.width,\\\n                        row.ycenter - 0.5 * row.height,\\\n                        row.width,\\\n                        row.height,\\\n                    ],\\\n                    confidence=row.confidence,\\\n                )\\\n                for row in df.itertuples()\\\n            ]\n        )\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nmodel = fout.load_torch_hub_image_model(\n    \"ultralytics/yolov5\",\n    \"yolov5s\",\n    hub_kwargs=dict(pretrained=True),\n    output_processor=YOLOv5OutputProcessor(),\n    raw_inputs=True,\n)\n\n# Generate predictions for a single image\nimg = np.asarray(Image.open(dataset.first().filepath))\npredictions = model.predict(img)\nprint(predictions)  # &lt;Detections: {...}&gt;\n\n# Generate predictions for all images in a collection\ndataset.limit(10).apply_model(model, label_field=\"yolov5\")\ndataset.count(\"yolov5.detections\")  # 26\n</code></pre> <p>Note</p> <p>Did you know? Ultralytics YOLOv5 is natively available in the FiftyOne Model Zoo. You should also check out the Ultralytics integration!</p>"},{"location":"integrations/pytorch_hub/#example-dinov2","title":"Example: DINOv2 \u00b6","text":"<p>Here\u2019s how to load DINOv2 and use it to compute embeddings:</p> <pre><code>from PIL import Image\nimport numpy as np\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.utils.torch as fout\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nmodel = fout.load_torch_hub_image_model(\n    \"facebookresearch/dinov2\",\n    \"dinov2_vits14\",\n    image_patch_size=14,\n    embeddings_layer=\"head\",\n)\nassert model.has_embeddings\n\n# Embed a single image\nimg = np.asarray(Image.open(dataset.first().filepath))\nembedding = model.embed(img)\nprint(embedding.shape)  # (384,)\n\n# Embed all images in a collection\nembeddings = dataset.limit(10).compute_embeddings(model)\nprint(embeddings.shape)  # (10, 384)\n</code></pre> <p>Note</p> <p>Did you know? DINOv2 is natively available in the FiftyOne Model Zoo!</p>"},{"location":"integrations/pytorch_hub/#adding-hub-models-to-your-local-zoo","title":"Adding Hub models to your local zoo \u00b6","text":"<p>You can add PyTorch Hub models to your local model zoo and then load and use them via the <code>fiftyone.zoo</code> package and the CLI using the same syntax that you would with the publicly available models:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = fo.load_dataset(\"...\")\nmodel = foz.load_zoo_model(\"your-custom-model\")\n\ndataset.apply_model(model, ...)\ndataset.compute_embeddings(model, ...)\n</code></pre>"},{"location":"integrations/pytorch_hub/#example-dinov2_1","title":"Example: DINOv2 \u00b6","text":"<p>Here\u2019s how to add DINOv2 to your local model zoo and then load it to compute embeddings.</p> <ol> <li>Create a custom manifest file and add DINOv2 to it:</li> </ol> <pre><code>{\n    \"models\": [\\\n        {\\\n            \"base_name\": \"dinov2-vits14\",\\\n            \"description\": \"DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-S/14 distilled\",\\\n            \"source\": \"https://github.com/facebookresearch/dinov2\",\\\n            \"default_deployment_config_dict\": {\\\n                \"type\": \"fiftyone.utils.torch.TorchImageModel\",\\\n                \"config\": {\\\n                    \"entrypoint_fcn\": \"fiftyone.utils.torch.load_torch_hub_raw_model\",\\\n                    \"entrypoint_args\": {\\\n                        \"repo_or_dir\": \"facebookresearch/dinov2\",\\\n                        \"model\": \"dinov2_vits14\"\\\n                    },\\\n                    \"image_patch_size\": 14,\\\n                    \"embeddings_layer\": \"head\"\\\n                }\\\n            }\\\n        }\\\n    ]\n}\n</code></pre> <ol> <li>Expose your manifest to FiftyOne by setting this environment variable:</li> </ol> <pre><code>export FIFTYONE_MODEL_ZOO_MANIFEST_PATHS=/path/to/custom-manifest.json\n</code></pre> <ol> <li>Now you can load and use the model using <code>load_zoo_model()</code>:</li> </ol> <pre><code>import numpy as np\nfrom PIL import Image\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nmodel = foz.load_zoo_model(\"dinov2-vits14\")\nassert model.has_embeddings\n\n# Embed a single image\nimg = np.asarray(Image.open(dataset.first().filepath))\nembedding = model.embed(img)\nprint(embedding.shape)  # (384,)\n\n# Embed all images in a collection\nembeddings = dataset.limit(10).compute_embeddings(model)\nprint(embeddings.shape)  # (10, 384)\n</code></pre>"},{"location":"integrations/qdrant/","title":"Qdrant Integration \u00b6","text":"<p>Qdrant is one of the most popular vector search engines available, and we\u2019ve made it easy to use Qdrant\u2019s vector search capabilities on your computer vision data directly from FiftyOne!</p> <p>Follow these simple instructions to configure your Qdrant server and get started using Qdrant + FiftyOne.</p> <p>FiftyOne provides an API to create Qdrant collections, upload vectors, and run similarity queries, both programmatically in Python and via point-and-click in the App.</p> <p>Note</p> <p>Did you know? You can search by natural language using Qdrant similarity indexes!</p> <p></p>"},{"location":"integrations/qdrant/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The basic workflow to use Qdrant to create a similarity index on your FiftyOne datasets and use this to query your data is as follows:</p> <ol> <li> <p>Start a Qdrant service locally</p> </li> <li> <p>Load a dataset into FiftyOne</p> </li> <li> <p>Compute embedding vectors for samples or patches in your dataset, or select a model to use to generate embeddings</p> </li> <li> <p>Use the <code>compute_similarity()</code> method to generate a Qdrant similarity index for the samples or object patches in a dataset by setting the parameter <code>backend=\"qdrant\"</code> and specifying a <code>brain_key</code> of your choice</p> </li> <li> <p>Use this Qdrant similarity index to query your data with <code>sort_by_similarity()</code></p> </li> <li> <p>If desired, delete the index</p> </li> </ol> <p>The example below demonstrates this workflow.</p> <p>Note</p> <p>You must launch a Qdrant server and install the Qdrant Python client to run this example:</p> <pre><code>docker pull qdrant/qdrant\ndocker run -p 6333:6333 qdrant/qdrant\n\npip install qdrant-client\n</code></pre> <p>Note that, if you are using a custom Qdrant server, you can store your credentials as described in this section to avoid entering them manually each time you interact with your Qdrant index.</p> <p>First let\u2019s load a dataset into FiftyOne and compute embeddings for the samples:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\n# Step 1: Load your data into FiftyOne\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Steps 2 and 3: Compute embeddings and create a similarity index\nqdrant_index = fob.compute_similarity(\n    dataset,\n    brain_key=\"qdrant_index\",\n    backend=\"qdrant\",\n)\n</code></pre> <p>Once the similarity index has been generated, we can query our data in FiftyOne by specifying the <code>brain_key</code>:</p> <pre><code># Step 4: Query your data\nquery = dataset.first().id  # query by sample ID\nview = dataset.sort_by_similarity(\n    query,\n    brain_key=\"qdrant_index\",\n    k=10,  # limit to 10 most similar samples\n)\n\n# Step 5 (optional): Cleanup\n\n# Delete the Qdrant collection\nqdrant_index.cleanup()\n\n# Delete run record from FiftyOne\ndataset.delete_brain_run(\"qdrant_index\")\n</code></pre> <p>Note</p> <p>Skip to this section for a variety of common Qdrant query patterns.</p>"},{"location":"integrations/qdrant/#setup","title":"Setup \u00b6","text":"<p>The easiest way to get started with Qdrant is to install locally via Docker:</p> <pre><code>docker pull qdrant/qdrant\ndocker run -p 6333:6333 qdrant/qdrant\n</code></pre>"},{"location":"integrations/qdrant/#installing-the-qdrant-client","title":"Installing the Qdrant client \u00b6","text":"<p>In order to use the Qdrant backend, you must also install the Qdrant Python client:</p> <pre><code>pip install qdrant-client\n</code></pre>"},{"location":"integrations/qdrant/#using-the-qdrant-backend","title":"Using the Qdrant backend \u00b6","text":"<p>By default, calling <code>compute_similarity()</code> or <code>sort_by_similarity()</code> will use an sklearn backend.</p> <p>To use the Qdrant backend, simply set the optional <code>backend</code> parameter of <code>compute_similarity()</code> to <code>\"qdrant\"</code>:</p> <pre><code>import fiftyone.brain as fob\n\nfob.compute_similarity(..., backend=\"qdrant\", ...)\n</code></pre> <p>Alternatively, you can permanently configure FiftyOne to use the Qdrant backend by setting the following environment variable:</p> <pre><code>export FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND=qdrant\n</code></pre> <p>or by setting the <code>default_similarity_backend</code> parameter of your brain config located at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"default_similarity_backend\": \"qdrant\"\n}\n</code></pre>"},{"location":"integrations/qdrant/#authentication","title":"Authentication \u00b6","text":"<p>If you are using a custom Qdrant server, you can provide your credentials in a variety of ways.</p> <p>Environment variables (recommended)</p> <p>The recommended way to configure your Qdrant credentials is to store them in the environment variables shown below, which are automatically accessed by FiftyOne whenever a connection to Qdrant is made.</p> <pre><code>export FIFTYONE_BRAIN_SIMILARITY_QDRANT_URL=localhost:6333\nexport FIFTYONE_BRAIN_SIMILARITY_QDRANT_API_KEY=XXXXXXXX\nexport FIFTYONE_BRAIN_SIMILARITY_QDRANT_GRPC_PORT=6334\nexport FIFTYONE_BRAIN_SIMILARITY_QDRANT_PREFER_GRPC=false\n</code></pre> <p>The <code>API_KEY</code>, <code>GRPC_PORT</code>, and <code>PREFER_GRPC</code> environment variables are optional.</p> <p>FiftyOne Brain config</p> <p>You can also store your credentials in your brain config located at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"similarity_backends\": {\n        \"qdrant\": {\n            \"url\": \"http://localhost:6333\",\n            \"api_key\": \"XXXXXXXX\",\n            \"grpc_port\": 6334,\n            \"prefer_grpc\": false\n        }\n    }\n}\n</code></pre> <p>Note that this file will not exist until you create it.</p> <p>Keyword arguments</p> <p>You can manually provide credentials as keyword arguments each time you call methods like <code>compute_similarity()</code> that require connections to Qdrant:</p> <pre><code>import fiftyone.brain as fob\n\nqdrant_index = fob.compute_similarity(\n    ...\n    backend=\"qdrant\",\n    brain_key=\"qdrant_index\",\n    url=\"http://localhost:6333\",\n    api_key=\"XXXXXXXX\",\n    grpc_port=6334,\n    prefer_grpc=False\n)\n</code></pre> <p>Note that, when using this strategy, you must manually provide the credentials when loading an index later via <code>load_brain_results()</code>:</p> <pre><code>qdrant_index = dataset.load_brain_results(\n    \"qdrant_index\",\n    url=\"http://localhost:6333\",\n    api_key=\"XXXXXXXX\",\n    grpc_port=6334,\n    prefer_grpc=False\n)\n</code></pre>"},{"location":"integrations/qdrant/#qdrant-config-parameters","title":"Qdrant config parameters \u00b6","text":"<p>The Qdrant backend supports a variety of query parameters that can be used to customize your similarity queries. These parameters broadly fall into four categories:</p> <ol> <li> <p>Basic vector database parameters</p> </li> <li> <p>Hierarchical navigable small world (HNSW) parameters</p> </li> <li> <p>Write-ahead-log (WAL) parameters</p> </li> <li> <p>Performance/optimizers parameters</p> </li> </ol> <p>For detailed information on these parameters, see the Qdrant documentation.</p> <p>You can specify these parameters via any of the strategies described in the previous section. Here\u2019s an example of a brain config that includes all of the available parameters:</p> <pre><code>{\n    \"similarity_backends\": {\n        \"qdrant\": {\n            \"metric\": \"cosine\",\n            \"replication_factor\": null,\n            \"shard_number\": null,\n            \"write_consistency_factor\": null,\n            \"hnsw_config\": {\n                \"m\": 16,\n                \"ef_construct\": 100,\n                \"full_scan_threshold\": 10000,\n                \"max_indexing_threads\": null,\n                \"on_disk\": null,\n                \"payload_m\": null\n            },\n            \"optimizers_config\": {\n                \"deleted_threshold\": 0.2,\n                \"vacuum_min_vector_number\": 1000,\n                \"default_segment_number\": 0,\n                \"max_segment_size\": null,\n                \"memmap_threshold\": null,\n                \"indexing_threshold\": 20000,\n                \"flush_interval_sec\": 5,\n                \"max_optimization_threads\": 1\n            },\n            \"wal_config\": {\n                \"wal_capacity_mb\": 32,\n                \"wal_segments_ahead\": 0\n            }\n        }\n    }\n}\n</code></pre> <p>However, typically these parameters are directly passed to <code>compute_similarity()</code> to configure a specific new index:</p> <pre><code>qdrant_index = fob.compute_similarity(\n    ...\n    backend=\"qdrant\",\n    brain_key=\"qdrant_index\",\n    collection_name=\"your-collection-name\",\n    metric=\"cosine\",\n    replication_factor=1,\n)\n</code></pre>"},{"location":"integrations/qdrant/#managing-brain-runs","title":"Managing brain runs \u00b6","text":"<p>FiftyOne provides a variety of methods that you can use to manage brain runs.</p> <p>For example, you can call <code>list_brain_runs()</code> to see the available brain keys on a dataset:</p> <pre><code>import fiftyone.brain as fob\n\n# List all brain runs\ndataset.list_brain_runs()\n\n# Only list similarity runs\ndataset.list_brain_runs(type=fob.Similarity)\n\n# Only list specific similarity runs\ndataset.list_brain_runs(\n    type=fob.Similarity,\n    patches_field=\"ground_truth\",\n    supports_prompts=True,\n)\n</code></pre> <p>Or, you can use <code>get_brain_info()</code> to retrieve information about the configuration of a brain run:</p> <pre><code>info = dataset.get_brain_info(brain_key)\nprint(info)\n</code></pre> <p>Use <code>load_brain_results()</code> to load the <code>SimilarityIndex</code> instance for a brain run.</p> <p>You can use <code>rename_brain_run()</code> to rename the brain key associated with an existing similarity results run:</p> <pre><code>dataset.rename_brain_run(brain_key, new_brain_key)\n</code></pre> <p>Finally, you can use <code>delete_brain_run()</code> to delete the record of a similarity index computation from your FiftyOne dataset:</p> <pre><code>dataset.delete_brain_run(brain_key)\n</code></pre> <p>Note</p> <p>Calling <code>delete_brain_run()</code> only deletes the record of the brain run from your FiftyOne dataset; it will not delete any associated Qdrant collection, which you can do as follows:</p> <pre><code># Delete the Qdrant collection\nqdrant_index = dataset.load_brain_results(brain_key)\nqdrant_index.cleanup()\n</code></pre>"},{"location":"integrations/qdrant/#examples","title":"Examples \u00b6","text":"<p>This section demonstrates how to perform some common vector search workflows on a FiftyOne dataset using the Qdrant backend.</p> <p>Note</p> <p>All of the examples below assume you have configured your Qdrant server as described in this section.</p>"},{"location":"integrations/qdrant/#create-a-similarity-index","title":"Create a similarity index \u00b6","text":"<p>In order to create a new Qdrant similarity index, you need to specify either the <code>embeddings</code> or <code>model</code> argument to <code>compute_similarity()</code>. Here\u2019s a few possibilities:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nmodel_name = \"clip-vit-base32-torch\"\nmodel = foz.load_zoo_model(model_name)\nbrain_key = \"qdrant_index\"\n\n# Option 1: Compute embeddings on the fly from model name\nfob.compute_similarity(\n    dataset,\n    model=model_name,\n    backend=\"qdrant\",\n    brain_key=brain_key,\n)\n\n# Option 2: Compute embeddings on the fly from model instance\nfob.compute_similarity(\n    dataset,\n    model=model,\n    backend=\"qdrant\",\n    brain_key=brain_key,\n)\n\n# Option 3: Pass precomputed embeddings as a numpy array\nembeddings = dataset.compute_embeddings(model)\nfob.compute_similarity(\n    dataset,\n    embeddings=embeddings,\n    backend=\"qdrant\",\n    brain_key=brain_key,\n)\n\n# Option 4: Pass precomputed embeddings by field name\ndataset.compute_embeddings(model, embeddings_field=\"embeddings\")\nfob.compute_similarity(\n    dataset,\n    embeddings=\"embeddings\",\n    backend=\"qdrant\",\n    brain_key=brain_key,\n)\n</code></pre> <p>Note</p> <p>You can customize the Qdrant collection by passing any supported parameters as extra kwargs.</p>"},{"location":"integrations/qdrant/#create-a-patch-similarity-index","title":"Create a patch similarity index \u00b6","text":"<p>You can also create a similarity index for object patches within your dataset by including the <code>patches_field</code> argument to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    patches_field=\"ground_truth\",\n    model=\"clip-vit-base32-torch\",\n    backend=\"qdrant\",\n    brain_key=\"qdrant_patches\",\n)\n</code></pre> <p>Note</p> <p>You can customize the Qdrant collection by passing any supported parameters as extra kwargs.</p>"},{"location":"integrations/qdrant/#connect-to-an-existing-index","title":"Connect to an existing index \u00b6","text":"<p>If you have already created a Qdrant collection storing the embedding vectors for the samples or patches in your dataset, you can connect to it by passing the <code>collection_name</code> to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",      # zoo model used (if applicable)\n    embeddings=False,                   # don't compute embeddings\n    collection_name=\"your-collection\",  # the existing Qdrant collection\n    brain_key=\"qdrant_index\",\n    backend=\"qdrant\",\n)\n</code></pre>"},{"location":"integrations/qdrant/#addremove-embeddings-from-an-index","title":"Add/remove embeddings from an index \u00b6","text":"<p>You can use <code>add_to_index()</code> and <code>remove_from_index()</code> to add and remove embeddings from an existing Qdrant index.</p> <p>These methods can come in handy if you modify your FiftyOne dataset and need to update the Qdrant index to reflect these changes:</p> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nqdrant_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"qdrant_index\",\n    backend=\"qdrant\",\n)\nprint(qdrant_index.total_index_size)  # 200\n\nview = dataset.take(10)\nids = view.values(\"id\")\n\n# Delete 10 samples from a dataset\ndataset.delete_samples(view)\n\n# Delete the corresponding vectors from the index\nqdrant_index.remove_from_index(sample_ids=ids)\n\n# Add 20 samples to a dataset\nsamples = [fo.Sample(filepath=\"tmp%d.jpg\" % i) for i in range(20)]\nsample_ids = dataset.add_samples(samples)\n\n# Add corresponding embeddings to the index\nembeddings = np.random.rand(20, 512)\nqdrant_index.add_to_index(embeddings, sample_ids)\n\nprint(qdrant_index.total_index_size)  # 210\n</code></pre>"},{"location":"integrations/qdrant/#retrieve-embeddings-from-an-index","title":"Retrieve embeddings from an index \u00b6","text":"<p>You can use <code>get_embeddings()</code> to retrieve embeddings from a Qdrant index by ID:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nqdrant_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"qdrant_index\",\n    backend=\"qdrant\",\n)\n\n# Retrieve embeddings for the entire dataset\nids = dataset.values(\"id\")\nembeddings, sample_ids, _ = qdrant_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (200, 512)\nprint(sample_ids.shape)  # (200,)\n\n# Retrieve embeddings for a view\nids = dataset.take(10).values(\"id\")\nembeddings, sample_ids, _ = qdrant_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (10, 512)\nprint(sample_ids.shape)  # (10,)\n</code></pre>"},{"location":"integrations/qdrant/#querying-a-qdrant-index","title":"Querying a Qdrant index \u00b6","text":"<p>You can query a Qdrant index by appending a <code>sort_by_similarity()</code> stage to any dataset or view. The query can be any of the following:</p> <ul> <li> <p>An ID (sample or patch)</p> </li> <li> <p>A query vector of same dimension as the index</p> </li> <li> <p>A list of IDs (samples or patches)</p> </li> <li> <p>A text prompt (if supported by the model)</p> </li> </ul> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"qdrant_index\",\n    backend=\"qdrant\",\n)\n\n# Query by vector\nquery = np.random.rand(512)  # matches the dimension of CLIP embeddings\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"qdrant_index\")\n\n# Query by sample ID\nquery = dataset.first().id\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"qdrant_index\")\n\n# Query by a list of IDs\nquery = [dataset.first().id, dataset.last().id]\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"qdrant_index\")\n\n# Query by text prompt\nquery = \"a photo of a dog\"\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"qdrant_index\")\n</code></pre> <p>Note</p> <p>Performing a similarity search on a <code>DatasetView</code> will only return results from the view; if the view contains samples that were not included in the index, they will never be included in the result.</p> <p>This means that you can index an entire <code>Dataset</code> once and then perform searches on subsets of the dataset by constructing views that contain the images of interest.</p>"},{"location":"integrations/qdrant/#accessing-the-qdrant-client","title":"Accessing the Qdrant client \u00b6","text":"<p>You can use the <code>client</code> property of a Qdrant index to directly access the underlying Qdrant client instance and use its methods as desired:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nqdrant_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"qdrant_index\",\n    backend=\"qdrant\",\n)\n\nqdrant_client = qdrant_index.client\nprint(qdrant_client)\nprint(qdrant_client.get_collections())\n</code></pre>"},{"location":"integrations/qdrant/#advanced-usage","title":"Advanced usage \u00b6","text":"<p>As previously mentioned, you can customize your Qdrant collections by providing optional parameters to <code>compute_similarity()</code>.</p> <p>In particular, the <code>hnsw_config</code>, <code>wal_config</code>, and <code>optimizers_config</code> parameters may impact the quality of your query results, as well as the time and memory required to perform approximate nearest neighbor searches. Additionally, you can specify parameters like <code>replication_factor</code> and <code>shard_number</code> to further tune performance.</p> <p>Here\u2019s an example of creating a similarity index backed by a customized Qdrant collection. Just for fun, we\u2019ll specify a custom collection name, use dot product similarity, and populate the index for only a subset of our dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Create a custom Qdrant index\nqdrant_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    embeddings=False,  # we'll add embeddings below\n    metric=\"dotproduct\",\n    brain_key=\"qdrant_index\",\n    backend=\"qdrant\",\n    collection_name=\"custom-quickstart-index\",\n    replication_factor=2,\n    shard_number=2,\n)\n\n# Add embeddings for a subset of the dataset\nview = dataset.take(10)\nembeddings, sample_ids, _ = qdrant_index.compute_embeddings(view)\nqdrant_index.add_to_index(embeddings, sample_ids)\n\nqdrant_client = qdrant_index.client\nprint(qdrant_client.get_collections())\n</code></pre>"},{"location":"integrations/redis/","title":"Redis Vector Search Integration \u00b6","text":"<p>Redis is the leading open source in-memory data store, and we\u2019ve made it easy to use Redis\u2019 vector search capabilities on your computer vision data directly from FiftyOne!</p> <p>Follow these simple instructions to configure a Redis server and get started using Redis + FiftyOne.</p> <p>FiftyOne provides an API to create Redis vector search indexes, upload vectors, and run similarity queries, both programmatically in Python and via point-and-click in the App.</p> <p>Note</p> <p>Did you know? You can search by natural language using Redis similarity indexes!</p> <p></p>"},{"location":"integrations/redis/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The basic workflow to use Redis to create a similarity index on your FiftyOne datasets and use this to query your data is as follows:</p> <ol> <li> <p>Start a Redis service locally</p> </li> <li> <p>Load a dataset into FiftyOne</p> </li> <li> <p>Compute embedding vectors for samples or patches in your dataset, or select a model to use to generate embeddings</p> </li> <li> <p>Use the <code>compute_similarity()</code> method to generate a Redis similarity index for the samples or object patches in a dataset by setting the parameter <code>backend=\"redis\"</code> and specifying a <code>brain_key</code> of your choice</p> </li> <li> <p>Use this Redis similarity index to query your data with <code>sort_by_similarity()</code></p> </li> <li> <p>If desired, delete the index</p> </li> </ol> <p>The example below demonstrates this workflow.</p> <p>Note</p> <p>You must launch a Redis server and install the Redis Python client to run this example:</p> <pre><code>brew tap redis-stack/redis-stack\nbrew install redis-stack\nredis-stack-server\n\npip install redis\n</code></pre> <p>Note that, if you are using a custom Redis server, you can store your credentials as described in this section to avoid entering them manually each time you interact with your Redis index.</p> <p>First let\u2019s load a dataset into FiftyOne and compute embeddings for the samples:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\n# Step 1: Load your data into FiftyOne\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Steps 2 and 3: Compute embeddings and create a similarity index\nredis_index = fob.compute_similarity(\n    dataset,\n    brain_key=\"redis_index\",\n    backend=\"redis\",\n)\n</code></pre> <p>Once the similarity index has been generated, we can query our data in FiftyOne by specifying the <code>brain_key</code>:</p> <pre><code># Step 4: Query your data\nquery = dataset.first().id  # query by sample ID\nview = dataset.sort_by_similarity(\n    query,\n    brain_key=\"redis_index\",\n    k=10,  # limit to 10 most similar samples\n)\n\n# Step 5 (optional): Cleanup\n\n# Delete the Redis vector search index\nredis_index.cleanup()\n\n# Delete run record from FiftyOne\ndataset.delete_brain_run(\"redis_index\")\n</code></pre> <p>Note</p> <p>Skip to this section for a variety of common Redis query patterns.</p>"},{"location":"integrations/redis/#setup","title":"Setup \u00b6","text":"<p>The easiest way to get started with Redis is to install Redis Stack:</p> <pre><code>brew tap redis-stack/redis-stack\nbrew install redis-stack\nredis-stack-server\n</code></pre>"},{"location":"integrations/redis/#installing-the-redis-client","title":"Installing the Redis client \u00b6","text":"<p>In order to use the Redis backend, you must also install the Redis Python client:</p> <pre><code>pip install redis\n</code></pre>"},{"location":"integrations/redis/#using-the-redis-backend","title":"Using the Redis backend \u00b6","text":"<p>By default, calling <code>compute_similarity()</code> or <code>sort_by_similarity()</code> will use an sklearn backend.</p> <p>To use the Redis backend, simply set the optional <code>backend</code> parameter of <code>compute_similarity()</code> to <code>\"redis\"</code>:</p> <pre><code>import fiftyone.brain as fob\n\nfob.compute_similarity(..., backend=\"redis\", ...)\n</code></pre> <p>Alternatively, you can permanently configure FiftyOne to use the Redis backend by setting the following environment variable:</p> <pre><code>export FIFTYONE_BRAIN_DEFAULT_SIMILARITY_BACKEND=redis\n</code></pre> <p>or by setting the <code>default_similarity_backend</code> parameter of your brain config located at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"default_similarity_backend\": \"redis\"\n}\n</code></pre>"},{"location":"integrations/redis/#authentication","title":"Authentication \u00b6","text":"<p>If you are using a custom Redis server, you can provide your credentials in a variety of ways.</p> <p>Environment variables (recommended)</p> <p>The recommended way to configure your Redis credentials is to store them in the environment variables shown below, which are automatically accessed by FiftyOne whenever a connection to Redis is made.</p> <pre><code>export FIFTYONE_BRAIN_SIMILARITY_REDIS_HOST=localhost\nexport FIFTYONE_BRAIN_SIMILARITY_REDIS_PORT=6379\nexport FIFTYONE_BRAIN_SIMILARITY_REDIS_DB=0\nexport FIFTYONE_BRAIN_SIMILARITY_REDIS_USERNAME=username\nexport FIFTYONE_BRAIN_SIMILARITY_REDIS_PASSWORD=password\n</code></pre> <p>FiftyOne Brain config</p> <p>You can also store your credentials in your brain config located at <code>~/.fiftyone/brain_config.json</code>:</p> <pre><code>{\n    \"similarity_backends\": {\n        \"redis\": {\n            \"host\": \"localhost\",\n            \"port\": 6379,\n            \"db\": 0,\n            \"username\": \"username\",\n            \"password\": \"password\"\n        }\n    }\n}\n</code></pre> <p>Note that this file will not exist until you create it.</p> <p>Keyword arguments</p> <p>You can manually provide credentials as keyword arguments each time you call methods like <code>compute_similarity()</code> that require connections to Redis:</p> <pre><code>import fiftyone.brain as fob\n\nredis_index = fob.compute_similarity(\n    ...\n    backend=\"redis\",\n    brain_key=\"redis_index\",\n    host=\"localhost\",\n    port=6379,\n    db=0,\n    username=\"username\",\n    password=\"password\",\n)\n</code></pre> <p>Note that, when using this strategy, you must manually provide the credentials when loading an index later via <code>load_brain_results()</code>:</p> <pre><code>redis_index = dataset.load_brain_results(\n    \"redis_index\",\n    host=\"localhost\",\n    port=6379,\n    db=0,\n    username=\"username\",\n    password=\"password\",\n)\n</code></pre>"},{"location":"integrations/redis/#redis-config-parameters","title":"Redis config parameters \u00b6","text":"<p>The Redis backend supports a variety of query parameters that can be used to customize your similarity queries. These parameters include:</p> <ul> <li> <p>index_name ( None): the name of the Redis vector search index to use or create. If not specified, a new unique name is generated automatically</p> </li> <li> <p>metric ( \u201ccosine\u201d): the distance/similarity metric to use when creating a new index. The supported values are <code>(\"cosine\", \"dotproduct\", \"euclidean\")</code></p> </li> <li> <p>algorithm ( \u201cFLAT\u201d): the search algorithm to use. The supported values are <code>(\"FLAT\", \"HNSW\")</code></p> </li> </ul> <p>For detailed information on these parameters, see the Redis documentation.</p> <p>You can specify these parameters via any of the strategies described in the previous section. Here\u2019s an example of a brain config that includes all of the available parameters:</p> <pre><code>{\n    \"similarity_backends\": {\n        \"redis\": {\n            \"index_name\": \"your-index\",\n            \"metric\": \"cosine\",\n            \"algorithm\": \"FLAT\"\n        }\n    }\n}\n</code></pre> <p>However, typically these parameters are directly passed to <code>compute_similarity()</code> to configure a specific new index:</p> <pre><code>redis_index = fob.compute_similarity(\n    ...\n    backend=\"redis\",\n    brain_key=\"redis_index\",\n    index_name=\"your-index\",\n    metric=\"cosine\",\n    algorithm=\"FLAT\",\n)\n</code></pre>"},{"location":"integrations/redis/#managing-brain-runs","title":"Managing brain runs \u00b6","text":"<p>FiftyOne provides a variety of methods that you can use to manage brain runs.</p> <p>For example, you can call <code>list_brain_runs()</code> to see the available brain keys on a dataset:</p> <pre><code>import fiftyone.brain as fob\n\n# List all brain runs\ndataset.list_brain_runs()\n\n# Only list similarity runs\ndataset.list_brain_runs(type=fob.Similarity)\n\n# Only list specific similarity runs\ndataset.list_brain_runs(\n    type=fob.Similarity,\n    patches_field=\"ground_truth\",\n    supports_prompts=True,\n)\n</code></pre> <p>Or, you can use <code>get_brain_info()</code> to retrieve information about the configuration of a brain run:</p> <pre><code>info = dataset.get_brain_info(brain_key)\nprint(info)\n</code></pre> <p>Use <code>load_brain_results()</code> to load the <code>SimilarityIndex</code> instance for a brain run.</p> <p>You can use <code>rename_brain_run()</code> to rename the brain key associated with an existing similarity results run:</p> <pre><code>dataset.rename_brain_run(brain_key, new_brain_key)\n</code></pre> <p>Finally, you can use <code>delete_brain_run()</code> to delete the record of a similarity index computation from your FiftyOne dataset:</p> <pre><code>dataset.delete_brain_run(brain_key)\n</code></pre> <p>Note</p> <p>Calling <code>delete_brain_run()</code> only deletes the record of the brain run from your FiftyOne dataset; it will not delete any associated Redis index, which you can do as follows:</p> <pre><code># Delete the Redis vector search index\nredis_index = dataset.load_brain_results(brain_key)\nredis_index.cleanup()\n</code></pre>"},{"location":"integrations/redis/#examples","title":"Examples \u00b6","text":"<p>This section demonstrates how to perform some common vector search workflows on a FiftyOne dataset using the Redis backend.</p> <p>Note</p> <p>All of the examples below assume you have configured your Redis server as described in this section.</p>"},{"location":"integrations/redis/#create-a-similarity-index","title":"Create a similarity index \u00b6","text":"<p>In order to create a new Redis similarity index, you need to specify either the <code>embeddings</code> or <code>model</code> argument to <code>compute_similarity()</code>. Here\u2019s a few possibilities:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nmodel_name = \"clip-vit-base32-torch\"\nmodel = foz.load_zoo_model(model_name)\nbrain_key = \"redis_index\"\n\n# Option 1: Compute embeddings on the fly from model name\nfob.compute_similarity(\n    dataset,\n    model=model_name,\n    backend=\"redis\",\n    brain_key=brain_key,\n)\n\n# Option 2: Compute embeddings on the fly from model instance\nfob.compute_similarity(\n    dataset,\n    model=model,\n    backend=\"redis\",\n    brain_key=brain_key,\n)\n\n# Option 3: Pass precomputed embeddings as a numpy array\nembeddings = dataset.compute_embeddings(model)\nfob.compute_similarity(\n    dataset,\n    embeddings=embeddings,\n    backend=\"redis\",\n    brain_key=brain_key,\n)\n\n# Option 4: Pass precomputed embeddings by field name\ndataset.compute_embeddings(model, embeddings_field=\"embeddings\")\nfob.compute_similarity(\n    dataset,\n    embeddings=\"embeddings\",\n    backend=\"redis\",\n    brain_key=brain_key,\n)\n</code></pre> <p>Note</p> <p>You can customize the Redis index by passing any supported parameters as extra kwargs.</p>"},{"location":"integrations/redis/#create-a-patch-similarity-index","title":"Create a patch similarity index \u00b6","text":"<p>You can also create a similarity index for object patches within your dataset by including the <code>patches_field</code> argument to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    patches_field=\"ground_truth\",\n    model=\"clip-vit-base32-torch\",\n    backend=\"redis\",\n    brain_key=\"redis_patches\",\n)\n</code></pre> <p>Note</p> <p>You can customize the Redis index by passing any supported parameters as extra kwargs.</p>"},{"location":"integrations/redis/#connect-to-an-existing-index","title":"Connect to an existing index \u00b6","text":"<p>If you have already created a Redis index storing the embedding vectors for the samples or patches in your dataset, you can connect to it by passing the <code>index_name</code> to <code>compute_similarity()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",      # zoo model used (if applicable)\n    embeddings=False,                   # don't compute embeddings\n    index_name=\"your-index\",            # the existing Redis index\n    brain_key=\"redis_index\",\n    backend=\"redis\",\n)\n</code></pre>"},{"location":"integrations/redis/#addremove-embeddings-from-an-index","title":"Add/remove embeddings from an index \u00b6","text":"<p>You can use <code>add_to_index()</code> and <code>remove_from_index()</code> to add and remove embeddings from an existing Redis index.</p> <p>These methods can come in handy if you modify your FiftyOne dataset and need to update the Redis index to reflect these changes:</p> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nredis_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"redis_index\",\n    backend=\"redis\",\n)\nprint(redis_index.total_index_size)  # 200\n\nview = dataset.take(10)\nids = view.values(\"id\")\n\n# Delete 10 samples from a dataset\ndataset.delete_samples(view)\n\n# Delete the corresponding vectors from the index\nredis_index.remove_from_index(sample_ids=ids)\n\n# Add 20 samples to a dataset\nsamples = [fo.Sample(filepath=\"tmp%d.jpg\" % i) for i in range(20)]\nsample_ids = dataset.add_samples(samples)\n\n# Add corresponding embeddings to the index\nembeddings = np.random.rand(20, 512)\nredis_index.add_to_index(embeddings, sample_ids)\n\nprint(redis_index.total_index_size)  # 210\n</code></pre>"},{"location":"integrations/redis/#retrieve-embeddings-from-an-index","title":"Retrieve embeddings from an index \u00b6","text":"<p>You can use <code>get_embeddings()</code> to retrieve embeddings from a Redis index by ID:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nredis_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"redis_index\",\n    backend=\"redis\",\n)\n\n# Retrieve embeddings for the entire dataset\nids = dataset.values(\"id\")\nembeddings, sample_ids, _ = redis_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (200, 512)\nprint(sample_ids.shape)  # (200,)\n\n# Retrieve embeddings for a view\nids = dataset.take(10).values(\"id\")\nembeddings, sample_ids, _ = redis_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (10, 512)\nprint(sample_ids.shape)  # (10,)\n</code></pre>"},{"location":"integrations/redis/#querying-a-redis-index","title":"Querying a Redis index \u00b6","text":"<p>You can query a Redis index by appending a <code>sort_by_similarity()</code> stage to any dataset or view. The query can be any of the following:</p> <ul> <li> <p>An ID (sample or patch)</p> </li> <li> <p>A query vector of same dimension as the index</p> </li> <li> <p>A list of IDs (samples or patches)</p> </li> <li> <p>A text prompt (if supported by the model)</p> </li> </ul> <pre><code>import numpy as np\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"redis_index\",\n    backend=\"redis\",\n)\n\n# Query by vector\nquery = np.random.rand(512)  # matches the dimension of CLIP embeddings\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"redis_index\")\n\n# Query by sample ID\nquery = dataset.first().id\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"redis_index\")\n\n# Query by a list of IDs\nquery = [dataset.first().id, dataset.last().id]\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"redis_index\")\n\n# Query by text prompt\nquery = \"a photo of a dog\"\nview = dataset.sort_by_similarity(query, k=10, brain_key=\"redis_index\")\n</code></pre> <p>Note</p> <p>Performing a similarity search on a <code>DatasetView</code> will only return results from the view; if the view contains samples that were not included in the index, they will never be included in the result.</p> <p>This means that you can index an entire <code>Dataset</code> once and then perform searches on subsets of the dataset by constructing views that contain the images of interest.</p>"},{"location":"integrations/redis/#accessing-the-redis-client","title":"Accessing the Redis client \u00b6","text":"<p>You can use the <code>client</code> property of a Redis index to directly access the underlying Redis client instance and use its methods as desired:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nredis_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"redis_index\",\n    backend=\"redis\",\n)\n\nredis_client = redis_index.client\nindex_name = redis_index.config.index_name\nprint(redis_client)\nprint(redis_client.ft(index_name).info())\n</code></pre>"},{"location":"integrations/redis/#advanced-usage","title":"Advanced usage \u00b6","text":"<p>As previously mentioned, you can customize your Redis index by providing optional parameters to <code>compute_similarity()</code>.</p> <p>In particular, the <code>algorithm</code> parameter may impact the quality of your query results, as well as the time and memory required to perform approximate nearest neighbor searches.</p> <p>Here\u2019s an example of creating a similarity index backed by a customized Redis index. Just for fun, we\u2019ll specify a custom index name, use dot product similarity, and populate the index for only a subset of our dataset:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Create a custom Redis index\nredis_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    embeddings=False,  # we'll add embeddings below\n    brain_key=\"redis_index\",\n    backend=\"redis\",\n    index_name=\"custom-quickstart-index\",\n    metric=\"dotproduct\",\n    algorithm=\"HNSW\",\n)\n\n# Add embeddings for a subset of the dataset\nview = dataset.take(10)\nembeddings, sample_ids, _ = redis_index.compute_embeddings(view)\nredis_index.add_to_index(embeddings, sample_ids)\n\nredis_client = redis_index.client\nindex_name = redis_index.config.index_name\nprint(redis_client.ft(index_name).info())\n</code></pre>"},{"location":"integrations/super_gradients/","title":"Super Gradients Integration \u00b6","text":"<p>FiftyOne integrates natively with Deci AI\u2019s SuperGradients library, so you can run inference with YOLO-NAS architectures on your FiftyOne datasets with just a few lines of code!</p>"},{"location":"integrations/super_gradients/#setup","title":"Setup \u00b6","text":"<p>To get started with SuperGradients, just install the <code>super-gradients</code> package:</p> <pre><code>pip install super-gradients\n</code></pre>"},{"location":"integrations/super_gradients/#inference","title":"Inference \u00b6","text":"<p>You can directly pass SuperGradients YOLO-NAS models to your FiftyOne dataset\u2019s <code>apply_model()</code> method:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\nfrom super_gradients.training import models\n\ndataset = foz.load_zoo_dataset(\"quickstart\", max_samples=25)\ndataset.select_fields().keep_fields()\n\nmodel = models.get(\"yolo_nas_m\", pretrained_weights=\"coco\")\n# model = models.get(\"yolo_nas_l\", pretrained_weights=\"coco\")\n# model = models.get(\"yolo_nas_s\", pretrained_weights=\"coco\")\n\ndataset.apply_model(model, label_field=\"yolo_nas\", confidence_thresh=0.7)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"integrations/super_gradients/#model-zoo","title":"Model zoo \u00b6","text":"<p>SuperGradients YOLO-NAS is also available directly from the FiftyOne Model Zoo!</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\nmodel = foz.load_zoo_model(\"yolo-nas-torch\")\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\ndataset.apply_model(model, label_field=\"yolo_nas\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"integrations/ultralytics/","title":"Ultralytics Integration \u00b6","text":"<p>FiftyOne integrates natively with Ultralytics, so you can load, fine-tune, and run inference with your favorite Ultralytics models on your FiftyOne datasets with just a few lines of code!</p>"},{"location":"integrations/ultralytics/#setup","title":"Setup \u00b6","text":"<p>To get started with Ultralytics, just install the following packages:</p> <pre><code>pip install \"ultralytics&gt;=8.1.0\" \"torch&gt;=1.8\"\n</code></pre>"},{"location":"integrations/ultralytics/#inference","title":"Inference \u00b6","text":"<p>The examples below show how to run inference with various Ultralytics models on the following sample dataset:</p> <pre><code># Suppress Ultralytics logging\nimport os; os.environ[\"YOLO_VERBOSE\"] = \"False\"\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.utils.ultralytics as fou\n\nfrom ultralytics import YOLO\n\n# Load an example dataset\ndataset = foz.load_zoo_dataset(\"quickstart\", max_samples=25)\ndataset.select_fields().keep_fields()\n</code></pre>"},{"location":"integrations/ultralytics/#image-classification","title":"Image classification \u00b6","text":"<p>You can directly pass Ultralytics <code>YOLO</code> classification models to <code>apply_model()</code>:</p> <pre><code># YOLOv8\nmodel = YOLO(\"yolov8n-cls.pt\")\n# model = YOLO(\"yolov8s-cls.pt\")\n# model = YOLO(\"yolov8m-cls.pt\")\n# model = YOLO(\"yolov8l-cls.pt\")\n# model = YOLO(\"yolov8x-cls.pt\")\n\ndataset.apply_model(model, label_field=\"classif\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"integrations/ultralytics/#object-detection","title":"Object detection \u00b6","text":"<p>You can directly pass Ultralytics <code>YOLO</code> or <code>RTDETR</code> detection models to <code>apply_model()</code>:</p> <pre><code># YOLOv8\nmodel = YOLO(\"yolov8s.pt\")\n# model = YOLO(\"yolov8m.pt\")\n# model = YOLO(\"yolov8l.pt\")\n# model = YOLO(\"yolov8x.pt\")\n\n# YOLOv5\n# model = YOLO(\"yolov5s.pt\")\n# model = YOLO(\"yolov5m.pt\")\n# model = YOLO(\"yolov5l.pt\")\n# model = YOLO(\"yolov5x.pt\")\n\n# YOLOv9\n# model = YOLO(\"yolov9c.pt\")\n# model = YOLO(\"yolov9e.pt\")\n\n# YOLOv10\n# model = YOLO(\"yolov10n.pt)\n# model = YOLO(\"yolov10s.pt)\n# model = YOLO(\"yolov10m.pt)\n# model = YOLO(\"yolov10l.pt)\n# model = YOLO(\"yolov10x.pt)\n\n# YOLOv11\n# model = YOLO(\"yolo11n.pt)\n# model = YOLO(\"yolo11s.pt)\n# model = YOLO(\"yolo11m.pt)\n# model = YOLO(\"yolo11l.pt)\n# model = YOLO(\"yolo11x.pt)\n\n# RTDETR\n# model = YOLO(\"rtdetr-l.pt\")\n# model = YOLO(\"rtdetr-x.pt\")\n\ndataset.apply_model(model, label_field=\"boxes\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Alternatively, you can use the <code>to_detections()</code> utility to manually convert Ultralytics predictions to FiftyOne format:</p> <pre><code>for sample in dataset.iter_samples(progress=True):\n    result = model(sample.filepath)[0]\n    sample[\"boxes\"] = fou.to_detections(result)\n    sample.save()\n</code></pre> <p></p> <p>You can also load any of these models directly from the FiftyOne Model Zoo:</p> <pre><code>model_name = \"yolov5l-coco-torch\"\n# model_name = \"yolov8m-coco-torch\"\n# model_name = \"yolov9e-coco-torch\"\n# model_name = \"yolov10s-coco-torch\"\n# model_name = \"yolo11x-coco-torch\"\n# model_name = \"rtdetr-l-coco-torch\"\n\nmodel = foz.load_zoo_model(\n    model_name,\n    label_field=\"boxes\",\n    confidence_thresh=0.5,\n    iou_thresh=0.5,\n)\n\ndataset.apply_model(model)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>You can use <code>list_zoo_models()</code> to see all available YOLO models that are compatible with Ultralytics or SuperGradients:</p> <pre><code>print(foz.list_zoo_models(tags=\"yolo\"))\n</code></pre> <p>In general, YOLO model names will contain \u201cyolov\u201d, followed by the version number, then the model size (\u201cn\u201d, \u201cs\u201d, \u201cm\u201d, \u201cl\u201d, or \u201cx\u201d), and an indicator of the label classes (\u201ccoco\u201d for MS COCO or \u201cworld\u201d for open-world), followed by \u201ctorch\u201d.</p>"},{"location":"integrations/ultralytics/#instance-segmentation","title":"Instance segmentation \u00b6","text":"<p>You can directly pass Ultralytics YOLO segmentation models to <code>apply_model()</code>:</p> <pre><code>model = YOLO(\"yolov8s-seg.pt\")\n# model = YOLO(\"yolov8m-seg.pt\")\n# model = YOLO(\"yolov8l-seg.pt\")\n# model = YOLO(\"yolov8x-seg.pt\")\n\n# model = YOLO(\"yolo11s-seg.pt\")\n# model = YOLO(\"yolo11m-seg.pt\")\n# model = YOLO(\"yolo11l-seg.pt\")\n# model = YOLO(\"yolo11x-seg.pt\")\n\ndataset.apply_model(model, label_field=\"instances\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Alternatively, you can use the <code>to_instances()</code> and <code>to_polylines()</code> utilities to manually convert Ultralytics predictions into the desired FiftyOne format:</p> <pre><code>for sample in dataset.iter_samples(progress=True):\n    result = model(sample.filepath)[0]\n    sample[\"detections\"] = fou.to_detections(result)\n    sample[\"instances\"] = fou.to_instances(result)\n    sample[\"polylines\"] = fou.to_polylines(result)\n    sample.save()\n</code></pre> <p></p> <p>You can also load YOLOv8, YOLOv9, and YOLO11 segmentation models from the FiftyOne Model Zoo:</p> <pre><code>model_name = \"yolov8n-seg-coco-torch\"\n# model_name = \"yolov8s-seg-coco-torch\"\n# model_name = \"yolov8m-seg-coco-torch\"\n# model_name = \"yolov8l-seg-coco-torch\"\n# model_name = \"yolov8x-seg-coco-torch\"\n\n# model_name = \"yolov9c-seg-coco-torch\"\n# model_name = \"yolov9e-seg-coco-torch\"\n\n# model_name = \"yolo11n-seg-coco-torch\"\n# model_name = \"yolo11s-seg-coco-torch\"\n# model_name = \"yolo11m-seg-coco-torch\"\n# model_name = \"yolo11l-seg-coco-torch\"\n# model_name = \"yolo11x-seg-coco-torch\"\n\nmodel = foz.load_zoo_model(model_name, label_field=\"yolo_seg\")\n\ndataset.apply_model(model)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"integrations/ultralytics/#keypoints","title":"Keypoints \u00b6","text":"<p>You can directly pass Ultralytics YOLO pose models to <code>apply_model()</code>:</p> <pre><code>model = YOLO(\"yolov8s-pose.pt\")\n# model = YOLO(\"yolov8m-pose.pt\")\n# model = YOLO(\"yolov8l-pose.pt\")\n# model = YOLO(\"yolov8x-pose.pt\")\n\ndataset.apply_model(model, label_field=\"keypoints\")\n\n# Store the COCO-pose keypoint skeleton so the App can render it\ndataset.default_skeleton = fo.KeypointSkeleton(\n    labels=[\\\n        \"nose\", \"left eye\", \"right eye\", \"left ear\", \"right ear\",\\\n        \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\",\\\n        \"left wrist\", \"right wrist\", \"left hip\", \"right hip\",\\\n        \"left knee\", \"right knee\", \"left ankle\", \"right ankle\",\\\n    ],\n    edges=[\\\n        [11, 5, 3, 1, 0, 2, 4, 6, 12],\\\n        [9, 7, 5, 6, 8, 10],\\\n        [15, 13, 11, 12, 14, 16],\\\n    ],\n)\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Alternatively, you can use the <code>to_keypoints()</code> utility to manually convert Ultralytics predictions to FiftyOne format:</p> <pre><code>for sample in dataset.iter_samples(progress=True):\n    result = model(sample.filepath)[0]\n    sample[\"keypoints\"] = fou.to_keypoints(result)\n    sample.save()\n</code></pre> <p></p>"},{"location":"integrations/ultralytics/#oriented-bounding-boxes","title":"Oriented bounding boxes \u00b6","text":"<p>You can directly pass Ultralytics YOLO oriented bounding box models to <code>apply_model()</code>:</p> <pre><code>model = YOLO(\"yolov8n-obb.pt\")\n# model = YOLO(\"yolov8s-obb.pt\")\n# model = YOLO(\"yolov8m-obb.pt\")\n# model = YOLO(\"yolov8l-obb.pt\")\n# model = YOLO(\"yolov8x-obb.pt\")\n\ndataset.apply_model(model, label_field=\"oriented_boxes\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>You can also load YOLOv8 oriented bounding box models from the FiftyOne Model Zoo:</p> <pre><code>model_name = \"yolov8n-obb-dotav1-torch\"\n# model_name = \"yolov8s-obb-dotav1-torch\"\n# model_name = \"yolov8m-obb-dotav1-torch\"\n# model_name = \"yolov8l-obb-dotav1-torch\"\n# model_name = \"yolov8x-obb-dotav1-torch\"\n\nmodel = foz.load_zoo_model(model_name)\n\ndataset.apply_model(model, label_field=\"oriented_boxes\")\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>The oriented bounding box models are trained on the DOTA dataset, which consists of drone images with oriented bounding boxes. The models are trained to predict on bird\u2019s eye view images, so applying them to regular images may not yield good results.</p>"},{"location":"integrations/ultralytics/#open-vocabulary-detection","title":"Open vocabulary detection \u00b6","text":"<p>FiftyOne\u2019s Ultralytics integration also supports real-time open vocabulary object detection via YOLO World.</p> <p>The usage syntax is the same as for regular object detection, with the caveat that you can set the classes that the model should detect:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\nfrom ultralytics import YOLO\n\n## Load dataset\ndataset = foz.load_zoo_dataset(\n    \"voc-2007\", split=\"validation\", max_samples=100\n)\ndataset.select_fields().keep_fields()\n\n## Load model\nmodel = YOLO(\"yolov8l-world.pt\")\n# model = YOLO(\"yolov8s-world.pt\")\n# model =  YOLO(\"yolov8m-world.pt\")\n# model =  YOLO(\"yolov8x-world.pt\")\n\n## Set open vocabulary classes\nmodel.set_classes(\n    [\"plant\", \"window\", \"keyboard\", \"human baby\", \"computer monitor\"]\n)\n\nlabel_field = \"yolo_world_detections\"\n\n## Apply model\ndataset.apply_model(model, label_field=label_field)\n\n## Visualize the detection patches\npatches = dataset.to_patches(label_field)\nsession = fo.launch_app(patches)\n</code></pre> <p></p> <p>You can also load these open-vocabulary models from the FiftyOne Model Zoo, optionally specifying the classes that the model should detect:</p> <pre><code>model_name = \"yolov8l-world-torch\"\n# model_name = \"yolov8m-world-torch\"\n# model_name = \"yolov8x-world-torch\"\n\nmodel = foz.load_zoo_model(\n    model_name,\n    classes=[\"plant\", \"window\", \"keyboard\", \"human baby\", \"computer monitor\"],\n)\n\ndataset.apply_model(model, label_field=\"yolo_world_detections\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"integrations/ultralytics/#batch-inference","title":"Batch inference \u00b6","text":"<p>When using <code>apply_model()</code>, you can request batch inference by passing the optional <code>batch_size</code> parameter:</p> <pre><code>dataset.apply_model(model, label_field=\"predictions\", batch_size=16)\n</code></pre> <p>The manual inference loops can be also executed using batch inference via the pattern below:</p> <pre><code>from fiftyone.core.utils import iter_batches\n\nfilepaths = dataset.values(\"filepath\")\nbatch_size = 16\n\npredictions = []\nfor paths in iter_batches(filepaths, batch_size):\n    results = model(paths)\n    predictions.extend(fou.to_detections(results))\n\ndataset.set_values(\"predictions\", predictions)\n</code></pre> <p>Note</p> <p>See this section for more information about performing batch updates to your FiftyOne datasets.</p>"},{"location":"integrations/ultralytics/#training","title":"Training \u00b6","text":"<p>You can use FiftyOne\u2019s builtin YOLOv5 exporter to export your FiftyOne datasets for use with Ultralytics models.</p> <p>For example, the code below prepares a random subset of the Open Images v7 dataset for fine-tuning:</p> <pre><code>import fiftyone as fo\nimport fiftyone.utils.ultralytics as fou\nimport fiftyone.zoo as foz\n\n# The path to export the dataset\nEXPORT_DIR = \"/tmp/oiv7-yolo\"\n\n# Prepare train split\n\ntrain = foz.load_zoo_dataset(\n    \"open-images-v7\",\n    split=\"train\",\n    label_types=[\"detections\"],\n    max_samples=100,\n)\n\n# YOLO format requires a common classes list\nclasses = train.default_classes\n\ntrain.export(\n    export_dir=EXPORT_DIR,\n    dataset_type=fo.types.YOLOv5Dataset,\n    label_field=\"ground_truth\",\n    split=\"train\",\n    classes=classes,\n)\n\n# Prepare validation split\n\nvalidation = foz.load_zoo_dataset(\n    \"open-images-v7\",\n    split=\"validation\",\n    label_types=[\"detections\"],\n    max_samples=10,\n)\n\nvalidation.export(\n    export_dir=EXPORT_DIR,\n    dataset_type=fo.types.YOLOv5Dataset,\n    label_field=\"ground_truth\",\n    split=\"val\",  # Ultralytics uses 'val'\n    classes=classes,\n)\n</code></pre> <p>From here, training an Ultralytics model is as simple as passing the path to the dataset YAML file:</p> <pre><code>from ultralytics import YOLO\n\n# The path to the `dataset.yaml` file we created above\nYAML_FILE = \"/tmp/oiv7-yolo/dataset.yaml\"\n\n# Load a model\nmodel = YOLO(\"yolov8s.pt\")  # load a pretrained model\n# model = YOLO(\"yolov8s.yaml\")  # build a model from scratch\n\n# Train the model\nmodel.train(data=YAML_FILE, epochs=3)\n\n# Evaluate model on the validation set\nmetrics = model.val()\n\n# Export the model\npath = model.export(format=\"onnx\")\n</code></pre>"},{"location":"integrations/v7/","title":"V7 Integration \u00b6","text":"<p>V7 is one of the leading image and video annotation tools available, and we\u2019ve made it easy to upload your data directly from FiftyOne to V7 for labeling.</p> <p>Create a V7 account and follow these simple setup instructions to get up and running.</p> <p>Note</p> <p>Did you know? You can request, manage, and import annotations from within the FiftyOne App by installing the @voxel51/annotation plugin!</p> <p>FiftyOne provides an API to upload data, define label schemas, and download annotations using V7, all programmatically in Python. All of the following label types are supported, for both image and video datasets:</p> <ul> <li> <p>Classifications</p> </li> <li> <p>Detections</p> </li> <li> <p>Polygons</p> </li> <li> <p>Keypoints</p> </li> </ul> <p></p>"},{"location":"integrations/v7/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>The basic workflow to use V7 to add or edit labels on your FiftyOne datasets is as follows:</p> <ol> <li> <p>Load a labeled or unlabeled dataset into FiftyOne</p> </li> <li> <p>Explore the dataset using the App or dataset views to locate either unlabeled samples that you wish to annotate or labeled samples whose annotations you want to edit</p> </li> <li> <p>Use the <code>annotate()</code> method on your dataset or view to upload the samples and optionally their existing labels to V7 by setting the parameter <code>backend=\"darwin\"</code></p> </li> <li> <p>In V7, perform the necessary annotation work</p> </li> <li> <p>Back in FiftyOne, load your dataset and use the <code>load_annotations()</code> method to merge the annotations back into your FiftyOne dataset</p> </li> <li> <p>If desired, delete the V7 project and the record of the annotation run from your FiftyOne dataset</p> </li> </ol> <p>The example below demonstrates this workflow.</p> <p>Note</p> <p>You must create an account at https://www.v7labs.com/sign-up and follow the simple setup instructions in this section in order to run this example.</p> <p>First, we create the annotation tasks in V7:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\n# Step 1: Load your data into FiftyOne\n\ndataset = foz.load_zoo_dataset(\n    \"quickstart\", dataset_name=\"v7-annotation-example\"\n)\ndataset.persistent = True\n\ndataset.evaluate_detections(\n    \"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\"\n)\n\n# Step 2: Locate a subset of your data requiring annotation\n\n# Create a view that contains only high confidence false positive model\n# predictions, with samples containing the most false positives first\nmost_fp_view = (\n    dataset\n    .filter_labels(\"predictions\", (F(\"confidence\") &gt; 0.8) &amp; (F(\"eval\") == \"fp\"))\n    .sort_by(F(\"predictions.detections\").length(), reverse=True)\n)\n\n# Retrieve the sample with the most high confidence false positives\nsample_id = most_fp_view.first().id\nview = dataset.select(sample_id)\n\n# Step 3: Send samples to V7\n\n# A unique identifier for this run\nanno_key = \"v7_basic_recipe\"\n\nlabel_schema = {\n    \"new_ground_truth\": {\n        \"type\": \"detections\",\n        \"classes\": dataset.distinct(\"ground_truth.detections.label\"),\n    },\n}\n\nview.annotate(\n    anno_key,\n    backend=\"darwin\",\n    label_schema=label_schema,\n    launch_editor=True,\n    dataset_slug=anno_key,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Step 4: Perform annotation in V7 and save the tasks\n</code></pre> <p>Then, once the annotation work is complete, we merge the annotations back into FiftyOne:</p> <pre><code>import fiftyone as fo\n\nanno_key = \"v7_basic_recipe\"\n\n# Step 5: Merge annotations back into FiftyOne dataset\n\ndataset = fo.load_dataset(\"v7-annotation-example\")\ndataset.load_annotations(anno_key)\n\n# Load the view that was annotated in the App\nview = dataset.load_annotation_view(anno_key)\nsession = fo.launch_app(view=view)\n\n# Step 6: Cleanup\n\n# Delete tasks from V7\nresults = dataset.load_annotation_results(anno_key)\nresults.cleanup()\n\n# Delete run record (not the labels) from FiftyOne\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>Note</p> <p>See this section to see a variety of common V7 annotation patterns.</p>"},{"location":"integrations/v7/#setup","title":"Setup \u00b6","text":"<p>You can get started with V7 by creating an account and downloading an API key.</p>"},{"location":"integrations/v7/#installing-the-v7-backend","title":"Installing the V7 backend \u00b6","text":"<p>In order to use the V7 backend, you must install the <code>darwin_fiftyone</code> Python package:</p> <pre><code>pip install darwin_fiftyone\n</code></pre> <p>and register the <code>darwin</code> backend with FiftyOne, which you can do either by setting the following environment variables:</p> <pre><code>export FIFTYONE_ANNOTATION_BACKENDS=*,darwin\nexport FIFTYONE_DARWIN_CONFIG_CLS=darwin_fiftyone.DarwinBackendConfig\nexport FIFTYONE_DARWIN_API_KEY=XXXXXXXXX\n</code></pre> <p>or by adding the following parameters to your annotation config located at <code>~/.fiftyone/annotation_config.json</code>:</p> <pre><code>{\n    \"backends\": {\n        \"darwin\": {\n            \"config_cls\": \"darwin_fiftyone.DarwinBackendConfig\",\n            \"api_key\": \"XXXXXXXXX\"\n        }\n    }\n}\n</code></pre> <p>Note that this file may not exist if you haven\u2019t previously customized your annotation backends.</p>"},{"location":"integrations/v7/#using-the-v7-backend","title":"Using the V7 backend \u00b6","text":"<p>By default, calling <code>annotate()</code> will use the CVAT backend.</p> <p>To use the V7 backend, simply set the optional <code>backend</code> parameter of <code>annotate()</code> to <code>\"darwin\"</code>:</p> <pre><code>view.annotate(anno_key, backend=\"darwin\", ...)\n</code></pre> <p>Alternatively, you can permanently configure FiftyOne to use the V7 backend by setting the <code>FIFTYONE_ANNOTATION_DEFAULT_BACKEND</code> environment variable:</p> <pre><code>export FIFTYONE_ANNOTATION_DEFAULT_BACKEND=darwin\n</code></pre> <p>or by setting the <code>default_backend</code> parameter of your annotation config located at <code>~/.fiftyone/annotation_config.json</code>:</p> <pre><code>{\n    \"default_backend\": \"darwin\"\n}\n</code></pre>"},{"location":"integrations/v7/#authentication","title":"Authentication \u00b6","text":"<p>In order to connect to V7, you must provide your API key, which can be done in a variety of ways.</p> <p>Environment variables (recommended)</p> <p>The recommended way to configure your V7 API key is to store it in the <code>FIFTYONE_DARWIN_API_KEY</code> environment variable. This is automatically accessed by FiftyOne whenever a connection to V7 is made.</p> <pre><code>export FIFTYONE_DARWIN_API_KEY=...\n</code></pre> <p>FiftyOne annotation config</p> <p>You can also store your credentials in your annotation config located at <code>~/.fiftyone/annotation_config.json</code>:</p> <pre><code>{\n    \"backends\": {\n        \"darwin\": {\n            \"api_key\": ...,\n        }\n    }\n}\n</code></pre> <p>Note that this file will not exist until you create it.</p> <p>Keyword arguments</p> <p>You can manually provide your API key as a keyword argument each time you call methods like <code>annotate()</code> and <code>load_annotations()</code> that require connections to V7:</p> <pre><code>view.annotate(\n    anno_key,\n    backend=\"darwin\",\n    label_field=\"ground_truth\",\n    dataset_slug=anno_key,\n    api_key=...,\n)\n</code></pre>"},{"location":"integrations/v7/#requesting-annotations","title":"Requesting annotations \u00b6","text":"<p>Use the <code>annotate()</code> method to send the samples and optionally existing labels in a <code>Dataset</code> or <code>DatasetView</code> to V7 for annotation.</p> <p>The basic syntax is:</p> <pre><code>anno_key = \"...\"\nview.annotate(anno_key, backend=\"darwin\", ...)\n</code></pre> <p>The <code>anno_key</code> argument defines a unique identifier for the annotation run, and you will provide it to methods like <code>load_annotations()</code>, <code>get_annotation_info()</code>, <code>load_annotation_results()</code>, <code>rename_annotation_run()</code>, and <code>delete_annotation_run()</code> to manage the run in the future.</p> <p>Note</p> <p>Calling <code>annotate()</code> will upload the source media files to the V7 server.</p> <p>In addition, <code>annotate()</code> provides various parameters that you can use to customize the annotation tasks that you wish to be performed.</p> <p>The following parameters are supported by all annotation backends:</p> <ul> <li> <p>backend ( None): the annotation backend to use. Use <code>\"darwin\"</code> for the V7 backend. The supported values are <code>fiftyone.annotation_config.backends.keys()</code> and the default is <code>fiftyone.annotation_config.default_backend</code></p> </li> <li> <p>media_field ( \u201cfilepath\u201d): the sample field containing the path to the source media to upload</p> </li> <li> <p>launch_editor ( False): whether to launch the annotation backend\u2019s editor after uploading the samples</p> </li> </ul> <p>The following parameters allow you to configure the labeling schema to use for your annotation tasks. See this section for more details:</p> <ul> <li> <p>label_schema ( None): a dictionary defining the label schema to use. If this argument is provided, it takes precedence over <code>label_field</code> and <code>label_type</code></p> </li> <li> <p>label_field ( None): a string indicating a new or existing label field to annotate</p> </li> <li> <p>label_type ( None): a string indicating the type of labels to annotate. The possible label types are:</p> </li> <li> <p><code>\"classification\"</code>: a single classification stored in     <code>Classification</code> fields</p> </li> <li> <p><code>\"classifications\"</code>: multilabel classifications stored in     <code>Classifications</code> fields</p> </li> <li> <p><code>\"detections\"</code>: object detections stored in <code>Detections</code> fields</p> </li> <li> <p><code>\"polygons\"</code>: polygons stored in <code>Polylines</code> fields with their     <code>filled</code> attributes set to     <code>True</code></p> </li> <li> <p><code>\"keypoints\"</code>: keypoints stored in <code>Keypoints</code> fields</p> </li> </ul> <p>All new label fields must have their type specified via this argument or in <code>label_schema</code></p> <ul> <li> <p>classes ( None): a list of strings indicating the class options for <code>label_field</code> or all fields in <code>label_schema</code> without classes specified. All new label fields must have a class list provided via one of the supported methods. For existing label fields, if classes are not provided by this argument nor <code>label_schema</code>, the observed labels on your dataset are used</p> </li> <li> <p>allow_additions ( True): whether to allow new labels to be added. Only applicable when editing existing label fields</p> </li> <li> <p>allow_deletions ( True): whether to allow labels to be deleted. Only applicable when editing existing label fields</p> </li> <li> <p>allow_label_edits ( True): whether to allow the <code>label</code> attribute of existing labels to be modified. Only applicable when editing existing fields with <code>label</code> attributes</p> </li> <li> <p>allow_spatial_edits ( True): whether to allow edits to the spatial properties (bounding boxes, vertices, keypoints, etc) of labels. Only applicable when editing existing spatial label fields</p> </li> </ul> <p>In addition, the following V7-specific parameters can also be provided:</p> <ul> <li> <p>dataset_slug ( None): the name of the dataset to use or create in Darwin. This is currently mandatory</p> </li> <li> <p>external_storage ( None): the sluggified name of a Darwin external storage to use. If provided, indicates that all files should be treated as external storage</p> </li> </ul>"},{"location":"integrations/v7/#label-schema","title":"Label schema \u00b6","text":"<p>The <code>label_schema</code>, <code>label_field</code>, <code>label_type</code>, and <code>classes</code> parameters to <code>annotate()</code> allow you to define the annotation schema that you wish to be used.</p> <p>The label schema may define new label field(s) that you wish to populate, and it may also include existing label field(s), in which case you can add, delete, or edit the existing labels on your FiftyOne dataset.</p> <p>The <code>label_schema</code> argument is the most flexible way to define how to construct tasks in V7. In its most verbose form, it is a dictionary that defines the label type, annotation type, and possible classes for each label field:</p> <pre><code>anno_key = \"...\"\n\nlabel_schema = {\n    \"new_field\": {\n        \"type\": \"classifications\",\n        \"classes\": [\"class1\", \"class2\"],\n    },\n}\n\ndataset.annotate(\n    anno_key,\n    backend=\"darwin\",\n    label_schema=label_schema,\n    dataset_slug=\"dataset_slug\",\n)\n</code></pre> <p>Alternatively, if you are only editing or creating a single label field, you can use the <code>label_field</code>, <code>label_type</code>, and <code>classes</code> parameters to specify the components of the label schema individually:</p> <pre><code>anno_key = \"...\"\n\nlabel_field = \"new_field\",\nlabel_type = \"classifications\"\nclasses = [\"class1\", \"class2\"]\n\ndataset.annotate(\n    anno_key,\n    backend=\"darwin\",\n    label_field=label_field,\n    label_type=label_type,\n    classes=classes,\n    dataset_slug=\"dataset_slug\",\n)\n</code></pre> <p>When you are annotating existing label fields, you can omit some of these parameters from <code>annotate()</code>, as FiftyOne can infer the appropriate values to use:</p> <ul> <li> <p>label_type: if omitted, the <code>Label</code> type of the field will be used to infer the appropriate value for this parameter</p> </li> <li> <p>classes: if omitted, the observed labels on your dataset will be used to construct a classes list</p> </li> </ul> <p>Warning</p> <p>Annotating multiple fields is not yet supported by the <code>darwin</code> backend. Please check back soon!</p>"},{"location":"integrations/v7/#label-attributes","title":"Label attributes \u00b6","text":"<p>Warning</p> <p>Label attributes are not yet supported by the <code>darwin</code> backend. Please check back soon!</p>"},{"location":"integrations/v7/#loading-annotations","title":"Loading annotations \u00b6","text":"<p>After your annotations tasks in the annotation backend are complete, you can use the <code>load_annotations()</code> method to download them and merge them back into your FiftyOne dataset.</p> <pre><code>view.load_annotations(anno_key)\n</code></pre> <p>The <code>anno_key</code> parameter is the unique identifier for the annotation run that you provided when calling <code>annotate()</code>. You can use <code>list_annotation_runs()</code> to see the available keys on a dataset.</p> <p>Note</p> <p>By default, calling <code>load_annotations()</code> will not delete any information for the run from the annotation backend.</p> <p>However, you can pass <code>cleanup=True</code> to delete the V7 dataset associated with the run after the annotations are downloaded.</p> <p>Warning</p> <p>The <code>dest_field</code> parameter of <code>load_annotations()</code> is not yet supported by the <code>darwin</code> backend. Check back soon!</p>"},{"location":"integrations/v7/#managing-annotation-runs","title":"Managing annotation runs \u00b6","text":"<p>FiftyOne provides a variety of methods that you can use to manage in-progress or completed annotation runs.</p> <p>For example, you can call <code>list_annotation_runs()</code> to see the available annotation keys on a dataset:</p> <pre><code>dataset.list_annotation_runs()\n</code></pre> <p>Or, you can use <code>get_annotation_info()</code> to retrieve information about the configuration of an annotation run:</p> <pre><code>info = dataset.get_annotation_info(anno_key)\nprint(info)\n</code></pre> <p>Use <code>load_annotation_results()</code> to load the <code>AnnotationResults</code> instance for an annotation run.</p> <p>All results objects provide a <code>cleanup()</code> method that you can use to delete all information associated with a run from the annotation backend.</p> <pre><code>results = dataset.load_annotation_results(anno_key)\nresults.cleanup()\n</code></pre> <p>You can use <code>rename_annotation_run()</code> to rename the annotation key associated with an existing annotation run:</p> <pre><code>dataset.rename_annotation_run(anno_key, new_anno_key)\n</code></pre> <p>Finally, you can use <code>delete_annotation_run()</code> to delete the record of an annotation run from your FiftyOne dataset:</p> <pre><code>dataset.delete_annotation_run(anno_key)\n</code></pre> <p>Note</p> <p>Calling <code>delete_annotation_run()</code> only deletes the record of the annotation run from your FiftyOne dataset; it will not delete any annotations loaded onto your dataset via <code>load_annotations()</code>, nor will it delete any associated information from the annotation backend.</p>"},{"location":"integrations/v7/#examples","title":"Examples \u00b6","text":"<p>This section demonstrates how to perform some common annotation workflows on a FiftyOne dataset using the V7 backend.</p> <p>Note</p> <p>All of the examples below assume you have configured your V7 backend as described in this section.</p>"},{"location":"integrations/v7/#adding-new-label-fields","title":"Adding new label fields \u00b6","text":"<p>In order to annotate a new label field, you can provide the <code>label_field</code>, <code>label_type</code>, and <code>classes</code> parameters to <code>annotate()</code> to define the annotation schema for the field:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"v7_new_field\"\n\nview.annotate(\n    anno_key,\n    backend=\"darwin\",\n    label_field=\"new_classifications\",\n    label_type=\"classifications\",\n    classes=[\"dog\", \"cat\", \"person\"],\n    dataset_slug=anno_key,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Create annotations in V7\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre> <p>Alternatively, you can use the <code>label_schema</code> argument to define the same labeling task:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"v7_new_field\"\n\nlabel_schema = {\n    \"new_classifications\": {\n        \"type\": \"classifications\",\n        \"classes\": [\"dog\", \"cat\", \"person\"],\n    }\n}\n\nview.annotate(\n    anno_key,\n    backend=\"darwin\",\n    label_schema=label_schema,\n    dataset_slug=anno_key,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Create annotations in V7\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre>"},{"location":"integrations/v7/#editing-existing-labels","title":"Editing existing labels \u00b6","text":"<p>A common use case is to fix annotation mistakes that you discovered in your datasets through FiftyOne.</p> <p>You can easily edit the labels in an existing field of your FiftyOne dataset by simply passing the name of the field via the <code>label_field</code> parameter of <code>annotate()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\nview = dataset.take(1)\n\nanno_key = \"v7_existing_field\"\n\nview.annotate(\n    anno_key,\n    backend=\"darwin\",\n    label_field=\"ground_truth\",\n    dataset_slug=anno_key,\n    launch_editor=True,\n)\nprint(dataset.get_annotation_info(anno_key))\n\n# Modify/add/delete bounding boxes and their attributes in V7\n\ndataset.load_annotations(anno_key, cleanup=True)\ndataset.delete_annotation_run(anno_key)\n</code></pre>"},{"location":"models/","title":"FiftyOne and Models","text":"<p>Landing page for models</p>"},{"location":"models/hugging_face_models/","title":"Using Models from Hugging Faces","text":""},{"location":"models/hugging_face_models/#voxel51-workspace","title":"Voxel51 workspace","text":""},{"location":"models/hugging_face_models/#other-workspaces","title":"Other Workspaces","text":""},{"location":"models/model_zoo/","title":"FiftyOne Model Zoo \u00b6","text":"<p>The FiftyOne Model Zoo provides a powerful interface for downloading models and applying them to your FiftyOne datasets.</p> <p>It provides native access to hundreds of pre-trained models, and it also supports downloading arbitrary public or private models whose definitions are provided via GitHub repositories or URLs.</p> <p>Note</p> <p>Zoo models may require additional packages such as PyTorch or TensorFlow (or specific versions of them) in order to be used. See this section for more information on viewing/installing package requirements for models.</p> <p>If you try to load a zoo model without the proper packages installed, you will receive an error message that will explain what you need to install.</p> <p>Depending on your compute environment, some package requirement failures may be erroneous. In such cases, you can suppress error messages.</p>"},{"location":"models/model_zoo/#built-in-models","title":"Built-in models \u00b6","text":"<p>The Model Zoo provides built-in access to hundreds of pre-trained models that you can apply to your datasets with a few simple commands.</p> <p>Explore the models in the zoo</p> <p>Note</p> <p>Did you know? You can also pass custom models to methods like <code>apply_model()</code> and <code>compute_embeddings()</code>!</p>"},{"location":"models/model_zoo/#remotely-sourced-models","title":"Remotely-sourced models \u00b6","text":"<p>The Model Zoo also supports downloading and applying models whose definitions are provided via GitHub repositories or URLs.</p> <p>Learn how to download remote models</p>"},{"location":"models/model_zoo/#model-interface","title":"Model interface \u00b6","text":"<p>All models in the Model Zoo are exposed via the <code>Model</code> class, which defines a common interface for loading models and generating predictions with defined input and output data formats.</p> <p>Grok the Model interface</p>"},{"location":"models/model_zoo/#api-reference","title":"API reference \u00b6","text":"<p>The Model Zoo can be accessed via the Python library and the CLI. Consult the API reference belwo to see how to download, apply, and manage zoo models.</p> <p>Check out the API reference</p>"},{"location":"models/model_zoo/#basic-recipe","title":"Basic recipe \u00b6","text":"<p>Methods for working with the Model Zoo are conveniently exposed via the Python library and the CLI. The basic recipe is that you load a model from the zoo and then apply it to a dataset (or a subset of the dataset specified by a <code>DatasetView</code>) using methods such as <code>apply_model()</code> and <code>compute_embeddings()</code>.</p>"},{"location":"models/model_zoo/#prediction","title":"Prediction \u00b6","text":"<p>The Model Zoo provides a number of convenient methods for generating predictions with zoo models for your datasets.</p> <p>For example, the code sample below shows a self-contained example of loading a Faster R-CNN model from the model zoo and adding its predictions to the COCO-2017 dataset from the Dataset Zoo:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n# List available zoo models\nprint(foz.list_zoo_models())\n\n# Download and load a model\nmodel = foz.load_zoo_model(\"faster-rcnn-resnet50-fpn-coco-torch\")\n\n# Load some samples from the COCO-2017 validation split\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=\"coco-2017-validation-sample\",\n    max_samples=50,\n    shuffle=True,\n)\n\n#\n# Choose some samples to process. This can be the entire dataset, or a\n# subset of the dataset. In this case, we'll choose some samples at\n# random\n#\nsamples = dataset.take(25)\n\n#\n# Generate predictions for each sample and store the results in the\n# `faster_rcnn` field of the dataset, discarding all predictions with\n# confidence below 0.5\n#\nsamples.apply_model(model, label_field=\"faster_rcnn\", confidence_thresh=0.5)\nprint(samples)\n\n# Visualize predictions in the App\nsession = fo.launch_app(view=samples)\n</code></pre>"},{"location":"models/model_zoo/#embeddings","title":"Embeddings \u00b6","text":"<p>Many models in the Model Zoo expose embeddings for their predictions:</p> <pre><code>import fiftyone.zoo as foz\n\n# Load zoo model\nmodel = foz.load_zoo_model(\"inception-v3-imagenet-torch\")\n\n# Check if model exposes embeddings\nprint(model.has_embeddings)  # True\n</code></pre> <p>For models that expose embeddings, you can generate embeddings for all samples in a dataset (or a subset of it specified by a <code>DatasetView</code>) by calling <code>compute_embeddings()</code>:</p> <pre><code>import fiftyone.zoo as foz\n\n# Load zoo model\nmodel = foz.load_zoo_model(\"inception-v3-imagenet-torch\")\nprint(model.has_embeddings)  # True\n\n# Load zoo dataset\ndataset = foz.load_zoo_dataset(\"imagenet-sample\")\n\n# Select some samples to process\nsamples = dataset.take(10)\n\n#\n# Option 1: Generate embeddings for each sample and return them in a\n# `num_samples x dim` array\n#\nembeddings = samples.compute_embeddings(model)\n\n#\n# Option 2: Generate embeddings for each sample and store them in an\n# `embeddings` field of the dataset\n#\nsamples.compute_embeddings(model, embeddings_field=\"embeddings\")\n</code></pre> <p>You can also use <code>compute_patch_embeddings()</code> to generate embeddings for image patches defined by another label field, e.g,. the detections generated by a detection model.</p>"},{"location":"models/model_zoo/#logits","title":"Logits \u00b6","text":"<p>Many classifiers in the Model Zoo can optionally store logits for their predictions.</p> <p>Note</p> <p>Storing logits for predictions enables you to run Brain methods such as label mistakes and sample hardness on your datasets!</p> <p>You can check if a model exposes logits via <code>has_logits()</code>:</p> <pre><code>import fiftyone.zoo as foz\n\n# Load zoo model\nmodel = foz.load_zoo_model(\"inception-v3-imagenet-torch\")\n\n# Check if model has logits\nprint(model.has_logits)  # True\n</code></pre> <p>For models that expose logits, you can store logits for all predictions generated by <code>apply_model()</code> by passing the optional <code>store_logits=True</code> argument:</p> <pre><code>import fiftyone.zoo as foz\n\n# Load zoo model\nmodel = foz.load_zoo_model(\"inception-v3-imagenet-torch\")\nprint(model.has_logits)  # True\n\n# Load zoo dataset\ndataset = foz.load_zoo_dataset(\"imagenet-sample\")\n\n# Select some samples to process\nsamples = dataset.take(10)\n\n# Generate predictions and populate their `logits` fields\nsamples.apply_model(model, store_logits=True)\n</code></pre>"},{"location":"models/model_zoo/api/","title":"Model Zoo API Reference \u00b6","text":"<p>You can interact with the Model Zoo either via the Python library or the CLI.</p>"},{"location":"models/model_zoo/api/#listing-zoo-models","title":"Listing zoo models \u00b6","text":""},{"location":"models/model_zoo/api/#getting-information-about-zoo-models","title":"Getting information about zoo models \u00b6","text":""},{"location":"models/model_zoo/api/#downloading-zoo-models","title":"Downloading zoo models \u00b6","text":""},{"location":"models/model_zoo/api/#installing-zoo-model-requirements","title":"Installing zoo model requirements \u00b6","text":""},{"location":"models/model_zoo/api/#loading-zoo-models","title":"Loading zoo models \u00b6","text":"<p>You can load a zoo model via <code>load_zoo_model()</code>.</p> <p>By default, the model will be automatically downloaded from the web the first time you access it if it is not already downloaded:</p> <pre><code>import fiftyone.zoo as foz\n\n# The model will be downloaded from the web the first time you access it\nmodel = foz.load_zoo_model(\"faster-rcnn-resnet50-fpn-coco-torch\")\n</code></pre> <p>You can also provide additional arguments to <code>load_zoo_model()</code> to customize the import behavior:</p> <pre><code># Load the zoo model and install any necessary requirements in order to\n# use it (logging warnings if any issues arise)\nmodel = foz.load_zoo_model(\n    \"faster-rcnn-resnet50-fpn-coco-torch\",\n    install_requirements=True,\n    error_level=1,\n)\n</code></pre> <p>Note</p> <p>By default, FiftyOne will attempt to ensure that any requirements such as Python packages or CUDA versions are satisfied before loading the model, and an error will be raised if a requirement is not satisfied.</p> <p>You can customize this behavior via the <code>error_level</code> argument to <code>load_zoo_model()</code>, or you can permanently adjust this behavior by setting the <code>requirement_error_level</code> parameter of your FiftyOne config.</p> <p>An <code>error_level</code> of <code>0</code> will raise an error if a requirement is not satisfied, <code>1</code> will log a warning if the requirement is not satisfied, and <code>2</code> will ignore unsatisfied requirements.</p> <p>If you are using a <code>conda</code> environment, it is recommended you use an <code>error_level</code> of <code>1</code> or <code>2</code>, since FiftyOne uses <code>pip</code> to check for requirements.</p>"},{"location":"models/model_zoo/api/#applying-zoo-models","title":"Applying zoo models \u00b6","text":""},{"location":"models/model_zoo/api/#generating-embeddings-with-zoo-models","title":"Generating embeddings with zoo models \u00b6","text":""},{"location":"models/model_zoo/api/#controlling-where-zoo-models-are-downloaded","title":"Controlling where zoo models are downloaded \u00b6","text":"<p>By default, zoo models are downloaded into subdirectories of <code>fiftyone.config.model_zoo_dir</code> corresponding to their names.</p> <p>You can customize this backend by modifying the <code>model_zoo_dir</code> setting of your FiftyOne config.</p>"},{"location":"models/model_zoo/api/#deleting-zoo-models","title":"Deleting zoo models \u00b6","text":""},{"location":"models/model_zoo/api/#adding-models-to-the-zoo","title":"Adding models to the zoo \u00b6","text":"<p>We frequently add new models to the Model Zoo, which will automatically become accessible to you when you update your FiftyOne package.</p> <p>Note</p> <p>FiftyOne is open source! You are welcome to contribute models to the public model zoo by submitting a pull request to the GitHub repository.</p> <p>You can also add your own models to your local model zoo, enabling you to work with these models via the <code>fiftyone.zoo</code> package and the CLI using the same syntax that you would with publicly available models.</p> <p>To add model(s) to your local zoo, you simply write a JSON manifest file in the format below to tell FiftyOne about the model(s). For example, the manifest below adds a second copy of the <code>yolo-v2-coco-tf1</code> model to the zoo under the alias <code>yolo-v2-coco-tf1-high-conf</code> that only returns predictions whose confidence is at least 0.5:</p> <pre><code>{\n    \"models\": [\\\n        {\\\n            \"base_name\": \"yolo-v2-coco-tf1-high-conf\",\\\n            \"base_filename\": \"yolo-v2-coco-high-conf.weights\",\\\n            \"version\": null,\\\n            \"description\": \"A YOLOv2 model with confidence threshold set to 0.5\",\\\n            \"manager\": {\\\n                \"type\": \"fiftyone.core.models.ModelManager\",\\\n                \"config\": {\\\n                    \"google_drive_id\": \"1ajuPZws47SOw3xJc4Wvk1yuiB3qv8ycr\"\\\n                }\\\n            },\\\n            \"default_deployment_config_dict\": {\\\n                \"type\": \"fiftyone.utils.eta.ETAModel\",\\\n                \"config\": {\\\n                    \"type\": \"eta.detectors.YOLODetector\",\\\n                    \"config\": {\\\n                        \"config_dir\": \"{{eta}}/tensorflow/darkflow/cfg/\",\\\n                        \"config_path\": \"{{eta}}/tensorflow/darkflow/cfg/yolo.cfg\",\\\n                        \"confidence_thresh\": 0.5\\\n                    }\\\n                }\\\n            },\\\n            \"requirements\": {\\\n                \"cpu\": {\\\n                    \"support\": true,\\\n                    \"packages\": [\"tensorflow&lt;2\"]\\\n                },\\\n                \"gpu\": {\\\n                    \"support\": true,\\\n                    \"packages\": [\"tensorflow-gpu&lt;2\"]\\\n                }\\\n            },\\\n            \"tags\": [\"detection\", \"coco\", \"tf1\"],\\\n            \"date_added\": \"2020-12-11 13:45:51\"\\\n        }\\\n    ]\n}\n</code></pre> <p>Note</p> <p>Adjusting the hard-coded threshold of the above model is possible via JSON-only changes in this case because the underlying eta.detectors.YOLODetector class exposes this as a parameter.</p> <p>In practice, there is no need to hard-code confidence thresholds in models, since the <code>apply_model()</code> method supports supplying an optional confidence threshold that is applied post-facto to the predictions generated by any model.</p> <p>Models manifest JSON files should have a <code>models</code> key that contains a list of serialized <code>ZooModel class definitions</code> that describe how to download and load the model.</p> <p>Finally, expose your new models(s) to FiftyOne by adding your manifest to the <code>model_zoo_manifest_paths</code> parameter of your FiftyOne config. One way to do this is to set the <code>FIFTYONE_MODEL_ZOO_MANIFEST_PATHS</code> environment variable:</p> <pre><code>export FIFTYONE_MODEL_ZOO_MANIFEST_PATHS=/path/to/custom/manifest.json\n</code></pre> <p>Now you can load and apply the <code>yolo-v2-coco-tf1-high-conf</code> model as you would any other zoo model:</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Load custom model\nmodel = foz.load_zoo_model(\"yolo-v2-coco-tf1-high-conf\")\n\n# Apply model to a dataset\ndataset = fo.load_dataset(...)\ndataset.apply_model(model, label_field=\"predictions\")\n</code></pre>"},{"location":"models/model_zoo/design/","title":"Model Interface \u00b6","text":"<p>All models in the Model Zoo are exposed via the <code>Model</code> class, which defines a common interface for loading models and generating predictions with defined input and output data formats.</p> <p>Note</p> <p>If you write a wrapper for your custom model that implements the <code>Model</code> interface, then you can pass your models to built-in methods like <code>apply_model()</code> and <code>compute_embeddings()</code> too!</p> <p>FiftyOne provides classes that make it easy to deploy models in custom frameworks easy. For example, if you have a PyTorch model that processes images, you can likely use <code>TorchImageModel</code> to run it using FiftyOne.</p>"},{"location":"models/model_zoo/design/#prediction","title":"Prediction \u00b6","text":"<p>Inside built-in methods like <code>apply_model()</code>, predictions of a <code>Model</code> instance are generated using the following pattern:</p> <p>By convention, <code>Model</code> instances must implement the context manager interface, which handles any necessary setup and teardown required to use the model.</p> <p>Predictions are generated via the <code>Model.predict()</code> interface method, which takes an image/video as input and returns the predictions.</p> <p>In order to be compatible with built-in methods like <code>apply_model()</code>, models should support the following basic signature of running inference and storing the output labels:</p> <pre><code>labels = model.predict(arg)\nsample.add_labels(labels, label_field=label_field)\n</code></pre> <p>where the model should, at minimum, support <code>arg</code> values that are:</p> <ul> <li> <p>Image models: uint8 numpy arrays (HWC)</p> </li> <li> <p>Video models: <code>eta.core.video.VideoReader</code> instances</p> </li> </ul> <p>and the output <code>labels</code> can be any of the following:</p> <ul> <li>A <code>Label</code> instance, in which case the labels are directly saved in the specified <code>label_field</code> of the sample</li> </ul> <pre><code># Single sample-level label\nsample[label_field] = labels\n</code></pre> <ul> <li>A dict mapping keys to <code>Label</code> instances. In this case, the labels are added as follows:</li> </ul> <pre><code># Multiple sample-level labels\nfor key, value in labels.items():\n    sample[label_key(key)] = value\n</code></pre> <ul> <li>A dict mapping frame numbers to <code>Label</code> instances. In this case, the provided labels are interpreted as frame-level labels that should be added as follows:</li> </ul> <pre><code># Single set of per-frame labels\nsample.frames.merge(\n    {\n        frame_number: {label_field: label}\n        for frame_number, label in labels.items()\n    }\n)\n</code></pre> <ul> <li>A dict mapping frame numbers to dicts mapping keys to <code>Label</code> instances. In this case, the provided labels are interpreted as frame-level labels that should be added as follows:</li> </ul> <pre><code># Multiple per-frame labels\nsample.frames.merge(\n    {\n        frame_number: {label_key(k): v for k, v in frame_dict.items()}\n        for frame_number, frame_dict in labels.items()\n    }\n)\n</code></pre> <p>In the above snippets, the <code>label_key</code> function maps label dict keys to field names, and is defined from <code>label_field</code> as follows:</p> <pre><code>if isinstance(label_field, dict):\n    label_key = lambda k: label_field.get(k, k)\nelif label_field is not None:\n    label_key = lambda k: label_field + \"_\" + k\nelse:\n    label_key = lambda k: k\n</code></pre> <p>For models that support batching, the <code>Model</code> interface also provides a <code>predict_all()</code> method that can provide an efficient implementation of predicting on a batch of data.</p> <p>Note</p> <p>Built-in methods like <code>apply_model()</code> provide a <code>batch_size</code> parameter that can be used to control the batch size used when performing inference with models that support efficient batching.</p> <p>Note</p> <p>PyTorch models can implement the <code>TorchModelMixin</code> mixin, in which case DataLoaders are used to efficiently feed data to the models during inference.</p>"},{"location":"models/model_zoo/design/#embeddings","title":"Embeddings \u00b6","text":"<p>Models that can compute embeddings for their input data can expose this capability by implementing the <code>EmbeddingsMixin</code> mixin.</p> <p>Inside built-in methods like <code>compute_embeddings()</code>, embeddings for a collection of samples are generated using an analogous pattern to the prediction code shown above, except that the embeddings are generated using <code>Model.embed()</code> in place of <code>Model.predict()</code>.</p> <p>By convention, <code>Model.embed()</code> should return a numpy array containing the embedding.</p> <p>Note</p> <p>Embeddings are typically 1D vectors, but this is not strictly required.</p> <p>For models that support batching, the <code>EmbeddingsMixin</code> interface also provides a <code>embed_all()</code> method that can provide an efficient implementation of embedding a batch of data.</p>"},{"location":"models/model_zoo/design/#logits","title":"Logits \u00b6","text":"<p>Models that generate logits for their predictions can expose them to FiftyOne by implementing the <code>LogitsMixin</code> mixin.</p> <p>Inside built-in methods like <code>apply_model()</code>, if the user requests logits, the model\u2019s <code>store_logits</code> property is set to indicate that the model should store logits in the <code>Label</code> instances that it produces during inference.</p>"},{"location":"models/model_zoo/design/#custom-models","title":"Custom models \u00b6","text":"<p>FiftyOne provides a <code>TorchImageModel</code> class that you can use to load your own custom Torch model and pass it to built-in methods like <code>apply_model()</code> and <code>compute_embeddings()</code>.</p> <p>For example, the snippet below loads a pretrained model from <code>torchvision</code> and uses it both as a classifier and to generate image embeddings:</p> <pre><code>import os\nimport eta\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.utils.torch as fout\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nlabels_path = os.path.join(\n    eta.constants.RESOURCES_DIR, \"imagenet-labels-no-background.txt\"\n)\nconfig = fout.TorchImageModelConfig(\n    {\n        \"entrypoint_fcn\": \"torchvision.models.mobilenet.mobilenet_v2\",\n        \"entrypoint_args\": {\"weights\": \"MobileNet_V2_Weights.DEFAULT\"},\n        \"output_processor_cls\": \"fiftyone.utils.torch.ClassifierOutputProcessor\",\n        \"labels_path\": labels_path,\n        \"image_min_dim\": 224,\n        \"image_max_dim\": 2048,\n        \"image_mean\": [0.485, 0.456, 0.406],\n        \"image_std\": [0.229, 0.224, 0.225],\n        \"embeddings_layer\": \"&lt;classifier.1\",\n    }\n)\nmodel = fout.TorchImageModel(config)\n\ndataset.apply_model(model, label_field=\"imagenet\")\nembeddings = dataset.compute_embeddings(model)\n</code></pre> <p>The necessary configuration is provided via the <code>TorchImageModelConfig</code> class, which exposes a number of built-in mechanisms for defining the model to load and any necessary preprocessing and post-processing.</p> <p>Under the hood, the torch model is loaded via:</p> <pre><code>torch_model = entrypoint_fcn(**entrypoint_args)\n</code></pre> <p>which is assumed to return a <code>torch.nn.Module</code> whose <code>__call__()</code> method directly accepts Torch tensors (NCHW) as input.</p> <p>The <code>TorchImageModelConfig</code> class provides a number of built-in mechanisms for specifying the required preprocessing for your model, such as resizing and normalization. In the above example, <code>image_min_dim</code>, <code>image_max_dim</code>, <code>image_mean</code>, and <code>image_std</code> are used.</p> <p>The <code>output_processor_cls</code> parameter of <code>TorchImageModelConfig</code> must be set to the fully-qualified class name of an <code>OutputProcessor</code> subclass that defines how to translate the model\u2019s raw output into the suitable FiftyOne <code>Label</code> types, and is instantiated as follows:</p> <pre><code>output_processor = output_processor_cls(classes=classes, **output_processor_args)\n</code></pre> <p>where your model\u2019s classes can be specified via any of the <code>classes</code>, <code>labels_string</code>, or <code>labels_path</code> parameters of <code>TorchImageModelConfig</code>.</p> <p>The following built-in output processors are available for use:</p> <ul> <li> <p><code>ClassifierOutputProcessor</code></p> </li> <li> <p><code>DetectorOutputProcessor</code></p> </li> <li> <p><code>InstanceSegmenterOutputProcessor</code></p> </li> <li> <p><code>KeypointDetectorOutputProcessor</code></p> </li> <li> <p><code>SemanticSegmenterOutputProcessor</code></p> </li> </ul> <p>or you can write your own <code>OutputProcessor</code> subclass.</p> <p>Finally, if you would like to pass your custom model to methods like <code>compute_embeddings()</code>, set the <code>embeddings_layer</code> parameter to the name of a layer whose output to expose as embeddings (or prepend <code>&lt;</code> to use the input tensor instead).</p> <p>Note</p> <p>Did you know? You can also register your custom model under a name of your choice so that it can be loaded and used as follows:</p> <pre><code>model = foz.load_zoo_model(\"your-custom-model\")\ndataset.apply_model(model, label_field=\"predictions\")\n</code></pre>"},{"location":"models/model_zoo/models/","title":"Built-In Zoo Models \u00b6","text":"<p>This page lists all of the natively available models in the FiftyOne Model Zoo.</p> <p>Check out the API reference for complete instructions for using the Model Zoo.</p>"},{"location":"models/model_zoo/models/#alexnet-imagenet-torch","title":"alexnet-imagenet-torch","text":"<p>AlexNet model architecture from \"One weird trick for parallelizing convolutional neural networks\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Alexnet</p>"},{"location":"models/model_zoo/models/#centernet-hg104-1024-coco-tf2","title":"centernet-hg104-1024-coco-tf2","text":"<p>CenterNet model from \"Objects as Points\" with the Hourglass-104 backbone trained on COCO resized to 1024x1024</p> <p>Detection,Coco,TensorFlow-2,Centernet</p>"},{"location":"models/model_zoo/models/#centernet-hg104-512-coco-tf2","title":"centernet-hg104-512-coco-tf2","text":"<p>CenterNet model from \"Objects as Points\" with the Hourglass-104 backbone trained on COCO resized to 512x512</p> <p>Detection,Coco,TensorFlow-2,Centernet</p>"},{"location":"models/model_zoo/models/#centernet-mobilenet-v2-fpn-512-coco-tf2","title":"centernet-mobilenet-v2-fpn-512-coco-tf2","text":"<p>CenterNet model from \"Objects as Points\" with the MobileNetV2 backbone trained on COCO resized to 512x512</p> <p>Detection,Coco,TensorFlow-2,Centernet,Mobilenet</p>"},{"location":"models/model_zoo/models/#centernet-resnet101-v1-fpn-512-coco-tf2","title":"centernet-resnet101-v1-fpn-512-coco-tf2","text":"<p>CenterNet model from \"Objects as Points\" with the ResNet-101v1 backbone + FPN trained on COCO resized to 512x512</p> <p>Detection,Coco,TensorFlow-2,Centernet,Resnet</p>"},{"location":"models/model_zoo/models/#centernet-resnet50-v1-fpn-512-coco-tf2","title":"centernet-resnet50-v1-fpn-512-coco-tf2","text":"<p>CenterNet model from \"Objects as Points\" with the ResNet-50-v1 backbone + FPN trained on COCO resized to 512x512</p> <p>Detection,Coco,TensorFlow-2,Centernet,Resnet</p>"},{"location":"models/model_zoo/models/#centernet-resnet50-v2-512-coco-tf2","title":"centernet-resnet50-v2-512-coco-tf2","text":"<p>CenterNet model from \"Objects as Points\" with the ResNet-50v2 backbone trained on COCO resized to 512x512</p> <p>Detection,Coco,TensorFlow-2,Centernet,Resnet</p>"},{"location":"models/model_zoo/models/#classification-transformer-torch","title":"classification-transformer-torch","text":"<p>Hugging Face Transformers model for image classification</p> <p>Classification,Logits,Embeddings,PyTorch,Transformers</p>"},{"location":"models/model_zoo/models/#clip-vit-base32-torch","title":"clip-vit-base32-torch","text":"<p>CLIP text/image encoder from \"Learning Transferable Visual Models From Natural Language Supervision\" trained on 400M text-image pairs</p> <p>Classification,Logits,Embeddings,PyTorch,Clip,Zero-shot</p>"},{"location":"models/model_zoo/models/#deeplabv3-cityscapes-tf","title":"deeplabv3-cityscapes-tf","text":"<p>DeepLabv3+ semantic segmentation model from \"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation\" with Xception backbone trained on the Cityscapes dataset</p> <p>Segmentation,Cityscapes,TensorFlow,Deeplabv3</p>"},{"location":"models/model_zoo/models/#deeplabv3-mnv2-cityscapes-tf","title":"deeplabv3-mnv2-cityscapes-tf","text":"<p>DeepLabv3+ semantic segmentation model from \"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation\" with MobileNetV2 backbone trained on the Cityscapes dataset</p> <p>Segmentation,Cityscapes,TensorFlow,Deeplabv3</p>"},{"location":"models/model_zoo/models/#deeplabv3-resnet101-coco-torch","title":"deeplabv3-resnet101-coco-torch","text":"<p>DeepLabV3 model from \"Rethinking Atrous Convolution for Semantic Image Segmentation\" with ResNet-101 backbone trained on COCO</p> <p>Segmentation,Coco,PyTorch,Resnet,Deeplabv3</p>"},{"location":"models/model_zoo/models/#deeplabv3-resnet50-coco-torch","title":"deeplabv3-resnet50-coco-torch","text":"<p>DeepLabV3 model from \"Rethinking Atrous Convolution for Semantic Image Segmentation\" with ResNet-50 backbone trained on COCO</p> <p>Segmentation,Coco,PyTorch,Resnet,Deeplabv3</p>"},{"location":"models/model_zoo/models/#densenet121-imagenet-torch","title":"densenet121-imagenet-torch","text":"<p>Densenet-121 model from \"Densely Connected Convolutional Networks\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Densenet</p>"},{"location":"models/model_zoo/models/#densenet161-imagenet-torch","title":"densenet161-imagenet-torch","text":"<p>Densenet-161 model from \"Densely Connected Convolutional Networks\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Densenet</p>"},{"location":"models/model_zoo/models/#densenet169-imagenet-torch","title":"densenet169-imagenet-torch","text":"<p>Densenet-169 model from \"Densely Connected Convolutional Networks\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Densenet</p>"},{"location":"models/model_zoo/models/#densenet201-imagenet-torch","title":"densenet201-imagenet-torch","text":"<p>Densenet-201 model from \"Densely Connected Convolutional Networks\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Densenet</p>"},{"location":"models/model_zoo/models/#depth-estimation-transformer-torch","title":"depth-estimation-transformer-torch","text":"<p>Hugging Face Transformers model for monocular depth estimation</p> <p>Depth,PyTorch,Transformers</p>"},{"location":"models/model_zoo/models/#detection-transformer-torch","title":"detection-transformer-torch","text":"<p>Hugging Face Transformers model for object detection</p> <p>Detection,Logits,Embeddings,PyTorch,Transformers</p>"},{"location":"models/model_zoo/models/#dinov2-vitb14-reg-torch","title":"dinov2-vitb14-reg-torch","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-B/14 distilled</p> <p>Embeddings,PyTorch,Dinov2</p>"},{"location":"models/model_zoo/models/#dinov2-vitb14-torch","title":"dinov2-vitb14-torch","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-B/14 distilled</p> <p>Embeddings,PyTorch,Dinov2</p>"},{"location":"models/model_zoo/models/#dinov2-vitg14-reg-torch","title":"dinov2-vitg14-reg-torch","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-g/14</p> <p>Embeddings,PyTorch,Dinov2</p>"},{"location":"models/model_zoo/models/#dinov2-vitg14-torch","title":"dinov2-vitg14-torch","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-g/14</p> <p>Embeddings,PyTorch,Dinov2</p>"},{"location":"models/model_zoo/models/#dinov2-vitl14-reg-torch","title":"dinov2-vitl14-reg-torch","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-L/14 distilled</p> <p>Embeddings,PyTorch,Dinov2</p>"},{"location":"models/model_zoo/models/#dinov2-vitl14-torch","title":"dinov2-vitl14-torch","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-L/14 distilled</p> <p>Embeddings,PyTorch,Dinov2</p>"},{"location":"models/model_zoo/models/#dinov2-vits14-reg-torch","title":"dinov2-vits14-reg-torch","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-S/14 distilled</p> <p>Embeddings,PyTorch,Dinov2</p>"},{"location":"models/model_zoo/models/#dinov2-vits14-torch","title":"dinov2-vits14-torch","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-S/14 distilled</p> <p>Embeddings,PyTorch,Dinov2</p>"},{"location":"models/model_zoo/models/#efficientdet-d0-512-coco-tf2","title":"efficientdet-d0-512-coco-tf2","text":"<p>EfficientDet-D0 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO resized to 512x512</p> <p>Detection,Coco,TensorFlow-2,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d0-coco-tf1","title":"efficientdet-d0-coco-tf1","text":"<p>EfficientDet-D0 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO</p> <p>Detection,Coco,TensorFlow-1,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d1-640-coco-tf2","title":"efficientdet-d1-640-coco-tf2","text":"<p>EfficientDet-D1 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO resized to 640x640</p> <p>Detection,Coco,TensorFlow-2,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d1-coco-tf1","title":"efficientdet-d1-coco-tf1","text":"<p>EfficientDet-D1 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO</p> <p>Detection,Coco,TensorFlow-1,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d2-768-coco-tf2","title":"efficientdet-d2-768-coco-tf2","text":"<p>EfficientDet-D2 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO resized to 768x768</p> <p>Detection,Coco,TensorFlow-2,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d2-coco-tf1","title":"efficientdet-d2-coco-tf1","text":"<p>EfficientDet-D2 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO</p> <p>Detection,Coco,TensorFlow-1,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d3-896-coco-tf2","title":"efficientdet-d3-896-coco-tf2","text":"<p>EfficientDet-D3 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO resized to 896x896</p> <p>Detection,Coco,TensorFlow-2,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d3-coco-tf1","title":"efficientdet-d3-coco-tf1","text":"<p>EfficientDet-D3 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO</p> <p>Detection,Coco,TensorFlow-1,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d4-1024-coco-tf2","title":"efficientdet-d4-1024-coco-tf2","text":"<p>EfficientDet-D4 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO resized to 1024x1024</p> <p>Detection,Coco,TensorFlow-2,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d4-coco-tf1","title":"efficientdet-d4-coco-tf1","text":"<p>EfficientDet-D4 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO</p> <p>Detection,Coco,TensorFlow-1,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d5-1280-coco-tf2","title":"efficientdet-d5-1280-coco-tf2","text":"<p>EfficientDet-D5 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO resized to 1280x1280</p> <p>Detection,Coco,TensorFlow-2,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d5-coco-tf1","title":"efficientdet-d5-coco-tf1","text":"<p>EfficientDet-D5 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO</p> <p>Detection,Coco,TensorFlow-1,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d6-1280-coco-tf2","title":"efficientdet-d6-1280-coco-tf2","text":"<p>EfficientDet-D6 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO resized to 1280x1280</p> <p>Detection,Coco,TensorFlow-2,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d6-coco-tf1","title":"efficientdet-d6-coco-tf1","text":"<p>EfficientDet-D6 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO</p> <p>Detection,Coco,TensorFlow-1,Efficientdet</p>"},{"location":"models/model_zoo/models/#efficientdet-d7-1536-coco-tf2","title":"efficientdet-d7-1536-coco-tf2","text":"<p>EfficientDet-D7 model from \"EfficientDet: Scalable and Efficient Object Detection\" trained on COCO resized to 1536x1536</p> <p>Detection,Coco,TensorFlow-2,Efficientdet</p>"},{"location":"models/model_zoo/models/#faster-rcnn-inception-resnet-atrous-v2-coco-tf","title":"faster-rcnn-inception-resnet-atrous-v2-coco-tf","text":"<p>Faster R-CNN model from \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\" atrous version with Inception backbone trained on COCO</p> <p>Detection,Coco,TensorFlow,Faster-rcnn,Inception,Resnet</p>"},{"location":"models/model_zoo/models/#faster-rcnn-inception-resnet-atrous-v2-lowproposals-coco-tf","title":"faster-rcnn-inception-resnet-atrous-v2-lowproposals-coco-tf","text":"<p>Faster R-CNN model from \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\" atrous version with low-proposals and Inception backbone trained on COCO</p> <p>Detection,Coco,TensorFlow,Faster-rcnn,Inception,Resnet</p>"},{"location":"models/model_zoo/models/#faster-rcnn-inception-v2-coco-tf","title":"faster-rcnn-inception-v2-coco-tf","text":"<p>Faster R-CNN model from \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\" with Inception v2 backbone trained on COCO</p> <p>Detection,Coco,TensorFlow,Faster-rcnn,Inception</p>"},{"location":"models/model_zoo/models/#faster-rcnn-nas-coco-tf","title":"faster-rcnn-nas-coco-tf","text":"<p>Faster R-CNN model from \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\" with NAS-net backbone trained on COCO</p> <p>Detection,Coco,TensorFlow,Faster-rcnn</p>"},{"location":"models/model_zoo/models/#faster-rcnn-nas-lowproposals-coco-tf","title":"faster-rcnn-nas-lowproposals-coco-tf","text":"<p>Faster R-CNN model from \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\" with low-proposals and NAS-net backbone trained on COCO</p> <p>Detection,Coco,TensorFlow,Faster-rcnn</p>"},{"location":"models/model_zoo/models/#faster-rcnn-resnet101-coco-tf","title":"faster-rcnn-resnet101-coco-tf","text":"<p>Faster R-CNN model from \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\" with ResNet-101 backbone trained on COCO</p> <p>Detection,Coco,TensorFlow,Faster-rcnn,Resnet</p>"},{"location":"models/model_zoo/models/#faster-rcnn-resnet101-lowproposals-coco-tf","title":"faster-rcnn-resnet101-lowproposals-coco-tf","text":"<p>Faster R-CNN model from \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\" with low-proposals and ResNet-101 backbone trained on COCO</p> <p>Detection,Coco,TensorFlow,Faster-rcnn,Resnet</p>"},{"location":"models/model_zoo/models/#faster-rcnn-resnet50-coco-tf","title":"faster-rcnn-resnet50-coco-tf","text":"<p>Faster R-CNN model from \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\" with ResNet-50 backbone trained on COCO</p> <p>Detection,Coco,TensorFlow,Faster-rcnn,Resnet</p>"},{"location":"models/model_zoo/models/#faster-rcnn-resnet50-fpn-coco-torch","title":"faster-rcnn-resnet50-fpn-coco-torch","text":"<p>Faster R-CNN model from \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\" with ResNet-50 FPN backbone trained on COCO</p> <p>Detection,Coco,PyTorch,Faster-rcnn,Resnet</p>"},{"location":"models/model_zoo/models/#faster-rcnn-resnet50-lowproposals-coco-tf","title":"faster-rcnn-resnet50-lowproposals-coco-tf","text":"<p>Faster R-CNN model from \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\" with low-proposals and ResNet-50 backbone trained on COCO</p> <p>Detection,Coco,TensorFlow,Faster-rcnn,Resnet</p>"},{"location":"models/model_zoo/models/#fcn-resnet101-coco-torch","title":"fcn-resnet101-coco-torch","text":"<p>FCN model from \"Fully Convolutional Networks for Semantic Segmentation\" with ResNet-101 backbone trained on COCO</p> <p>Segmentation,Coco,PyTorch,Fcn,Resnet</p>"},{"location":"models/model_zoo/models/#fcn-resnet50-coco-torch","title":"fcn-resnet50-coco-torch","text":"<p>FCN model from \"Fully Convolutional Networks for Semantic Segmentation\" with ResNet-50 backbone trained on COCO</p> <p>Segmentation,Coco,PyTorch,Fcn,Resnet</p>"},{"location":"models/model_zoo/models/#googlenet-imagenet-torch","title":"googlenet-imagenet-torch","text":"<p>GoogLeNet (Inception v1) model from \"Going Deeper with Convolutions\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Googlenet</p>"},{"location":"models/model_zoo/models/#inception-resnet-v2-imagenet-tf1","title":"inception-resnet-v2-imagenet-tf1","text":"<p>Inception v2 model from \"Rethinking the Inception Architecture for Computer Vision\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,TensorFlow-1,Inception,Resnet</p>"},{"location":"models/model_zoo/models/#inception-v3-imagenet-torch","title":"inception-v3-imagenet-torch","text":"<p>Inception v3 model from \"Rethinking the Inception Architecture for Computer Vision\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Inception</p>"},{"location":"models/model_zoo/models/#inception-v4-imagenet-tf1","title":"inception-v4-imagenet-tf1","text":"<p>Inception v4 model from \"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,TensorFlow-1,Inception</p>"},{"location":"models/model_zoo/models/#keypoint-rcnn-resnet50-fpn-coco-torch","title":"keypoint-rcnn-resnet50-fpn-coco-torch","text":"<p>Keypoint R-CNN model from \"Mask R-CNN\" with ResNet-50 FPN backbone trained on COCO</p> <p>Keypoints,Coco,PyTorch,Keypoint-rcnn,Resnet</p>"},{"location":"models/model_zoo/models/#mask-rcnn-inception-resnet-v2-atrous-coco-tf","title":"mask-rcnn-inception-resnet-v2-atrous-coco-tf","text":"<p>Mask R-CNN model from \"Mask R-CNN\" atrous version with Inception backbone trained on COCO</p> <p>Instances,Coco,TensorFlow,Mask-rcnn,Inception,Resnet</p>"},{"location":"models/model_zoo/models/#mask-rcnn-inception-v2-coco-tf","title":"mask-rcnn-inception-v2-coco-tf","text":"<p>Mask R-CNN model from \"Mask R-CNN\" with Inception backbone trained on COCO</p> <p>Instances,Coco,TensorFlow,Mask-rcnn,Inception</p>"},{"location":"models/model_zoo/models/#mask-rcnn-resnet101-atrous-coco-tf","title":"mask-rcnn-resnet101-atrous-coco-tf","text":"<p>Mask R-CNN model from \"Mask R-CNN\" atrous version with ResNet-101 backbone trained on COCO</p> <p>Instances,Coco,TensorFlow,Mask-rcnn,Resnet</p>"},{"location":"models/model_zoo/models/#mask-rcnn-resnet50-atrous-coco-tf","title":"mask-rcnn-resnet50-atrous-coco-tf","text":"<p>Mask R-CNN model from \"Mask R-CNN\" atrous version with ResNet-50 backbone trained on COCO</p> <p>Instances,Coco,TensorFlow,Mask-rcnn,Resnet</p>"},{"location":"models/model_zoo/models/#mask-rcnn-resnet50-fpn-coco-torch","title":"mask-rcnn-resnet50-fpn-coco-torch","text":"<p>Mask R-CNN model from \"Mask R-CNN\" with ResNet-50 FPN backbone trained on COCO</p> <p>Instances,Coco,PyTorch,Mask-rcnn,Resnet</p>"},{"location":"models/model_zoo/models/#med-sam-2-video-torch","title":"med-sam-2-video-torch","text":"<p>Fine-tuned SAM2-hiera-tiny model from \"Medical SAM 2 - Segment Medical Images as Video via Segment Anything Model 2\"</p> <p>Segment-anything,PyTorch,Zero-shot,Video,Med-sam</p>"},{"location":"models/model_zoo/models/#mnasnet05-imagenet-torch","title":"mnasnet0.5-imagenet-torch","text":"<p>MNASNet model from from \"MnasNet: Platform-Aware Neural Architecture Search for Mobile\" with depth multiplier of 0.5 trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Mnasnet</p>"},{"location":"models/model_zoo/models/#mnasnet10-imagenet-torch","title":"mnasnet1.0-imagenet-torch","text":"<p>MNASNet model from \"MnasNet: Platform-Aware Neural Architecture Search for Mobile\" with depth multiplier of 1.0 trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Mnasnet</p>"},{"location":"models/model_zoo/models/#mobilenet-v2-imagenet-tf1","title":"mobilenet-v2-imagenet-tf1","text":"<p>MobileNetV2 model from \"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,TensorFlow-1,Mobilenet</p>"},{"location":"models/model_zoo/models/#mobilenet-v2-imagenet-torch","title":"mobilenet-v2-imagenet-torch","text":"<p>MobileNetV2 model from \"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Mobilenet</p>"},{"location":"models/model_zoo/models/#open-clip-torch","title":"open-clip-torch","text":"<p>OPEN CLIP text/image encoder from \"Learning Transferable Visual Models From Natural Language Supervision\" trained on 400M text-image pairs</p> <p>Classification,Logits,Embeddings,PyTorch,Clip,Zero-shot</p>"},{"location":"models/model_zoo/models/#resnet-v1-50-imagenet-tf1","title":"resnet-v1-50-imagenet-tf1","text":"<p>ResNet-50 v1 model from \"Deep Residual Learning for Image Recognition\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,TensorFlow-1,Resnet</p>"},{"location":"models/model_zoo/models/#resnet-v2-50-imagenet-tf1","title":"resnet-v2-50-imagenet-tf1","text":"<p>ResNet-50 v2 model from \"Deep Residual Learning for Image Recognition\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,TensorFlow-1,Resnet</p>"},{"location":"models/model_zoo/models/#resnet101-imagenet-torch","title":"resnet101-imagenet-torch","text":"<p>ResNet-101 model from \"Deep Residual Learning for Image Recognition\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Resnet</p>"},{"location":"models/model_zoo/models/#resnet152-imagenet-torch","title":"resnet152-imagenet-torch","text":"<p>ResNet-152 model from \"Deep Residual Learning for Image Recognition\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Resnet</p>"},{"location":"models/model_zoo/models/#resnet18-imagenet-torch","title":"resnet18-imagenet-torch","text":"<p>ResNet-18 model from \"Deep Residual Learning for Image Recognition\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Resnet</p>"},{"location":"models/model_zoo/models/#resnet34-imagenet-torch","title":"resnet34-imagenet-torch","text":"<p>ResNet-34 model from \"Deep Residual Learning for Image Recognition\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Resnet</p>"},{"location":"models/model_zoo/models/#resnet50-imagenet-torch","title":"resnet50-imagenet-torch","text":"<p>ResNet-50 model from \"Deep Residual Learning for Image Recognition\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Resnet</p>"},{"location":"models/model_zoo/models/#resnext101-32x8d-imagenet-torch","title":"resnext101-32x8d-imagenet-torch","text":"<p>ResNeXt-101 32x8d model from \"Aggregated Residual Transformations for Deep Neural Networks\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Resnext</p>"},{"location":"models/model_zoo/models/#resnext50-32x4d-imagenet-torch","title":"resnext50-32x4d-imagenet-torch","text":"<p>ResNeXt-50 32x4d model from \"Aggregated Residual Transformations for Deep Neural Networks\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Resnext</p>"},{"location":"models/model_zoo/models/#retinanet-resnet50-fpn-coco-torch","title":"retinanet-resnet50-fpn-coco-torch","text":"<p>RetinaNet model from \"Focal Loss for Dense Object Detection\" with ResNet-50 FPN backbone trained on COCO</p> <p>Detection,Coco,PyTorch,Retinanet,Resnet</p>"},{"location":"models/model_zoo/models/#rfcn-resnet101-coco-tf","title":"rfcn-resnet101-coco-tf","text":"<p>R-FCN object detection model from \"R-FCN: Object Detection via Region-based Fully Convolutional Networks\" with ResNet-101 backbone trained on COCO</p> <p>Detection,Coco,TensorFlow,Rfcn,Resnet</p>"},{"location":"models/model_zoo/models/#rtdetr-l-coco-torch","title":"rtdetr-l-coco-torch","text":"<p>RT-DETR-l model trained on COCO</p> <p>Detection,Coco,PyTorch,Transformer,Rtdetr</p>"},{"location":"models/model_zoo/models/#rtdetr-x-coco-torch","title":"rtdetr-x-coco-torch","text":"<p>RT-DETR-x model trained on COCO</p> <p>Detection,Coco,PyTorch,Transformer,Rtdetr</p>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-base-plus-image-torch","title":"segment-anything-2-hiera-base-plus-image-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot</p>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-base-plus-video-torch","title":"segment-anything-2-hiera-base-plus-video-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot,Video</p>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-large-image-torch","title":"segment-anything-2-hiera-large-image-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot</p>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-large-video-torch","title":"segment-anything-2-hiera-large-video-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot,Video</p>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-small-image-torch","title":"segment-anything-2-hiera-small-image-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot</p>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-small-video-torch","title":"segment-anything-2-hiera-small-video-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot,Video</p>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-tiny-image-torch","title":"segment-anything-2-hiera-tiny-image-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot</p>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-tiny-video-torch","title":"segment-anything-2-hiera-tiny-video-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot,Video</p>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-base-plus-image-torch","title":"segment-anything-2.1-hiera-base-plus-image-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot</p>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-base-plus-video-torch","title":"segment-anything-2.1-hiera-base-plus-video-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot,Video</p>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-large-image-torch","title":"segment-anything-2.1-hiera-large-image-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot</p>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-large-video-torch","title":"segment-anything-2.1-hiera-large-video-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot,Video</p>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-small-image-torch","title":"segment-anything-2.1-hiera-small-image-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot</p>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-small-video-torch","title":"segment-anything-2.1-hiera-small-video-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot,Video</p>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-tiny-image-torch","title":"segment-anything-2.1-hiera-tiny-image-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot</p>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-tiny-video-torch","title":"segment-anything-2.1-hiera-tiny-video-torch","text":"<p>Segment Anything Model 2 (SAM2) from \"SAM2: Segment Anything in Images and Videos\"</p> <p>Segment-anything,PyTorch,Zero-shot,Video</p>"},{"location":"models/model_zoo/models/#segment-anything-vitb-torch","title":"segment-anything-vitb-torch","text":"<p>Segment Anything Model (SAM) from \"Segment Anything\" with ViT-B/16 backbone trained on SA-1B</p> <p>Segment-anything,Sa-1b,PyTorch,Zero-shot</p>"},{"location":"models/model_zoo/models/#segment-anything-vith-torch","title":"segment-anything-vith-torch","text":"<p>Segment Anything Model (SAM) from \"Segment Anything\" with ViT-H/16 backbone trained on SA-1B</p> <p>Segment-anything,Sa-1b,PyTorch,Zero-shot</p>"},{"location":"models/model_zoo/models/#segment-anything-vitl-torch","title":"segment-anything-vitl-torch","text":"<p>Segment Anything Model (SAM) from \"Segment Anything\" with ViT-L/16 backbone trained on SA-1B</p> <p>Segment-anything,Sa-1b,PyTorch,Zero-shot</p>"},{"location":"models/model_zoo/models/#segmentation-transformer-torch","title":"segmentation-transformer-torch","text":"<p>Hugging Face Transformers model for semantic segmentation</p> <p>Segmentation,PyTorch,Transformers</p>"},{"location":"models/model_zoo/models/#shufflenetv2-05x-imagenet-torch","title":"shufflenetv2-0.5x-imagenet-torch","text":"<p>ShuffleNetV2 model from \"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\" with 0.5x output channels trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Shufflenet</p>"},{"location":"models/model_zoo/models/#shufflenetv2-10x-imagenet-torch","title":"shufflenetv2-1.0x-imagenet-torch","text":"<p>ShuffleNetV2 model from \"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\" with 1.0x output channels trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Shufflenet</p>"},{"location":"models/model_zoo/models/#squeezenet-11-imagenet-torch","title":"squeezenet-1.1-imagenet-torch","text":"<p>SqueezeNet 1.1 model from \"the official SqueezeNet repo\" trained on ImageNet</p> <p>Classification,Imagenet,PyTorch,Squeezenet</p>"},{"location":"models/model_zoo/models/#squeezenet-imagenet-torch","title":"squeezenet-imagenet-torch","text":"<p>SqueezeNet model from \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and\" trained on ImageNet</p> <p>Classification,Imagenet,PyTorch,Squeezenet</p>"},{"location":"models/model_zoo/models/#ssd-inception-v2-coco-tf","title":"ssd-inception-v2-coco-tf","text":"<p>Inception Single Shot Detector model from \"SSD: Single Shot MultiBox Detector\" trained on COCO</p> <p>Detection,Coco,TensorFlow,Ssd,Inception</p>"},{"location":"models/model_zoo/models/#ssd-mobilenet-v1-coco-tf","title":"ssd-mobilenet-v1-coco-tf","text":"<p>Single Shot Detector model from \"SSD: Single Shot MultiBox Detector\" with MobileNetV1 backbone trained on COCO</p> <p>Detection,Coco,TensorFlow,Ssd,Mobilenet</p>"},{"location":"models/model_zoo/models/#ssd-mobilenet-v1-fpn-640-coco17","title":"ssd-mobilenet-v1-fpn-640-coco17","text":"<p>MobileNetV1 model from \"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" resized to 640x640</p> <p>Detection,Coco,TensorFlow-2,Ssd,Mobilenet</p>"},{"location":"models/model_zoo/models/#ssd-mobilenet-v1-fpn-coco-tf","title":"ssd-mobilenet-v1-fpn-coco-tf","text":"<p>FPN Single Shot Detector model from \"SSD: Single Shot MultiBox Detector\" with MobileNetV1 backbone trained on COCO</p> <p>Detection,Coco,TensorFlow,Ssd,Mobilenet</p>"},{"location":"models/model_zoo/models/#ssd-mobilenet-v2-320-coco17","title":"ssd-mobilenet-v2-320-coco17","text":"<p>MobileNetV2 model from \"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" resized to 320x320</p> <p>Detection,Coco,TensorFlow-2,Ssd,Mobilenet</p>"},{"location":"models/model_zoo/models/#ssd-resnet50-fpn-coco-tf","title":"ssd-resnet50-fpn-coco-tf","text":"<p>FPN Single Shot Detector model from \"SSD: Single Shot MultiBox Detector\" with ResNet-50 backbone trained on COCO</p> <p>Detection,Coco,TensorFlow,Ssd,Resnet</p>"},{"location":"models/model_zoo/models/#vgg11-bn-imagenet-torch","title":"vgg11-bn-imagenet-torch","text":"<p>VGG-11 model from \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" with batch normalization trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Vgg</p>"},{"location":"models/model_zoo/models/#vgg11-imagenet-torch","title":"vgg11-imagenet-torch","text":"<p>VGG-11 model from \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Vgg</p>"},{"location":"models/model_zoo/models/#vgg13-bn-imagenet-torch","title":"vgg13-bn-imagenet-torch","text":"<p>VGG-13 model from \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" with batch normalization trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Vgg</p>"},{"location":"models/model_zoo/models/#vgg13-imagenet-torch","title":"vgg13-imagenet-torch","text":"<p>VGG-13 model from \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Vgg</p>"},{"location":"models/model_zoo/models/#vgg16-bn-imagenet-torch","title":"vgg16-bn-imagenet-torch","text":"<p>VGG-16 model from \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" with batch normalization trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Vgg</p>"},{"location":"models/model_zoo/models/#vgg16-imagenet-tf1","title":"vgg16-imagenet-tf1","text":"<p>VGG-16 model from \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,TensorFlow-1,Vgg</p>"},{"location":"models/model_zoo/models/#vgg16-imagenet-torch","title":"vgg16-imagenet-torch","text":"<p>VGG-16 model from \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Vgg</p>"},{"location":"models/model_zoo/models/#vgg19-bn-imagenet-torch","title":"vgg19-bn-imagenet-torch","text":"<p>VGG-19 model from \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" with batch normalization trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Vgg</p>"},{"location":"models/model_zoo/models/#vgg19-imagenet-torch","title":"vgg19-imagenet-torch","text":"<p>VGG-19 model from \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Vgg</p>"},{"location":"models/model_zoo/models/#wide-resnet101-2-imagenet-torch","title":"wide-resnet101-2-imagenet-torch","text":"<p>Wide ResNet-101-2 model from \"Wide Residual Networks\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Wide-resnet</p>"},{"location":"models/model_zoo/models/#wide-resnet50-2-imagenet-torch","title":"wide-resnet50-2-imagenet-torch","text":"<p>Wide ResNet-50-2 model from \"Wide Residual Networks\" trained on ImageNet</p> <p>Classification,Embeddings,Logits,Imagenet,PyTorch,Wide-resnet</p>"},{"location":"models/model_zoo/models/#yolo-nas-torch","title":"yolo-nas-torch","text":"<p>YOLO-NAS is an open-source training library for advanced computer vision models. It specializes in accuracy and efficiency, supporting tasks like object detection</p> <p>Detection,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolo-v2-coco-tf1","title":"yolo-v2-coco-tf1","text":"<p>YOLOv2 model from \"YOLO9000: Better, Faster, Stronger\" trained on COCO</p> <p>Detection,Coco,TensorFlow-1,Yolo</p>"},{"location":"models/model_zoo/models/#yolo11l-coco-torch","title":"yolo11l-coco-torch","text":"<p>YOLO11-L model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolo11l-seg-coco-torch","title":"yolo11l-seg-coco-torch","text":"<p>YOLO11-L Segmentation model trained on COCO</p> <p>Segmentation,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolo11m-coco-torch","title":"yolo11m-coco-torch","text":"<p>YOLO11-M model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolo11m-seg-coco-torch","title":"yolo11m-seg-coco-torch","text":"<p>YOLO11-M Segmentation model trained on COCO</p> <p>Segmentation,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolo11n-coco-torch","title":"yolo11n-coco-torch","text":"<p>YOLO11-N model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolo11n-seg-coco-torch","title":"yolo11n-seg-coco-torch","text":"<p>YOLO11-N Segmentation model trained on COCO</p> <p>Segmentation,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolo11s-coco-torch","title":"yolo11s-coco-torch","text":"<p>YOLO11-S model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolo11s-seg-coco-torch","title":"yolo11s-seg-coco-torch","text":"<p>YOLO11-S Segmentation model trained on COCO</p> <p>Segmentation,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolo11x-coco-torch","title":"yolo11x-coco-torch","text":"<p>YOLO11-X model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolo11x-seg-coco-torch","title":"yolo11x-seg-coco-torch","text":"<p>YOLO11-X Segmentation model trained on COCO</p> <p>Segmentation,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov10l-coco-torch","title":"yolov10l-coco-torch","text":"<p>YOLOv10-L model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov10m-coco-torch","title":"yolov10m-coco-torch","text":"<p>YOLOv10-M model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov10n-coco-torch","title":"yolov10n-coco-torch","text":"<p>YOLOv10-N model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov10s-coco-torch","title":"yolov10s-coco-torch","text":"<p>YOLOv10-S model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov10x-coco-torch","title":"yolov10x-coco-torch","text":"<p>YOLOv10-X model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov5l-coco-torch","title":"yolov5l-coco-torch","text":"<p>Ultralytics YOLOv5l model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov5m-coco-torch","title":"yolov5m-coco-torch","text":"<p>Ultralytics YOLOv5m model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov5n-coco-torch","title":"yolov5n-coco-torch","text":"<p>Ultralytics YOLOv5n model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov5s-coco-torch","title":"yolov5s-coco-torch","text":"<p>Ultralytics YOLOv5s model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov5x-coco-torch","title":"yolov5x-coco-torch","text":"<p>Ultralytics YOLOv5x model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8l-coco-torch","title":"yolov8l-coco-torch","text":"<p>Ultralytics YOLOv8l model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8l-obb-dotav1-torch","title":"yolov8l-obb-dotav1-torch","text":"<p>YOLOv8l Oriented Bounding Box model</p> <p>Detection,PyTorch,Yolo,Polylines,Obb</p>"},{"location":"models/model_zoo/models/#yolov8l-oiv7-torch","title":"yolov8l-oiv7-torch","text":"<p>Ultralytics YOLOv8l model trained Open Images v7</p> <p>Detection,Oiv7,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8l-seg-coco-torch","title":"yolov8l-seg-coco-torch","text":"<p>Ultralytics YOLOv8l Segmentation model trained on COCO</p> <p>Segmentation,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8l-world-torch","title":"yolov8l-world-torch","text":"<p>YOLOv8l-World model</p> <p>Detection,PyTorch,Yolo,Zero-shot</p>"},{"location":"models/model_zoo/models/#yolov8m-coco-torch","title":"yolov8m-coco-torch","text":"<p>Ultralytics YOLOv8m model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8m-obb-dotav1-torch","title":"yolov8m-obb-dotav1-torch","text":"<p>YOLOv8m Oriented Bounding Box model</p> <p>Detection,PyTorch,Yolo,Polylines,Obb</p>"},{"location":"models/model_zoo/models/#yolov8m-oiv7-torch","title":"yolov8m-oiv7-torch","text":"<p>Ultralytics YOLOv8m model trained Open Images v7</p> <p>Detection,Oiv7,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8m-seg-coco-torch","title":"yolov8m-seg-coco-torch","text":"<p>Ultralytics YOLOv8m Segmentation model trained on COCO</p> <p>Segmentation,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8m-world-torch","title":"yolov8m-world-torch","text":"<p>YOLOv8m-World model</p> <p>Detection,PyTorch,Yolo,Zero-shot</p>"},{"location":"models/model_zoo/models/#yolov8n-coco-torch","title":"yolov8n-coco-torch","text":"<p>Ultralytics YOLOv8n model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8n-obb-dotav1-torch","title":"yolov8n-obb-dotav1-torch","text":"<p>YOLOv8n Oriented Bounding Box model</p> <p>Detection,PyTorch,Yolo,Polylines,Obb</p>"},{"location":"models/model_zoo/models/#yolov8n-oiv7-torch","title":"yolov8n-oiv7-torch","text":"<p>Ultralytics YOLOv8n model trained on Open Images v7</p> <p>Detection,Oiv7,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8n-seg-coco-torch","title":"yolov8n-seg-coco-torch","text":"<p>Ultralytics YOLOv8n Segmentation model trained on COCO</p> <p>Segmentation,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8s-coco-torch","title":"yolov8s-coco-torch","text":"<p>Ultralytics YOLOv8s model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8s-obb-dotav1-torch","title":"yolov8s-obb-dotav1-torch","text":"<p>YOLOv8s Oriented Bounding Box model</p> <p>Detection,PyTorch,Yolo,Polylines,Obb</p>"},{"location":"models/model_zoo/models/#yolov8s-oiv7-torch","title":"yolov8s-oiv7-torch","text":"<p>Ultralytics YOLOv8s model trained on Open Images v7</p> <p>Detection,Oiv7,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8s-seg-coco-torch","title":"yolov8s-seg-coco-torch","text":"<p>Ultralytics YOLOv8s Segmentation model trained on COCO</p> <p>Segmentation,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8s-world-torch","title":"yolov8s-world-torch","text":"<p>YOLOv8s-World model</p> <p>Detection,PyTorch,Yolo,Zero-shot</p>"},{"location":"models/model_zoo/models/#yolov8x-coco-torch","title":"yolov8x-coco-torch","text":"<p>Ultralytics YOLOv8x model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8x-obb-dotav1-torch","title":"yolov8x-obb-dotav1-torch","text":"<p>YOLOv8x Oriented Bounding Box model</p> <p>Detection,PyTorch,Yolo,Polylines,Obb</p>"},{"location":"models/model_zoo/models/#yolov8x-oiv7-torch","title":"yolov8x-oiv7-torch","text":"<p>Ultralytics YOLOv8x model trained Open Images v7</p> <p>Detection,Oiv7,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8x-seg-coco-torch","title":"yolov8x-seg-coco-torch","text":"<p>Ultralytics YOLOv8x Segmentation model trained on COCO</p> <p>Segmentation,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov8x-world-torch","title":"yolov8x-world-torch","text":"<p>YOLOv8x-World model</p> <p>Detection,PyTorch,Yolo,Zero-shot</p>"},{"location":"models/model_zoo/models/#yolov9c-coco-torch","title":"yolov9c-coco-torch","text":"<p>YOLOv9-C model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov9c-seg-coco-torch","title":"yolov9c-seg-coco-torch","text":"<p>YOLOv9-C Segmentation model trained on COCO</p> <p>Segmentation,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov9e-coco-torch","title":"yolov9e-coco-torch","text":"<p>YOLOv9-E model trained on COCO</p> <p>Detection,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#yolov9e-seg-coco-torch","title":"yolov9e-seg-coco-torch","text":"<p>YOLOv9-E Segmentation model trained on COCO</p> <p>Segmentation,Coco,PyTorch,Yolo</p>"},{"location":"models/model_zoo/models/#zero-shot-classification-transformer-torch","title":"zero-shot-classification-transformer-torch","text":"<p>Hugging Face Transformers model for zero-shot image classification</p> <p>Classification,Logits,Embeddings,PyTorch,Transformers,Zero-shot</p>"},{"location":"models/model_zoo/models/#zero-shot-detection-transformer-torch","title":"zero-shot-detection-transformer-torch","text":"<p>Hugging Face Transformers model for zero-shot object detection</p> <p>Detection,Logits,Embeddings,PyTorch,Transformers,Zero-shot</p>"},{"location":"models/model_zoo/models/#torch-models","title":"Torch models \u00b6","text":""},{"location":"models/model_zoo/models/#alexnet-imagenet-torch_1","title":"alexnet-imagenet-torch \u00b6","text":"<p>AlexNet model architecture from One weird trick for parallelizing convolutional neural networks trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>alexnet-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 233.10 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, alexnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"alexnet-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#classification-transformer-torch_1","title":"classification-transformer-torch \u00b6","text":"<p>Hugging Face Transformers model for image classification.</p> <p>Details</p> <ul> <li> <p>Model name: <code>classification-transformer-torch</code></p> </li> <li> <p>Model source: https://huggingface.co/docs/transformers/tasks/image_classification</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, logits, embeddings, torch, transformers</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision, transformers</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"classification-transformer-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#clip-vit-base32-torch_1","title":"clip-vit-base32-torch \u00b6","text":"<p>CLIP text/image encoder from Learning Transferable Visual Models From Natural Language Supervision trained on 400M text-image pairs.</p> <p>Details</p> <ul> <li> <p>Model name: <code>clip-vit-base32-torch</code></p> </li> <li> <p>Model source: https://github.com/openai/CLIP</p> </li> <li> <p>Model size: 337.58 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, logits, embeddings, torch, clip, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"clip-vit-base32-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n\n#\n# Make zero-shot predictions with custom classes\n#\n\nmodel = foz.load_zoo_model(\n    \"clip-vit-base32-torch\",\n    text_prompt=\"A photo of a\",\n    classes=[\"person\", \"dog\", \"cat\", \"bird\", \"car\", \"tree\", \"chair\"],\n)\n\ndataset.apply_model(model, label_field=\"predictions\")\nsession.refresh()\n</code></pre>"},{"location":"models/model_zoo/models/#deeplabv3-resnet101-coco-torch_1","title":"deeplabv3-resnet101-coco-torch \u00b6","text":"<p>DeepLabV3 model from Rethinking Atrous Convolution for Semantic Image Segmentation with ResNet-101 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>deeplabv3-resnet101-coco-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 233.22 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, resnet, deeplabv3</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"deeplabv3-resnet101-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#deeplabv3-resnet50-coco-torch_1","title":"deeplabv3-resnet50-coco-torch \u00b6","text":"<p>DeepLabV3 model from Rethinking Atrous Convolution for Semantic Image Segmentation with ResNet-50 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>deeplabv3-resnet50-coco-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 160.51 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, resnet, deeplabv3</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"deeplabv3-resnet50-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#densenet121-imagenet-torch_1","title":"densenet121-imagenet-torch \u00b6","text":"<p>Densenet-121 model from Densely Connected Convolutional Networks trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>densenet121-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 30.84 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, densenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"densenet121-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#densenet161-imagenet-torch_1","title":"densenet161-imagenet-torch \u00b6","text":"<p>Densenet-161 model from Densely Connected Convolutional Networks trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>densenet161-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 110.37 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, densenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"densenet161-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#densenet169-imagenet-torch_1","title":"densenet169-imagenet-torch \u00b6","text":"<p>Densenet-169 model from Densely Connected Convolutional Networks trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>densenet169-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 54.71 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, densenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"densenet169-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#densenet201-imagenet-torch_1","title":"densenet201-imagenet-torch \u00b6","text":"<p>Densenet-201 model from Densely Connected Convolutional Networks trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>densenet201-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 77.37 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, densenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"densenet201-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#depth-estimation-transformer-torch_1","title":"depth-estimation-transformer-torch \u00b6","text":"<p>Hugging Face Transformers model for monocular depth estimation.</p> <p>Details</p> <ul> <li> <p>Model name: <code>depth-estimation-transformer-torch</code></p> </li> <li> <p>Model source: https://huggingface.co/docs/transformers/tasks/monocular_depth_estimation</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>depth, torch, transformers</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision, transformers</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"depth-estimation-transformer-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#detection-transformer-torch_1","title":"detection-transformer-torch \u00b6","text":"<p>Hugging Face Transformers model for object detection.</p> <p>Details</p> <ul> <li> <p>Model name: <code>detection-transformer-torch</code></p> </li> <li> <p>Model source: https://huggingface.co/docs/transformers/tasks/object_detection</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>detection, logits, embeddings, torch, transformers</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision, transformers</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"detection-transformer-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#dinov2-vitb14-reg-torch_1","title":"dinov2-vitb14-reg-torch \u00b6","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-B/14 distilled.</p> <p>Details</p> <ul> <li> <p>Model name: <code>dinov2-vitb14-reg-torch</code></p> </li> <li> <p>Model source: https://github.com/facebookresearch/dinov2</p> </li> <li> <p>Model size: 330.35 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>embeddings, torch, dinov2</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"dinov2-vitb14-reg-torch\")\n\nembeddings = dataset.compute_embeddings(model)\n</code></pre>"},{"location":"models/model_zoo/models/#dinov2-vitb14-torch_1","title":"dinov2-vitb14-torch \u00b6","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-B/14 distilled.</p> <p>Details</p> <ul> <li> <p>Model name: <code>dinov2-vitb14-torch</code></p> </li> <li> <p>Model source: https://github.com/facebookresearch/dinov2</p> </li> <li> <p>Model size: 330.33 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>embeddings, torch, dinov2</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"dinov2-vitb14-torch\")\n\nembeddings = dataset.compute_embeddings(model)\n</code></pre>"},{"location":"models/model_zoo/models/#dinov2-vitg14-reg-torch_1","title":"dinov2-vitg14-reg-torch \u00b6","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-g/14.</p> <p>Details</p> <ul> <li> <p>Model name: <code>dinov2-vitg14-reg-torch</code></p> </li> <li> <p>Model source: https://github.com/facebookresearch/dinov2</p> </li> <li> <p>Model size: 4.23 GB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>embeddings, torch, dinov2</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"dinov2-vitg14-reg-torch\")\n\nembeddings = dataset.compute_embeddings(model)\n</code></pre>"},{"location":"models/model_zoo/models/#dinov2-vitg14-torch_1","title":"dinov2-vitg14-torch \u00b6","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-g/14.</p> <p>Details</p> <ul> <li> <p>Model name: <code>dinov2-vitg14-torch</code></p> </li> <li> <p>Model source: https://github.com/facebookresearch/dinov2</p> </li> <li> <p>Model size: 4.23 GB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>embeddings, torch, dinov2</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"dinov2-vitg14-torch\")\n\nembeddings = dataset.compute_embeddings(model)\n</code></pre>"},{"location":"models/model_zoo/models/#dinov2-vitl14-reg-torch_1","title":"dinov2-vitl14-reg-torch \u00b6","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-L/14 distilled.</p> <p>Details</p> <ul> <li> <p>Model name: <code>dinov2-vitl14-reg-torch</code></p> </li> <li> <p>Model source: https://github.com/facebookresearch/dinov2</p> </li> <li> <p>Model size: 1.13 GB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>embeddings, torch, dinov2</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"dinov2-vitl14-reg-torch\")\n\nembeddings = dataset.compute_embeddings(model)\n</code></pre>"},{"location":"models/model_zoo/models/#dinov2-vitl14-torch_1","title":"dinov2-vitl14-torch \u00b6","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-L/14 distilled.</p> <p>Details</p> <ul> <li> <p>Model name: <code>dinov2-vitl14-torch</code></p> </li> <li> <p>Model source: https://github.com/facebookresearch/dinov2</p> </li> <li> <p>Model size: 1.13 GB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>embeddings, torch, dinov2</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"dinov2-vitl14-torch\")\n\nembeddings = dataset.compute_embeddings(model)\n</code></pre>"},{"location":"models/model_zoo/models/#dinov2-vits14-reg-torch_1","title":"dinov2-vits14-reg-torch \u00b6","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-S/14 distilled.</p> <p>Details</p> <ul> <li> <p>Model name: <code>dinov2-vits14-reg-torch</code></p> </li> <li> <p>Model source: https://github.com/facebookresearch/dinov2</p> </li> <li> <p>Model size: 84.20 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>embeddings, torch, dinov2</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"dinov2-vits14-reg-torch\")\n\nembeddings = dataset.compute_embeddings(model)\n</code></pre>"},{"location":"models/model_zoo/models/#dinov2-vits14-torch_1","title":"dinov2-vits14-torch \u00b6","text":"<p>DINOv2: Learning Robust Visual Features without Supervision. Model: ViT-S/14 distilled.</p> <p>Details</p> <ul> <li> <p>Model name: <code>dinov2-vits14-torch</code></p> </li> <li> <p>Model source: https://github.com/facebookresearch/dinov2</p> </li> <li> <p>Model size: 84.19 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>embeddings, torch, dinov2</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"dinov2-vits14-torch\")\n\nembeddings = dataset.compute_embeddings(model)\n</code></pre>"},{"location":"models/model_zoo/models/#faster-rcnn-resnet50-fpn-coco-torch_1","title":"faster-rcnn-resnet50-fpn-coco-torch \u00b6","text":"<p>Faster R-CNN model from Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks with ResNet-50 FPN backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>faster-rcnn-resnet50-fpn-coco-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 159.74 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, faster-rcnn, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"faster-rcnn-resnet50-fpn-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#fcn-resnet101-coco-torch_1","title":"fcn-resnet101-coco-torch \u00b6","text":"<p>FCN model from Fully Convolutional Networks for Semantic Segmentation with ResNet-101 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>fcn-resnet101-coco-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 207.71 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, fcn, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"fcn-resnet101-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#fcn-resnet50-coco-torch_1","title":"fcn-resnet50-coco-torch \u00b6","text":"<p>FCN model from Fully Convolutional Networks for Semantic Segmentation with ResNet-50 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>fcn-resnet50-coco-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 135.01 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, fcn, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"fcn-resnet50-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#googlenet-imagenet-torch_1","title":"googlenet-imagenet-torch \u00b6","text":"<p>GoogLeNet (Inception v1) model from Going Deeper with Convolutions trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>googlenet-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 49.73 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, googlenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>scipy, torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"googlenet-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#inception-v3-imagenet-torch_1","title":"inception-v3-imagenet-torch \u00b6","text":"<p>Inception v3 model from Rethinking the Inception Architecture for Computer Vision trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>inception-v3-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 103.81 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, inception</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>scipy, torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"inception-v3-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#keypoint-rcnn-resnet50-fpn-coco-torch_1","title":"keypoint-rcnn-resnet50-fpn-coco-torch \u00b6","text":"<p>Keypoint R-CNN model from Mask R-CNN with ResNet-50 FPN backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>keypoint-rcnn-resnet50-fpn-coco-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 226.05 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>keypoints, coco, torch, keypoint-rcnn, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"keypoint-rcnn-resnet50-fpn-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#mask-rcnn-resnet50-fpn-coco-torch_1","title":"mask-rcnn-resnet50-fpn-coco-torch \u00b6","text":"<p>Mask R-CNN model from Mask R-CNN with ResNet-50 FPN backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>mask-rcnn-resnet50-fpn-coco-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 169.84 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>instances, coco, torch, mask-rcnn, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"mask-rcnn-resnet50-fpn-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#med-sam-2-video-torch_1","title":"med-sam-2-video-torch \u00b6","text":"<p>Fine-tuned SAM2-hiera-tiny model from Medical SAM 2 - Segment Medical Images as Video via Segment Anything Model 2.</p> <p>Details</p> <ul> <li> <p>Model name: <code>med-sam-2-video-torch</code></p> </li> <li> <p>Model source: https://github.com/MedicineToken/Medical-SAM2</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot, video, med-SAM</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\nfrom fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\"Voxel51/BTCV-CT-as-video-MedSAM2-dataset\")[:2]\n\n# Retaining detections from a single frame in the middle\n# Note that SAM2 only propagates segmentation masks forward in a video\n(\n    dataset\n    .match_frames(F(\"frame_number\") != 100)\n    .set_field(\"frames.gt_detections\", None)\n    .save()\n)\n\nmodel = foz.load_zoo_model(\"med-sam-2-video-torch\")\n\n# Segment inside boxes and propagate to all frames\ndataset.apply_model(\n    model,\n    label_field=\"pred_segmentations\",\n    prompt_field=\"frames.gt_detections\",\n)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#mnasnet05-imagenet-torch_1","title":"mnasnet0.5-imagenet-torch \u00b6","text":"<p>MNASNet model from from MnasNet: Platform-Aware Neural Architecture Search for Mobile with depth multiplier of 0.5 trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>mnasnet0.5-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 8.59 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, mnasnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"mnasnet0.5-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#mnasnet10-imagenet-torch_1","title":"mnasnet1.0-imagenet-torch \u00b6","text":"<p>MNASNet model from MnasNet: Platform-Aware Neural Architecture Search for Mobile with depth multiplier of 1.0 trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>mnasnet1.0-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 16.92 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, mnasnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"mnasnet1.0-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#mobilenet-v2-imagenet-torch_1","title":"mobilenet-v2-imagenet-torch \u00b6","text":"<p>MobileNetV2 model from MobileNetV2: Inverted Residuals and Linear Bottlenecks trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>mobilenet-v2-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 13.55 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, mobilenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#open-clip-torch_1","title":"open-clip-torch \u00b6","text":"<p>OPEN CLIP text/image encoder from Learning Transferable Visual Models From Natural Language Supervision trained on 400M text-image pairs.</p> <p>Details</p> <ul> <li> <p>Model name: <code>open-clip-torch</code></p> </li> <li> <p>Model source: https://github.com/mlfoundations/open_clip</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, logits, embeddings, torch, clip, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision, open_clip_torch</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"open-clip-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n\n#\n# Make zero-shot predictions with custom classes\n#\n\nmodel = foz.load_zoo_model(\n    \"open-clip-torch\",\n    text_prompt=\"A photo of a\",\n    classes=[\"person\", \"dog\", \"cat\", \"bird\", \"car\", \"tree\", \"chair\"],\n)\n\ndataset.apply_model(model, label_field=\"predictions\")\nsession.refresh()\n</code></pre>"},{"location":"models/model_zoo/models/#resnet101-imagenet-torch_1","title":"resnet101-imagenet-torch \u00b6","text":"<p>ResNet-101 model from Deep Residual Learning for Image Recognition trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>resnet101-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 170.45 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"resnet101-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#resnet152-imagenet-torch_1","title":"resnet152-imagenet-torch \u00b6","text":"<p>ResNet-152 model from Deep Residual Learning for Image Recognition trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>resnet152-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 230.34 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"resnet152-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#resnet18-imagenet-torch_1","title":"resnet18-imagenet-torch \u00b6","text":"<p>ResNet-18 model from Deep Residual Learning for Image Recognition trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>resnet18-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 44.66 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"resnet18-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#resnet34-imagenet-torch_1","title":"resnet34-imagenet-torch \u00b6","text":"<p>ResNet-34 model from Deep Residual Learning for Image Recognition trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>resnet34-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 83.26 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"resnet34-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#resnet50-imagenet-torch_1","title":"resnet50-imagenet-torch \u00b6","text":"<p>ResNet-50 model from Deep Residual Learning for Image Recognition trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>resnet50-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 97.75 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"resnet50-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#resnext101-32x8d-imagenet-torch_1","title":"resnext101-32x8d-imagenet-torch \u00b6","text":"<p>ResNeXt-101 32x8d model from Aggregated Residual Transformations for Deep Neural Networks trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>resnext101-32x8d-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 339.59 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, resnext</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"resnext101-32x8d-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#resnext50-32x4d-imagenet-torch_1","title":"resnext50-32x4d-imagenet-torch \u00b6","text":"<p>ResNeXt-50 32x4d model from Aggregated Residual Transformations for Deep Neural Networks trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>resnext50-32x4d-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 95.79 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, resnext</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"resnext50-32x4d-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#retinanet-resnet50-fpn-coco-torch_1","title":"retinanet-resnet50-fpn-coco-torch \u00b6","text":"<p>RetinaNet model from Focal Loss for Dense Object Detection with ResNet-50 FPN backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>retinanet-resnet50-fpn-coco-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 130.27 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, retinanet, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision&gt;=0.8.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"retinanet-resnet50-fpn-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#rtdetr-l-coco-torch_1","title":"rtdetr-l-coco-torch \u00b6","text":"<p>RT-DETR-l model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>rtdetr-l-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/rtdetr/</p> </li> <li> <p>Model size: 63.43 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, transformer, rtdetr</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.2.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"rtdetr-l-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#rtdetr-x-coco-torch_1","title":"rtdetr-x-coco-torch \u00b6","text":"<p>RT-DETR-x model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>rtdetr-x-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/rtdetr/</p> </li> <li> <p>Model size: 129.47 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, transformer, rtdetr</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.2.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"rtdetr-x-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-base-plus-image-torch_1","title":"segment-anything-2-hiera-base-plus-image-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2-hiera-base-plus-image-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2-hiera-base-plus-image-torch\")\n\n# Segment inside boxes\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"ground_truth\",  # can contain Detections or Keypoints\n)\n\n# Full automatic segmentations\ndataset.apply_model(model, label_field=\"auto\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-base-plus-video-torch_1","title":"segment-anything-2-hiera-base-plus-video-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2-hiera-base-plus-video-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot, video</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=2)\n\n# Only retain detections in the first frame\n(\n    dataset\n    .match_frames(F(\"frame_number\") &gt; 1)\n    .set_field(\"frames.detections\", None)\n    .save()\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2-hiera-base-plus-video-torch\")\n\n# Segment inside boxes and propagate to all frames\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"frames.detections\",  # can contain Detections or Keypoints\n)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-large-image-torch_1","title":"segment-anything-2-hiera-large-image-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2-hiera-large-image-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2-hiera-large-image-torch\")\n\n# Segment inside boxes\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"ground_truth\",  # can contain Detections or Keypoints\n)\n\n# Full automatic segmentations\ndataset.apply_model(model, label_field=\"auto\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-large-video-torch_1","title":"segment-anything-2-hiera-large-video-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2-hiera-large-video-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot, video</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=2)\n\n# Only retain detections in the first frame\n(\n    dataset\n    .match_frames(F(\"frame_number\") &gt; 1)\n    .set_field(\"frames.detections\", None)\n    .save()\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2-hiera-large-video-torch\")\n\n# Segment inside boxes and propagate to all frames\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"frames.detections\",  # can contain Detections or Keypoints\n)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-small-image-torch_1","title":"segment-anything-2-hiera-small-image-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2-hiera-small-image-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2-hiera-small-image-torch\")\n\n# Segment inside boxes\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"ground_truth\",  # can contain Detections or Keypoints\n)\n\n# Full automatic segmentations\ndataset.apply_model(model, label_field=\"auto\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-small-video-torch_1","title":"segment-anything-2-hiera-small-video-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2-hiera-small-video-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot, video</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=2)\n\n# Only retain detections in the first frame\n(\n    dataset\n    .match_frames(F(\"frame_number\") &gt; 1)\n    .set_field(\"frames.detections\", None)\n    .save()\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2-hiera-small-video-torch\")\n\n# Segment inside boxes and propagate to all frames\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"frames.detections\",  # can contain Detections or Keypoints\n)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-tiny-image-torch_1","title":"segment-anything-2-hiera-tiny-image-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2-hiera-tiny-image-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2-hiera-tiny-image-torch\")\n\n# Segment inside boxes\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"ground_truth\",  # can contain Detections or Keypoints\n)\n\n# Full automatic segmentations\ndataset.apply_model(model, label_field=\"auto\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-2-hiera-tiny-video-torch_1","title":"segment-anything-2-hiera-tiny-video-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2-hiera-tiny-video-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot, video</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=2)\n\n# Only retain detections in the first frame\n(\n    dataset\n    .match_frames(F(\"frame_number\") &gt; 1)\n    .set_field(\"frames.detections\", None)\n    .save()\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2-hiera-tiny-video-torch\")\n\n# Segment inside boxes and propagate to all frames\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"frames.detections\",  # can contain Detections or Keypoints\n)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-base-plus-image-torch_1","title":"segment-anything-2.1-hiera-base-plus-image-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2.1-hiera-base-plus-image-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2.1-hiera-base-plus-image-torch\")\n\n# Segment inside boxes\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"ground_truth\",  # can contain Detections or Keypoints\n)\n\n# Full automatic segmentations\ndataset.apply_model(model, label_field=\"auto\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-base-plus-video-torch_1","title":"segment-anything-2.1-hiera-base-plus-video-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2.1-hiera-base-plus-video-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot, video</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=2)\n\n# Only retain detections in the first frame\n(\n    dataset\n    .match_frames(F(\"frame_number\") &gt; 1)\n    .set_field(\"frames.detections\", None)\n    .save()\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2.1-hiera-base-plus-video-torch\")\n\n# Segment inside boxes and propagate to all frames\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"frames.detections\",  # can contain Detections or Keypoints\n)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-large-image-torch_1","title":"segment-anything-2.1-hiera-large-image-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2.1-hiera-large-image-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2.1-hiera-large-image-torch\")\n\n# Segment inside boxes\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"ground_truth\",  # can contain Detections or Keypoints\n)\n\n# Full automatic segmentations\ndataset.apply_model(model, label_field=\"auto\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-large-video-torch_1","title":"segment-anything-2.1-hiera-large-video-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2.1-hiera-large-video-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot, video</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=2)\n\n# Only retain detections in the first frame\n(\n    dataset\n    .match_frames(F(\"frame_number\") &gt; 1)\n    .set_field(\"frames.detections\", None)\n    .save()\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2.1-hiera-large-video-torch\")\n\n# Segment inside boxes and propagate to all frames\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"frames.detections\",  # can contain Detections or Keypoints\n)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-small-image-torch_1","title":"segment-anything-2.1-hiera-small-image-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2.1-hiera-small-image-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2.1-hiera-small-image-torch\")\n\n# Segment inside boxes\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"ground_truth\",  # can contain Detections or Keypoints\n)\n\n# Full automatic segmentations\ndataset.apply_model(model, label_field=\"auto\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-small-video-torch_1","title":"segment-anything-2.1-hiera-small-video-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2.1-hiera-small-video-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot, video</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=2)\n\n# Only retain detections in the first frame\n(\n    dataset\n    .match_frames(F(\"frame_number\") &gt; 1)\n    .set_field(\"frames.detections\", None)\n    .save()\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2.1-hiera-small-video-torch\")\n\n# Segment inside boxes and propagate to all frames\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"frames.detections\",  # can contain Detections or Keypoints\n)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-tiny-image-torch_1","title":"segment-anything-2.1-hiera-tiny-image-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2.1-hiera-tiny-image-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2.1-hiera-tiny-image-torch\")\n\n# Segment inside boxes\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"ground_truth\",  # can contain Detections or Keypoints\n)\n\n# Full automatic segmentations\ndataset.apply_model(model, label_field=\"auto\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-21-hiera-tiny-video-torch_1","title":"segment-anything-2.1-hiera-tiny-video-torch \u00b6","text":"<p>Segment Anything Model 2 (SAM2) from SAM2: Segment Anything in Images and Videos.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-2.1-hiera-tiny-video-torch</code></p> </li> <li> <p>Model source: https://ai.meta.com/sam2/</p> </li> <li> <p>Model size: 148.68 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, torch, zero-shot, video</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\ndataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=2)\n\n# Only retain detections in the first frame\n(\n    dataset\n    .match_frames(F(\"frame_number\") &gt; 1)\n    .set_field(\"frames.detections\", None)\n    .save()\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-2.1-hiera-tiny-video-torch\")\n\n# Segment inside boxes and propagate to all frames\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"frames.detections\",  # can contain Detections or Keypoints\n)\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-vitb-torch_1","title":"segment-anything-vitb-torch \u00b6","text":"<p>Segment Anything Model (SAM) from Segment Anything with ViT-B/16 backbone trained on SA-1B.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-vitb-torch</code></p> </li> <li> <p>Model source: https://segment-anything.com</p> </li> <li> <p>Model size: 715.34 KB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, sa-1b, torch, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision, segment-anything</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-vitb-torch\")\n\n# Segment inside boxes\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"ground_truth\",  # can contain Detections or Keypoints\n)\n\n# Full automatic segmentations\ndataset.apply_model(model, label_field=\"auto\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-vith-torch_1","title":"segment-anything-vith-torch \u00b6","text":"<p>Segment Anything Model (SAM) from Segment Anything with ViT-H/16 backbone trained on SA-1B.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-vith-torch</code></p> </li> <li> <p>Model source: https://segment-anything.com</p> </li> <li> <p>Model size: 4.78 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, sa-1b, torch, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision, segment-anything</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-vith-torch\")\n\n# Segment inside boxes\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"ground_truth\",  # can contain Detections or Keypoints\n)\n\n# Full automatic segmentations\ndataset.apply_model(model, label_field=\"auto\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segment-anything-vitl-torch_1","title":"segment-anything-vitl-torch \u00b6","text":"<p>Segment Anything Model (SAM) from Segment Anything with ViT-L/16 backbone trained on SA-1B.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segment-anything-vitl-torch</code></p> </li> <li> <p>Model source: https://segment-anything.com</p> </li> <li> <p>Model size: 2.33 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segment-anything, sa-1b, torch, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision, segment-anything</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"segment-anything-vitl-torch\")\n\n# Segment inside boxes\ndataset.apply_model(\n    model,\n    label_field=\"segmentations\",\n    prompt_field=\"ground_truth\",  # can contain Detections or Keypoints\n)\n\n# Full automatic segmentations\ndataset.apply_model(model, label_field=\"auto\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#segmentation-transformer-torch_1","title":"segmentation-transformer-torch \u00b6","text":"<p>Hugging Face Transformers model for semantic segmentation.</p> <p>Details</p> <ul> <li> <p>Model name: <code>segmentation-transformer-torch</code></p> </li> <li> <p>Model source: https://huggingface.co/docs/transformers/tasks/semantic_segmentation</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, torch, transformers</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision, transformers</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"segmentation-transformer-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#shufflenetv2-05x-imagenet-torch_1","title":"shufflenetv2-0.5x-imagenet-torch \u00b6","text":"<p>ShuffleNetV2 model from ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design with 0.5x output channels trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>shufflenetv2-0.5x-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 5.28 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, shufflenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"shufflenetv2-0.5x-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#shufflenetv2-10x-imagenet-torch_1","title":"shufflenetv2-1.0x-imagenet-torch \u00b6","text":"<p>ShuffleNetV2 model from ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design with 1.0x output channels trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>shufflenetv2-1.0x-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 8.79 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, shufflenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"shufflenetv2-1.0x-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#squeezenet-11-imagenet-torch_1","title":"squeezenet-1.1-imagenet-torch \u00b6","text":"<p>SqueezeNet 1.1 model from the official SqueezeNet repo trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>squeezenet-1.1-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 4.74 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>classification, imagenet, torch, squeezenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"squeezenet-1.1-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#squeezenet-imagenet-torch_1","title":"squeezenet-imagenet-torch \u00b6","text":"<p>SqueezeNet model from SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>squeezenet-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 4.79 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>classification, imagenet, torch, squeezenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"squeezenet-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#vgg11-bn-imagenet-torch_1","title":"vgg11-bn-imagenet-torch \u00b6","text":"<p>VGG-11 model from Very Deep Convolutional Networks for Large-Scale Image Recognition with batch normalization trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>vgg11-bn-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 506.88 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, vgg</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"vgg11-bn-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#vgg11-imagenet-torch_1","title":"vgg11-imagenet-torch \u00b6","text":"<p>VGG-11 model from Very Deep Convolutional Networks for Large-Scale Image Recognition trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>vgg11-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 506.84 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, vgg</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"vgg11-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#vgg13-bn-imagenet-torch_1","title":"vgg13-bn-imagenet-torch \u00b6","text":"<p>VGG-13 model from Very Deep Convolutional Networks for Large-Scale Image Recognition with batch normalization trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>vgg13-bn-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 507.59 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, vgg</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"vgg13-bn-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#vgg13-imagenet-torch_1","title":"vgg13-imagenet-torch \u00b6","text":"<p>VGG-13 model from Very Deep Convolutional Networks for Large-Scale Image Recognition trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>vgg13-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 507.54 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, vgg</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"vgg13-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#vgg16-bn-imagenet-torch_1","title":"vgg16-bn-imagenet-torch \u00b6","text":"<p>VGG-16 model from Very Deep Convolutional Networks for Large-Scale Image Recognition with batch normalization trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>vgg16-bn-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 527.87 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, vgg</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"vgg16-bn-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#vgg16-imagenet-torch_1","title":"vgg16-imagenet-torch \u00b6","text":"<p>VGG-16 model from Very Deep Convolutional Networks for Large-Scale Image Recognition trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>vgg16-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 527.80 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, vgg</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"vgg16-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#vgg19-bn-imagenet-torch_1","title":"vgg19-bn-imagenet-torch \u00b6","text":"<p>VGG-19 model from Very Deep Convolutional Networks for Large-Scale Image Recognition with batch normalization trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>vgg19-bn-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 548.14 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, vgg</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"vgg19-bn-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#vgg19-imagenet-torch_1","title":"vgg19-imagenet-torch \u00b6","text":"<p>VGG-19 model from Very Deep Convolutional Networks for Large-Scale Image Recognition trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>vgg19-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 548.05 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, vgg</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"vgg19-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#wide-resnet101-2-imagenet-torch_1","title":"wide-resnet101-2-imagenet-torch \u00b6","text":"<p>Wide ResNet-101-2 model from Wide Residual Networks trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>wide-resnet101-2-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 242.90 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, wide-resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"wide-resnet101-2-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#wide-resnet50-2-imagenet-torch_1","title":"wide-resnet50-2-imagenet-torch \u00b6","text":"<p>Wide ResNet-50-2 model from Wide Residual Networks trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>wide-resnet50-2-imagenet-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/vision/main/models.html</p> </li> <li> <p>Model size: 131.82 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, torch, wide-resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"wide-resnet50-2-imagenet-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolo-nas-torch_1","title":"yolo-nas-torch \u00b6","text":"<p>YOLO-NAS is an open-source training library for advanced computer vision models. It specializes in accuracy and efficiency, supporting tasks like object detection.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolo-nas-torch</code></p> </li> <li> <p>Model source: https://github.com/Deci-AI/super-gradients</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision, super-gradients</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolo-nas-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolo11l-coco-torch_1","title":"yolo11l-coco-torch \u00b6","text":"<p>YOLO11-L model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolo11l-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov11/</p> </li> <li> <p>Model size: 49.01 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.3.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolo11l-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolo11l-seg-coco-torch_1","title":"yolo11l-seg-coco-torch \u00b6","text":"<p>YOLO11-L Segmentation model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolo11l-seg-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolo11/#__tabbed_1_2</p> </li> <li> <p>Model size: 53.50 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.3.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolo11l-seg-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolo11m-coco-torch_1","title":"yolo11m-coco-torch \u00b6","text":"<p>YOLO11-M model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolo11m-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov11/</p> </li> <li> <p>Model size: 38.80 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.3.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolo11m-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolo11m-seg-coco-torch_1","title":"yolo11m-seg-coco-torch \u00b6","text":"<p>YOLO11-M Segmentation model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolo11m-seg-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolo11/#__tabbed_1_2</p> </li> <li> <p>Model size: 43.30 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.3.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolo11m-seg-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolo11n-coco-torch_1","title":"yolo11n-coco-torch \u00b6","text":"<p>YOLO11-N model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolo11n-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov11/</p> </li> <li> <p>Model size: 5.35 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.3.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolo11n-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolo11n-seg-coco-torch_1","title":"yolo11n-seg-coco-torch \u00b6","text":"<p>YOLO11-N Segmentation model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolo11n-seg-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolo11/#__tabbed_1_2</p> </li> <li> <p>Model size: 5.90 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.3.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolo11n-seg-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolo11s-coco-torch_1","title":"yolo11s-coco-torch \u00b6","text":"<p>YOLO11-S model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolo11s-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov11/</p> </li> <li> <p>Model size: 18.42 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.3.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolo11s-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolo11s-seg-coco-torch_1","title":"yolo11s-seg-coco-torch \u00b6","text":"<p>YOLO11-S Segmentation model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolo11s-seg-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolo11/#__tabbed_1_2</p> </li> <li> <p>Model size: 19.71 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.3.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolo11s-seg-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolo11x-coco-torch_1","title":"yolo11x-coco-torch \u00b6","text":"<p>YOLO11-X model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolo11x-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov11/</p> </li> <li> <p>Model size: 109.33 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.3.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolo11x-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolo11x-seg-coco-torch_1","title":"yolo11x-seg-coco-torch \u00b6","text":"<p>YOLO11-X Segmentation model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolo11x-seg-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolo11/#__tabbed_1_2</p> </li> <li> <p>Model size: 119.30 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.3.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolo11x-seg-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov10l-coco-torch_1","title":"yolov10l-coco-torch \u00b6","text":"<p>YOLOv10-L model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov10l-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov10/</p> </li> <li> <p>Model size: 50.00 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.2.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov10l-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov10m-coco-torch_1","title":"yolov10m-coco-torch \u00b6","text":"<p>YOLOv10-M model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov10m-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov10/</p> </li> <li> <p>Model size: 32.09 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.2.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov10m-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov10n-coco-torch_1","title":"yolov10n-coco-torch \u00b6","text":"<p>YOLOv10-N model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov10n-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov10/</p> </li> <li> <p>Model size: 5.59 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.2.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov10n-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov10s-coco-torch_1","title":"yolov10s-coco-torch \u00b6","text":"<p>YOLOv10-S model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov10s-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov10/</p> </li> <li> <p>Model size: 15.85 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.2.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov10s-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov10x-coco-torch_1","title":"yolov10x-coco-torch \u00b6","text":"<p>YOLOv10-X model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov10x-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov10/</p> </li> <li> <p>Model size: 61.41 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.2.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov10x-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov5l-coco-torch_1","title":"yolov5l-coco-torch \u00b6","text":"<p>Ultralytics YOLOv5l model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov5l-coco-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/hub/ultralytics_yolov5</p> </li> <li> <p>Model size: 192.88 KB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov5l-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov5m-coco-torch_1","title":"yolov5m-coco-torch \u00b6","text":"<p>Ultralytics YOLOv5m model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov5m-coco-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/hub/ultralytics_yolov5</p> </li> <li> <p>Model size: 81.91 KB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov5m-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov5n-coco-torch_1","title":"yolov5n-coco-torch \u00b6","text":"<p>Ultralytics YOLOv5n model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov5n-coco-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/hub/ultralytics_yolov5</p> </li> <li> <p>Model size: 7.75 KB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov5n-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov5s-coco-torch_1","title":"yolov5s-coco-torch \u00b6","text":"<p>Ultralytics YOLOv5s model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov5s-coco-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/hub/ultralytics_yolov5</p> </li> <li> <p>Model size: 28.25 KB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov5s-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov5x-coco-torch_1","title":"yolov5x-coco-torch \u00b6","text":"<p>Ultralytics YOLOv5x model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov5x-coco-torch</code></p> </li> <li> <p>Model source: https://pytorch.org/hub/ultralytics_yolov5</p> </li> <li> <p>Model size: 352.05 KB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov5x-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8l-coco-torch_1","title":"yolov8l-coco-torch \u00b6","text":"<p>Ultralytics YOLOv8l model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8l-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov8/</p> </li> <li> <p>Model size: 83.70 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8l-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8l-obb-dotav1-torch_1","title":"yolov8l-obb-dotav1-torch \u00b6","text":"<p>YOLOv8l Oriented Bounding Box model.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8l-obb-dotav1-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/tasks/obb/</p> </li> <li> <p>Model size: 85.36 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, torch, yolo, polylines, obb</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.1.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8l-obb-dotav1-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8l-oiv7-torch_1","title":"yolov8l-oiv7-torch \u00b6","text":"<p>Ultralytics YOLOv8l model trained Open Images v7.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8l-oiv7-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/datasets/detect/open-images-v7</p> </li> <li> <p>Model size: 83.70 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, oiv7, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8l-oiv7-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8l-seg-coco-torch_1","title":"yolov8l-seg-coco-torch \u00b6","text":"<p>Ultralytics YOLOv8l Segmentation model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8l-seg-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov8/</p> </li> <li> <p>Model size: 88.11 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8l-seg-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8l-world-torch_1","title":"yolov8l-world-torch \u00b6","text":"<p>YOLOv8l-World model.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8l-world-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolo-world/</p> </li> <li> <p>Model size: 91.23 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, torch, yolo, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.1.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8l-world-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8m-coco-torch_1","title":"yolov8m-coco-torch \u00b6","text":"<p>Ultralytics YOLOv8m model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8m-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov8/</p> </li> <li> <p>Model size: 49.70 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8m-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8m-obb-dotav1-torch_1","title":"yolov8m-obb-dotav1-torch \u00b6","text":"<p>YOLOv8m Oriented Bounding Box model.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8m-obb-dotav1-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/tasks/obb/</p> </li> <li> <p>Model size: 50.84 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, torch, yolo, polylines, obb</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.1.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8m-obb-dotav1-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8m-oiv7-torch_1","title":"yolov8m-oiv7-torch \u00b6","text":"<p>Ultralytics YOLOv8m model trained Open Images v7.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8m-oiv7-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/datasets/detect/open-images-v7</p> </li> <li> <p>Model size: 49.70 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, oiv7, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8m-oiv7-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8m-seg-coco-torch_1","title":"yolov8m-seg-coco-torch \u00b6","text":"<p>Ultralytics YOLOv8m Segmentation model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8m-seg-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov8/</p> </li> <li> <p>Model size: 52.36 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8m-seg-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8m-world-torch_1","title":"yolov8m-world-torch \u00b6","text":"<p>YOLOv8m-World model.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8m-world-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolo-world/</p> </li> <li> <p>Model size: 55.89 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, torch, yolo, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.1.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8m-world-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8n-coco-torch_1","title":"yolov8n-coco-torch \u00b6","text":"<p>Ultralytics YOLOv8n model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8n-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov8/</p> </li> <li> <p>Model size: 6.23 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8n-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8n-obb-dotav1-torch_1","title":"yolov8n-obb-dotav1-torch \u00b6","text":"<p>YOLOv8n Oriented Bounding Box model.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8n-obb-dotav1-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/tasks/obb/</p> </li> <li> <p>Model size: 6.24 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, torch, yolo, polylines, obb</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.1.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8n-obb-dotav1-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8n-oiv7-torch_1","title":"yolov8n-oiv7-torch \u00b6","text":"<p>Ultralytics YOLOv8n model trained on Open Images v7.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8n-oiv7-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/datasets/detect/open-images-v7</p> </li> <li> <p>Model size: 6.23 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, oiv7, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8n-oiv7-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8n-seg-coco-torch_1","title":"yolov8n-seg-coco-torch \u00b6","text":"<p>Ultralytics YOLOv8n Segmentation model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8n-seg-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov8/</p> </li> <li> <p>Model size: 6.73 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8n-seg-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8s-coco-torch_1","title":"yolov8s-coco-torch \u00b6","text":"<p>Ultralytics YOLOv8s model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8s-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov8/</p> </li> <li> <p>Model size: 21.53 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8s-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8s-obb-dotav1-torch_1","title":"yolov8s-obb-dotav1-torch \u00b6","text":"<p>YOLOv8s Oriented Bounding Box model.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8s-obb-dotav1-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/tasks/obb/</p> </li> <li> <p>Model size: 22.17 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, torch, yolo, polylines, obb</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.1.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8s-obb-dotav1-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8s-oiv7-torch_1","title":"yolov8s-oiv7-torch \u00b6","text":"<p>Ultralytics YOLOv8s model trained on Open Images v7.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8s-oiv7-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/datasets/detect/open-images-v7</p> </li> <li> <p>Model size: 21.53 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, oiv7, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8s-oiv7-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8s-seg-coco-torch_1","title":"yolov8s-seg-coco-torch \u00b6","text":"<p>Ultralytics YOLOv8s Segmentation model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8s-seg-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov8/</p> </li> <li> <p>Model size: 22.79 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8s-seg-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8s-world-torch_1","title":"yolov8s-world-torch \u00b6","text":"<p>YOLOv8s-World model.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8s-world-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolo-world/</p> </li> <li> <p>Model size: 25.91 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, torch, yolo, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.1.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8s-world-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8x-coco-torch_1","title":"yolov8x-coco-torch \u00b6","text":"<p>Ultralytics YOLOv8x model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8x-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov8/</p> </li> <li> <p>Model size: 130.53 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8x-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8x-obb-dotav1-torch_1","title":"yolov8x-obb-dotav1-torch \u00b6","text":"<p>YOLOv8x Oriented Bounding Box model.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8x-obb-dotav1-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/tasks/obb/</p> </li> <li> <p>Model size: 133.07 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, torch, yolo, polylines, obb</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.1.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8x-obb-dotav1-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8x-oiv7-torch_1","title":"yolov8x-oiv7-torch \u00b6","text":"<p>Ultralytics YOLOv8x model trained Open Images v7.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8x-oiv7-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/datasets/detect/open-images-v7</p> </li> <li> <p>Model size: 130.53 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, oiv7, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8x-oiv7-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8x-seg-coco-torch_1","title":"yolov8x-seg-coco-torch \u00b6","text":"<p>Ultralytics YOLOv8x Segmentation model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8x-seg-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov8/</p> </li> <li> <p>Model size: 137.40 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8x-seg-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov8x-world-torch_1","title":"yolov8x-world-torch \u00b6","text":"<p>YOLOv8x-World model.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov8x-world-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolo-world/</p> </li> <li> <p>Model size: 141.11 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, torch, yolo, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.1.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov8x-world-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov9c-coco-torch_1","title":"yolov9c-coco-torch \u00b6","text":"<p>YOLOv9-C model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov9c-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov9/</p> </li> <li> <p>Model size: 49.40 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.1.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov9c-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov9c-seg-coco-torch_1","title":"yolov9c-seg-coco-torch \u00b6","text":"<p>YOLOv9-C Segmentation model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov9c-seg-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov9/#__tabbed_1_2</p> </li> <li> <p>Model size: 107.20 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.1.42</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov9c-seg-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov9e-coco-torch_1","title":"yolov9e-coco-torch \u00b6","text":"<p>YOLOv9-E model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov9e-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov9/</p> </li> <li> <p>Model size: 112.09 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.1.0</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov9e-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolov9e-seg-coco-torch_1","title":"yolov9e-seg-coco-torch \u00b6","text":"<p>YOLOv9-E Segmentation model trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolov9e-seg-coco-torch</code></p> </li> <li> <p>Model source: https://docs.ultralytics.com/models/yolov9/#__tabbed_1_2</p> </li> <li> <p>Model size: 232.20 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, coco, torch, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch&gt;=1.7.0, torchvision&gt;=0.8.1, ultralytics&gt;=8.1.42</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolov9e-seg-coco-torch\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#zero-shot-classification-transformer-torch_1","title":"zero-shot-classification-transformer-torch \u00b6","text":"<p>Hugging Face Transformers model for zero-shot image classification.</p> <p>Details</p> <ul> <li> <p>Model name: <code>zero-shot-classification-transformer-torch</code></p> </li> <li> <p>Model source: https://huggingface.co/docs/transformers/tasks/zero_shot_image_classification</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, logits, embeddings, torch, transformers, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision, transformers</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\n    \"zero-shot-classification-transformer-torch\",\n    classes=[\"person\", \"dog\", \"cat\", \"bird\", \"car\", \"tree\", \"chair\"],\n)\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#zero-shot-detection-transformer-torch_1","title":"zero-shot-detection-transformer-torch \u00b6","text":"<p>Hugging Face Transformers model for zero-shot object detection.</p> <p>Details</p> <ul> <li> <p>Model name: <code>zero-shot-detection-transformer-torch</code></p> </li> <li> <p>Model source: https://huggingface.co/docs/transformers/tasks/zero_shot_object_detection</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>detection, logits, embeddings, torch, transformers, zero-shot</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>Packages: <code>torch, torchvision, transformers</code></p> </li> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\n    \"zero-shot-detection-transformer-torch\",\n    classes=[\"person\", \"dog\", \"cat\", \"bird\", \"car\", \"tree\", \"chair\"],\n)\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#tensorflow-models","title":"TensorFlow models \u00b6","text":""},{"location":"models/model_zoo/models/#centernet-hg104-1024-coco-tf2_1","title":"centernet-hg104-1024-coco-tf2 \u00b6","text":"<p>CenterNet model from Objects as Points with the Hourglass-104 backbone trained on COCO resized to 1024x1024.</p> <p>Details</p> <ul> <li> <p>Model name: <code>centernet-hg104-1024-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 1.33 GB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, centernet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"centernet-hg104-1024-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#centernet-hg104-512-coco-tf2_1","title":"centernet-hg104-512-coco-tf2 \u00b6","text":"<p>CenterNet model from Objects as Points with the Hourglass-104 backbone trained on COCO resized to 512x512.</p> <p>Details</p> <ul> <li> <p>Model name: <code>centernet-hg104-512-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 1.49 GB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, centernet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"centernet-hg104-512-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#centernet-mobilenet-v2-fpn-512-coco-tf2_1","title":"centernet-mobilenet-v2-fpn-512-coco-tf2 \u00b6","text":"<p>CenterNet model from Objects as Points with the MobileNetV2 backbone trained on COCO resized to 512x512.</p> <p>Details</p> <ul> <li> <p>Model name: <code>centernet-mobilenet-v2-fpn-512-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 41.98 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, centernet, mobilenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"centernet-mobilenet-v2-fpn-512-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#centernet-resnet101-v1-fpn-512-coco-tf2_1","title":"centernet-resnet101-v1-fpn-512-coco-tf2 \u00b6","text":"<p>CenterNet model from Objects as Points with the ResNet-101v1 backbone + FPN trained on COCO resized to 512x512.</p> <p>Details</p> <ul> <li> <p>Model name: <code>centernet-resnet101-v1-fpn-512-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 329.96 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, centernet, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"centernet-resnet101-v1-fpn-512-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#centernet-resnet50-v1-fpn-512-coco-tf2_1","title":"centernet-resnet50-v1-fpn-512-coco-tf2 \u00b6","text":"<p>CenterNet model from Objects as Points with the ResNet-50-v1 backbone + FPN trained on COCO resized to 512x512.</p> <p>Details</p> <ul> <li> <p>Model name: <code>centernet-resnet50-v1-fpn-512-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 194.61 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, centernet, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"centernet-resnet50-v1-fpn-512-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#centernet-resnet50-v2-512-coco-tf2_1","title":"centernet-resnet50-v2-512-coco-tf2 \u00b6","text":"<p>CenterNet model from Objects as Points with the ResNet-50v2 backbone trained on COCO resized to 512x512.</p> <p>Details</p> <ul> <li> <p>Model name: <code>centernet-resnet50-v2-512-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 226.95 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, centernet, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"centernet-resnet50-v2-512-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#deeplabv3-cityscapes-tf_1","title":"deeplabv3-cityscapes-tf \u00b6","text":"<p>DeepLabv3+ semantic segmentation model from Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation with Xception backbone trained on the Cityscapes dataset.</p> <p>Details</p> <ul> <li> <p>Model name: <code>deeplabv3-cityscapes-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md</p> </li> <li> <p>Model size: 158.04 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, cityscapes, tf, deeplabv3</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"deeplabv3-cityscapes-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#deeplabv3-mnv2-cityscapes-tf_1","title":"deeplabv3-mnv2-cityscapes-tf \u00b6","text":"<p>DeepLabv3+ semantic segmentation model from Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation with MobileNetV2 backbone trained on the Cityscapes dataset.</p> <p>Details</p> <ul> <li> <p>Model name: <code>deeplabv3-mnv2-cityscapes-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md</p> </li> <li> <p>Model size: 8.37 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>segmentation, cityscapes, tf, deeplabv3</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"deeplabv3-mnv2-cityscapes-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d0-512-coco-tf2_1","title":"efficientdet-d0-512-coco-tf2 \u00b6","text":"<p>EfficientDet-D0 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO resized to 512x512.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d0-512-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 29.31 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d0-512-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d0-coco-tf1_1","title":"efficientdet-d0-coco-tf1 \u00b6","text":"<p>EfficientDet-D0 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d0-coco-tf1</code></p> </li> <li> <p>Model source: https://github.com/voxel51/automl/tree/master/efficientdet</p> </li> <li> <p>Model size: 38.20 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf1, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=1.14,&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=1.14,&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d0-coco-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d1-640-coco-tf2_1","title":"efficientdet-d1-640-coco-tf2 \u00b6","text":"<p>EfficientDet-D1 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO resized to 640x640.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d1-640-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 49.44 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d1-640-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d1-coco-tf1_1","title":"efficientdet-d1-coco-tf1 \u00b6","text":"<p>EfficientDet-D1 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d1-coco-tf1</code></p> </li> <li> <p>Model source: https://github.com/voxel51/automl/tree/master/efficientdet</p> </li> <li> <p>Model size: 61.64 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf1, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=1.14,&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=1.14,&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d1-coco-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d2-768-coco-tf2_1","title":"efficientdet-d2-768-coco-tf2 \u00b6","text":"<p>EfficientDet-D2 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO resized to 768x768.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d2-768-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 60.01 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d2-768-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d2-coco-tf1_1","title":"efficientdet-d2-coco-tf1 \u00b6","text":"<p>EfficientDet-D2 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d2-coco-tf1</code></p> </li> <li> <p>Model source: https://github.com/voxel51/automl/tree/master/efficientdet</p> </li> <li> <p>Model size: 74.00 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf1, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=1.14,&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=1.14,&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d2-coco-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d3-896-coco-tf2_1","title":"efficientdet-d3-896-coco-tf2 \u00b6","text":"<p>EfficientDet-D3 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO resized to 896x896.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d3-896-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 88.56 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d3-896-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d3-coco-tf1_1","title":"efficientdet-d3-coco-tf1 \u00b6","text":"<p>EfficientDet-D3 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d3-coco-tf1</code></p> </li> <li> <p>Model source: https://github.com/voxel51/automl/tree/master/efficientdet</p> </li> <li> <p>Model size: 106.44 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf1, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=1.14,&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=1.14,&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d3-coco-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d4-1024-coco-tf2_1","title":"efficientdet-d4-1024-coco-tf2 \u00b6","text":"<p>EfficientDet-D4 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO resized to 1024x1024.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d4-1024-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 151.15 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d4-1024-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d4-coco-tf1_1","title":"efficientdet-d4-coco-tf1 \u00b6","text":"<p>EfficientDet-D4 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d4-coco-tf1</code></p> </li> <li> <p>Model source: https://github.com/voxel51/automl/tree/master/efficientdet</p> </li> <li> <p>Model size: 175.33 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf1, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=1.14,&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=1.14,&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d4-coco-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d5-1280-coco-tf2_1","title":"efficientdet-d5-1280-coco-tf2 \u00b6","text":"<p>EfficientDet-D5 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO resized to 1280x1280.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d5-1280-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 244.41 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d5-1280-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d5-coco-tf1_1","title":"efficientdet-d5-coco-tf1 \u00b6","text":"<p>EfficientDet-D5 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d5-coco-tf1</code></p> </li> <li> <p>Model source: https://github.com/voxel51/automl/tree/master/efficientdet</p> </li> <li> <p>Model size: 275.81 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf1, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=1.14,&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=1.14,&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d5-coco-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d6-1280-coco-tf2_1","title":"efficientdet-d6-1280-coco-tf2 \u00b6","text":"<p>EfficientDet-D6 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO resized to 1280x1280.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d6-1280-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 375.63 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d6-1280-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d6-coco-tf1_1","title":"efficientdet-d6-coco-tf1 \u00b6","text":"<p>EfficientDet-D6 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d6-coco-tf1</code></p> </li> <li> <p>Model source: https://github.com/voxel51/automl/tree/master/efficientdet</p> </li> <li> <p>Model size: 416.43 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf1, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=1.14,&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=1.14,&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d6-coco-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#efficientdet-d7-1536-coco-tf2_1","title":"efficientdet-d7-1536-coco-tf2 \u00b6","text":"<p>EfficientDet-D7 model from EfficientDet: Scalable and Efficient Object Detection trained on COCO resized to 1536x1536.</p> <p>Details</p> <ul> <li> <p>Model name: <code>efficientdet-d7-1536-coco-tf2</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 376.20 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, efficientdet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"efficientdet-d7-1536-coco-tf2\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#faster-rcnn-inception-resnet-atrous-v2-coco-tf_1","title":"faster-rcnn-inception-resnet-atrous-v2-coco-tf \u00b6","text":"<p>Faster R-CNN model from Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks atrous version with Inception backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>faster-rcnn-inception-resnet-atrous-v2-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 234.46 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, faster-rcnn, inception, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"faster-rcnn-inception-resnet-atrous-v2-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#faster-rcnn-inception-resnet-atrous-v2-lowproposals-coco-tf_1","title":"faster-rcnn-inception-resnet-atrous-v2-lowproposals-coco-tf \u00b6","text":"<p>Faster R-CNN model from Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks atrous version with low-proposals and Inception backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>faster-rcnn-inception-resnet-atrous-v2-lowproposals-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 234.46 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, faster-rcnn, inception, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"faster-rcnn-inception-resnet-atrous-v2-lowproposals-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#faster-rcnn-inception-v2-coco-tf_1","title":"faster-rcnn-inception-v2-coco-tf \u00b6","text":"<p>Faster R-CNN model from Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks with Inception v2 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>faster-rcnn-inception-v2-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 52.97 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, faster-rcnn, inception</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"faster-rcnn-inception-v2-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#faster-rcnn-nas-coco-tf_1","title":"faster-rcnn-nas-coco-tf \u00b6","text":"<p>Faster R-CNN model from Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks with NAS-net backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>faster-rcnn-nas-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 404.95 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, faster-rcnn</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"faster-rcnn-nas-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#faster-rcnn-nas-lowproposals-coco-tf_1","title":"faster-rcnn-nas-lowproposals-coco-tf \u00b6","text":"<p>Faster R-CNN model from Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks with low-proposals and NAS-net backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>faster-rcnn-nas-lowproposals-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 404.88 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, faster-rcnn</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"faster-rcnn-nas-lowproposals-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#faster-rcnn-resnet101-coco-tf_1","title":"faster-rcnn-resnet101-coco-tf \u00b6","text":"<p>Faster R-CNN model from Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks with ResNet-101 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>faster-rcnn-resnet101-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 186.41 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, faster-rcnn, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"faster-rcnn-resnet101-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#faster-rcnn-resnet101-lowproposals-coco-tf_1","title":"faster-rcnn-resnet101-lowproposals-coco-tf \u00b6","text":"<p>Faster R-CNN model from Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks with low-proposals and ResNet-101 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>faster-rcnn-resnet101-lowproposals-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 186.41 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, faster-rcnn, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"faster-rcnn-resnet101-lowproposals-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#faster-rcnn-resnet50-coco-tf_1","title":"faster-rcnn-resnet50-coco-tf \u00b6","text":"<p>Faster R-CNN model from Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks with ResNet-50 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>faster-rcnn-resnet50-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 113.57 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, faster-rcnn, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"faster-rcnn-resnet50-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#faster-rcnn-resnet50-lowproposals-coco-tf_1","title":"faster-rcnn-resnet50-lowproposals-coco-tf \u00b6","text":"<p>Faster R-CNN model from Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks with low-proposals and ResNet-50 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>faster-rcnn-resnet50-lowproposals-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 113.57 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, faster-rcnn, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"faster-rcnn-resnet50-lowproposals-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#inception-resnet-v2-imagenet-tf1_1","title":"inception-resnet-v2-imagenet-tf1 \u00b6","text":"<p>Inception v2 model from Rethinking the Inception Architecture for Computer Vision trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>inception-resnet-v2-imagenet-tf1</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/tree/archive/research/slim#pre-trained-models</p> </li> <li> <p>Model size: 213.81 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, tf1, inception, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"inception-resnet-v2-imagenet-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#inception-v4-imagenet-tf1_1","title":"inception-v4-imagenet-tf1 \u00b6","text":"<p>Inception v4 model from Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>inception-v4-imagenet-tf1</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/tree/archive/research/slim#pre-trained-models</p> </li> <li> <p>Model size: 163.31 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, tf1, inception</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"inception-v4-imagenet-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#mask-rcnn-inception-resnet-v2-atrous-coco-tf_1","title":"mask-rcnn-inception-resnet-v2-atrous-coco-tf \u00b6","text":"<p>Mask R-CNN model from Mask R-CNN atrous version with Inception backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>mask-rcnn-inception-resnet-v2-atrous-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 254.51 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>instances, coco, tf, mask-rcnn, inception, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"mask-rcnn-inception-resnet-v2-atrous-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#mask-rcnn-inception-v2-coco-tf_1","title":"mask-rcnn-inception-v2-coco-tf \u00b6","text":"<p>Mask R-CNN model from Mask R-CNN with Inception backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>mask-rcnn-inception-v2-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 64.03 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>instances, coco, tf, mask-rcnn, inception</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"mask-rcnn-inception-v2-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#mask-rcnn-resnet101-atrous-coco-tf_1","title":"mask-rcnn-resnet101-atrous-coco-tf \u00b6","text":"<p>Mask R-CNN model from Mask R-CNN atrous version with ResNet-101 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>mask-rcnn-resnet101-atrous-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 211.56 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>instances, coco, tf, mask-rcnn, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"mask-rcnn-resnet101-atrous-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#mask-rcnn-resnet50-atrous-coco-tf_1","title":"mask-rcnn-resnet50-atrous-coco-tf \u00b6","text":"<p>Mask R-CNN model from Mask R-CNN atrous version with ResNet-50 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>mask-rcnn-resnet50-atrous-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 138.29 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>instances, coco, tf, mask-rcnn, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"mask-rcnn-resnet50-atrous-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#mobilenet-v2-imagenet-tf1_1","title":"mobilenet-v2-imagenet-tf1 \u00b6","text":"<p>MobileNetV2 model from MobileNetV2: Inverted Residuals and Linear Bottlenecks trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>mobilenet-v2-imagenet-tf1</code></p> </li> <li> <p>Model source: None</p> </li> <li> <p>Model size: 13.64 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, tf1, mobilenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"mobilenet-v2-imagenet-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#resnet-v1-50-imagenet-tf1_1","title":"resnet-v1-50-imagenet-tf1 \u00b6","text":"<p>ResNet-50 v1 model from Deep Residual Learning for Image Recognition trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>resnet-v1-50-imagenet-tf1</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/tree/archive/research/slim#pre-trained-models</p> </li> <li> <p>Model size: 97.84 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, tf1, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"resnet-v1-50-imagenet-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#resnet-v2-50-imagenet-tf1_1","title":"resnet-v2-50-imagenet-tf1 \u00b6","text":"<p>ResNet-50 v2 model from Deep Residual Learning for Image Recognition trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>resnet-v2-50-imagenet-tf1</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/tree/archive/research/slim#pre-trained-models</p> </li> <li> <p>Model size: 97.86 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, tf1, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"resnet-v2-50-imagenet-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#rfcn-resnet101-coco-tf_1","title":"rfcn-resnet101-coco-tf \u00b6","text":"<p>R-FCN object detection model from R-FCN: Object Detection via Region-based Fully Convolutional Networks with ResNet-101 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>rfcn-resnet101-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 208.16 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, rfcn, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"rfcn-resnet101-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#ssd-inception-v2-coco-tf_1","title":"ssd-inception-v2-coco-tf \u00b6","text":"<p>Inception Single Shot Detector model from SSD: Single Shot MultiBox Detector trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>ssd-inception-v2-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 97.50 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, ssd, inception</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"ssd-inception-v2-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#ssd-mobilenet-v1-coco-tf_1","title":"ssd-mobilenet-v1-coco-tf \u00b6","text":"<p>Single Shot Detector model from SSD: Single Shot MultiBox Detector with MobileNetV1 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>ssd-mobilenet-v1-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 27.83 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, ssd, mobilenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"ssd-mobilenet-v1-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#ssd-mobilenet-v1-fpn-640-coco17_1","title":"ssd-mobilenet-v1-fpn-640-coco17 \u00b6","text":"<p>MobileNetV1 model from MobileNetV2: Inverted Residuals and Linear Bottlenecks resized to 640x640.</p> <p>Details</p> <ul> <li> <p>Model name: <code>ssd-mobilenet-v1-fpn-640-coco17</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 43.91 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, ssd, mobilenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"ssd-mobilenet-v1-fpn-640-coco17\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#ssd-mobilenet-v1-fpn-coco-tf_1","title":"ssd-mobilenet-v1-fpn-coco-tf \u00b6","text":"<p>FPN Single Shot Detector model from SSD: Single Shot MultiBox Detector with MobileNetV1 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>ssd-mobilenet-v1-fpn-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 48.97 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, ssd, mobilenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"ssd-mobilenet-v1-fpn-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#ssd-mobilenet-v2-320-coco17_1","title":"ssd-mobilenet-v2-320-coco17 \u00b6","text":"<p>MobileNetV2 model from MobileNetV2: Inverted Residuals and Linear Bottlenecks resized to 320x320.</p> <p>Details</p> <ul> <li> <p>Model name: <code>ssd-mobilenet-v2-320-coco17</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf2_detection_zoo.md</p> </li> <li> <p>Model size: 43.91 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf2, ssd, mobilenet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&gt;=2|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&gt;=2|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"ssd-mobilenet-v2-320-coco17\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#ssd-resnet50-fpn-coco-tf_1","title":"ssd-resnet50-fpn-coco-tf \u00b6","text":"<p>FPN Single Shot Detector model from SSD: Single Shot MultiBox Detector with ResNet-50 backbone trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>ssd-resnet50-fpn-coco-tf</code></p> </li> <li> <p>Model source: https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md</p> </li> <li> <p>Model size: 128.07 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf, ssd, resnet</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow|tensorflow-macos</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu|tensorflow&gt;=2|tensorflow-macos</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"ssd-resnet50-fpn-coco-tf\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#vgg16-imagenet-tf1_1","title":"vgg16-imagenet-tf1 \u00b6","text":"<p>VGG-16 model from Very Deep Convolutional Networks for Large-Scale Image Recognition trained on ImageNet.</p> <p>Details</p> <ul> <li> <p>Model name: <code>vgg16-imagenet-tf1</code></p> </li> <li> <p>Model source: https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md</p> </li> <li> <p>Model size: 527.80 MB</p> </li> <li> <p>Exposes embeddings? yes</p> </li> <li> <p>Tags: <code>classification, embeddings, logits, imagenet, tf1, vgg</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"imagenet-sample\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"vgg16-imagenet-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/models/#yolo-v2-coco-tf1_1","title":"yolo-v2-coco-tf1 \u00b6","text":"<p>YOLOv2 model from YOLO9000: Better, Faster, Stronger trained on COCO.</p> <p>Details</p> <ul> <li> <p>Model name: <code>yolo-v2-coco-tf1</code></p> </li> <li> <p>Model source: https://github.com/thtrieu/darkflow</p> </li> <li> <p>Model size: 194.49 MB</p> </li> <li> <p>Exposes embeddings? no</p> </li> <li> <p>Tags: <code>detection, coco, tf1, yolo</code></p> </li> </ul> <p>Requirements</p> <ul> <li> <p>CPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow&lt;2</code></p> </li> <li> <p>GPU support</p> </li> <li> <p>yes</p> </li> <li> <p>Packages: <code>tensorflow-gpu&lt;2</code></p> </li> </ul> <p>Example usage</p> <pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=fo.get_default_dataset_name(),\n    max_samples=50,\n    shuffle=True,\n)\n\nmodel = foz.load_zoo_model(\"yolo-v2-coco-tf1\")\n\ndataset.apply_model(model, label_field=\"predictions\")\n\nsession = fo.launch_app(dataset)\n</code></pre>"},{"location":"models/model_zoo/remote/","title":"Remotely-Sourced Zoo Models \u00b6","text":"<p>This page describes how to work with and create zoo models whose definitions are hosted via GitHub repositories or public URLs.</p> <p>Note</p> <p>To download from a private GitHub repository that you have access to, provide your GitHub personal access token by setting the <code>GITHUB_TOKEN</code> environment variable.</p>"},{"location":"models/model_zoo/remote/#working-with-remotely-sourced-models","title":"Working with remotely-sourced models \u00b6","text":"<p>Working with remotely-sourced zoo models is just like built-in zoo models, as both varieties support the full zoo API.</p> <p>When specifying remote sources, you can provide any of the following:</p> <ul> <li> <p>A GitHub repo URL like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;</code></p> </li> <li> <p>A GitHub ref like <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/tree/&lt;branch&gt;</code> or <code>https://github.com/&lt;user&gt;/&lt;repo&gt;/commit/&lt;commit&gt;</code></p> </li> <li> <p>A GitHub ref string like <code>&lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;]</code></p> </li> <li> <p>A publicly accessible URL of an archive (eg zip or tar) file</p> </li> </ul> <p>Here\u2019s the basic recipe for working with remotely-sourced zoo models:</p>"},{"location":"models/model_zoo/remote/#creating-remotely-sourced-models","title":"Creating remotely-sourced models \u00b6","text":"<p>A remote source of models is defined by a directory with the following contents:</p> <pre><code>manifest.json\n__init__.py\n    def download_model(model_name, model_path):\n        pass\n\n    def load_model(model_name, model_path, **kwargs):\n        pass\n</code></pre> <p>Each component is described in detail below.</p> <p>Note</p> <p>By convention, model sources also contain an optional <code>README.md</code> file that provides additional information about the models that it contains and example syntaxes for downloading and working with them.</p>"},{"location":"models/model_zoo/remote/#manifestjson","title":"manifest.json \u00b6","text":"<p>The remote source\u2019s <code>manifest.json</code> file defines relevant metadata about the model(s) that it contains:</p> Field Required? Description <code>base_name</code> yes The base name of the model (no version info) <code>base_filename</code> The base filename or directory of the model (no version info), if applicable.This is required in order for<code>list_downloaded_zoo_models()</code>to detect the model and <code>delete_zoo_model()</code>to delete the local copy if it is downloaded <code>author</code> The author of the model <code>version</code> The version of the model (if applicable).If a version is provided, then users can refer to a specific version of the model byappending <code>@&lt;ver&gt;</code> to its name when using methods like<code>load_zoo_model()</code>, otherwise the latestversion of the model is loaded by default <code>url</code> The URL at which the model is hosted <code>license</code> The license under which the model is distributed <code>source</code> The original source of the model <code>description</code> A brief description of the model <code>tags</code> A list of tags for the model. Useful in conjunction with<code>list_zoo_models()</code> <code>size_bytes</code> The size of the model on disk <code>date_added</code> The time that the model was added to the source <code>requirements</code> JSON description of the model\u2019s package/runtime requirements <code>manager</code> A <code>fiftyone.core.models.ModelManagerConfig</code> dict that describes the remotelocation of the model and how to download it. If this is not provided, then adownload_model() function must be provided <code>default_deployment_config_dict</code> A <code>fiftyone.core.models.ModelConfig</code> dict describing how to load the model. Ifthis is not provided, then a load_model() functionmust be provided <p>It can also provide optional metadata about the remote source itself:</p> Field Required? Description <code>name</code> A name for the remote model source <code>url</code> The URL of the remote model source <p>Here\u2019s an exaxmple model manifest file that declares a single model:</p> <pre><code>{\n    \"name\": \"voxel51/openai-clip\",\n    \"url\": \"https://github.com/voxel51/openai-clip\",\n    \"models\": [\\\n        {\\\n            \"base_name\": \"voxel51/clip-vit-base32-torch\",\\\n            \"base_filename\": \"CLIP-ViT-B-32.pt\",\\\n            \"author\": \"OpenAI\",\\\n            \"license\": \"MIT\",\\\n            \"source\": \"https://github.com/openai/CLIP\",\\\n            \"description\": \"CLIP text/image encoder from Learning Transferable Visual Models From Natural Language Supervision (https://arxiv.org/abs/2103.00020) trained on 400M text-image pairs\",\\\n            \"tags\": [\\\n                \"classification\",\\\n                \"logits\",\\\n                \"embeddings\",\\\n                \"torch\",\\\n                \"clip\",\\\n                \"zero-shot\"\\\n            ],\\\n            \"size_bytes\": 353976522,\\\n            \"date_added\": \"2022-04-12 17:49:51\",\\\n            \"requirements\": {\\\n                \"packages\": [\"torch\", \"torchvision\"],\\\n                \"cpu\": {\\\n                    \"support\": true\\\n                },\\\n                \"gpu\": {\\\n                    \"support\": true\\\n                }\\\n            }\\\n        }\\\n    ]\n}\n</code></pre>"},{"location":"models/model_zoo/remote/#download-model","title":"Download model \u00b6","text":"<p>If a remote source contains model(s) that don\u2019t use the <code>manager</code> key in its manifest, then it must contain an <code>__init__.py</code> file that defines a <code>download_model()</code> method with the signature below:</p> <pre><code>def download_model(model_name, model_path):\n    \"\"\"Downloads the model.\n\n    Args:\n        model_name: the name of the model to download, as declared by the\n            ``base_name`` and optional ``version`` fields of the manifest\n        model_path: the absolute filename or directory to which to download the\n            model, as declared by the ``base_filename`` field of the manifest\n    \"\"\"\n\n    # Determine where to download `model_name` from\n    url = ...\n\n    # Download `url` to `model_path`\n    ...\n</code></pre> <p>This method is called under-the-hood when a user calls <code>download_zoo_model()</code> or <code>load_zoo_model()</code>, and its job is to download any relevant files from the web and organize and/or prepare them as necessary at the provided path.</p>"},{"location":"models/model_zoo/remote/#load-model","title":"Load model \u00b6","text":"<p>If a remote source contains model(s) that don\u2019t use the <code>default_deployment_config_dict</code> key in its manifest, then it must contain an <code>__init__.py</code> file that defines a <code>load_model()</code> method with the signature below:</p> <pre><code>def load_model(model_name, model_path, **kwargs):\n    \"\"\"Loads the model.\n\n    Args:\n        model_name: the name of the model to load, as declared by the\n            ``base_name`` and optional ``version`` fields of the manifest\n        model_path: the absolute filename or directory to which the model was\n            donwloaded, as declared by the ``base_filename`` field of the\n            manifest\n        **kwargs: optional keyword arguments that configure how the model\n            is loaded\n\n    Returns:\n        a :class:`fiftyone.core.models.Model`\n    \"\"\"\n\n    # The directory containing this file\n    model_dir = os.path.dirname(model_path)\n\n    # Consturct the specified `Model` instance, generally by importing\n    # other modules in `model_dir`\n    model = ...\n\n    return model\n</code></pre> <p>This method\u2019s job is to load the <code>Model</code> instance for the specified model whose associated weights are stored at the provided path.</p> <p>Note</p> <p>Refer to this page for more information about wrapping models in the <code>Model</code> interface.</p> <p>Remotely-sourced models can optionally support customized loading by accepting optional keyword arguments to their <code>load_model()</code> method.</p> <p>When <code>load_zoo_model(name_or_url, ..., **kwargs)</code> is called, any <code>kwargs</code> are passed through to <code>load_model(..., **kwargs)</code>.</p> <p>Note</p> <p>Check out voxel51/openai-clip for an example of a remote model source.</p>"},{"location":"plugins/","title":"Plugins Overview \u00b6","text":"<p>FiftyOne provides a powerful plugin framework that allows for extending and customizing the functionality of the tool to suit your specific needs.</p> <p>With plugins, you can add new functionality to the FiftyOne App, create integrations with other tools and APIs, render custom panels, and add custom actions to menus.</p> <p>With FiftyOne Teams, you can even write plugins that allow users to execute long-running tasks from within the App that run on a connected compute cluster.</p> <p>Get started with plugins by installing some popular plugins, then try your hand at writing your own!</p> <p>Note</p> <p>Check out the FiftyOne plugins repository for a growing collection of plugins that you can easily download and use locally.</p>"},{"location":"plugins/#getting-started","title":"Getting started \u00b6","text":"<p>What can plugins do for you? Get started by installing any of these plugins available in the FiftyOne Plugins repository:</p> @voxel51/annotation \u270f\ufe0f Utilities for integrating FiftyOne with annotation tools @voxel51/brain \ud83e\udde0 Utilities for working with the FiftyOne Brain @voxel51/dashboard \ud83d\udcca Create your own custom dashboards from within the App @voxel51/evaluation \u2705 Utilities for evaluating models with FiftyOne @voxel51/io \ud83d\udcc1 A collection of import/export utilities @voxel51/indexes \ud83d\udcc8 Utilities for working with FiftyOne database indexes @voxel51/runs \ud83d\udcc8 Utilities for working with custom runs @voxel51/utils \u2692\ufe0f Call your favorite SDK utilities from the App @voxel51/voxelgpt \ud83e\udd16 An AI assistant that can query visual datasets, search the FiftyOne docs, and answer general computer vision questions @voxel51/zoo \ud83c\udf0e Download datasets and run inference with models from the FiftyOne Zoo, all without leaving the App <p>For example, do you wish you could import data from within the App? With the @voxel51/io, plugin, you can!</p> <p></p> <p>Want to send data for annotation from within the App? Sure thing! Just install the @voxel51/annotation plugin:</p> <p></p> <p>Have model predictions on your dataset that you want to evaluate? The @voxel51/evaluation plugin makes it easy:</p> <p></p> <p>Need to compute embedding for your dataset? Kick off the task with the @voxel51/brain plugin and proceed with other work while the execution happens in the background:</p> <p></p> <p>Want to create a custom dashboard that displays statistics of interest about the current dataset? Just install the @voxel51/dashboard plugin and build away:</p> <p></p> <p>Note</p> <p>When you choose delegated execution in the App, these tasks are automatically scheduled for execution on your connected orchestrator and you can continue with other work!</p> <p>FiftyOne also includes a number of builtin features that are implemented as plugins. For example, Panels are miniature full-featured data applications that you can open in App Spaces and interactively manipulate to explore your dataset and update/respond to updates from other spaces that are currently open in the App.</p> <p>Does your dataset have geolocation data? Use the Map panel to view it:</p> <p></p> <p>Want to visualize embeddings in the App? Just open the Embeddings panel:</p> <p></p> <p>Note</p> <p>Look interesting? Learn how to develop your own plugins!</p>"},{"location":"plugins/developing_plugins/","title":"Developing Plugins \u00b6","text":"<p>This page describes how to write your own FiftyOne plugins.</p> <p>Note</p> <p>Check out the FiftyOne plugins repository for a growing collection of plugins that you can use as examples when developing your own.</p>"},{"location":"plugins/developing_plugins/#design-overview","title":"Design overview \u00b6","text":"<p>Plugins are composed of one or more panels, operators, and components.</p> <p>Together these building blocks enable you to build full-featured interactive data applications that tailor FiftyOne to your specific use case and workflow. Whether you\u2019re working with images, videos, or other data types, a plugin can help you streamline your machine learning workflows and co-develop your data and models.</p> <p></p>"},{"location":"plugins/developing_plugins/#plugin-types","title":"Plugin types \u00b6","text":"<p>FiftyOne plugins can be written in Python or JavaScript (JS), or a combination of both.</p> <p>Python plugins are built using the <code>fiftyone</code> package, pip packages, and your own Python. They can consist of panels and operators.</p> <p>JS plugins are built using the <code>@fiftyone</code> TypeScript packages, npm packages, and your own TypeScript. They can consist of panels, operators, and custom components.</p>"},{"location":"plugins/developing_plugins/#panels","title":"Panels \u00b6","text":"<p>Panels are miniature full-featured data applications that you can open in App spaces and interactively manipulate to explore your dataset and update/respond to updates from other spaces that are currently open in the App.</p> <p>FiftyOne natively includes the following Panels:</p> <ul> <li> <p>Samples panel: the media grid that loads by default when you launch the App</p> </li> <li> <p>Histograms panel: a dashboard of histograms for the fields of your dataset</p> </li> <li> <p>Embeddings panel: a canvas for working with embeddings visualizations</p> </li> <li> <p>Map panel: visualizes the geolocation data of datasets that have a <code>GeoLocation</code> field</p> </li> </ul> <p></p> <p>Note</p> <p>Jump to this section for more information about developing panels.</p>"},{"location":"plugins/developing_plugins/#operators","title":"Operators \u00b6","text":"<p>Operators are user-facing operations that allow you to interact with the data in your dataset. They can range from simple actions like checking a checkbox to more complex workflows such as requesting annotation of samples from a configurable backend. Operators can even be composed of other operators or be used to add functionality to custom panels.</p> <p>FiftyOne comes with a number of builtin <code>Python</code> and JavaScript operators for common tasks that are intended for either user-facing or internal plugin use.</p> <p></p> <p>Note</p> <p>Jump to this section for more information about developing operators.</p>"},{"location":"plugins/developing_plugins/#components","title":"Components \u00b6","text":"<p>Components are responsible for rendering and event handling in plugins. They provide the necessary functionality to display and interact with your plugin in the FiftyOne App. Components also implement form inputs and output rendering for operators, making it possible to customize the way an operator is rendered in the FiftyOne App.</p> <p>For example, FiftyOne comes with a wide variety of <code>builtin types</code> that you can leverage to build complex input and output forms for your operators.</p> <p></p> <p>Note</p> <p>Jump to this section for more information about developing components.</p>"},{"location":"plugins/developing_plugins/#development-setup","title":"Development setup \u00b6","text":"<p>In order to develop Python plugins, you can use either a release or source install of FiftyOne:</p> <pre><code>pip install fiftyone\n</code></pre> <p>In order to develop JS plugins, you will need a source install of FiftyOne and a vite config that links modules to your <code>fiftyone/app</code> directory.</p> <p>Note</p> <p>For JS plugins we recommend forking the FiftyOne Hello World JS Example repository and following the conventions there to build your JS plugin.</p>"},{"location":"plugins/developing_plugins/#anatomy-of-a-plugin","title":"Anatomy of a plugin \u00b6","text":"<p>FiftyOne recognizes plugins by searching for <code>fiftyone.yml</code> or <code>fiftyone.yaml</code> files within your plugins directory.</p> <p>Below is an example of a plugin directory with a typical Python plugin and JS plugin:</p> <pre><code>/path/to/your/plugins/dir/\n    my-js-plugin/\n        fiftyone.yml\n        package.json\n        dist/\n            index.umd.js\n    my-py-plugin/\n        fiftyone.yml\n        __init__.py\n        requirements.txt\n</code></pre> <p>Note</p> <p>If the source code for a plugin already exists on disk, you can make it into a plugin using <code>create_plugin()</code> or the fiftyone plugins create CLI command.</p> <p>This will copy the source code to the plugins directory and create a <code>fiftyone.yml</code> file for you if one does not already exist. Alternatively, you can manually copy the code into your plugins directory.</p> <p>If your FiftyOne App is already running, you may need to restart the server and refresh your browser to see new plugins.</p>"},{"location":"plugins/developing_plugins/#fiftyoneyml","title":"fiftyone.yml \u00b6","text":"<p>All plugins must contain a <code>fiftyone.yml</code> or <code>fiftyone.yaml</code> file, which is used to define the plugin\u2019s metadata, declare any operators and panels that it exposes, and declare any secrets that it may require. The following fields are available:</p> Field Required? Description <code>name</code> yes The name of the plugin <code>type</code> Declare that the directory defines a <code>plugin</code>. This can be omitted forbackwards compatibility, but it is recommended to specify this <code>author</code> The author of the plugin <code>version</code> The version of the plugin <code>url</code> The remote source (eg GitHub repository) where the directory containingthis file is hosted <code>license</code> The license under which the plugin is distributed <code>description</code> A brief description of the plugin <code>fiftyone.version</code> A semver version specifier (or <code>*</code>) describing the requiredFiftyOne version for the plugin to work properly <code>operators</code> A list of operator names registered by the plugin, if any <code>panels</code> A list of panel names registred by the plugin, if any <code>secrets</code> A list of secret keys that may be used by the plugin, if any <p>For example, the @voxel51/annotation plugin\u2019s <code>fiftyone.yml</code> looks like this:</p> <pre><code>name: \"@voxel51/annotation\"\ntype: plugin\nauthor: Voxel51\nversion: 1.0.0\nurl: https://github.com/voxel51/fiftyone-plugins/tree/main/plugins/annotation\nlicense: Apache 2.0\ndescription: Utilities for integrating FiftyOne with annotation tools\nfiftyone:\n  version: \"&gt;=0.22\"\noperators:\n  - request_annotations\n  - load_annotations\n  - get_annotation_info\n  - load_annotation_view\n  - rename_annotation_run\n  - delete_annotation_run\nsecrets:\n  - FIFTYONE_CVAT_URL\n  - FIFTYONE_CVAT_USERNAME\n  - FIFTYONE_CVAT_PASSWORD\n  - FIFTYONE_CVAT_EMAIL\n  - FIFTYONE_LABELBOX_URL\n  - FIFTYONE_LABELBOX_API_KEY\n  - FIFTYONE_LABELSTUDIO_URL\n  - FIFTYONE_LABELSTUDIO_API_KEY\n</code></pre> <p>Note</p> <p>Although it is not strictly required, we highly recommend using the <code>@user-or-org-name/plugin-name</code> naming convention when writing plugins.</p>"},{"location":"plugins/developing_plugins/#python-plugins","title":"Python plugins \u00b6","text":"<p>Python plugins should define the following files:</p> <ul> <li> <p><code>__init__.py</code> (required): entrypoint that defines the Python operators and panels that the plugin defines</p> </li> <li> <p><code>requirements.txt</code>: specifies the Python package requirements to run the plugin</p> </li> </ul>"},{"location":"plugins/developing_plugins/#js-plugins","title":"JS plugins \u00b6","text":"<p>JS plugins should define the following files:</p> <ul> <li> <p><code>package.json</code>: a JSON file containing additional information about the plugin, including the JS bundle file path</p> </li> <li> <p><code>dist/index.umd.js</code>: a JS bundle file for the plugin</p> </li> </ul>"},{"location":"plugins/developing_plugins/#publishing-plugins","title":"Publishing plugins \u00b6","text":"<p>You can publish your FiftyOne plugins either privately or publicly by simply uploading the source directory or a ZIP of it to GitHub or another file hosting service.</p> <p>Note</p> <p>Want to share your plugin with the FiftyOne community? Make a pull request into the FiftyOne Plugins repository to add it to the Community Plugins list!</p> <p>Any users with access to the plugin\u2019s hosted location can easily download it via the fiftyone plugins download CLI command:</p> <pre><code># Download plugin(s) from a GitHub repository\nfiftyone plugins download https://github.com/&lt;user&gt;/&lt;repo&gt;[/tree/branch]\n\n# Download plugin(s) by specifying the GitHub repository details\nfiftyone plugins download &lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;]\n\n# Download specific plugins from a GitHub repository\nfiftyone plugins download \\\\\n    https://github.com/&lt;user&gt;/&lt;repo&gt;[/tree/branch] \\\\\n    --plugin-names &lt;name1&gt; &lt;name2&gt; &lt;name3&gt;\n</code></pre> <p>Note</p> <p>GitHub repositories may contain multiple plugins. By default, all plugins that are found within the first three directory levels are installed, but you can select specific ones if desired as shown above.</p>"},{"location":"plugins/developing_plugins/#quick-examples","title":"Quick examples \u00b6","text":"<p>This section contains a few quick examples of plugins before we dive into the full details of the plugin system.</p> <p>Note</p> <p>The best way to learn how to write plugins is to use and inspect existing ones. Check out the FiftyOne plugins repository for a growing collection of plugins that you can use as examples when developing your own.</p>"},{"location":"plugins/developing_plugins/#example-plugin","title":"Example plugin \u00b6","text":"<p>The Hello World plugin defines both a JS Panel and a Python operator:</p> <p>Here\u2019s the plugin in action! The <code>Hello world</code> panel is available under the <code>+</code> icon next to the Samples tab and the <code>count_samples</code> operator is available in the operator browser:</p> <p></p>"},{"location":"plugins/developing_plugins/#example-python-operator","title":"Example Python operator \u00b6","text":"<p>Here\u2019s a simple Python operator that accepts a string input and then displays it to the user in the operator\u2019s output modal.</p> <pre><code>class SimpleInputExample(foo.Operator):\n    @property\n    def config(self):\n        return foo.OperatorConfig(\n            name=\"simple_input_example\",\n            label=\"Simple input example\",\n        )\n\n    def resolve_input(self, ctx):\n        inputs = types.Object()\n        inputs.str(\"message\", label=\"Message\", required=True)\n        header = \"Simple input example\"\n        return types.Property(inputs, view=types.View(label=header))\n\n    def execute(self, ctx):\n        return {\"message\": ctx.params[\"message\"]}\n\n    def resolve_output(self, ctx):\n        outputs = types.Object()\n        outputs.str(\"message\", label=\"Message\")\n        header = \"Simple input example: Success!\"\n        return types.Property(outputs, view=types.View(label=header))\n\ndef register(p):\n    p.register(SimpleInputExample)\n</code></pre> <p>In practice, operators would use the inputs to perform some operation on the current dataset.</p> <p>Note</p> <p>Remember that you must also include the operator\u2019s name in the plugin\u2019s fiftyone.yml:</p> <pre><code>operators:\n  - simple_input_example\n</code></pre>"},{"location":"plugins/developing_plugins/#example-python-panel","title":"Example Python panel \u00b6","text":"<p>Here\u2019s a simple Python panel that renders a button that shows a \u201cHello world!\u201d notification when clicked:</p> <pre><code>import fiftyone.operators as foo\nimport fiftyone.operators.types as types\n\nclass HelloWorldPanel(foo.Panel):\n    @property\n    def config(self):\n        return foo.PanelConfig(\n            name=\"hello_world_panel\",\n            label=\"Hello World Panel\"\n        )\n\n    def on_load(self, ctx):\n        ctx.panel.state.hello_message = \"Hello world!\"\n\n    def say_hello(self, ctx):\n        ctx.ops.notify(ctx.panel.state.hello_message)\n\n    def render(self, ctx):\n        panel = types.Object()\n        panel.btn(\n            \"hello_btn\",\n            label=\"Say Hello\",\n            icon=\"emoji_people\",\n            on_click=self.say_hello,\n            variant=\"contained\",\n        )\n\n        panel_view = types.GridView(\n            width=100, height=100, align_x=\"center\", align_y=\"center\"\n        )\n        return types.Property(panel, view=panel_view)\n\ndef register(p):\n    p.register(HelloWorldPanel)\n</code></pre> <p>Note</p> <p>Remember that you must also include the panel\u2019s name in the plugin\u2019s fiftyone.yml:</p> <pre><code>panels:\n  - hello_world_panel\n</code></pre> <p></p>"},{"location":"plugins/developing_plugins/#example-js-operator","title":"Example JS operator \u00b6","text":"<p>Here\u2019s how to define a JS operator that sets the currently selected samples in the App based on a list of sample IDs provided via a <code>samples</code> parameter.</p> <pre><code>import {Operator, OperatorConfig, types, registerOperator} from \"@fiftyone/operators\";\nconst PLUGIN_NAME = \"@my/plugin\";\n\nclass SetSelectedSamples extends Operator {\n    get config(): OperatorConfig {\n        return new OperatorConfig({\n            name: \"set_selected_samples\",\n            label: \"Set selected samples\",\n            unlisted: true,\n        });\n    }\n    useHooks(): {} {\n        return {\n            setSelected: fos.useSetSelected(),\n        };\n    }\n    async execute({ hooks, params }: ExecutionContext) {\n        hooks.setSelected(params.samples);\n    }\n}\n\nregisterOperator(SetSelectedSamples, PLUGIN_NAME);\n</code></pre> <p>Unlike Python operators, JS operators can use React hooks and the <code>@fiftyone/*</code> packages by defining a <code>useHook()</code> method. Any values return in this method will be available to the operator\u2019s <code>execute()</code> method via <code>ctx.hooks</code>.</p> <p>Note</p> <p>Marking the operator as <code>unlisted</code> omits it from the operator browser, which is useful when the operator is intended only for internal use by other plugin components.</p>"},{"location":"plugins/developing_plugins/#developing-operators","title":"Developing operators \u00b6","text":"<p>Operators allow you to define custom operations that accept parameters via input properties, execute some actions based on them, and optionally return outputs. They can be executed by users in the App or triggered internally by other operators.</p> <p>Operators can be defined in either Python or JS, and FiftyOne comes with a number of builtin <code>Python</code> and JS operators for common tasks.</p> <p>The <code>fiftyone.operators.types</code> module and <code>@fiftyone/operators</code> package define a rich builtin type system that operator developers can use to define the input and output properties of their operators without the need to build custom user interfaces from scratch. These types handle all aspects of input collection, validation, and component rendering for you.</p> <p>Operators can be composed for coordination between Python and the FiftyOne App, such as triggering a reload of samples/view to update the app with the changes made by the operator. Operators can also be scheduled to run by an orchestrator or triggered by other operators.</p>"},{"location":"plugins/developing_plugins/#operator-interface","title":"Operator interface \u00b6","text":"<p>The code block below describes the Python interface for defining operators. We\u2019ll dive into each component of the interface in more detail in the subsequent sections.</p> <p>Note</p> <p>The JS interface for defining operators is analogous. See this example JS operator for details.</p> <pre><code>import fiftyone.operators as foo\nimport fiftyone.operators.types as types\n\nclass ExampleOperator(foo.Operator):\n    @property\n    def config(self):\n        return foo.OperatorConfig(\n            # The operator's URI: f\"{plugin_name}/{name}\"\n            name=\"example_operator\",  # required\n\n            # The display name of the operator\n            label=\"Example operator\",  # required\n\n            # A description for the operator\n            description=\"An example description\"\n\n            # Whether to re-execute resolve_input() after each user input\n            dynamic=True/False,  # default False\n\n            # Whether the operator's execute() method returns a generator\n            # that should be iterated over until exhausted\n            execute_as_generator=True/False,  # default False\n\n            # Whether to hide this operator from the App's operator browser\n            # Set this to True if the operator is only for internal use\n            unlisted=True/False,  # default False\n\n            # Whether the operator should be executed every time a new App\n            # session starts\n            on_startup=True/False,  # default False\n\n            # Whether the operator should be executed every time a new\n            # dataset is opened in the App\n            on_dataset_open=True/False,  # default False\n\n            # Custom icons to use\n            # Can be a URL, a local path in the plugin directory, or the\n            # name of a MUI icon: https://marella.me/material-icons/demo\n            icon=\"/assets/icon.svg\",\n            light_icon=\"/assets/icon-light.svg\",  # light theme only\n            dark_icon=\"/assets/icon-dark.svg\",  # dark theme only\n\n            # Whether the operator supports immediate and/or delegated execution\n            allow_immediate_execution=True/False,    # default True\n            allow_delegated_execution=True/False,    # default False\n            default_choice_to_delegated=True/False,  # default False\n            resolve_execution_options_on_change=None,\n        )\n\n    def resolve_placement(self, ctx):\n        \"\"\"You can optionally implement this method to configure a button\n        or icon in the App that triggers this operator.\n\n        By default the operator only appears in the operator browser\n        (unless it is unlisted).\n\n        Returns:\n            a `types.Placement`\n        \"\"\"\n        return types.Placement(\n            # Make operator appear in the actions row above the sample grid\n            types.Places.SAMPLES_GRID_SECONDARY_ACTIONS,\n\n            # Use a button as the operator's placement\n            types.Button(\n                # A label for placement button visible on hover\n                label=\"Open Histograms Panel\",\n\n                # An icon for the button\n                # The default is a button with the `label` displayed\n                icon=\"/assets/icon.svg\",\n\n                # If False, don't show the operator's input prompt when we\n                # do not require user input\n                prompt=True/False  # False\n            )\n        )\n\n    def resolve_input(self, ctx):\n        \"\"\"Implement this method to collect user inputs as parameters\n        that are stored in `ctx.params`.\n\n        Returns:\n            a `types.Property` defining the form's components\n        \"\"\"\n        inputs = types.Object()\n\n        # Use the builtin `types` and the current `ctx.params` to define\n        # the necessary user input data\n        inputs.str(\"key\", ...)\n\n        # When `dynamic=True`, you'll often use the current `ctx` to\n        # conditionally render different components\n        if ctx.params[\"key\"] == \"value\" and len(ctx.view) &lt; 100:\n            # do something\n        else:\n            # do something else\n\n        return types.Property(inputs, view=types.View(label=\"Example operator\"))\n\n    def resolve_delegation(self, ctx):\n        \"\"\"Implement this method if you want to programmatically *force*\n        this operation to be delegated or executed immediately.\n\n        Returns:\n            whether the operation should be delegated (True), run\n            immediately (False), or None to defer to\n            `resolve_execution_options()` to specify the available options\n        \"\"\"\n        return len(ctx.view) &gt; 1000  # delegate for larger views\n\n    def resolve_execution_options(self, ctx):\n        \"\"\"Implement this method if you want to dynamically configure the\n        execution options available to this operator based on the current\n        `ctx`.\n\n        Returns:\n            an `ExecutionOptions` instance\n        \"\"\"\n        should_delegate = len(ctx.view) &gt; 1000  # delegate for larger views\n        return foo.ExecutionOptions(\n            allow_immediate_execution=True,\n            allow_delegated_execution=True,\n            default_choice_to_delegated=should_delegate,\n        )\n\n    def execute(self, ctx):\n        \"\"\"Executes the actual operation based on the hydrated `ctx`.\n        All operators must implement this method.\n\n        This method can optionally be implemented as `async`.\n\n        Returns:\n            an optional dict of results values\n        \"\"\"\n        # Use ctx.params, ctx.dataset, ctx.view, etc to perform the\n        # necessary computation\n        value = ctx.params[\"key\"]\n        view = ctx.view\n        n = len(view)\n\n        # Use ctx.ops to trigger builtin operations\n        ctx.ops.clear_selected_samples()\n        ctx.ops.set_view(view=view)\n\n        # Use ctx.trigger to call other operators as necessary\n        ctx.trigger(\"operator_uri\", params={\"key\": value})\n\n        # If `execute_as_generator=True`, this method may yield multiple\n        # messages\n        for i, sample in enumerate(current_view, 1):\n            # do some computation\n            yield ctx.ops.set_progress(progress=i/n)\n\n        yield ctx.ops.reload_dataset()\n\n        return {\"value\": value, ...}\n\n    def resolve_output(self, ctx):\n        \"\"\"Implement this method if your operator renders an output form\n        to the user.\n\n        Returns:\n            a `types.Property` defining the components of the output form\n        \"\"\"\n        outputs = types.Object()\n\n        # Use the builtin `types` and the current `ctx.params` and\n        # `ctx.results` as necessary to define the necessary output form\n        outputs.define_property(\"value\", ...)\n\n        return types.Property(outputs, view=types.View(label=\"Example operator\"))\n\ndef register(p):\n    \"\"\"Always implement this method and register() each operator that your\n    plugin defines.\n    \"\"\"\n    p.register(ExampleOperator)\n</code></pre> <p>Note</p> <p>Remember that you must also include the operator\u2019s name in the plugin\u2019s fiftyone.yml:</p> <pre><code>operators:\n  - example_operator\n</code></pre>"},{"location":"plugins/developing_plugins/#operator-config","title":"Operator config \u00b6","text":"<p>Every operator must define a <code>config</code> property that defines its name, display name, and other optional metadata about its execution:</p> <pre><code>@property\ndef config(self):\n    return foo.OperatorConfig(\n        # The operator's URI: f\"{plugin_name}/{name}\"\n        name=\"example_operator\",  # required\n\n        # The display name of the operator\n        label=\"Example operator\",  # required\n\n        # A description for the operator\n        description=\"An example description\"\n\n        # Whether to re-execute resolve_input() after each user input\n        dynamic=True/False,  # default False\n\n        # Whether the operator's execute() method returns a generator\n        # that should be iterated over until exhausted\n        execute_as_generator=True/False,  # default False\n\n        # Whether to hide this operator from the App's operator browser\n        # Set this to True if the operator is only for internal use\n        unlisted=True/False,  # default False\n\n        # Whether the operator should be executed every time a new App\n        # session starts\n        on_startup=True/False,  # default False\n\n        # Whether the operator should be executed every time a new dataset\n        # is opened in the App\n        on_dataset_open=True/False,  # default False\n\n        # Custom icons to use\n        # Can be a URL, a local path in the plugin directory, or the\n        # name of a MUI icon: https://marella.me/material-icons/demo\n        icon=\"/assets/icon.svg\",\n        light_icon=\"/assets/icon-light.svg\",  # light theme only\n        dark_icon=\"/assets/icon-dark.svg\",  # dark theme only\n\n        # Whether the operator supports immediate and/or delegated execution\n        allow_immediate_execution=True/False,    # default True\n        allow_delegated_execution=True/False,    # default False\n        default_choice_to_delegated=True/False,  # default False\n        resolve_execution_options_on_change=None,\n    )\n</code></pre>"},{"location":"plugins/developing_plugins/#execution-context","title":"Execution context \u00b6","text":"<p>An <code>ExecutionContext</code> is passed to each of the operator\u2019s methods at runtime. This <code>ctx</code> contains static information about the current state of the App (dataset, view, panel, selection, etc) as well as dynamic information about the current parameters and results.</p> <p>An <code>ExecutionContext</code> contains the following properties:</p> <ul> <li> <p><code>ctx.params</code>: a dict containing the operator\u2019s current input parameter values</p> </li> <li> <p><code>ctx.dataset_name</code>: the name of the current dataset</p> </li> <li> <p><code>ctx.dataset</code> - the current <code>Dataset</code> instance</p> </li> <li> <p><code>ctx.view</code> - the current <code>DatasetView</code> instance</p> </li> <li> <p><code>ctx.spaces</code> - the current Spaces layout in the App</p> </li> <li> <p><code>ctx.current_sample</code> - the ID of the active sample in the App modal, if any</p> </li> <li> <p><code>ctx.selected</code> - the list of currently selected samples in the App, if any</p> </li> <li> <p><code>ctx.selected_labels</code> - the list of currently selected labels in the App, if any</p> </li> <li> <p><code>ctx.extended_selection</code> - the extended selection of the view, if any</p> </li> <li> <p><code>ctx.group_slice</code> - the active group slice in the App, if any</p> </li> <li> <p><code>ctx.user_id</code> - the ID of the user that invoked the operator, if known</p> </li> <li> <p><code>ctx.user</code> - an object of information about the user that invoked the operator, if known, including the user\u2019s <code>id</code>, <code>name</code>, <code>email</code>, <code>role</code>, and <code>dataset_permission</code></p> </li> <li> <p><code>ctx.user_request_token</code> - the request token authenticating the user executing the operation, if known</p> </li> <li> <p><code>ctx.panel_id</code> - the ID of the panel that invoked the operator, if any</p> </li> <li> <p><code>ctx.panel</code> - a <code>PanelRef</code> instance that you can use to read and write the state and data of the current panel, if the operator was invoked from a panel</p> </li> <li> <p><code>ctx.delegated</code> - whether the operation was delegated</p> </li> <li> <p><code>ctx.requesting_delegated_execution</code> - whether delegated execution was requested for the operation</p> </li> <li> <p><code>ctx.delegation_target</code> - the orchestrator to which the operation should be delegated, if applicable</p> </li> <li> <p><code>ctx.ops</code> - an <code>Operations</code> instance that you can use to trigger builtin operations on the current context</p> </li> <li> <p><code>ctx.trigger</code> - a method that you can use to trigger arbitrary operations on the current context</p> </li> <li> <p><code>ctx.secrets</code> - a dict of secrets for the plugin, if any</p> </li> <li> <p><code>ctx.results</code> - a dict containing the outputs of the <code>execute()</code> method, if it has been called</p> </li> <li> <p><code>ctx.hooks</code> (JS only) - the return value of the operator\u2019s <code>useHooks()</code> method</p> </li> </ul>"},{"location":"plugins/developing_plugins/#operator-inputs","title":"Operator inputs \u00b6","text":"<p>Operators can optionally implement <code>resolve_input()</code> to define user input forms that are presented to the user as a modal in the App when the operator is invoked.</p> <p>The basic objective of <code>resolve_input()</code> is to populate the <code>ctx.params</code> dict with user-provided parameter values, which are retrieved from the various subproperties of the <code>Property</code> returned by the method ( <code>inputs</code> in the examples below).</p> <p>The <code>fiftyone.operators.types</code> module defines a rich builtin type system that you can use to define the necessary input properties. These types handle all aspects of input collection, validation, and component rendering for you!</p> <p>For example, here\u2019s a simple example of collecting a single string input from the user:</p> <pre><code>def resolve_input(self, ctx):\n    inputs = types.Object()\n    inputs.str(\"message\", label=\"Message\", required=True)\n    return types.Property(inputs, view=types.View(label=\"Static example\"))\n\ndef execute(self, ctx):\n    the_message = ctx.params[\"message\"]\n</code></pre> <p>If the operator\u2019s config declares <code>dynamic=True</code>, then <code>resolve_input()</code> will be called after each user input, which allows you to construct dynamic forms whose components may contextually change based on the already provided values and any other aspects of the execution context:</p> <pre><code>import fiftyone.brain as fob\n\ndef resolve_input(self, ctx):\n    inputs = types.Object()\n    brain_keys = ctx.dataset.list_brain_runs()\n\n    if not brain_keys:\n        warning = types.Warning(label=\"This dataset has no brain runs\")\n        prop = inputs.view(\"warning\", warning)\n        prop.invalid = True  # so form's `Execute` button is disabled\n        return\n\n    choices = types.DropdownView()\n    for brain_key in brain_keys:\n        choices.add_choice(brain_key, label=brain_key)\n\n    inputs.str(\n        \"brain_key\",\n        required=True,\n        label=\"Brain key\",\n        description=\"Choose a brain key to use\",\n        view=choices,\n    )\n\n    brain_key = ctx.params.get(\"brain_key\", None)\n    if brain_key is None:\n        return  # single `brain_key`\n\n    info = ctx.dataset.get_brain_info(brain_key)\n\n    if isinstance(info.config, fob.SimilarityConfig):\n        # We found a similarity config; render some inputs specific to that\n        inputs.bool(\n            \"upgrade\",\n            label\"Compute visualization\",\n            description=\"Generate an embeddings visualization for this index?\",\n            view=types.CheckboxView(),\n        )\n\n    return types.Property(inputs, view=types.View(label=\"Dynamic example\"))\n</code></pre> <p>Remember that properties automatically handle validation for you. So if you configure a property as <code>required=True</code> but the user has not provided a value, the property will automatically be marked as <code>invalid=True</code>. The operator\u2019s <code>Execute</code> button will be enabled if and only if all input properties are valid (recursively searching nested objects).</p> <p>Note</p> <p>As the example above shows, you can manually set a property to invalid by setting its <code>invalid</code> property.</p> <p>Note</p> <p>Avoid expensive computations in <code>resolve_input()</code> or else the form may take too long to render, especially for dynamic inputs where the method is called after every user input.</p>"},{"location":"plugins/developing_plugins/#delegated-execution","title":"Delegated execution \u00b6","text":"<p>By default, operations are executed immediately after their inputs are provided in the App or they are triggered programmatically.</p> <p>However, many interesting operations like model inference, embeddings computation, evaluation, and exports are computationally intensive and/or not suitable for immediate execution.</p> <p>In such cases, delegated operations come to the rescue by allowing users to schedule potentially long-running tasks that are executed in the background while you continue to use the App.</p> <p>Note</p> <p>FiftyOne Teams deployments come out of the box with a connected compute cluster for executing delegated operations at scale.</p> <p>In FiftyOne Open Source, you can use delegated operations at small scale by running them locally.</p> <p>There are a variety of options available for configuring whether a given operation should be delegated or executed immediately.</p>"},{"location":"plugins/developing_plugins/#execution-options","title":"Execution options \u00b6","text":"<p>You can provide the optional properties described below in the operator\u2019s config to specify the available execution modes for the operator:</p> <pre><code>@property\ndef config(self):\n    return foo.OperatorConfig(\n        # Other parameters...\n\n        # Whether to allow immediate execution\n        allow_immediate_execution=True/False,    # default True\n\n        # Whether to allow delegated execution\n        allow_delegated_execution=True/False,    # default False\n\n        # Whether the default execution mode should be delegated, if both\n        # options are available\n        default_choice_to_delegated=True/False,  # default False\n\n        # Whether to resolve execution options dynamically when the\n        # operator's inputs change. By default, this behavior will match\n        # the operator's ``dynamic`` setting\n        resolve_execution_options_on_change=True/False/None,  # default None\n    )\n</code></pre> <p>When the operator\u2019s input form is rendered in the App, the <code>Execute|Schedule</code> button at the bottom of the modal will contextually show whether the operation will be executed immediately, scheduled for delegated execution, or allow the user to choose between the supported options if there are multiple:</p> <p></p>"},{"location":"plugins/developing_plugins/#dynamic-execution-options","title":"Dynamic execution options \u00b6","text":"<p>Operators may also implement <code>resolve_execution_options()</code> to dynamically configure the available execution options based on the current execution context:</p> <pre><code># Option 1: recommend delegation for larger views\ndef resolve_execution_options(self, ctx):\n    should_delegate = len(ctx.view) &gt; 1000\n    return foo.ExecutionOptions(\n        allow_immediate_execution=True,\n        allow_delegated_execution=True,\n        default_choice_to_delegated=should_delegate,\n    )\n\n# Option 2: force delegation for larger views\ndef resolve_execution_options(self, ctx):\n    delegate = len(ctx.view) &gt; 1000\n    return foo.ExecutionOptions(\n        allow_immediate_execution=not delegate,\n        allow_delegated_execution=delegate,\n    )\n</code></pre> <p>If implemented, this method will override any static execution parameters included in the operator\u2019s config as described in the previous section.</p>"},{"location":"plugins/developing_plugins/#forced-delegation","title":"Forced delegation \u00b6","text":"<p>Operators can implement <code>resolve_delegation()</code> to force a particular operation to be delegated (by returning <code>True</code>) or executed immediately (by returning <code>False</code>) based on the current execution context.</p> <p>For example, you could decide whether to delegate execution based on the size of the current view:</p> <pre><code>def resolve_delegation(self, ctx):\n    # Force delegation for large views and immediate execution for small views\n    return len(ctx.view) &gt; 1000\n</code></pre> <p>If <code>resolve_delegation()</code> is not implemented or returns <code>None</code>, then the choice of execution mode is deferred to the prior mechanisms described above.</p>"},{"location":"plugins/developing_plugins/#reporting-progress","title":"Reporting progress \u00b6","text":"<p>Delegated operations can report their execution progress by calling <code>set_progress()</code> on their execution context from within <code>execute()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.core.storage as fos\nimport fiftyone.core.utils as fou\n\ndef execute(self, ctx):\n    images_dir = ctx.params[\"images_dir\"]\n\n    filepaths = fos.list_files(images_dir, abs_paths=True, recursive=True)\n\n    num_added = 0\n    num_total = len(filepaths)\n    for batch in fou.iter_batches(filepaths, 100):\n        samples = [fo.Sample(filepath=f) for f in batch]\n        ctx.dataset.add_samples(samples)\n\n        num_added += len(batch)\n        ctx.set_progress(progress=num_added / num_total)\n</code></pre> <p>Note</p> <p>FiftyOne Teams users can view the current progress of their delegated operations from the Runs page of the Teams App!</p> <p>For your convenience, all builtin methods of the FiftyOne SDK that support rendering progress bars provide an optional <code>progress</code> method that you can use trigger calls to <code>set_progress()</code> using the pattern show below:</p> <pre><code>import fiftyone as fo\n\ndef execute(self, ctx):\n    images_dir = ctx.params[\"images_dir\"]\n\n    # Custom logic that controls how progress is reported\n    def set_progress(pb):\n        if pb.complete:\n            ctx.set_progress(progress=1, label=\"Operation complete\")\n        else:\n            ctx.set_progress(progress=pb.progress)\n\n    # Option 1: report progress every five seconds\n    progress = fo.report_progress(set_progress, dt=5.0)\n\n    # Option 2: report progress at 10 equally-spaced increments\n    # progress = fo.report_progress(set_progress, n=10)\n\n    ctx.dataset.add_images_dir(images_dir, progress=progress)\n</code></pre> <p>You can also use the builtin <code>ProgressHandler</code> class to automatically forward logging messages to <code>set_progress()</code> as <code>label</code> values using the pattern shown below:</p> <pre><code>import logging\nimport fiftyone.operators as foo\nimport fiftyone.zoo as foz\n\ndef execute(self, ctx):\n    name = ctx.params[\"name\"]\n\n    # Automatically report all `fiftyone` logging messages\n    with foo.ProgressHandler(ctx, logger=logging.getLogger(\"fiftyone\")):\n        foz.load_zoo_dataset(name, persistent=True)\n</code></pre>"},{"location":"plugins/developing_plugins/#operator-execution","title":"Operator execution \u00b6","text":"<p>All operators must implement <code>execute()</code>, which is where their main actions are performed.</p> <p>The <code>execute()</code> method takes an execution context as input whose <code>ctx.params</code> dict has been hydrated with parameters provided either by the user by filling out the operator\u2019s input form or directly provided by the operation that triggered it. The method can optionally return a dict of results values that will be made available via <code>ctx.results</code> when the operator\u2019s output form is rendered.</p>"},{"location":"plugins/developing_plugins/#synchronous-execution","title":"Synchronous execution \u00b6","text":"<p>Your execution method is free to make use of the full power of the FiftyOne SDK and any external dependencies that it needs.</p> <p>For example, you might perform inference on a model:</p> <pre><code>import fiftyone.zoo as foz\n\ndef execute(self, ctx):\n    name = ctx.params[\"name\"]\n    label_field = ctx.params[\"label_field\"]\n    confidence_thresh = ctx.params.get(\"confidence_thresh\", None)\n\n    model = foz.load_zoo_model(name)\n    ctx.view.apply_model(\n        model, label_field=label_field, confidence_thresh=confidence_thresh\n    )\n\n    num_predictions = ctx.view.count(f\"{label_field}.detections\")\n    return {\"num_predictions\": num_predictions}\n</code></pre> <p>Note</p> <p>When an operator\u2019s <code>execute()</code> method throws an error it will be displayed to the user in the browser.</p>"},{"location":"plugins/developing_plugins/#asynchronous-execution","title":"Asynchronous execution \u00b6","text":"<p>The <code>execute()</code> method can also be <code>async</code>:</p> <pre><code>import aiohttp\n\nasync def execute(self, ctx):\n    # do something async\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as resp:\n            r = await resp.json()\n</code></pre>"},{"location":"plugins/developing_plugins/#operator-composition","title":"Operator composition \u00b6","text":"<p>Many operators are designed to be composed with other operators to build up more complex behaviors. You can trigger other operations from within an operator\u2019s <code>execute()</code> method via <code>ctx.ops</code> and <code>ctx.trigger</code>.</p> <p>The <code>ctx.ops</code> property of an execution context exposes all builtin <code>Python</code> and JavaScript in a conveniently documented functional interface. For example, many operations involve updating the current state of the App:</p> <pre><code>def execute(self, ctx):\n    # Dataset\n    ctx.ops.open_dataset(\"...\")\n    ctx.ops.reload_dataset()\n\n    # View/sidebar\n    ctx.ops.set_view(name=\"...\")  # saved view by name\n    ctx.ops.set_view(view=view)  # arbitrary view\n    ctx.ops.clear_view()\n    ctx.ops.clear_sidebar_filters()\n\n    # Selected samples\n    ctx.ops.set_selected_samples([...]))\n    ctx.ops.clear_selected_samples()\n\n    # Selected labels\n    ctx.ops.set_selected_labels([...])\n    ctx.ops.clear_selected_labels()\n\n    # Panels\n    ctx.ops.open_panel(\"Embeddings\")\n    ctx.ops.close_panel(\"Embeddings\")\n</code></pre> <p>The <code>ctx.trigger</code> property is a lower-level function that allows you to invoke arbitrary operations by providing their URI and parameters, including all builtin operations as well as any operations installed via custom plugins. For example, here\u2019s how to trigger the same App-related operations from above:</p> <pre><code>def execute(self, ctx):\n    # Dataset\n    ctx.trigger(\"open_dataset\", params=dict(name=\"...\"))\n    ctx.trigger(\"reload_dataset\")  # refreshes the App\n\n    # View/sidebar\n    ctx.trigger(\"set_view\", params=dict(name=\"...\"))  # saved view by name\n    ctx.trigger(\"set_view\", params=dict(view=view._serialize()))  # arbitrary view\n    ctx.trigger(\"clear_view\")\n    ctx.trigger(\"clear_sidebar_filters\")\n\n    # Selected samples\n    ctx.trigger(\"set_selected_samples\", params=dict(samples=[...]))\n    ctx.trigger(\"clear_selected_samples\")\n\n    # Selected labels\n    ctx.trigger(\"set_selected_labels\", params=dict(labels=[...]))\n    ctx.trigger(\"clear_selected_labels\")\n\n    # Panels\n    ctx.trigger(\"open_panel\", params=dict(name=\"Embeddings\"))\n    ctx.trigger(\"close_panel\", params=dict(name=\"Embeddings\"))\n</code></pre>"},{"location":"plugins/developing_plugins/#generator-execution","title":"Generator execution \u00b6","text":"<p>If your operator\u2019s config declares that it is a generator via <code>execute_as_generator=True</code>, then its <code>execute()</code> method should <code>yield</code> calls to <code>ctx.ops</code> methods or <code>ctx.trigger()</code>, both of which trigger another operation and return a <code>GeneratedMessage</code> instance containing the result of the invocation.</p> <p>For example, a common generator pattern is to use the builtin <code>set_progress</code> operator to render a progress bar tracking the progress of an operation:</p> <pre><code>def execute(self, ctx):\n    # render a progress bar tracking the execution\n    for i in range(n):\n        # [process a chunk here]\n\n        # Option 1: ctx.ops\n        yield ctx.ops.set_progress(progress=i/n, label=f\"Processed {i}/{n}\")\n\n        # Option 2: ctx.trigger\n        yield ctx.trigger(\n            \"set_progress\",\n            dict(progress=i/n, label=f\"Processed {i}/{n}\"),\n        )\n</code></pre> <p>Note</p> <p>Check out the VoxelGPT plugin for a more sophisticated example of using generator execution to stream an LLM\u2019s response to a panel.</p>"},{"location":"plugins/developing_plugins/#accessing-secrets","title":"Accessing secrets \u00b6","text":"<p>Some plugins may require sensitive information such as API tokens and login credentials in order to function. Any secrets that a plugin requires are in its fiftyone.yml.</p> <p>For example, the @voxel51/annotation plugin declares the following secrets:</p> <pre><code>secrets:\n  - FIFTYONE_CVAT_URL\n  - FIFTYONE_CVAT_USERNAME\n  - FIFTYONE_CVAT_PASSWORD\n  - FIFTYONE_CVAT_EMAIL\n  - FIFTYONE_LABELBOX_URL\n  - FIFTYONE_LABELBOX_API_KEY\n  - FIFTYONE_LABELSTUDIO_URL\n  - FIFTYONE_LABELSTUDIO_API_KEY\n</code></pre> <p>As the naming convention implies, any necessary secrets are provided by users by setting environment variables with the appropriate names. For example, if you want to use the CVAT backend with the @voxel51/annotation plugin, you would set:</p> <pre><code>FIFTYONE_CVAT_URL=...\nFIFTYONE_CVAT_USERNAME=...\nFIFTYONE_CVAT_PASSWORD=...\nFIFTYONE_CVAT_EMAIL=...\n</code></pre> <p>At runtime, the plugin\u2019s execution context is automatically hydrated with any available secrets that are declared by the plugin. Operators can access these secrets via the <code>ctx.secrets</code> dict:</p> <pre><code>def execute(self, ctx):\n   url = ctx.secrets[\"FIFTYONE_CVAT_URL\"]\n   username = ctx.secrets[\"FIFTYONE_CVAT_USERNAME\"]\n   password = ctx.secrets[\"FIFTYONE_CVAT_PASSWORD\"]\n   email = ctx.secrets[\"FIFTYONE_CVAT_EMAIL\"]\n</code></pre>"},{"location":"plugins/developing_plugins/#operator-outputs","title":"Operator outputs \u00b6","text":"<p>Operators can optionally implement <code>resolve_output()</code> to define read-only output forms that are presented to the user as a modal in the App after the operator\u2019s execution completes.</p> <p>The basic objective of <code>resolve_output()</code> is to define properties that describe how to render the values in <code>ctx.results</code> for the user. As with input forms, you can use the <code>fiftyone.operators.types</code> module to define the output properties.</p> <p>For example, the output form below renders the number of samples ( <code>count</code>) computed during the operator\u2019s execution:</p> <pre><code>def execute(self, ctx):\n    # computation here...\n\n    return {\"count\": count}\n\ndef resolve_output(self, ctx):\n    outputs = types.Object()\n    outputs.int(\n        \"count\",\n        label=\"Count\",\n        description=f\"The number of samples in the current {target}\",\n    )\n    return types.Property(outputs)\n</code></pre> <p>Note</p> <p>All properties in output forms are implicitly rendered as read-only.</p>"},{"location":"plugins/developing_plugins/#operator-placement","title":"Operator placement \u00b6","text":"<p>By default, operators are only accessible from the operator browser. However, you can place a custom button, icon, menu item, etc. in the App that will trigger the operator when clicked in any location supported by the <code>types.Places</code> enum.</p> <p>For example, you can use:</p> <ul> <li><code>types.Places.SAMPLES_GRID_ACTIONS</code> </li> <li><code>types.Places.SAMPLES_GRID_SECONDARY_ACTIONS</code> </li> <li><code>types.Places.SAMPLES_VIEWER_ACTIONS</code> </li> <li><code>types.Places.EMBEDDINGS_ACTIONS</code> </li> <li><code>types.Places.HISTOGRAM_ACTIONS</code> </li> <li><code>types.Places.MAP_ACTIONS</code> </li> </ul> <p>You can add a placement for an operator by implementing the <code>resolve_placement()</code> method as demonstrated below:</p>"},{"location":"plugins/developing_plugins/#developing-panels","title":"Developing panels \u00b6","text":"<p>Panels are miniature full-featured data applications that you can open in App spaces and interactively manipulate to explore your dataset and update/respond to updates from other spaces that are currently open in the App.</p> <p>Panels can be defined in either Python or JS, and FiftyOne comes with a number of builtin panels for common tasks.</p> <p>Panels can be scoped to the App\u2019s grid view or modal view via their config. Grid panels enable extensibility at the macro level, allowing you to work with entire datasets or views, while modal panels provide extensibility at the micro level, focusing on individual samples and scenarios.</p> <p>Panels, like operators, can make use of the <code>fiftyone.operators.types</code> module and the <code>@fiftyone/operators</code> package, which define a rich builtin type system that panel developers can use to implement the layout and associated events that define the panel.</p> <p>Panels can trigger both Python and JS operators, either programmatically or by interactively launching a prompt that users can fill out to provide the necessary parameters for the operator\u2019s execution. This powerful composability allows panels to define interactive workflows that guide the user through executing workflows on their data and then interactively exploring and analyzing the results of the computation.</p> <p>Panels can also interact with other components of the App, such as responding to changes in (or programmatically updating) the current dataset, view, current selection, or active sample in the modal.</p>"},{"location":"plugins/developing_plugins/#panel-interface","title":"Panel interface \u00b6","text":"<p>The code block below describes the Python interface for defining panels. We\u2019ll dive into each component of the interface in more detail in the subsequent sections.</p> <p>Note</p> <p>See this section for more information on developing panels in JS.</p> <pre><code>import fiftyone.operators as foo\nimport fiftyone.operators.types as types\n\nclass ExamplePanel(foo.Panel):\n    @property\n    def config(self):\n        return foo.PanelConfig(\n            # The panel's URI: f\"{plugin_name}/{name}\"\n            name=\"example_panel\",  # required\n\n            # The display name of the panel in the \"+\" menu\n            label=\"Example panel\",  # required\n\n            # Custom icons to use in the \"+\"\" menu\n            # Can be a URL, a local path in the plugin directory, or the\n            # name of a MUI icon: https://marella.me/material-icons/demo\n            icon=\"/assets/icon.svg\",\n            light_icon=\"developer_mode\",  # light theme only\n            dark_icon=\"developer_mode\",  # dark theme only\n\n            # Whether to allow multiple instances of the panel to be opened\n            allow_multiple=False,\n\n            # Whether the panel should be available in the grid, modal, or both\n            # Possible values: \"grid\", \"modal\", \"grid modal\"\n            surfaces=\"grid\",  # default = \"grid\"\n\n            # Markdown-formatted text that describes the panel. This is\n            # rendererd in a tooltip when the help icon in the panel\n            # title is hovered over\n            help_markdown=\"A description of the panel\",\n        )\n\n    def render(self, ctx):\n        \"\"\"Implement this method to define your panel's layout and events.\n\n        This method is called after every panel event is executed (panel\n        load, button callback, context change event, etc).\n\n        Returns:\n            a `types.Property` defining the panel's components\n        \"\"\"\n        panel = types.Object()\n\n        brain_keys = ctx.panel.get_state(\"brain_keys\", [])\n\n        # Define a menu of actions for the panel\n        menu = panel.menu(\"menu\", variant=\"square\", color=\"51\")\n        menu.enum(\n            \"brain_key\",\n            label=\"Choose a brain key\",  # placeholder text\n            values=brain_keys,\n            on_change=self.on_change_brain_key,  # custom event callback\n        )\n        menu.btn(\n            \"learn_more\",\n            label=\"Learn more\",  # tooltip text\n            icon=\"help\",  # material UI icon\n            on_click=self.on_click_learn_more,  # custom event callback\n        )\n\n        # Define components that appear in the panel's main body\n        panel.str(\"event\", label=\"The last event\", view=types.LabelValueView())\n        panel.obj(\n            \"event_data\", label=\"The last event data\", view=types.JSONView()\n        )\n\n        # Display a checkbox to toggle between plot and compute visualization button\n        show_compute_visualization_btn = ctx.panel.get_state(\n            \"show_start_button\", True\n        )\n        panel.bool(\n            \"show_start_button\",\n            label=\"Show compute visualization button\",\n            on_change=self.on_change_show_start_button,\n        )\n\n        # You can use conditional logic to dynamically change the layout\n        # based on the current panel state\n        if show_compute_visualization_btn:\n            # Define a button with a custom on click event\n            panel.btn(\n                \"start\",\n                label=\"Compute visualization\",  # button text\n                on_click=self.on_click_start,  # custom event callback\n                variant=\"contained\",  # button style\n            )\n        else:\n            # Define an interactive plot with custom callbacks\n            panel.plot(\n                \"embeddings\",\n                config={},  # plotly config\n                layout={},  # plotly layout config\n                on_selected=self.on_selected_embeddings,  # custom event callback\n                height=\"400px\",\n            )\n\n        return types.Property(\n            panel, view=types.GridView(orientation=\"vertical\")\n        )\n\n    #######################################################################\n    # Builtin events\n    #######################################################################\n\n    def on_load(self, ctx):\n        \"\"\"Implement this method to set panel state/data when the panel\n        initially loads.\n        \"\"\"\n        event = {\n            \"data\": None,\n            \"description\": \"the panel is loaded\",\n        }\n        ctx.panel.set_state(\"event\", \"on_load\")\n        ctx.panel.set_data(\"event_data\", event)\n\n        # Get the list of brain keys to populate `brain_key` dropdown\n        visualization_keys = ctx.dataset.list_brain_runs(\"visualization\")\n        ctx.panel.set_state(\"brain_keys\", visualization_keys)\n\n        # Show compute visualization button by default\n        ctx.panel.set_state(\"show_start_button\", True)\n\n    def on_unload(self, ctx):\n        \"\"\"Implement this method to set panel state/data when the panel is\n        being closed.\n        \"\"\"\n        event = {\n            \"data\": None,\n            \"description\": \"the panel is unloaded\",\n        }\n        ctx.panel.set_state(\"event\", \"on_unload\")\n        ctx.panel.set_data(\"event_data\", event)\n\n    def on_change_ctx(self, ctx):\n        \"\"\"Implement this method to set panel state/data when any aspect\n        of the execution context (view, selected samples, filters, etc.) changes.\n\n        The current execution context will be available via ``ctx``.\n        \"\"\"\n        event = {\n            \"data\": {\n                \"view\": ctx.view._serialize(),\n                \"selected\": ctx.selected,\n                \"has_custom_view\": ctx.has_custom_view,\n            },\n            \"description\": \"the current ExecutionContext\",\n        }\n        ctx.panel.set_state(\"event\", \"on_change_ctx\")\n        ctx.panel.set_data(\"event_data\", event)\n\n    def on_change_dataset(self, ctx):\n        \"\"\"Implement this method to set panel state/data when the current\n        dataset is changed.\n\n        The new dataset will be available via ``ctx.dataset``.\n        \"\"\"\n        event = {\n            \"data\": ctx.dataset.name,\n            \"description\": \"the current dataset name\",\n        }\n        ctx.panel.set_state(\"event\", \"on_change_dataset\")\n        ctx.panel.set_data(\"event_data\", event)\n\n    def on_change_view(self, ctx):\n        \"\"\"Implement this method to set panel state/data when the current\n        view is changed.\n\n        The new view will be available via ``ctx.view``.\n        \"\"\"\n        event = {\n            \"data\": ctx.view._serialize(),\n            \"description\": \"the current view\",\n        }\n        ctx.panel.set_state(\"event\", \"on_change_view\")\n        ctx.panel.set_data(\"event_data\", event)\n\n    def on_change_spaces(self, ctx):\n        \"\"\"Implement this method to set panel state/data when the current\n        spaces layout changes.\n\n        The current spaces layout will be available via ``ctx.spaces``.\n        \"\"\"\n        event = {\n            \"data\": ctx.spaces,\n            \"description\": \"the current spaces layout\",\n        }\n        ctx.panel.set_state(\"event\", \"on_change_spaces\")\n        ctx.panel.set_data(\"event_data\", event)\n\n    def on_change_current_sample(self, ctx):\n        \"\"\"Implement this method to set panel state/data when a new sample\n        is loaded in the Sample modal.\n\n        The ID of the new sample will be available via\n        ``ctx.current_sample``.\n        \"\"\"\n        event = {\n            \"data\": ctx.current_sample,\n            \"description\": \"the current sample\",\n        }\n        ctx.panel.set_state(\"event\", \"on_change_current_sample\")\n        ctx.panel.set_data(\"event_data\", event)\n\n    def on_change_selected(self, ctx):\n        \"\"\"Implement this method to set panel state/data when the current\n        selection changes (eg in the Samples panel).\n\n        The IDs of the current selected samples will be available via\n        ``ctx.selected``.\n        \"\"\"\n        event = {\n            \"data\": ctx.selected,\n            \"description\": \"the current selection\",\n        }\n        ctx.panel.set_state(\"event\", \"on_change_selected\")\n        ctx.panel.set_data(\"event_data\", event)\n\n    def on_change_selected_labels(self, ctx):\n        \"\"\"Implement this method to set panel state/data when the current\n        selected labels change (eg in the Sample modal).\n\n        Information about the current selected labels will be available\n        via ``ctx.selected_labels``.\n        \"\"\"\n        event = {\n            \"data\": ctx.selected_labels,\n            \"description\": \"the current selected labels\",\n        }\n        ctx.panel.set_state(\"event\", \"on_change_selected_labels\")\n        ctx.panel.set_data(\"event_data\", event)\n\n    def on_change_extended_selection(self, ctx):\n        \"\"\"Implement this method to set panel state/data when the current\n        extended selection changes.\n\n        The IDs of the current extended selection will be available via\n        ``ctx.extended_selection``.\n        \"\"\"\n        event = {\n            \"data\": ctx.extended_selection,\n            \"description\": \"the current extended selection\",\n        }\n        ctx.panel.set_state(\"event\", \"on_change_extended_selection\")\n        ctx.panel.set_data(\"event_data\", event)\n\n    def on_change_group_slice(self, ctx):\n        \"\"\"Implement this method to set panel state/data when the current\n        group slice changes.\n\n        The current group slice will be available via ``ctx.group_slice``.\n        \"\"\"\n        event = {\n            \"data\": ctx.group_slice,\n            \"description\": \"the current group slice\",\n        }\n        ctx.panel.set_state(\"event\", \"on_change_group_slice\")\n        ctx.panel.set_data(\"event_data\", event)\n\n    #######################################################################\n    # Custom events\n    # These events are defined by user code above and, just like builtin\n    # events, take `ctx` as input and are followed by a call to render()\n    #######################################################################\n\n    def on_change_brain_key(self, ctx):\n        # Load expensive content based on current `brain_key`\n        brain_key = ctx.panel.get_state(\"menu.brain_key\")\n        results = ctx.dataset.load_brain_results(brain_key)\n\n        # Format results for plotly\n        x, y = zip(*results.points.tolist())\n        ids = results.sample_ids\n\n        plot_data = [\\\n            {\"x\": x, \"y\": y, \"ids\": ids, \"type\": \"scatter\", \"mode\": \"markers\"}\\\n        ]\n\n        # Store large content as panel data for efficiency\n        ctx.panel.set_data(\"embeddings\", plot_data)\n\n        # Show plot with embeddings data instead of the compute visualization button\n        ctx.panel.set_state(\"show_start_button\", False)\n\n    def on_click_start(self, ctx):\n        # Launch an interactive prompt for user to execute an operator\n        ctx.prompt(\"@voxel51/brain/compute_visualization\")\n\n        # Lightweight state update\n        ctx.panel.set_state(\"show_start_button\", False)\n\n    def on_click_learn_more(self, ctx):\n        # Trigger a builtin operation via `ctx.ops`\n        url = \"https://docs.voxel51.com/plugins/developing_plugins.html\"\n        ctx.ops.notify(f\"Check out {url} for more information\")\n\n    def on_selected_embeddings(self, ctx):\n        # Get selected points from event params\n        selected_points = ctx.params.get(\"data\", [])\n        selected_sample_ids = [d.get(\"id\", None) for d in selected_points]\n\n        # Conditionally trigger a builtin operation via `ctx.ops`\n        if len(selected_sample_ids) &gt; 0:\n            ctx.ops.set_extended_selection(selected_sample_ids)\n\n    def on_change_show_start_button(self, ctx):\n        # Get current state of the checkbox on change\n        current_state = ctx.params.get(\"value\", None)\n\ndef register(p):\n    \"\"\"Always implement this method and register() each panel that your\n    plugin defines.\n    \"\"\"\n    p.register(ExamplePanel)\n</code></pre> <p></p> <p>Note</p> <p>Remember that you must also include the panel\u2019s name in the plugin\u2019s fiftyone.yml:</p> <pre><code>panels:\n  - example_panel\n</code></pre>"},{"location":"plugins/developing_plugins/#panel-config","title":"Panel config \u00b6","text":"<p>Every panel must define a <code>config</code> property that defines its name, display name, surfaces, and other optional metadata about its behavior:</p> <pre><code>@property\ndef config(self):\n    return foo.PanelConfig(\n        # The panel's URI: f\"{plugin_name}/{name}\"\n        name=\"example_panel\",  # required\n\n        # The display name of the panel in the \"+\" menu\n        label=\"Example panel\",  # required\n\n        # Custom icons to use in the \"+\"\" menu\n        # Can be a URL, a local path in the plugin directory, or the\n        # name of a MUI icon: https://marella.me/material-icons/demo\n        icon=\"/assets/icon.svg\",\n        light_icon=\"/assets/icon-light.svg\",  # light theme only\n        dark_icon=\"/assets/icon-dark.svg\",  # dark theme only\n\n        # Whether to allow multiple instances of the panel to be opened\n        allow_multiple=False,\n\n        # Whether the panel should be available in the grid, modal, or both\n        # Possible values: \"grid\", \"modal\", \"grid modal\"\n        surfaces=\"grid\",  # default = \"grid\"\n\n        # Markdown-formatted text that describes the panel. This is\n        # rendererd in a tooltip when the help icon in the panel\n        # title is hovered over\n        help_markdown=\"A description of the panel\",\n    )\n</code></pre> <p>The <code>surfaces</code> key defines the panel\u2019s scope:</p> <ul> <li> <p>Grid panels can be accessed from the <code>+</code> button in the App\u2019s grid view, which allows you to build macro experiences that work with entire datasets or views</p> </li> <li> <p>Modal panels can be accessed from the <code>+</code> button in the App\u2019s modal view, which allows you to build interactions that focus on individual samples and scenarios</p> </li> </ul> <p>Note</p> <p>For an example of a modal panel, refer to the label count panel.</p>"},{"location":"plugins/developing_plugins/#execution-context_1","title":"Execution context \u00b6","text":"<p>An <code>ExecutionContext</code> is passed to each of the panel\u2019s methods at runtime. This <code>ctx</code> contains static information about the current state of the App (dataset, view, panel, selection, etc) as well as dynamic information about the panel\u2019s current state and data.</p> <p>See this section for a full description of the execution context.</p>"},{"location":"plugins/developing_plugins/#panel-state-and-data","title":"Panel state and data \u00b6","text":"<p>Panels provide two mechanisms for persisting information: panel state and panel data.</p>"},{"location":"plugins/developing_plugins/#basic-structure","title":"Basic structure \u00b6","text":"<p>Panel state can be accessed and updated via <code>ctx.panel.state</code>, and panel data can be updated (but not accessed) via <code>ctx.panel.data</code>.</p> <p>Under the hood, panel state and data is merged into a single nested object that maps 1-1 to the structure and naming of the properties defined by the panel\u2019s <code>render()</code> method.</p> <p>The example code below shows how to access and update panel state.</p> <p>Note</p> <p>Since panel state and panel data are merged into a single object, it is important to avoid naming conflicts between state and data keys. If a key is present in both panel state and data, the value in panel data will be used.</p> <pre><code>class CounterPanel(foo.Panel):\n    @property\n    def config(self):\n        return foo.PanelConfig(\n            name=\"counter_panel\", label=\"Counter Panel\", icon=\"123\"\n        )\n\n    def on_load(self, ctx):\n        ctx.panel.state.v_stack = {\"h_stack\": {\"count\": 3}}\n\n    def increment(self, ctx):\n        count = ctx.panel.state.get(\"v_stack.h_stack.count\", 0)\n        ctx.panel.state.set(\"v_stack.h_stack.count\", count + 1)\n\n    def decrement(self, ctx):\n        count = ctx.panel.get_state(\"v_stack.h_stack.count\", 0)\n        ctx.panel.set_state(\"v_stack.h_stack.count\", count - 1)\n\n    def render(self, ctx):\n        panel = types.Object()\n\n        # Define a vertical stack object with the name 'v_stack'\n        # key: 'v_stack'\n        v_stack = panel.v_stack(\"v_stack\", align_x=\"center\", gap=2)\n\n        # Define a horizontal stack object with the name 'h_stack' on 'v_stack'\n        # key: 'v_stack.h_stack'\n        h_stack = v_stack.h_stack(\"h_stack\", align_y=\"center\")\n\n        # Get state\n        v_stack_state = ctx.panel.state.v_stack\n        h_stack_state = v_stack_state[\"h_stack\"] if v_stack_state is not None else None\n        count = h_stack_state[\"count\"] if h_stack_state is not None else 0\n\n        # Add a message to the horizontal stack object with the name 'count'\n        # key: 'v_stack.h_stack.count'\n        h_stack.message(\"count\", f\"Count: {count}\")\n\n        # Add a button to the horizontal stack object with the name 'increment'\n        # key: 'v_stack.h_stack.increment'\n        h_stack.btn(\n            \"increment\",\n            label=\"Increment\",\n            icon=\"add\",\n            on_click=self.increment,\n            variant=\"contained\",\n        )\n\n        # Add a button to the horizontal stack object with the name 'decrement'\n        # key: 'v_stack.h_stack.count'\n        h_stack.btn(\n            \"decrement\",\n            label=\"Decrement\",\n            icon=\"remove\",\n            on_click=self.decrement,\n            variant=\"contained\",\n        )\n\n        return types.Property(panel)\n</code></pre> <p></p>"},{"location":"plugins/developing_plugins/#panel-state","title":"Panel state \u00b6","text":"<p>Panel state is included in every <code>render()</code> call and event callback and is analogous to operator parameters:</p> <ul> <li> <p>The values of any components defined in a panel\u2019s <code>render()</code> method are available via corresponding state properties of the same name</p> </li> <li> <p>The current panel state is readable during a panel\u2019s execution</p> </li> </ul> <pre><code>def render(self, ctx):\n    panel = types.Object()\n\n    menu = panel.menu(\"menu\", ...)\n    actions = menu.btn_group(\"actions\")\n    actions.enum(\n        \"mode\",\n        values=[\"foo\", \"bar\"],\n        on_change=self.on_change_mode,\n        ...\n    )\n\n    panel.str(\"user_input\", default=\"spam\")\n\ndef on_change_mode(self, ctx):\n    # Object-based interface\n    mode = ctx.panel.state.menu.actions.mode\n    user_input = ctx.panel.state.user_input\n\n    # Functional interface\n    mode = ctx.panel.get_state(\"menu.actions.mode\")\n    user_input = ctx.panel.get_state(\"user_input\")\n</code></pre> <p>Panel state can be programmatically updated in panel methods via the two syntaxes shown below:</p> <pre><code>def on_change_view(self, ctx):\n    # Top-level state attributes can be modified by setting properties\n    ctx.panel.state.foo = \"bar\"\n\n    # Use set_state() to efficiently apply nested updates\n    ctx.panel.set_state(\"foo.bar\", {\"spam\": \"eggs\"})\n</code></pre> <p>Warning</p> <p>Don\u2019t directly modify panel state in <code>render()</code>, just like how <code>setState()</code> should not be called in React\u2019s render().</p> <p>Instead set panel state in event callbacks as demonstrated above.</p>"},{"location":"plugins/developing_plugins/#panel-data","title":"Panel data \u00b6","text":"<p>Panel data is designed to store larger content such as plot data that is loaded once and henceforward stored only clientside to avoid unnecessary/expensive reloads and serverside serialization during the lifecycle of the panel.</p> <pre><code>def on_load(self, ctx):\n    self.update_plot_data(ctx)\n\ndef render(self, ctx):\n    panel = types.Object()\n\n    menu = panel.menu(\"menu\", ...)\n    actions = menu.btn_group(\"actions\")\n    actions.enum(\n        \"brain_key\",\n        label=\"Brain key\",\n        values=[\"foo\", \"bar\"],\n        default=None,\n        on_change=self.update_plot_data,\n    )\n\n    panel.plot(\"embeddings\", config=..., layout=...)\n\n    return types.Property(panel)\n\ndef update_plot_data(self, ctx):\n    brain_key = ctx.panel.state.menu.actions.brain_key\n    if brain_key is None:\n        return\n\n    # Load expensive content based on current `brain_key`\n    results = ctx.dataset.load_brain_results(brain_key)\n\n    # Store large content as panel data for efficiency\n    data = {\"points\": results.points, ...}\n    ctx.panel.set_data(\"embeddings\", data)\n</code></pre> <p>Note how the panel\u2019s <code>on_load()</code> hook is implemented so that panel data can be hydrated when the panel is initially loaded, and then subsequently plot data is loaded only when the <code>brain_key</code> property is modified.</p> <p>Note</p> <p>Panel data is never readable in Python; it is only implicitly used by the types you define when they are rendered clientside.</p>"},{"location":"plugins/developing_plugins/#execution-store","title":"Execution store \u00b6","text":"<p>Panels can store data in the execution store, which is a key-value store that is persisted beyond the lifetime of the panel. This is useful for storing information that should persist across panel instances and App sessions, such as cached data, long-lived panel state, or user preferences.</p> <p>You can create/retrieve execution stores scoped to the current <code>ctx.dataset</code> via <code>ctx.store</code>:</p> <pre><code>def on_load(ctx):\n    # Retrieve a store scoped to the current `ctx.dataset`\n    # The store is automatically created if necessary\n    store = ctx.store(\"my_store\")\n\n    # Load a pre-existing value from the store\n    user_choice = store.get(\"user_choice\")\n\n    # Store data with a TTL to ensure it is evicted after `ttl` seconds\n    store.set(\"my_key\", {\"foo\": \"bar\"}, ttl=60)\n\n    # List all keys in the store\n    print(store.list_keys())  # [\"user_choice\", \"my_key\"]\n\n    # Retrieve data from the store\n    print(store.get(\"my_key\"))  # {\"foo\": \"bar\"}\n\n    # Retrieve metadata about a key\n    print(store.get_metadata(\"my_key\"))\n    # {\"created_at\": ..., \"updated_at\": ..., \"expires_at\": ...}\n\n    # Delete a key from the store\n    store.delete(\"my_key\")\n\n    # Clear all data in the store\n    store.clear()\n</code></pre> <p>Note</p> <p>Did you know? Any execution stores associated with a dataset are automatically deleted when the dataset is deleted.</p> <p>For advanced use cases, it is also possible to create and use global stores that are available to all datasets via the <code>ExecutionStore</code> class:</p> <pre><code>from fiftyone.operators import ExecutionStore\n\n# Retrieve a global store\n# The store is automatically created if necessary\nstore = ExecutionStore.create(\"my_store\")\n\n# Store data with a TTL to ensure it is evicted after `ttl` seconds\nstore.set(\"my_key\", {\"foo\": \"bar\"}, ttl=60)\n\n# List all keys in the global store\nprint(store.list_keys())  # [\"my_key\"]\n\n# Retrieve data from the global store\nprint(store.get(\"my_key\"))  # {\"foo\": \"bar\"}\n\n# Retrieve metadata about a key\nprint(store.get_metadata(\"my_key\"))\n# {\"created_at\": ..., \"updated_at\": ..., \"expires_at\": ...}\n\n# Delete a key from the global store\nstore.delete(\"my_key\")\n\n# Clear all data in the global store\nstore.clear()\n</code></pre> <p>Warning</p> <p>Global stores have no automatic garbage collection, so take care when creating and using global stores whose keys do not utilize TTLs.</p>"},{"location":"plugins/developing_plugins/#saved-workspaces","title":"Saved workspaces \u00b6","text":"<p>Saved workspaces may contain any number of Python panels!</p> <p>When a workspace is saved, the current panel state of any panels in the layout is persisted as part of the workspace\u2019s definition. Thus when the workspace is loaded later, all panels will \u201cremember\u201d their state.</p> <p>Panel data (which may be large), on the other hand, is not explicitly persisted. Instead it should be hydrated when the panel is loaded using the pattern demonstrated here.</p>"},{"location":"plugins/developing_plugins/#accessing-secrets_1","title":"Accessing secrets \u00b6","text":"<p>Panels can access secrets defined by their plugin.</p> <p>At runtime, the panel\u2019s execution context is automatically hydrated with any available secrets that are declared by the plugin. Panels can access these secrets via the <code>ctx.secrets</code> dict:</p> <pre><code>def on_load(self, ctx):\n    url = ctx.secrets[\"FIFTYONE_CVAT_URL\"]\n    username = ctx.secrets[\"FIFTYONE_CVAT_USERNAME\"]\n    password = ctx.secrets[\"FIFTYONE_CVAT_PASSWORD\"]\n    email = ctx.secrets[\"FIFTYONE_CVAT_EMAIL\"]\n</code></pre>"},{"location":"plugins/developing_plugins/#common-patterns","title":"Common patterns \u00b6","text":"<p>Most panels make use of common patterns like callbacks, menus, interactive plots, and walkthrough layouts.</p> <p>Learning the patterns described below will help you build panels faster and avoid roadblocks along the way.</p> <p>Note</p> <p>Check out the panel examples plugin to see a collection of fully-functional panels that demonstrate the common patterns below.</p>"},{"location":"plugins/developing_plugins/#callbacks","title":"Callbacks \u00b6","text":"<p>Most panel components support callback methods like <code>on_click</code> and <code>on_change</code> that you can implement to perform operations and trigger state updates when users interact with the components.</p> <p>For example, the code below shows how clicking a button or changing the state of a slider can initiate callbacks that trigger operators, open other panels, and programmatically modify the current state.</p> <p>Note</p> <p>All callback functions have access to the current <code>ExecutionContext</code> via their <code>ctx</code> argument and can use it to get/update panel state and trigger other operations.</p> <pre><code>def on_load(self, ctx):\n    # Set initial slider state\n    ctx.panel.state.slider_value = 5\n\ndef open_compute(self, ctx):\n    # Launch an interactive prompt for user to execute an operator\n    ctx.prompt(\"@voxel51/brain/compute_visualization\")\n\ndef open_embeddings(self, ctx):\n    # Open embeddings panel\n    ctx.trigger(\"open_panel\", params=dict(name=\"Embeddings\"))\n\ndef change_value(self, ctx):\n    # Grab current slider value from `ctx.params`\n    ctx.panel.state.slider_value = (\n        ctx.params[\"value\"] or ctx.params[\"panel_state\"][\"slider_value\"]\n    )\n\ndef render(self, ctx):\n    panel = types.Object()\n\n    # Define buttons that work with on_click callbacks\n    panel.btn(\n        \"button_1\",\n        label=\"Compute visualization\",\n        on_click=self.open_compute,\n    )\n    panel.btn(\n        \"button_2\",\n        label=\"Open embeddings panel\",\n        on_click=self.open_embeddings,\n    )\n\n    # Define a slider with an `on_change` callback\n    slider = types.SliderView(\n        data=ctx.panel.state.slider_value, label=\"Example Slider\"\n    )\n    schema = {\"min\": 0, \"max\": 10, \"multipleOf\": 1}\n    panel.int(\n        \"slider_value\", view=slider, on_change=self.change_value, **schema\n    )\n</code></pre> <p>Note</p> <p>Did you know? You can use <code>ctx.params</code> in a callback to access the state of the property that triggered the action.</p>"},{"location":"plugins/developing_plugins/#dropdown-menus","title":"Dropdown menus \u00b6","text":"<p>Dropdown menus can be a useful tool to build panels whose layout/content dynamically changes based on the current state of the menu.</p> <p>Here\u2019s an example of a dropdown menu with selectable options that alters the panel layout based on user input.</p> <p>Note</p> <p>Panels also support a <code>menu()</code> property that provides a convenient syntax for defining a group of dropdowns, buttons, etc that can be anchored to a particular position in your panel (e.g., top-left).</p> <p>Check out this section for an example panel that makes use of <code>menu()</code>.</p> <pre><code>class DropdownMenuExample(foo.Panel):\n    @property\n    def config(self):\n        return foo.PanelConfig(\n            name=\"example_dropdown_menu\",\n            label=\"Examples: Dropdown Menu\",\n        )\n\n    def on_load(self, ctx):\n        ctx.panel.state.selection = None\n\n    def alter_selection(self, ctx):\n        ctx.panel.state.selection = ctx.params[\"value\"]\n\n    def refresh_page(self, ctx):\n        ctx.ops.reload_dataset()\n\n    def reload_samples(self, ctx):\n        ctx.ops.reload_samples()\n\n    def say_hi(self, ctx):\n        ctx.ops.notify(\"Hi!\", variant=\"success\")\n\n    def render(self, ctx):\n        panel = types.Object()\n\n        panel.md(\n            \"\"\"\n            ### Welcome to the Python Panel Dropdown Menu Example\n            Use the menu below to select what you would like to do next!\n\n            ---\n\n        \"\"\",\n            name=\"header\",\n            width=50,  # 50% of current panel width\n            height=\"200px\",\n        )\n\n        # Define a dropdown menu and add choices\n        dropdown = types.DropdownView()\n        dropdown.add_choice(\n            \"refresh\",\n            label=\"Display Refresh Button\",\n            description=\"Displays button that will refresh the FiftyOne App\",\n        )\n        dropdown.add_choice(\n            \"reload_samples\",\n            label=\"Display Reload Samples Button\",\n            description=\"Displays button that will reload the samples view\",\n        )\n        dropdown.add_choice(\n            \"say_hi\",\n            label=\"Display Hi Button\",\n            description=\"Displays button that will say hi\",\n        )\n\n        # Add dropdown menu to the panel as a view and use the `on_change`\n        # callback to trigger `alter_selection`\n        panel.view(\n            \"dropdown\",\n            view=dropdown,\n            label=\"Dropdown Menu\",\n            on_change=self.alter_selection,\n        )\n\n        # Change panel visual state dependent on dropdown menu selection\n        if ctx.panel.state.selection == \"refresh\":\n            panel.btn(\n                \"refresh\",\n                label=\"Refresh FiftyOne\",\n                on_click=self.refresh_page,\n                variant=\"contained\",\n            )\n        elif ctx.panel.state.selection == \"reload_samples\":\n            panel.btn(\n                \"reload_samples\",\n                label=\"Reload Samples\",\n                on_click=self.reload_samples,\n                variant=\"contained\",\n            )\n        elif ctx.panel.state.selection == \"say_hi\":\n            panel.btn(\n                \"say_hi\",\n                label=\"Say Hi\",\n                on_click=self.say_hi,\n                variant=\"contained\",\n            )\n\n        return types.Property(\n            panel,\n            view=types.GridView(\n                height=100,\n                width=100,\n                align_x=\"center\",\n                align_y=\"center\",\n                orientation=\"vertical\",\n            ),\n        )\n</code></pre> <p></p>"},{"location":"plugins/developing_plugins/#interactive-plots","title":"Interactive plots \u00b6","text":"<p>Panels provide native support for defining interactive plots that can render data from the current dataset and dynamically update or trigger actions as users interact with the plots.</p> <p>For example, here\u2019s a panel that displays a histogram of a specified field of the current dataset where clicking a bar loads the corresponding samples in the App.</p> <pre><code>import fiftyone.operators as foo\nimport fiftyone.operators.types as types\nfrom fiftyone import ViewField as F\n\nclass InteractivePlotExample(foo.Panel):\n    @property\n    def config(self):\n        return foo.PanelConfig(\n            name=\"example_interactive_plot\",\n            label=\"Examples: Interactive Plot\",\n            icon=\"bar_chart\",\n        )\n\n    def on_load(self, ctx):\n        # Get target field\n        target_field = (\n            ctx.panel.state.target_field or \"ground_truth.detections.label\"\n        )\n        ctx.panel.state.target_field = target_field\n\n        # Compute target histogram for current dataset\n        counts = ctx.dataset.count_values(target_field)\n        keys, values = zip(*sorted(counts.items(), key=lambda x: x[0]))\n\n        # Store as panel data for efficiency\n        ctx.panel.data.histogram = {\"x\": keys, \"y\": values, \"type\": \"bar\"}\n\n        # Launch panel in a horizontal split view\n        ctx.ops.split_panel(\"example_interactive_plot\", layout=\"horizontal\")\n\n    def on_change_view(self, ctx):\n        # Update histogram when current view changes\n        self.on_load(ctx)\n\n    def on_histogram_click(self, ctx):\n        # The histogram bar that the user clicked\n        value = ctx.params.get(\"x\")\n\n        # Create a view that matches the selected histogram bar\n        field = ctx.panel.state.target_field\n        view = _make_matching_view(ctx.dataset, field, value)\n\n        # Load view in App\n        if view is not None:\n            ctx.ops.set_view(view=view)\n\n    def reset(self, ctx):\n        ctx.ops.clear_view()\n        self.on_load(ctx)\n\n    def render(self, ctx):\n        panel = types.Object()\n\n        panel.plot(\n            \"histogram\",\n            layout={\n                \"title\": {\n                    \"text\": \"Interactive Histogram\",\n                    \"xanchor\": \"center\",\n                    \"yanchor\": \"top\",\n                    \"automargin\": True,\n                },\n                \"xaxis\": {\"title\": \"Labels\"},\n                \"yaxis\": {\"title\": \"Count\"},\n            },\n            on_click=self.on_histogram_click,\n            width=100,\n        )\n\n        panel.btn(\n            \"reset\",\n            label=\"Reset Chart\",\n            on_click=self.reset,\n            variant=\"contained\",\n        )\n\n        return types.Property(\n            panel,\n            view=types.GridView(\n                align_x=\"center\",\n                align_y=\"center\",\n                orientation=\"vertical\",\n                height=100,\n                width=100,\n                gap=2,\n                padding=0,\n            ),\n        )\n\ndef _make_matching_view(dataset, field, value):\n    if field.endswith(\".label\"):\n        root_field = field.split(\".\")[0]\n        return dataset.filter_labels(root_field, F(\"label\") == value)\n    elif field == \"tags\":\n        return dataset.match_tags(value)\n    else:\n        return dataset.match(F(field) == value)\n</code></pre> <p></p>"},{"location":"plugins/developing_plugins/#walkthroughs","title":"Walkthroughs \u00b6","text":"<p>You can use a combination of panel objects like markdown, buttons, arrow navigation, and layout containers to create guided walkthroughs similar to the ones at try.fiftyone.ai.</p> <p>Here\u2019s an example of a panel that leads the user through multiple steps of a guided workflow.</p> <pre><code>class WalkthroughExample(foo.Panel):\n    @property\n    def config(self):\n        return foo.PanelConfig(\n            name=\"example_walkthrough\",\n            label=\"Examples: Walkthrough\",\n        )\n\n    def on_load(self, ctx):\n        ctx.panel.state.page = 1\n        info_table = [\\\n            {\\\n                \"Dataset Name\": f\"{ctx.dataset.name}\",\\\n                \"Dataset Description\": \"FiftyOne Quick Start Zoo Dataset\",\\\n                \"Number of Samples\": f\"{ctx.dataset.count()}\",\\\n            },\\\n        ]\n\n    ctx.panel.state.info_table = info_table\n\n    def go_to_next_page(self, ctx):\n        ctx.panel.state.page = ctx.panel.state.page + 1\n\n    def go_to_previous_page(self, ctx):\n        ctx.panel.state.page = ctx.panel.state.page - 1\n\n    def reset_page(self, ctx):\n        ctx.panel.state.page = 1\n\n    def open_operator_io(self, ctx):\n        ctx.ops.open_panel(\"OperatorIO\")\n\n    def render(self, ctx):\n        panel = types.Object()\n\n        # Define a vertical stack to live inside your panel\n        stack = panel.v_stack(\n            \"welcome\", gap=2, width=75, align_x=\"center\", align_y=\"center\"\n        )\n        button_container = types.GridView(\n            gap=2, align_x=\"left\", align_y=\"center\"\n        )\n\n        page = ctx.panel.state.get(\"page\", 1)\n\n        if page == 1:\n            stack.md(\n                \"\"\"\n                ### A Tutorial Walkthrough\n\n                Welcome to the FiftyOne App! Here is a great example of what it looks like to create a tutorial style walkthrough via a Python Panel.\n            \"\"\",\n                name=\"markdown_screen_1\",\n            )\n            stack.media_player(\n                \"video\",\n                \"https://youtu.be/ad79nYk2keg\",\n                align_x=\"center\",\n                align_y=\"center\",\n            )\n        elif page == 2:\n            stack.md(\n                \"\"\"\n                ### Information About Your Dataset\n\n                Perhaps you would like to know some more information about your dataset?\n            \"\"\",\n                name=\"markdown_screen_2\",\n            )\n            table = types.TableView()\n            table.add_column(\"Dataset Name\", label=\"Dataset Name\")\n            table.add_column(\"Dataset Description\", label=\"Description\")\n            table.add_column(\"Number of Samples\", label=\"Number of Samples\")\n\n            panel.obj(\n                name=\"info_table\",\n                view=table,\n                label=\"Cool Info About Your Data\",\n            )\n        elif page == 3:\n            if ctx.panel.state.operator_status != \"opened\":\n                stack.md(\n                    \"\"\"\n                    ### One Last Trick\n\n                    If you want to do something cool, click the button below.\n                \"\"\",\n                    name=\"markdown_screen_3\",\n                )\n                btns = stack.obj(\"top_btns\", view=button_container)\n                btns.type.btn(\n                    \"open_operator_io\",\n                    label=\"Do Something Cool\",\n                    on_click=self.open_operator_io,\n                    variant=\"contained\"\n                )\n        else:\n            stack.md(\n                \"\"\"\n                #### How did you get here?\n                Looks like you found the end of the walkthrough. Or have you gotten a little lost in the grid? No worries, let's get you back to the walkthrough!\n            \"\"\"\n            )\n            btns = stack.obj(\"btns\", view=button_container)\n            btns.type.btn(\"reset\", label=\"Go Home\", on_click=self.reset_page)\n\n        # Arrow navigation to go to next or previous page\n        panel.arrow_nav(\n            \"arrow_nav\",\n            forward=page != 3,  # hidden for the last page\n            backward=page != 1,  # hidden for the first page\n            on_forward=self.go_to_next_page,\n            on_backward=self.go_to_previous_page,\n        )\n\n        return types.Property(\n            panel,\n            view=types.GridView(\n                height=100, width=100, align_x=\"center\", align_y=\"center\"\n            ),\n        )\n</code></pre> <p></p>"},{"location":"plugins/developing_plugins/#displaying-multimedia","title":"Displaying multimedia \u00b6","text":"<p>Displaying images, videos, and other forms of multimedia is straightforward in panels. You can embed third-party resources like URLs or load multimedia stored in local directories.</p> <p>Here are some examples of panels that load, render, and manipulate various forms of image and video data.</p> <p></p>"},{"location":"plugins/developing_plugins/#type-hints","title":"Type hints \u00b6","text":"<p>Defining the types of your panel\u2019s function arguments allows you to inspect the methods available to an object and will dramatically help you increase your speed of development.</p> <p>With type hints, your IDE can preview helpful docstrings, trace <code>fiftyone</code> source code, and see what methods exist on your object during the development process.</p> <p>For example, declaring that the <code>ctx</code> variable has type <code>ExecutionContext</code> allows you to reveal all of its available methods during development:</p> <pre><code>from fiftyone.operators import ExecutionContext\n\ndef on_load(ctx: ExecutionContext):\n    ctx.trigger()\n    ctx.ops()\n    ctx.secrets()\n\n    # Reveals the remaining methods available to ctx\n    ctx.\n    ...\n</code></pre>"},{"location":"plugins/developing_plugins/#developing-js-plugins","title":"Developing JS plugins \u00b6","text":"<p>This section describes how to develop JS-specific plugin components.</p>"},{"location":"plugins/developing_plugins/#getting-started","title":"Getting Started \u00b6","text":"<p>To start building your own JS plugin, refer to the hello-world-plugin-js repository. This repo serves as a starting point, providing examples of a build process, a JS panel, and a JS operator.</p> <p>The fiftyone-js-plugin-build package offers a utility for configuring vite to build your JS plugin bundle.</p>"},{"location":"plugins/developing_plugins/#component-types","title":"Component types \u00b6","text":"<p>JS plugins may register components to add or customize functionality within the FiftyOne App. Each component is registered with an activation function. The component will only be considered for rendering when the activation function returns <code>true</code>:</p> <ul> <li> <p>Panel: JS plugins can register panel components that can be opened by clicking the <code>+</code> next to any existing panel\u2019s tab</p> </li> <li> <p>Component: JS plugins can register generic components that can be used to render operator input and output</p> </li> </ul>"},{"location":"plugins/developing_plugins/#panels-and-components","title":"Panels and Components \u00b6","text":"<p>Here\u2019s some examples of using panels and components to add your own custom user interface and components to the FiftyOne App.</p>"},{"location":"plugins/developing_plugins/#hello-world-panel","title":"Hello world panel \u00b6","text":"<p>A simple plugin that renders \u201cHello world\u201d in a panel would look like this:</p> <pre><code>import { registerComponent, PluginComponentTypes } from \"@fiftyone/plugins\";\n\nfunction HelloWorld() {\n    return &lt;h1&gt;Hello world&lt;/h1&gt;;\n}\n\nregisterComponent({\n    name: \"HelloWorld\",\n    label: \"Hello world\",\n    component: HelloWorld,\n    type: PluginComponentTypes.Panel,\n    activator: () =&gt; true\n});\n</code></pre>"},{"location":"plugins/developing_plugins/#adding-a-custom-panel","title":"Adding a custom Panel \u00b6","text":"<pre><code>import * as fop from \"@fiftyone/plugins\";\nimport * as fos from \"@fiftyone/state\";\nimport * as foa from \"@fiftyone/aggregations\";\nimport AwesomeMap from \"react-mapping-library\";\n\nfunction CustomPanel() {\n    const dataset = useRecoilValue(fos.dataset);\n    const view = useRecoilValue(fos.view);\n    const filters = useRecoilValue(fos.filters);\n    const [aggregate, points, loading] = foa.useAggregation({\n        dataset,\n        filters,\n        view,\n    });\n\n    React.useEffect(() =&gt; {\n        aggregate(\n            [\\\n                new foa.aggregations.Values({\\\n                    fieldOrExpr: \"id\",\\\n                }),\\\n                new foa.aggregations.Values({\\\n                    fieldOrExpr: \"location.point.coordinates\",\\\n                }),\\\n            ],\n            dataset.name\n        );\n    }, [dataset, filters, view]);\n\n    if (loading) return &lt;h1&gt;Loading&lt;/h1&gt;;\n\n    return &lt;MyMap geoPoints={points} /&gt;;\n}\n\nfop.registerComponent({\n    // component to delegate to\n    component: CustomPanel,\n\n    // tell FiftyOne you want to provide a custom panel\n    type: PluginComponentTypes.Panel,\n\n    // used for the panel selector button\n    label: \"Map\",\n\n    // only show the Map panel when the dataset has Geo data\n    activator: ({ dataset }) =&gt; dataset.sampleFields.location,\n});\n</code></pre>"},{"location":"plugins/developing_plugins/#custom-operator-view-using-component-plugin","title":"Custom operator view using component plugin \u00b6","text":"<p>Creating and registering a custom view type:</p> <pre><code>import * as fop from \"@fiftyone/plugins\";\nimport { useState } from \"react\"\n\nfunction CustomOperatorView(props) {\n    // these props are provided to the component used as the view for an\n    // operator input/output field\n    const { errors, data, id, onChange, path, schema } = props\n\n    // schema may optionally include a view property which contains\n    // attributes such label, description, caption for\n    // the field. Schema will also provide a type property to indicate the type\n    // of value expected for the field (i.e. string, number, object, array, etc.)\n    const { default: defaultValue, view, type } = schema\n\n    // Schema may also provide a default value for the field\n    const [value, setValue] = useState(defaultValue)\n\n    return (\n        &lt;div&gt;\n            &lt;label htmlFor={id}&gt;{view.label}&lt;/label&gt;\n            &lt;input\n                value={value}\n                id={id}\n                type={type}\n                onChange={(e) =&gt; {\n                    // onChange function passed as a prop can be called with\n                    // path and value to set the current value for a field\n                    onChange(path, e.target.value)\n                }}\n            /&gt;\n        &lt;/div&gt;\n    )\n}\n\nfop.registerComponent({\n    // unique name you can use later to refer to the component plugin\n    name: \"CustomOperatorView\",\n\n    // component to delegate to\n    component: CustomOperatorView,\n\n    // tell FiftyOne you want to provide a custom component\n    type: PluginComponentTypes.Component,\n\n    // activate this plugin unconditionally\n    activator: () =&gt; true,\n});\n</code></pre> <p>Using the custom component as the view for a Python operator field:</p> <pre><code>import fiftyone.operators as foo\nimport fiftyone.operators.types as types\n\nclass CustomViewOperator(foo.Operator):\n    @property\n    def config(self):\n        return foo.OperatorConfig(\n            name=\"custom_view_operator\",\n            label=\"Custom View Operator\",\n        )\n\n    def resolve_input(self, ctx):\n        inputs = types.Object()\n        inputs.str(\n            \"name\",\n            label=\"Name\",\n            default=\"FiftyOne\",\n            # provide the name of a registered component plugin\n            view=types.View(component=\"CustomOperatorView\")\n        )\n        return types.Property(inputs)\n\n    def execute(self, ctx):\n        return {}\n</code></pre>"},{"location":"plugins/developing_plugins/#fiftyone-app-state","title":"FiftyOne App state \u00b6","text":"<p>There are a few ways to manage the state of your plugin. By default you should defer to existing state management in the FiftyOne App.</p> <p>For example, if you want to allow users to select samples, you can use the <code>@fiftyone/state</code> package.</p>"},{"location":"plugins/developing_plugins/#interactivity-and-state","title":"Interactivity and state \u00b6","text":"<p>If your plugin only has internal state, you can use existing state management to achieve your desired UX. For example, in a 3D visualizer, you might want to use Three.js and its object model, events, and state management. Or just use your own React hooks to maintain your plugin components internal state.</p> <p>If you want to allow users to interact with other aspects of FiftyOne through your plugin, you can use the <code>@fiftyone/state</code> package:</p> <pre><code>// note: similar to react hooks, these must be used in the context\n// of a React component\n\n// select a dataset\nconst selectLabel = fos.useOnSelectLabel();\n\n// in a callback\nselectLabel({ id: \"labelId\", field: \"fieldName\" });\n</code></pre> <p>The example above shows how you can coordinate or surface existing features of FiftyOne through your plugin via the <code>@fiftyone/state</code> package. This package provides hooks to access and modify the state of the FiftyOne App.</p>"},{"location":"plugins/developing_plugins/#recoil-atoms-and-selectors","title":"Recoil, atoms, and selectors \u00b6","text":"<p>You can also use a combination of your own and fiftyone\u2019s recoil <code>atoms</code> and <code>selectors</code>.</p> <p>Here\u2019s an example the combines both approaches in a hook that you could call from anywhere where hooks are supported (almost all plugin component types).</p> <pre><code>import {atom, useRecoilValue, useRecoilState} from 'recoil';\n\nconst myPluginmyPluginFieldsState = atom({\n    key: 'myPluginFields',\n    default: []\n})\n\nfunction useMyHook() {\n    const dataset = useRecoilValue(fos.dataset);\n    const [fields, setFields] = useRecoilState(myPluginFieldsState);\n\n    return {\n        dataset,\n        fields,\n        addField: (field) =&gt; setFields([...fields, field])\n    }\n}\n</code></pre>"},{"location":"plugins/developing_plugins/#panel-state_1","title":"Panel state \u00b6","text":"<p>Plugins that provide <code>PluginComponentTypes.Panel</code> components should use the <code>@fiftyone/spaces</code> package to manage their state. This package provides hooks to allow plugins to manage the state of individual panel instances.</p> <pre><code>import { usePanelStatePartial, usePanelTitle } from \"@fiftyone/spaces\";\nimport { Button } from '@fiftyone/components';\n\n// in your panel component, you can use the usePanelStatePartial hook\n// to read and write to the panel state\nfunction MyPanel() {\n    const [state, setState] = usePanelStatePartial('choice');\n    const setTitle = usePanelTitle();\n\n    React.useEffect(() =&gt; {\n      setTitle(`My Panel: ${state}`);\n    }, [state]);\n\n    return (\n      &lt;div&gt;\n        &lt;h1&gt;Choice: {state}&lt;/h1&gt;\n        &lt;Button onClick={() =&gt; setState('A')}&gt;A&lt;/Button&gt;\n        &lt;Button onClick={() =&gt; setState('B')}&gt;B&lt;/Button&gt;\n      &lt;/div&gt;\n    );\n}\n</code></pre>"},{"location":"plugins/developing_plugins/#reading-settings-in-your-plugin","title":"Reading settings in your plugin \u00b6","text":"<p>Plugins may support two styles of configuration settings:</p> <ul> <li> <p>System-wide plugin settings under the <code>plugins</code> key of your App config</p> </li> <li> <p>Dataset-specific plugin settings for any subset of the above values on a dataset\u2019s App config.</p> </li> </ul> <p>Plugin settings are used, for example, to allow the user to configure the default camera position of FiftyOne\u2019s builtin 3D visualizer.</p> <p>Here\u2019s an example of a system-wide plugin setting:</p> <pre><code>// app_config.json\n{\n  \"plugins\": {\n    \"my-plugin\": {\n      \"mysetting\": \"foo\"\n    }\n  }\n}\n</code></pre> <p>And here\u2019s how to customize that setting for a particular dataset:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.load_dataset(\"quickstart\")\ndataset.app_config.plugins[\"my-plugin\"] = {\"mysetting\": \"bar\"}\ndataset.save()\n</code></pre> <p>In your plugin implementation, you can read settings with the <code>useSettings</code> hook:</p> <pre><code>const { mysetting } = fop.useSettings(\"my-plugin\");\n</code></pre> <p>Note</p> <p>See the this page page for more information about configuring plugins.</p>"},{"location":"plugins/developing_plugins/#querying-fiftyone","title":"Querying FiftyOne \u00b6","text":"<p>A typical use case for a JS plugin is to provide a unique way of visualizing FiftyOne data. However some plugins may need to also fetch data in a unique way to efficiently visualize it.</p> <p>For example, a <code>PluginComponentType.Panel</code> plugin rendering a map of geo points may need to fetch data relative to where the user is currently viewing. In MongoDB, such a query would look like this:</p> <pre><code>{\n  $geoNear: {\n    near: { type: \"Point\", coordinates: [ -73.99279 , 40.719296 ] },\n    maxDistance: 2,\n    query: { category: \"Parks\" },\n  }\n}\n</code></pre> <p>In a FiftyOne plugin this same query can be performed using the <code>useAggregation()</code> method of the plugin SDK:</p> <pre><code>import * as fop from \"@fiftyone/plugins\";\nimport * as fos from \"@fiftyone/state\";\nimport * as foa from \"@fiftyone/aggregations\";\nimport * as recoil from \"recoil\";\n\nfunction useGeoDataNear() {\n    const dataset = useRecoilValue(fos.dataset);\n    const view = useRecoilValue(fos.view);\n    const filters = useRecoilValue(fos.filters);\n    const [aggregate, points, isLoading] = foa.useAggregation({\n        dataset,\n        filters,\n        view,\n    });\n    const availableFields = findAvailableFields(dataset.sampleFields);\n    const [selectedField, setField] = React.useState(availableFields[0]);\n\n    React.useEffect(() =&gt; {\n        aggregate([\\\n            new foa.aggregations.Values({\\\n                fieldOrExpr: \"location.point.coordinates\",\\\n            }),\\\n        ]);\n    }, []);\n\n    return {\n        points,\n        isLoading,\n        setField,\n        availableFields,\n        selectedField,\n    };\n}\n\nfunction MapPlugin() {\n    const { points, isLoading, setField, availableFields, selectedField } =\n        useGeoDataNear();\n\n    return (\n        &lt;Map\n            points={points}\n            onSelectField={(f) =&gt; setField(f)}\n            selectedField={selectedField}\n            locationFields={availableFields}\n        /&gt;\n    );\n}\n\nfop.registerComponent({\n    name: \"MapPlugin\",\n    label: \"Map\",\n    activator: ({ dataset }) =&gt; findAvailableFields(dataset.fields).length &gt; 0,\n});\n</code></pre>"},{"location":"plugins/developing_plugins/#plugin-runtime","title":"Plugin runtime \u00b6","text":""},{"location":"plugins/developing_plugins/#js-runtime","title":"JS runtime \u00b6","text":"<p>In JS, plugins are loaded from your plugins directory into the browser. The FiftyOne App server finds these plugins by looking for <code>package.json</code> files that include <code>fiftyone</code> as a property. This <code>fiftyone</code> property describes where the plugin executable (dist) is.</p>"},{"location":"plugins/developing_plugins/#python-runtime","title":"Python runtime \u00b6","text":"<p>Python operators are executed in two ways:</p>"},{"location":"plugins/developing_plugins/#immediate-execution","title":"Immediate execution \u00b6","text":"<p>By default, all operations are executed by the plugin server immediately after they are triggered, either programmatically or by the user in the App.</p> <p>The plugin server is launched by the FiftyOne App as a subprocess that is responsible for loading plugins and executing them. The plugin server is only accessible via ipc. Its interface (similar to JSON rpc) allows for functions to be called over interprocess communication. This allows for user python code to be isolated from core code. It also allows for the operating system to manage the separate process as it exists in the same process tree as the root process (ipython, Jupyter, etc).</p>"},{"location":"plugins/developing_plugins/#delegated-execution_1","title":"Delegated execution \u00b6","text":"<p>Python operations may also be delegated for execution in the background.</p> <p>When an operation is delegated, the following happens:</p> <ol> <li> <p>The operation\u2019s execution context is serialized and stored in the database</p> </li> <li> <p>The connected orchestrator picks up the task and executes it when resources are available</p> </li> </ol>"},{"location":"plugins/developing_plugins/#advanced-usage","title":"Advanced usage \u00b6","text":""},{"location":"plugins/developing_plugins/#storing-custom-runs","title":"Storing custom runs \u00b6","text":"<p>When users execute builtin methods like annotation, evaluation, and brain methods on their datasets, certain configuration and results information is stored on the dataset that can be accessed later; for example, see managing brain runs.</p> <p>FiftyOne also provides the ability to store custom runs on datasets, which can be used by plugin developers to persist arbitrary application-specific information that can be accessed later by users and/or plugins.</p> <p>The interface for creating custom runs is simple:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.Dataset(\"custom-runs-example\")\ndataset.persistent = True\n\nconfig = dataset.init_run()\nconfig.foo = \"bar\"  # add as many key-value pairs as you need\n\n# Also possible\n# config = fo.RunConfig(foo=\"bar\")\n\ndataset.register_run(\"custom\", config)\n\nresults = dataset.init_run_results(\"custom\")\nresults.spam = \"eggs\"  # add as many key-value pairs as you need\n\n# Also possible\n# results = fo.RunResults(dataset, config, \"custom\", spam=\"eggs\")\n\ndataset.save_run_results(\"custom\", results)\n</code></pre> <p>Note</p> <p><code>RunConfig</code> and <code>RunResults</code> can store any JSON serializable values.</p> <p><code>RunConfig</code> documents must be less than 16MB, although they are generally far smaller as they are intended to store only a handful of simple parameters.</p> <p><code>RunResults</code> instances are stored in GridFS and may exceed 16MB. They are only loaded when specifically accessed by a user.</p> <p>You can access custom runs at any time as follows:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.load_dataset(\"custom-runs-example\")\n\ninfo = dataset.get_run_info(\"custom\")\nprint(info)\n\nresults = dataset.load_run_results(\"custom\")\nprint(results)\n</code></pre> <pre><code>{\n    \"key\": \"custom\",\n    \"version\": \"0.22.3\",\n    \"timestamp\": \"2023-10-26T13:29:20.837595\",\n    \"config\": {\n        \"type\": \"run\",\n        \"method\": null,\n        \"cls\": \"fiftyone.core.runs.RunConfig\",\n        \"foo\": \"bar\"\n    }\n}\n</code></pre> <pre><code>{\n    \"cls\": \"fiftyone.core.runs.RunResults\",\n    \"spam\": \"eggs\"\n}\n</code></pre>"},{"location":"plugins/developing_plugins/#managing-custom-runs","title":"Managing custom runs \u00b6","text":"<p>FiftyOne provides a variety of methods that you can use to manage custom runs stored on datasets.</p> <p>Call <code>list_runs()</code> to see the available custom run keys on a dataset:</p> <pre><code>dataset.list_runs()\n</code></pre> <p>Use <code>get_run_info()</code> to retrieve information about the configuration of a custom run:</p> <pre><code>info = dataset.get_run_info(run_key)\nprint(info)\n</code></pre> <p>Use <code>init_run()</code> and <code>register_run()</code> to create a new custom run on a dataset:</p> <pre><code>config = dataset.init_run()\nconfig.foo = \"bar\"  # add as many key-value pairs as you need\n\ndataset.register_run(run_key, config)\n</code></pre> <p>Use <code>update_run_config()</code> to update the run config associated with an existing custom run:</p> <pre><code>dataset.update_run_config(run_key, config)\n</code></pre> <p>Use <code>init_run_results()</code> and <code>save_run_results()</code> to store run results for a custom run:</p> <pre><code>results = dataset.init_run_results(run_key)\nresults.spam = \"eggs\"  # add as many key-value pairs as you need\n\ndataset.save_run_results(run_key, results)\n\n# update existing results\ndataset.save_run_results(run_key, results, overwrite=True)\n</code></pre> <p>Use <code>load_run_results()</code> to load the results for a custom run:</p> <pre><code>results = dataset.load_run_results(run_key)\n</code></pre> <p>Use <code>rename_run()</code> to rename the run key associated with an existing custom run:</p> <pre><code>dataset.rename_run(run_key, new_run_key)\n</code></pre> <p>Use <code>delete_run()</code> to delete the record of a custom run from a dataset:</p> <pre><code>dataset.delete_run(run_key)\n</code></pre>"},{"location":"plugins/using_plugins/","title":"Using Plugins \u00b6","text":"<p>Every plugin that you download exposes one or more Panels and/or Operators that you can access from within the App.</p> <p>Note</p> <p>Check out the FiftyOne plugins repository for a growing collection of plugins that you can easily download and use locally.</p> <p>Then, after you get comfortable using prebuilt plugins, try your hand at writing your own!</p>"},{"location":"plugins/using_plugins/#downloading-plugins","title":"Downloading plugins \u00b6","text":"<p>To download and use a plugin, all you need is the plugin\u2019s GitHub repository or a link to a ZIP archive of the plugin\u2019s source code.</p> <p>You can download plugins using any of the methods described below:</p> <p>Note</p> <p>A GitHub repository may contain multiple plugins. By default, all plugins that are found within the first three directory levels are installed, but you can select specific ones if desired as shown above.</p> <p>Note</p> <p>All plugins are downloaded to your plugins directory.</p> <p>Note</p> <p>You can download plugins from private GitHub repositories that you have access to by providing your GitHub personal access token via the <code>GITHUB_TOKEN</code> environment variable.</p>"},{"location":"plugins/using_plugins/#your-plugins-directory","title":"Your plugins directory \u00b6","text":"<p>All plugins must be stored and are automatically downloaded to your plugins directory in order for FiftyOne to find them.</p> <p>By default, plugins are downloaded to <code>~/fiftyone/__plugins__</code>, but you can customize this directory by setting the <code>FIFTYONE_PLUGINS_DIR</code> environment variable:</p> <pre><code>export FIFTYONE_PLUGINS_DIR=/path/to/your/plugins\n</code></pre> <p>You can also permanently configure this directory by adding it to your FiftyOne config.</p> <pre><code>{\n    \"plugins_dir\": \"/path/to/your/plugins\"\n}\n</code></pre> <p>You can locate your current plugins directory by running the following command:</p> <pre><code>fiftyone config plugins_dir\n# ~/fiftyone/__plugins__\n</code></pre> <p>Note</p> <p>Your plugins directory must be readable by the FiftyOne server.</p>"},{"location":"plugins/using_plugins/#managing-plugins","title":"Managing plugins \u00b6","text":"<p>You can use the fiftyone plugins and fiftyone operators CLI methods to perform a variety of plugin-related actions.</p>"},{"location":"plugins/using_plugins/#listing-plugins","title":"Listing plugins \u00b6","text":"<p>You can use the fiftyone plugins list command to list the plugins that you\u2019ve downloaded or created locally:</p> <pre><code># List all locally available plugins\nfiftyone plugins list\n</code></pre> <pre><code># List enabled plugins\nfiftyone plugins list --enabled\n</code></pre> <pre><code># List disabled plugins\nfiftyone plugins list --disabled\n</code></pre> <pre><code>plugin               version   enabled  directory\n-------------------  -------  -------  ----------------------------------------------------------\n@voxel51/annotation  1.0.0    \u2713        ~/fiftyone/__plugins__/fiftyone-plugins/plugins/annotation\n@voxel51/brain       1.0.0    \u2713        ~/fiftyone/__plugins__/fiftyone-plugins/plugins/brain\n@voxel51/evaluation  1.0.0    \u2713        ~/fiftyone/__plugins__/fiftyone-plugins/plugins/evaluation\n@voxel51/indexes     1.0.0    \u2713        ~/fiftyone/__plugins__/fiftyone-plugins/plugins/indexes\n@voxel51/io          1.0.0    \u2713        ~/fiftyone/__plugins__/fiftyone-plugins/plugins/io\n@voxel51/utils       1.0.0    \u2713        ~/fiftyone/__plugins__/fiftyone-plugins/plugins/utils\n@voxel51/voxelgpt    1.0.0    \u2713        ~/fiftyone/__plugins__/voxelgpt\n@voxel51/zoo         1.0.0    \u2713        ~/fiftyone/__plugins__/fiftyone-plugins/plugins/zoo\n</code></pre>"},{"location":"plugins/using_plugins/#listing-operators","title":"Listing operators \u00b6","text":"<p>You can use the fiftyone operators list command to list the individual operators and panels within the plugins that you\u2019ve installed locally:</p> <pre><code># List all available operators and panels\nfiftyone operators list\n</code></pre> <pre><code># List enabled operators and panels\nfiftyone operators list --enabled\n</code></pre> <pre><code># List disabled operators and panels\nfiftyone operators list --disabled\n</code></pre> <pre><code># Only list panels\nfiftyone operators list --panels-only\n</code></pre> <pre><code>uri                                          enabled   builtin   panel   unlisted\n-------------------------------------------  --------  --------  ------  ---------\n@voxel51/annotation/request_annotations      \u2713\n@voxel51/annotation/load_annotations         \u2713\n@voxel51/annotation/get_annotation_info      \u2713\n@voxel51/annotation/load_annotation_view     \u2713\n@voxel51/annotation/rename_annotation_run    \u2713\n@voxel51/annotation/delete_annotation_run    \u2713\n@voxel51/brain/compute_visualization         \u2713\n@voxel51/brain/compute_similarity            \u2713\n@voxel51/brain/compute_uniqueness            \u2713\n@voxel51/brain/compute_mistakenness          \u2713\n@voxel51/brain/compute_hardness              \u2713\n@voxel51/brain/get_brain_info                \u2713\n@voxel51/brain/load_brain_view               \u2713\n@voxel51/brain/rename_brain_run              \u2713\n@voxel51/brain/delete_brain_run              \u2713\n@voxel51/evaluation/evaluate_model           \u2713\n@voxel51/evaluation/get_evaluation_info      \u2713\n@voxel51/evaluation/load_evaluation_view     \u2713\n@voxel51/evaluation/rename_evaluation        \u2713\n@voxel51/evaluation/delete_evaluation        \u2713\n@voxel51/io/import_samples                   \u2713\n@voxel51/io/merge_samples                    \u2713\n@voxel51/io/merge_labels                     \u2713\n@voxel51/io/export_samples                   \u2713\n@voxel51/io/draw_labels                      \u2713\n@voxel51/operators/clone_selected_samples    \u2713          \u2713\n@voxel51/operators/clone_sample_field        \u2713          \u2713\n@voxel51/operators/rename_sample_field       \u2713          \u2713\n@voxel51/operators/delete_selected_samples   \u2713          \u2713\n@voxel51/operators/delete_sample_field       \u2713          \u2713\n@voxel51/operators/print_stdout              \u2713          \u2713                 \u2713\n@voxel51/operators/list_files                \u2713          \u2713                 \u2713\n@voxel51/utils/create_dataset                \u2713\n@voxel51/utils/load_dataset                  \u2713\n@voxel51/utils/edit_dataset_info             \u2713\n@voxel51/utils/rename_dataset                \u2713\n@voxel51/utils/delete_dataset                \u2713\n@voxel51/utils/compute_metadata              \u2713\n@voxel51/utils/generate_thumbnails           \u2713\n@voxel51/utils/manage_plugins                \u2713\n@voxel51/zoo/load_zoo_dataset                \u2713\n@voxel51/zoo/apply_zoo_model                 \u2713\n</code></pre>"},{"location":"plugins/using_plugins/#downloading-plugins_1","title":"Downloading plugins \u00b6","text":"<p>You can use the fiftyone plugins download command to list the plugins that you\u2019ve downloaded or created locally:</p> <pre><code># Download plugins from a GitHub repository URL\nfiftyone plugins download &lt;github-repo-url&gt;\n</code></pre> <pre><code># Download plugins by specifying the GitHub repository details\nfiftyone plugins download &lt;user&gt;/&lt;repo&gt;[/&lt;ref&gt;]\n</code></pre> <pre><code># Download specific plugins from a URL with a custom search depth\nfiftyone plugins download \\\n    &lt;url&gt; \\\n    --plugin-names &lt;name1&gt; &lt;name2&gt; &lt;name3&gt; \\\n    --max-depth 2  # search nested directories for plugins\n</code></pre>"},{"location":"plugins/using_plugins/#getting-plugin-info","title":"Getting plugin info \u00b6","text":"<p>You can use the fiftyone plugins info command to view the available metadata about a plugin:</p> <pre><code>fiftyone plugins info @voxel51/annotation\n</code></pre> <pre><code>key                     value\n----------------------  --------------------------------------------------------------------\nname                    @voxel51/annotation\nauthor\nversion                 1.0.0\nurl                     https://github.com/voxel51/fiftyone-plugins/.../annotation/README.md\nlicense                 Apache 2.0\ndescription             Utilities for integrating FiftyOne with annotation tools\nfiftyone_compatibility  &gt;=0.22\noperators               request_annotations\n                        load_annotations\n                        get_annotation_info\n                        load_annotation_view\n                        rename_annotation_run\n                        delete_annotation_run\njs_bundle               dist/index.umd.js\npy_entry                __init__.py\njs_bundle_exists        False\njs_bundle_server_path\nhas_py                  True\nhas_js                  False\nserver_path             /plugins/fiftyone-plugins/plugins/annotation\nsecrets                 FIFTYONE_CVAT_URL\n                        FIFTYONE_CVAT_USERNAME\n                        FIFTYONE_CVAT_PASSWORD\n                        FIFTYONE_CVAT_EMAIL\n                        FIFTYONE_LABELBOX_URL\n                        FIFTYONE_LABELBOX_API_KEY\n                        FIFTYONE_LABELSTUDIO_URL\n                        FIFTYONE_LABELSTUDIO_API_KEY\ndirectory               ~/fiftyone/__plugins__/fiftyone-plugins/plugins/annotation\n</code></pre>"},{"location":"plugins/using_plugins/#getting-operator-info","title":"Getting operator info \u00b6","text":"<p>You can use the fiftyone operators info to view the available metadata about an individual operator or panel within a plugin:</p> <pre><code>fiftyone operators info @voxel51/io/import_samples\n</code></pre> <pre><code>key                                  value\n-----------------------------------  ----------------------\nname                                 import_samples\nlabel                                Import samples\ndescription\nexecute_as_generator                 True\nunlisted                             False\ndynamic                              True\non_startup                           False\non_dataset_open                      False\ndisable_schema_validation            False\ndelegation_target\nicon\ndark_icon                            /assets/icon-dark.svg\nlight_icon                           /assets/icon-light.svg\nallow_immediate_execution            True\nallow_delegated_execution            False\ndefault_choice_to_delegated          False\nresolve_execution_options_on_change  True\n</code></pre>"},{"location":"plugins/using_plugins/#installing-plugin-requirements","title":"Installing plugin requirements \u00b6","text":"<p>You can use the fiftyone plugins requirements command to view, install, and ensure installation of a plugin\u2019s requirements:</p> <pre><code># Print requirements for a plugin\nfiftyone plugins requirements &lt;name&gt; --print\n</code></pre> <pre><code># Install any requirements for the plugin\nfiftyone plugins requirements &lt;name&gt; --install\n</code></pre> <pre><code># Ensures that the requirements for the plugin are satisfied\nfiftyone plugins requirements &lt;name&gt; --ensure\n</code></pre>"},{"location":"plugins/using_plugins/#enabling-and-disabling-plugins","title":"Enabling and disabling plugins \u00b6","text":"<p>You can use the fiftyone plugins enable and fiftyone plugins disable commands to enable and disable plugins that you\u2019ve downloaded:</p> <pre><code># Enable a plugin\nfiftyone plugins enable &lt;name&gt;\n\n# Enable multiple plugins\nfiftyone plugins enable &lt;name1&gt; &lt;name2&gt; ...\n\n# Enable all plugins\nfiftyone plugins enable --all\n</code></pre> <pre><code># Disable a plugin\nfiftyone plugins disable &lt;name&gt;\n\n# Disable multiple plugins\nfiftyone plugins disable &lt;name1&gt; &lt;name2&gt; ...\n\n# Disable all plugins\nfiftyone plugins disable --all\n</code></pre> <p>Note</p> <p>Operators associated with disabled plugins will not appear in the App\u2019s operator browser.</p> <p>Plugin disablement is stored as an <code>enabled: false</code> entry in the plugin\u2019s config settings.</p> <pre><code>fiftyone plugins disable @voxel51/zoo\nfiftyone app config plugins\n</code></pre> <pre><code>{\n    \"map\": {\n        \"mapboxAccessToken\": \"XXXXXXXX\"\n    },\n    \"@voxel51/zoo\": {\n        \"enabled\": false\n    },\n    ...\n}\n</code></pre>"},{"location":"plugins/using_plugins/#deleting-plugins","title":"Deleting plugins \u00b6","text":"<p>You can use the fiftyone plugins delete command to delete plugins from your local machine.</p> <pre><code># Delete a plugin from local disk\nfiftyone plugins delete &lt;name&gt;\n</code></pre> <pre><code># Delete multiple plugins from local disk\nfiftyone plugins delete &lt;name1&gt; &lt;name2&gt; ...\n</code></pre> <pre><code># Delete all plugins from local disk\nfiftyone plugins delete --all\n</code></pre>"},{"location":"plugins/using_plugins/#configuring-plugins","title":"Configuring plugins \u00b6","text":"<p>Certain plugins support configuration. For those plugins, you can store:</p> <ul> <li> <p>System-wide plugin settings under the <code>plugins</code> key of your App config</p> </li> <li> <p>Dataset-specific plugin settings for any subset of the above values on a dataset\u2019s App config.</p> </li> </ul> <p>See the configuring plugins page for more information.</p>"},{"location":"plugins/using_plugins/#plugin-secrets","title":"Plugin secrets \u00b6","text":"<p>Some plugins may require sensitive information such as API tokens and login credentials in order to function. Any secrets that a plugin requires are documented under the <code>secrets</code> key of its <code>fiftyone.yml</code> file.</p> <p>For example, the @voxel51/annotation plugin declares the following secrets:</p> <pre><code>secrets:\n  - FIFTYONE_CVAT_URL\n  - FIFTYONE_CVAT_USERNAME\n  - FIFTYONE_CVAT_PASSWORD\n  - FIFTYONE_CVAT_EMAIL\n  - FIFTYONE_LABELBOX_URL\n  - FIFTYONE_LABELBOX_API_KEY\n  - FIFTYONE_LABELSTUDIO_URL\n  - FIFTYONE_LABELSTUDIO_API_KEY\n</code></pre> <p>Note</p> <p>You can use the fiftyone plugins info CLI command to print information about a plugin, including its required secrets.</p> <p>As the naming convention implies, any necessary secrets are provided by setting environment variables with the appropriate names. For example, if you want to use the CVAT backend with the @voxel51/annotation plugin, you would set:</p> <pre><code>FIFTYONE_CVAT_URL=...\nFIFTYONE_CVAT_USERNAME=...\nFIFTYONE_CVAT_PASSWORD=...\nFIFTYONE_CVAT_EMAIL=...\n</code></pre> <p>At runtime, the plugin\u2019s execution context will automatically be hydrated with any available secrets that are declared by the plugin. Operators access these secrets via the <code>ctx.secrets</code> dict:</p> <pre><code>def execute(self, ctx):\n    url = ctx.secrets[\"FIFTYONE_CVAT_URL\"]\n    username = ctx.secrets[\"FIFTYONE_CVAT_USERNAME\"]\n    password = ctx.secrets[\"FIFTYONE_CVAT_PASSWORD\"]\n    email = ctx.secrets[\"FIFTYONE_CVAT_EMAIL\"]\n</code></pre>"},{"location":"plugins/using_plugins/#using-panels","title":"Using panels \u00b6","text":"<p>Panels are miniature full-featured data applications that you can open in App Spaces and interactively manipulate to explore your dataset and update/respond to updates from other spaces that are currently open in the App.</p> <p>FiftyOne natively includes the following Panels:</p> <ul> <li> <p>Samples panel: the media grid that loads by default when you launch the App</p> </li> <li> <p>Histograms panel: a dashboard of histograms for the fields of your dataset</p> </li> <li> <p>Embeddings panel: a canvas for working with embeddings visualizations</p> </li> <li> <p>Map panel: visualizes the geolocation data of datasets that have a <code>GeoLocation</code> field</p> </li> </ul> <p>Any plugins that you\u2019ve installed may expose additional panels too.</p> <p>Click the <code>+</code> icon next to the \u201cSamples\u201d tab to open a new panel:</p> <p></p> <p>Note</p> <p>Did you know? You can also programmatically configure spaces in Python.</p>"},{"location":"plugins/using_plugins/#using-operators","title":"Using operators \u00b6","text":"<p>Operators are a powerful feature in FiftyOne that allow plugin developers to define custom operations that can be executed from within the App.</p> <p>Some operators may expose themselves as custom buttons, icons, or menu items throughout the App. However, the Operator Browser allows users to search through all available (enabled) operators.</p> <p>You can open the Operator Browser by clicking on the Operator Browser icon above the sample grid or by typing backtick ( <code>`</code>):</p> <p></p> <p>Operators provide dynamic input forms that collect the necessary user inputs. The actual operation is then performed by pressing the <code>Execute</code>/ <code>Schedule</code> button at the bottom of the form.</p> <p>Some Operators perform an immediate action when executed, while other Operators delegate their execution to another process.</p>"},{"location":"plugins/using_plugins/#executing-operators-via-sdk","title":"Executing operators via SDK \u00b6","text":"<p>Many operators are intended to be executed programmatically via the SDK rather than (or in addition to) executing them by filling out their input form in the App.</p>"},{"location":"plugins/using_plugins/#calling-operators","title":"Calling operators \u00b6","text":"<p>By convention, operators that are intended to be executed programmatically should implement <code>__call__()</code> so that users have a well-documented interface for invoking the operator as a function.</p> <p>For example, the @voxel51/utils/compute_metadata operator can be invoked like so:</p> <pre><code>import fiftyone as fo\nimport fiftyone.operators as foo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\ncompute_metadata = foo.get_operator(\"@voxel51/utils/compute_metadata\")\n\n# (Re)compute the dataset's metadata\ncompute_metadata(dataset, overwrite=True)\n</code></pre> <p>Note</p> <p>Notice that <code>get_operator()</code> is used to retrieve the operator by its URI.</p> <p>Behind the scenes, the operator\u2019s <code>__call__()</code> method is implemented as follows:</p> <pre><code>class ComputeMetadata(foo.Operator):\n    def __call__(\n        self,\n        sample_collection,\n        overwrite=False,\n        num_workers=None,\n    ):\n        ctx = dict(view=sample_collection.view())\n        params = dict(\n            overwrite=overwrite,\n            num_workers=num_workers,\n        )\n        return foo.execute_operator(self.uri, ctx, params=params)\n</code></pre> <p>which simply packages up the provided keyword arguments into the correct format for the operator\u2019s <code>ctx.params</code> and then passes them to <code>execute_operator()</code>, which performs the execution.</p> <p>For operators whose <code>execute()</code> method returns data, you can access it via the <code>result</code> property of the returned <code>ExecutionResult</code> object:</p> <pre><code>op = foo.get_operator(\"@an-operator/with-results\")\n\nresult = op(...)\nprint(result.result) # {...}\n</code></pre> <p>Note</p> <p>When working in notebook contexts, executing operators returns an <code>asyncio.Task</code> that you can <code>await</code> to retrieve the <code>ExecutionResult</code>:</p> <pre><code>op = foo.get_operator(\"@an-operator/with-results\")\n\nresult = await op(...)\nprint(result.result) # {...}\n</code></pre>"},{"location":"plugins/using_plugins/#requesting-delegation","title":"Requesting delegation \u00b6","text":"<p>Operators that support delegated execution can support this via the <code>__call__()</code> syntax by passing the <code>request_delegation=True</code> flag to <code>execute_operator()</code>.</p> <p>In fact, the @voxel51/utils/compute_metadata operator does just that:</p> <pre><code>class ComputeMetadata(foo.Operator):\n    return foo.OperatorConfig(\n        ...\n        allow_immediate_execution=True,\n        allow_delegated_execution=True,\n    )\n\n    def __call__(\n        self,\n        sample_collection,\n        overwrite=False,\n        num_workers=None,\n        delegate=False,\n    ):\n        ctx = dict(view=sample_collection.view())\n        params = dict(\n            overwrite=overwrite,\n            num_workers=num_workers,\n        )\n        return foo.execute_operator(\n            self.uri,\n            ctx,\n            params=params,\n            request_delegation=delegate,\n        )\n</code></pre> <p>which means that it can be invoked like so:</p> <pre><code>compute_metadata = foo.get_operator(\"@voxel51/utils/compute_metadata\")\n\n# Schedule a delegated operation to (re)compute metadata\ncompute_metadata(dataset, overwrite=True, delegate=True)\n</code></pre>"},{"location":"plugins/using_plugins/#direct-execution","title":"Direct execution \u00b6","text":"<p>You can programmatically execute any operator by directly calling <code>execute_operator()</code>:</p> <pre><code>import fiftyone as fo\nimport fiftyone.operators as foo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nctx = {\n    \"view\": dataset.take(10),\n    \"params\": dict(\n        export_type=\"LABELS_ONLY\",\n        dataset_type=\"COCO\",\n        labels_path=dict(absolute_path=\"/tmp/coco/labels.json\"),\n        label_field=\"ground_truth\",\n    )\n}\n\nfoo.execute_operator(\"@voxel51/io/export_samples\", ctx)\n</code></pre> <p>Note</p> <p>In general, to use <code>execute_operator()</code> you must inspect the operator\u2019s <code>execute()</code> implementation to understand what parameters are required.</p> <p>For operators whose <code>execute()</code> method returns data, you can access it via the <code>result</code> property of the returned <code>ExecutionResult</code> object:</p> <pre><code>result = foo.execute_operator(\"@an-operator/with-results\", ctx)\nprint(result.result)  # {...}\n</code></pre> <p>Note</p> <p>When working in notebook contexts, executing operators returns an <code>asyncio.Task</code> that you can <code>await</code> to retrieve the <code>ExecutionResult</code>:</p> <pre><code>result = await foo.execute_operator(\"@an-operator/with-results\", ctx)\nprint(result.result)  # {...}\n</code></pre>"},{"location":"plugins/using_plugins/#requesting-delegation_1","title":"Requesting delegation \u00b6","text":"<p>If an operation supports both immediate and delegated execution as specified by its execution options, you can request delegated execution by passing the <code>request_delegation=True</code> flag to <code>execute_operator()</code>:</p> <pre><code>foo.execute_operator(operator_uri, ctx=ctx, request_delegation=True)\n</code></pre> <p>This has the same effect as choosing <code>Schedule</code> from the dropdown in the operator\u2019s input modal when executing it from within the App:</p> <p></p> <p>Note</p> <p>FiftyOne Teams users can also specify an optional delegation target for their delegated operations:</p> <pre><code>foo.execute_operator(\n    operator_uri,\n    ctx=ctx,\n    request_delegation=True,\n    delegation_target=\"overnight\",\n)\n</code></pre>"},{"location":"plugins/using_plugins/#delegating-function-calls","title":"Delegating function calls \u00b6","text":"<p>The @voxel51/utils/delegate operator provides a general purpose utility for delegating execution of an arbitrary function call that can be expressed in any of the following forms:</p> <ul> <li> <p>Execute an arbitrary function: <code>fcn(*args, **kwargs)</code></p> </li> <li> <p>Apply a function to a dataset or view: <code>fcn(dataset_or_view, *args, **kwargs)</code></p> </li> <li> <p>Call an instance method of a dataset or view: <code>dataset_or_view.fcn(*args, **kwargs)</code></p> </li> </ul> <p>Here\u2019s some examples of delegating common tasks that can be expressed in the above forms:</p> <pre><code>import fiftyone as fo\nimport fiftyone.operators as foo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\ndelegate = foo.get_operator(\"@voxel51/utils/delegate\")\n\n# Compute metadata\ndelegate(\"compute_metadata\", dataset=dataset)\n\n# Compute visualization\ndelegate(\n    \"fiftyone.brain.compute_visualization\",\n    dataset=dataset,\n    brain_key=\"img_viz\",\n)\n\n# Export a view\ndelegate(\n    \"export\",\n    view=dataset.to_patches(\"ground_truth\"),\n    export_dir=\"/tmp/patches\",\n    dataset_type=\"fiftyone.types.ImageClassificationDirectoryTree\",\n    label_field=\"ground_truth\",\n)\n\n# Load the exported patches into a new dataset\ndelegate(\n    \"fiftyone.Dataset.from_dir\",\n    dataset_dir=\"/tmp/patches\",\n    dataset_type=\"fiftyone.types.ImageClassificationDirectoryTree\",\n    label_field=\"ground_truth\",\n    name=\"patches\",\n    persistent=True,\n)\n</code></pre>"},{"location":"plugins/using_plugins/#delegated-operations","title":"Delegated operations \u00b6","text":"<p>Delegated operations are a powerful feature of FiftyOne\u2019s plugin framework that allows you to schedule potentially long-running tasks from within the App that are executed in the background while you continue to work.</p> <p>Note</p> <p>FiftyOne Teams deployments come out of the box with a connected compute cluster for executing delegated operations at scale.</p> <p>In FiftyOne Open Source, you can use delegated operations at small scale by running them locally.</p> <p>For example, have model predictions on your dataset that you want to evaluate? The @voxel51/evaluation plugin makes it easy:</p> <p></p> <p>Need to compute embedding for your dataset so you can visualize them in the Embeddings panel? Kick off the task with the @voxel51/brain plugin and proceed with other work while the execution happens in the background:</p> <p></p> <p>Why is this awesome? Your AI stack needs a flexible data-centric component that enables you to organize and compute on your data. With delegated operations, FiftyOne becomes both a dataset management/visualization tool and a workflow automation tool that defines how your data-centric workflows like ingestion, curation, and evaluation are performed.</p> <p>Note</p> <p>Want to run delegated operations at scale? Contact us about FiftyOne Teams, an open source-compatible enterprise deployment of FiftyOne with multiuser collaboration features, native cloud dataset support, and much more!</p> <p>Think of FiftyOne Teams as the single source of truth on which you co-develop your data and models together \ud83d\udcc8</p>"},{"location":"plugins/using_plugins/#setting-up-an-orchestrator","title":"Setting up an orchestrator \u00b6","text":""},{"location":"plugins/using_plugins/#fiftyone-open-source","title":"FiftyOne Open Source \u00b6","text":"<p>FiftyOne Open Source users can run delegated operations via the fiftyone delegated launch CLI command:</p> <pre><code>fiftyone delegated launch\n</code></pre> <p>This command starts a service that will continuously check for any queued delegated operations and execute them serially in its process.</p> <p>You must also ensure that the allow_legacy_orchestrators config flag is set in the environment where you run the App/SDK, e.g. by setting:</p> <pre><code>export FIFTYONE_ALLOW_LEGACY_ORCHESTRATORS=true\n</code></pre>"},{"location":"plugins/using_plugins/#fiftyone-teams","title":"FiftyOne Teams \u00b6","text":"<p>FiftyOne Teams deployments come out of the box with a connected compute cluster for executing delegated operations at scale.</p> <p>This powerful feature allows users to install/build plugins that execute potentially long-running tasks in the background while users continue with other work in the App.</p> <p>Note</p> <p>Want to take advantage of this functionality? Contact us about FiftyOne Teams, an open source-compatible enterprise deployment of FiftyOne with multiuser collaboration features, native cloud dataset support, and much more!</p> <p>Think of FiftyOne Teams as the single source of truth on which you co-develop your data and models together \ud83d\udcc8</p>"},{"location":"plugins/using_plugins/#managing-delegated-operations","title":"Managing delegated operations \u00b6","text":"<p>The fiftyone delegated CLI command contains a number of useful utilities for viewing the status of your delegated operations.</p>"},{"location":"plugins/using_plugins/#listing-delegated-operations","title":"Listing delegated operations \u00b6","text":"<p>You can use the fiftyone delegated list command to list the delegated operations that you\u2019ve run:</p> <pre><code># List all delegated operations\nfiftyone delegated list\n</code></pre> <pre><code># List some specific delegated operations\nfiftyone delegated list \\\n    --dataset quickstart \\\n    --operator @voxel51/io/export_samples \\\n    --state COMPLETED \\\n    --sort-by COMPLETED_AT \\\n    --limit 10\n</code></pre>"},{"location":"plugins/using_plugins/#getting-delegated-operation-info","title":"Getting delegated operation info \u00b6","text":"<p>You can use the fiftyone delegated info command to view the available metadata about a delegated operation, including its inputs, execution status, and error stack trace, if applicable.</p> <pre><code># Print information about a delegated operation\nfiftyone delegated info &lt;id&gt;\n</code></pre>"},{"location":"plugins/using_plugins/#cleaning-up-delegated-operations","title":"Cleaning up delegated operations \u00b6","text":"<p>You can use the fiftyone delegated cleanup command to cleanup delegated operations:</p> <pre><code># Delete all failed operations associated with a given dataset\nfiftyone delegated cleanup --dataset quickstart --state FAILED\n</code></pre> <pre><code># Delete all delegated operations associated with non-existent datasets\nfiftyone delegated cleanup --orphan\n</code></pre> <pre><code># Print information about operations rather than actually deleting them\nfiftyone delegated cleanup --orphan --dry-run\n</code></pre>"},{"location":"teams/","title":"FiftyOne Teams \u00b6","text":"<p>FiftyOne Teams enables multiple users to securely collaborate on the same datasets and models, either on-premises or in the cloud, all built on top of the open source FiftyOne workflows that you\u2019re already relying on.</p> <p>FiftyOne Teams is fully backwards-compatible with the open source project, so all other pages on this site apply to Teams deployments as well.</p> <p></p>"},{"location":"teams/#overview","title":"Overview","text":"<p>What's FiftyOne Teams? We think you'll be glad you asked!</p> <p>Learn the basics</p>"},{"location":"teams/#installation","title":"Installation","text":"<p>Learn how to install the FiftyOne Teams Python SDK.</p> <p>Install the SDK</p>"},{"location":"teams/#cloud-backed-media","title":"Cloud-backed media","text":"<p>Integrate FiftyOne Teams with your media stored in the cloud.</p> <p>Get started with cloud media</p>"},{"location":"teams/#roles-and-permissions","title":"Roles and permissions","text":"<p>Learn how to configure roles, groups and access permissions for users of your deployment.</p> <p>Learn more about permissions</p>"},{"location":"teams/#dataset-versioning","title":"Dataset Versioning","text":"<p>Explore or revert to historical snapshots of your FiftyOne Teams datasets.</p> <p>Version your FiftyOne datasets</p>"},{"location":"teams/#fiftyone-teams-app","title":"FiftyOne Teams App","text":"<p>See how you can collaboratively work on datasets in the FiftyOne Teams App.</p> <p>Explore the Teams App</p>"},{"location":"teams/#data-lens-new","title":"Data Lens NEW","text":"<p>Use FiftyOne Teams to explore and import samples from external data sources.</p> <p>Connect your data lake</p>"},{"location":"teams/#data-quality-new","title":"Data Quality NEW","text":"<p>Automatically scan your data for quality issues and take action to resolve them.</p> <p>Find quality issues</p>"},{"location":"teams/#model-evaluation-new","title":"Model Evaluation NEW","text":"<p>Evaluate your models and interactively and visually analyze their performance.</p> <p>Evaluate models</p>"},{"location":"teams/#query-performance-new","title":"Query Performance NEW","text":"<p>Configure your massive datasets to support fast queries at scale.</p> <p>Fast queries at scale</p>"},{"location":"teams/#plugins","title":"Plugins","text":"<p>Learn how to install and manage shared plugins for your Teams deployment.</p> <p>Get plugins for Teams</p>"},{"location":"teams/#secrets","title":"Secrets","text":"<p>Safely store and access secrets in your Teams deployment.</p> <p>Learn more about secrets</p>"},{"location":"teams/#management-sdk","title":"Management SDK","text":"<p>Learn how to programmatically manage your deployment with the Management SDK.</p> <p>Manage your Teams deployment</p>"},{"location":"teams/#migrations","title":"Migrations","text":"<p>Learn how to migrate to FiftyOne Teams and manage your Teams deployment.</p> <p>Migrate to Teams</p> <p>Note</p> <p>Look interesting? Schedule a demo to get started with Teams</p>"},{"location":"teams/api_connection/","title":"API Connection \u00b6","text":"<p>This page describes how to create API keys and configure your SDK installation to connect to your Teams deployment\u2019s API.</p> <p>All actions taken via API connections are authenticated based on the user associated with the API key, which means that concepts like user roles and dataset permissions are enforced.</p> <p>Note</p> <p>API connections are currently in beta. The recommended stable solution is to use your Teams deployment\u2019s MongoDB connection.</p>"},{"location":"teams/api_connection/#configuring-an-api-connection","title":"Configuring an API connection \u00b6","text":"<p>You can configure an API connection by adding an API URI and API key to your FiftyOne config as described below:</p> Config field Environment variable Description <code>api_uri</code> <code>FIFTYONE_API_URI</code> The URI of your FiftyOne Teams API. Ask your deployment admin for this value. <code>api_key</code> <code>FIFTYONE_API_KEY</code> Your FiftyOne Teams API key. See here to generate one. <p>For example, you can set environment variables:</p> <pre><code>export FIFTYONE_API_URI==XXXXXXXX\nexport FIFTYONE_API_KEY==YYYYYYYY\n</code></pre> <p>See this page for more information about using your FiftyOne config.</p>"},{"location":"teams/api_connection/#generating-an-api-key","title":"Generating an API key \u00b6","text":"<p>Users can generate and manage API keys via the UI or the Management SDK.</p> <p>Note</p> <p>Guests cannot create or use API keys.</p>"},{"location":"teams/api_connection/#generating-keys-via-the-ui","title":"Generating keys via the UI \u00b6","text":"<p>You can access API key management features by clicking on your account icon in the upper-right of the FiftyOne Teams App and navigating to the \u201cSettings &gt; API keys\u201d page.</p> <p>A new key can be generated by clicking on \u201cGenerate API key\u201d and optionally providing a nickname for the key to identify what it is used for. Click \u201cGenerate key\u201d to complete the process.</p> <p></p> <p>Finally, copy the key and configure it locally using one of the options described here.</p> <p></p> <p>Warning</p> <p>Keys are only shown once. Copy the key immediately, as it will not be accessible again. API keys provide full programmatic access to perform actions as a user, so secure them as you would a password!</p>"},{"location":"teams/api_connection/#generating-keys-programmatically","title":"Generating keys programmatically \u00b6","text":"<p>You can also use <code>generate_api_key()</code> to generate API keys programmatically.</p> <p>Note</p> <p>Admins can generate API keys for other users, if desired.</p>"},{"location":"teams/api_connection/#deleting-an-api-key","title":"Deleting an API key \u00b6","text":"<p>To delete a key and remove its access, find the key to delete in the list and click \u201cDelete\u201d.</p> <p></p> <p>You can also programmatically delete API keys via <code>delete_api_key()</code>.</p> <p>Note</p> <p>Admins can delete API keys for other users, if desired.</p>"},{"location":"teams/cloud_media/","title":"Cloud-Backed Media \u00b6","text":"<p>FiftyOne Teams datasets may contain samples whose filepath refers to cloud object storage paths and/or publicly available URLs.</p> <p>Note</p> <p>Click here to see how to configure FiftyOne Teams to load your cloud credentials!</p>"},{"location":"teams/cloud_media/#cloud-media-caching","title":"Cloud media caching \u00b6","text":"<p>When you work with cloud-backed datasets using the Teams SDK, media files will automatically be downloaded and cached on the machine you\u2019re working from when you execute workflows such as model inference or Brain methods that require access to the pixels of the media files. This design minimizes bandwidth usage and can significantly improve performance in workflows where you access the same media file repeatedly:</p> <pre><code>import fiftyone as fo\nimport fiftyone.brain as fob\n\ndataset = fo.load_dataset(\"a-teams-dataset\")\n\n# Automatically downloads cloud media to your local cache for processing\nfob.compute_visualization(dataset, brain_key=\"img_viz\")\n</code></pre> <p>When launching the App locally using the Teams SDK, media will be served from your local cache whenever possible; otherwise it will be automatically retrieved from the cloud:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.load_dataset(\"a-teams-dataset\")\n\n# Any media you view will be automatically retrieved from the cloud\nsession = fo.launch_app(dataset)\n</code></pre> <p>By default, viewing media in the App will not add it to your local media cache, but, if desired, you can enable caching of images via the <code>cache_app_images</code> parameter of your media cache config. Viewing video datasets in the App will never cause previously uncached videos to be cached locally.</p> <p>Note</p> <p>We recommend that you populate the metadata on your datasets at creation time:</p> <pre><code>dataset.compute_metadata()\n</code></pre> <p>so that the dimensions of all media stored in the cloud are readily available when viewing datasets in the App. Otherwise, the App must pull this metadata each time you view a sample.</p>"},{"location":"teams/cloud_media/#media-cache-config","title":"Media cache config \u00b6","text":"<p>By default, your local media cache is located at <code>~/fiftyone/__cache__</code>, has a size of 32GB, will download media in batches of 128MB when using <code>download_context()</code>, and will use a thread pool whose size equals the number of virtual CPU cores on your machine to download media files.</p> <p>When the cache is full, local files are automatically deleted in reverse order of when they were last accessed (i.e., oldest deleted first).</p> <p>You can configure the behavior of FiftyOne Team\u2019s media cache in any of the following ways.</p> <p>1. Configure your media cache on a per-session basis by setting any of the following environment variables (default values shown):</p> <pre><code>export FIFTYONE_MEDIA_CACHE_DIR=/path/for/media-cache\nexport FIFTYONE_MEDIA_CACHE_SIZE_BYTES=34359738368  # 32GB\nexport FIFTYONE_MEDIA_CACHE_DOWNLOAD_SIZE_BYTES=134217728  # 128MB\nexport FIFTYONE_MEDIA_CACHE_NUM_WORKERS=16\nexport FIFTYONE_MEDIA_CACHE_APP_IMAGES=false\n</code></pre> <p>2. Create a media cache config file at <code>~/.fiftyone/media_cache_config.json</code> that contains any of the following keys (default values shown):</p> <pre><code>{\n    \"cache_dir\": \"/path/for/media-cache\",\n    \"cache_size_bytes\": 34359738368,\n    \"download_size_bytes\": 134217728,\n    \"num_workers\": 16,\n    \"cache_app_images\": false\n}\n</code></pre> <p>You can change the location of this file by setting the <code>FIFTYONE_MEDIA_CACHE_CONFIG_PATH</code> environment variable.</p> <p>If you combine multiple options above, environment variables will take precedence over JSON config settings.</p>"},{"location":"teams/cloud_media/#working-with-cloud-backed-datasets","title":"Working with cloud-backed datasets \u00b6","text":"<p>When writing Python code using the Teams client that may involve cloud-backed datasets, use <code>sample.local_path</code> instead of <code>sample.filepath</code> to retrieve the location of the locally cached version of a media file:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.load_dataset(\"a-teams-dataset\")\nsample = dataset.first()\n\nprint(sample.filepath)\n# ex: s3://voxel51-test/images/000001.jpg\n\nprint(sample.local_path)\n# ex: ~/fiftyone/__cache__/media/s3/voxel51-test/images/000001.jpg\n</code></pre> <p>Note</p> <p>If <code>sample.filepath</code> itself is a local path, then <code>sample.local_path</code> will simply return that path. In other words, it is safe to write all Teams Python code as if the dataset contains cloud-backed media.</p> <p>Note</p> <p>If you access <code>sample.local_path</code> and the corresponding media file is not cached locally, it will immediately be downloaded.</p> <p>You can use <code>download_media()</code> to efficiently download and cache the source media files for an entire dataset or view using the cache\u2019s full thread pool to maximize throughput:</p> <pre><code>import fiftyone as fo\n\n# Download media for a view\nview = dataset.shuffle().limit(10)\nview.download_media()\n\n# Download all media in the dataset\ndataset.download_media()\n</code></pre> <p>Note</p> <p>By default, <code>download_media()</code> will automatically skip any already cached media.</p> <p>You can also use <code>download_context()</code> to download smaller batches of media when iterating over samples in a collection:</p> <pre><code>import fiftyone as fo\n\ndataset = fo.load_dataset(\"a-teams-dataset\")\n\n# Download media using the default batching strategy\nwith dataset.download_context():\n    for sample in dataset:\n        sample.local_path  # already downloaded\n\n# Download media in batches of 50MB\nwith dataset.download_context(target_size_bytes=50 * 1024**2):\n    for sample in dataset:\n        sample.local_path  # already downloaded\n</code></pre> <p>Note</p> <p>You can configure the default size of each download batch via the <code>download_size_bytes</code> parameter of your media cache config.</p> <p>Download contexts provide a middle ground between the two extremes:</p> <pre><code># Download all media in advance\ndataset.download_media()\nfor sample in dataset:\n    sample.local_path  # already downloaded\n\n# Download individual images just in time\nfor sample in dataset:\n    sample.local_path   # downloads media now\n</code></pre> <p>Note</p> <p>Download contexts are useful if your cache is not large enough to store all the media in the dataset you\u2019re working with simultaneously.</p> <p>You can also use <code>get_local_paths()</code> to retrieve the list of local paths for each sample in a potentially cloud-backed dataset or view:</p> <pre><code># These methods support full datasets or views into them\nsample_collection = dataset\n# sample_collection = dataset.limit(10)\n\n# Retrieve the local paths for all media in a collection\nlocal_paths = sample_collection.get_local_paths()\n\nprint(local_paths[0])\n# ex: ~/fiftyone/__cache__/media/s3/voxel51-test/images/000001.jpg\n\n# Retrieve the possibly-cloud paths for all media in a collection\ncloud_paths = sample_collection.values(\"filepath\")\n\nprint(cloud_paths[0])\n# ex: s3://voxel51-test/images/000001.jpg\n</code></pre> <p>You can get information about currently cached media files for a sample collection by calling <code>cache_stats()</code>:</p> <pre><code># View cache stats for the current collection\nsample_collection.cache_stats()\n</code></pre> <pre><code>{'cache_dir': '~/fiftyone/__cache__',\n 'cache_size': 34359738368,\n 'cache_size_str': '32.0GB',\n 'current_size': 24412374,\n 'current_size_str': '23.3MB',\n 'current_count': 200,\n 'load_factor': 0.000710493593942374}\n</code></pre> <p>and you can call <code>clear_media()</code> to delete any cached copies of media in the collection:</p> <pre><code># Clear this collection's media from the cache\nsample_collection.clear_media()\n</code></pre> <p>You can also perform these operations on the full cache as follows:</p> <pre><code># View global cache stats\nprint(fo.media_cache.stats())\n</code></pre> <pre><code>{'cache_dir': '~/fiftyone/__cache__',\n 'cache_size': 34359738368,\n 'cache_size_str': '32.0GB',\n 'current_size': 49097587,\n 'current_size_str': '46.8MB',\n 'current_count': 600,\n 'load_factor': 0.0014289278478827327}\n</code></pre> <pre><code># Clear the entire cache\nfo.media_cache.clear()\n</code></pre> <p>The <code>fiftyone.core.storage</code> module also provides a number of convenient methods that can be used to manipulate cloud and/or local media.</p> <p>The <code>upload_media()</code> method provides a convenient wrapper for uploading a local dataset\u2019s media to the cloud:</p> <pre><code>import fiftyone.core.storage as fos\n\n# Create a dataset from media stored locally\ndataset = fo.Dataset.from_dir(\"/tmp/local\", ...)\n\n# Upload the dataset's media to the cloud\nfos.upload_media(\n    dataset,\n    \"s3://voxel51-test/your-media\",\n    update_filepaths=True,\n    progress=True,\n)\n</code></pre> <p>The <code>fiftyone.core.storage</code> module also provides a number of lower-level methods that you can use to work with cloud and local assets.</p> <pre><code>import fiftyone.core.storage as fos\n\ns3_paths = [\\\n    \"s3://voxel51-test/images/000001.jpg\",\\\n    \"s3://voxel51-test/images/000002.jpg\",\\\n    ...\\\n]\n\ngcs_paths = [\\\n    \"gs://voxel51-test/images/000001.jpg\",\\\n    \"gs://voxel51-test/images/000002.jpg\",\\\n    ...\\\n\\\n]\n\nlocal_paths = [\\\n    \"/tmp/voxel51-test/images/000001.jpg\",\\\n    \"/tmp/voxel51-test/images/000002.jpg\",\\\n    ...\\\n]\n</code></pre> <p>For example, you can use <code>list_files()</code> to list the contents of a folder:</p> <pre><code>cloud_paths = fos.list_files(\n    \"s3://voxel51-test\", abs_paths=True, recursive=True\n)\n\nprint(cloud_paths)[0]\n# ex: s3://voxel51-test/images/000001.jpg\n</code></pre> <p>or you can use <code>copy_files()</code> and <code>move_files()</code> to transfer files between destinations:</p> <pre><code># S3 -&gt; local\nfos.copy_files(s3_paths, local_paths)\nfos.move_files(s3_paths, local_paths)\n\n# local -&gt; S3\nfos.copy_files(local_paths, s3_paths)\nfos.move_files(local_paths, s3_paths)\n\n# S3 -&gt; GCS\nfos.copy_files(s3_paths, gcs_paths)\nfos.move_files(s3_paths, gcs_paths)\n</code></pre> <p>or you can use <code>delete_files()</code> to delete assets:</p> <pre><code>fos.delete_files(s3_paths)\nfos.delete_files(gcs_paths)\nfos.delete_files(local_paths)\n</code></pre> <p>Note</p> <p>All of the above methods use the media cache\u2019s thread pool to maximize throughput.</p>"},{"location":"teams/cloud_media/#api-reference","title":"API reference \u00b6","text":""},{"location":"teams/cloud_media/#dataset-methods","title":"<code>Dataset</code> methods \u00b6","text":"<pre><code>import fiftyone as fo\n\nfo.Dataset.download_media?\nfo.Dataset.download_scenes?\nfo.Dataset.download_context?\nfo.Dataset.get_local_paths?\nfo.Dataset.cache_stats?\nfo.Dataset.clear_media?\n</code></pre> <pre><code>fo.Dataset.download_media(\n    self,\n    media_fields=None,\n    group_slices=None,\n    include_assets=True,\n    update=False,\n    skip_failures=True,\n    progress=None,\n):\n    \"\"\"Downloads the source media files for all samples in the collection.\n\n    This method is only useful for collections that contain remote media.\n\n    Any existing files are not re-downloaded, unless ``update == True`` and\n    their checksums no longer match.\n\n    Args:\n        media_fields (None): a field or iterable of fields containing media\n            to download. By default, all media fields in the collection's\n            :meth:`app_config` are used\n        group_slices (None): an optional subset of group slices for which\n            to download media. Only applicable when the collection contains\n            groups\n        include_assets (True): whether to include 3D scene assets\n        update (False): whether to re-download media whose checksums no\n            longer match\n        skip_failures (True): whether to gracefully continue without\n            raising an error if a remote file cannot be downloaded\n        progress (None): whether to render a progress bar tracking the\n            progress of any downloads (True/False), use the default value\n            ``fiftyone.config.show_progress_bars`` (None), or a progress\n            callback function to invoke instead\n    \"\"\"\n</code></pre> <pre><code>fo.Dataset.download_scenes(\n    self,\n    update=False,\n    skip_failures=True,\n    progress=None,\n):\n    \"\"\"Downloads all ``.fo3d`` files for the samples in the collection.\n\n    This method is only useful for collections that contain remote media.\n\n    Any existing files are not re-downloaded, unless ``update == True`` and\n    their checksums no longer match.\n\n    Args:\n        update (False): whether to re-download files whose checksums no\n            longer match\n        skip_failures (True): whether to gracefully continue without\n            raising an error if a remote file cannot be downloaded\n        progress (None): whether to render a progress bar tracking the\n            progress of any downloads (True/False), use the default value\n            ``fiftyone.config.show_progress_bars`` (None), or a progress\n            callback function to invoke instead\n    \"\"\"\n</code></pre> <pre><code>fo.Dataset.download_context(\n    self,\n    batch_size=None,\n    target_size_bytes=None,\n    media_fields=None,\n    group_slices=None,\n    include_assets=True,\n    update=False,\n    skip_failures=True,\n    clear=False,\n    progress=None,\n):\n    \"\"\"Returns a context that can be used to pre-download media in batches\n    when iterating over samples in this collection.\n\n    This method is only useful for collections that contain remote media.\n\n    By default, all media will be downloaded when the context is entered,\n    but you can configure a batching strategy via the `batch_size` or\n    `target_size_bytes` parameters.\n\n    If no ``batch_size`` or ``target_size_bytes`` is provided, media are\n    downloaded in batches of ``fo.media_cache_config.download_size_bytes``.\n\n    Args:\n        batch_size (None): a sample batch size to use for each download\n        target_size_bytes (None): a target content size, in bytes, for each\n            download batch. If negative, all media is downloaded in one\n            batch\n        media_fields (None): a field or iterable of fields containing media\n            to download. By default, all media fields in the collection's\n            :meth:`app_config` are used\n        group_slices (None): an optional subset of group slices to download\n            media for. Only applicable when the collection contains groups\n        include_assets (True): whether to include 3D scene assets\n        update (False): whether to re-download media whose checksums no\n            longer match\n        skip_failures (True): whether to gracefully continue without\n            raising an error if a remote file cannot be downloaded\n        clear (False): whether to clear the media from the cache when the\n            context exits\n        progress (None): whether to render a progress bar tracking the\n            progress of any downloads (True/False), use the default value\n            ``fiftyone.config.show_progress_bars`` (None), or a progress\n            callback function to invoke instead\n        **kwargs: valid keyword arguments for :meth:`download_media`\n\n    Returns:\n        a :class:`DownloadContext`\n    \"\"\"\n</code></pre> <pre><code>fo.Dataset.get_local_paths(\n    self,\n    media_field=\"filepath\",\n    include_assets=True,\n    download=True,\n    skip_failures=True,\n    progress=None,\n):\n    \"\"\"Returns a list of local paths to the media files in this collection.\n\n    This method is only useful for collections that contain remote media.\n\n    Args:\n        media_field (\"filepath\"): the field containing the media paths\n        include_assets (True): whether to include 3D scene assets\n        download (True): whether to download any non-cached media files\n        skip_failures (True): whether to gracefully continue without\n            raising an error if a remote file cannot be downloaded\n        progress (None): whether to render a progress bar tracking the\n            progress of any downloads (True/False), use the default value\n            ``fiftyone.config.show_progress_bars`` (None), or a progress\n            callback function to invoke instead\n\n    Returns:\n        a list of local filepaths\n    \"\"\"\n</code></pre> <pre><code>fo.Dataset.cache_stats(\n    self,\n    media_fields=None,\n    group_slices=None,\n    include_assets=True,\n):\n    \"\"\"Returns a dictionary of stats about the cached media files in this\n    collection.\n\n    This method is only useful for collections that contain remote media.\n\n    Args:\n        media_fields (None): a field or iterable of fields containing media\n            paths. By default, all media fields in the collection's\n            :meth:`app_config` are included\n        group_slices (None): an optional subset of group slices to include.\n            Only applicable when the collection contains groups\n        include_assets (True): whether to include 3D scene assets\n\n    Returns:\n        a stats dict\n    \"\"\"\n</code></pre> <pre><code>fo.Dataset.clear_media(\n    self,\n    media_fields=None,\n    group_slices=None,\n    include_assets=True,\n):\n    \"\"\"Deletes any local copies of media files in this collection from the\n    media cache.\n\n    This method is only useful for collections that contain remote media.\n\n    Args:\n        media_fields (None): a field or iterable of fields containing media\n            paths to clear from the cache. By default, all media fields\n            in the collection's :meth:`app_config` are cleared\n        group_slices (None): an optional subset of group slices for which\n            to clear media. Only applicable when the collection contains\n            groups\n        include_assets (True): whether to include 3D scene assets\n    \"\"\"\n</code></pre>"},{"location":"teams/cloud_media/#fiftyonecorestorage","title":"<code>fiftyone.core.storage</code> \u00b6","text":"<pre><code>import fiftyone.core.storage as fos\n\nfos.list_files?\nfos.copy_files?\nfos.move_files?\nfos.delete_files?\nfos.upload_media?\n</code></pre> <pre><code>fos.list_files(\n    dirpath,\n    abs_paths=False,\n    recursive=False,\n    include_hidden_files=False,\n    sort=True,\n):\n    \"\"\"Lists the files in the given directory.\n\n    If the directory does not exist, an empty list is returned.\n\n    Args:\n        dirpath: the path to the directory to list\n        abs_paths (False): whether to return the absolute paths to the files\n        recursive (False): whether to recursively traverse subdirectories\n        include_hidden_files (False): whether to include dot files\n        sort (True): whether to sort the list of files\n\n    Returns:\n        a list of filepaths\n    \"\"\"\n</code></pre> <pre><code>fos.copy_files(inpaths, outpaths, skip_failures=False, progress=None):\n    \"\"\"Copies the files to the given locations.\n\n    Args:\n        inpaths: a list of input paths\n        outpaths: a list of output paths\n        skip_failures (False): whether to gracefully continue without\n            raising an error if a remote operation fails\n        progress (None): whether to render a progress bar (True/False), use the\n            default value ``fiftyone.config.show_progress_bars`` (None), or a\n            progress callback function to invoke instead\n    \"\"\"\n</code></pre> <pre><code>fos.move_files(inpaths, outpaths, skip_failures=False, progress=None):\n    \"\"\"Moves the files to the given locations.\n\n    Args:\n        inpaths: a list of input paths\n        outpaths: a list of output paths\n        skip_failures (False): whether to gracefully continue without raising\n            an error if a remote operation fails\n        progress (None): whether to render a progress bar (True/False), use the\n            default value ``fiftyone.config.show_progress_bars`` (None), or a\n            progress callback function to invoke instead\n    \"\"\"\n</code></pre> <pre><code>fos.delete_files(paths, skip_failures=False, progress=None):\n    \"\"\"Deletes the files from the given locations.\n\n    For local paths, any empty directories are also recursively deleted from\n    the resulting directory tree.\n\n    Args:\n        paths: a list of paths\n        skip_failures (False): whether to gracefully continue without raising\n            an error if a remote operation fails\n        progress (None): whether to render a progress bar (True/False), use the\n            default value ``fiftyone.config.show_progress_bars`` (None), or a\n            progress callback function to invoke instead\n    \"\"\"\n</code></pre> <pre><code>fos.upload_media(\n    sample_collection,\n    remote_dir,\n    rel_dir=None,\n    media_field=\"filepath\",\n    update_filepaths=False,\n    cache=False,\n    overwrite=False,\n    skip_failures=False,\n    progress=None,\n):\n    \"\"\"Uploads the source media files for the given collection to the given\n    remote directory.\n\n    Providing a ``rel_dir`` enables writing nested subfolders within\n    ``remote_dir`` matching the structure of the input collection's media. By\n    default, the files are written directly to ``remote_dir`` using their\n    basenames.\n\n    Args:\n        sample_collection: a\n            :class:`fiftyone.core.collections.SampleCollection`\n        remote_dir: a remote \"folder\" into which to upload\n        rel_dir (None): an optional relative directory to strip from each\n            filepath when constructing the corresponding remote path\n        media_field (\"filepath\"): the field containing the media paths\n        update_filepaths (False): whether to update the ``media_field`` of each\n            sample in the collection to its remote path\n        cache (False): whether to store the uploaded media in your local media\n            cache. The supported values are:\n\n            -   ``False`` (default): do not cache the media\n            -   ``True`` or ``\"copy\"``: copy the media into your local cache\n            -   ``\"move\"``: move the media into your local cache\n        overwrite (False): whether to overwrite (True) or skip (False) existing\n            remote files\n        skip_failures (False): whether to gracefully continue without raising\n            an error if a remote operation fails\n        progress (None): whether to render a progress bar (True/False), use the\n            default value ``fiftyone.config.show_progress_bars`` (None), or a\n            progress callback function to invoke instead\n\n    Returns:\n        the list of remote paths\n    \"\"\"\n</code></pre>"},{"location":"teams/cloud_media/#annotating-cloud-backed-datasets-with-cvat","title":"Annotating cloud-backed datasets with CVAT \u00b6","text":"<p>When using FiftyOne to annotate data with CVAT, you can optionally follow the instructions below to instruct CVAT to load media directly from S3, GCS, or MinIO buckets rather than the default behavior of uploading copies of the media to the CVAT server.</p> <p>First, follow these instructions to attach a cloud storage bucket to CVAT. Then, simply provide the <code>cloud_manifest</code> parameter to <code>annotate()</code> to specify the URL of the manifest file in your cloud bucket:</p> <pre><code>anno_key = \"cloud_annotations\"\n\nresults = dataset.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    cloud_manifest=\"s3://voxel51/manifest.jsonl\",\n)\n</code></pre> <p>Alternatively, if your cloud manifest has the default name <code>manifest.jsonl</code> and exists in the root of the bucket containing the data in the sample collection being annotated, then you can simply pass <code>cloud_manifest=True</code>:</p> <pre><code>results = dataset.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    cloud_manifest=True,\n)\n</code></pre> <p>Note</p> <p>The cloud manifest file must contain all media files in the sample collection being annotated.</p>"},{"location":"teams/cloud_media/#annotating-cloud-backed-datasets-with-v7-darwin","title":"Annotating cloud-backed datasets with V7 Darwin \u00b6","text":"<p>When using FiftyOne to annotate data with V7 Darwin, you can optionally follow the instructions below to instruct V7 to load media directly from S3, GCS, or Azure buckets rather than the default behavior of uploading copies of the media from your local machine.</p> <p>First, follow these instructions to configure external storage for V7. Then, simply provide the <code>external_storage</code> parameter to <code>annotate()</code> and specify the sluggified external storage name:</p> <pre><code>anno_key = \"cloud_annotations\"\n\nresults = dataset.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    external_storage=\"example-darwin-storage-slug\",\n    ...\n)\n</code></pre>"},{"location":"teams/cloud_media/#annotating-cloud-backed-datasets-with-labelbox","title":"Annotating cloud-backed datasets with Labelbox \u00b6","text":"<p>When using FiftyOne to annotate data with Labelbox, you can optionally follow the instructions below to instruct Labelbox to load media directly from S3 rather than the default behavior of uploading copies of the media.</p> <p>This assumes that you have configured the S3 integration for Labelbox. If so, then you can provide the <code>upload_media=False</code> keyword argument to the <code>annotate()</code> method to pass URLs for your S3-backed media when creating Labelbox data rows.</p> <pre><code>results = dataset.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    label_type=\"detections\",\n    classes=[\"dog\", \"cat\"],\n    backend=\"labelbox\",\n    upload_media=False,\n)\n</code></pre> <p>Note</p> <p>Google Cloud and Azure blob support will be added in the future. Currently, any Google Cloud, Azure, or local media will still be uploaded to Labelbox as usual.</p>"},{"location":"teams/cloud_media/#aws-lambda-and-google-cloud-functions","title":"AWS Lambda and Google Cloud Functions \u00b6","text":"<p>FiftyOne Teams can easily be used in AWS Lambda Functions and Google Cloud Functions.</p> <p>Requirements</p> <p>We recommend including Teams in your function\u2019s <code>requirements.txt</code> file by passing your token as a build environment variable, e.g., <code>FIFTYONE_TEAMS_TOKEN</code> and then using the syntax below to specify the version of the Teams client to use:</p> <pre><code>https://${FIFTYONE_TEAMS_TOKEN}@pypi.fiftyone.ai/packages/fiftyone-0.6.6-py3-none-any.whl\n</code></pre> <p>Runtime</p> <p>Lambda/GCFs cannot use services, so you must disable the media the cache by setting the following runtime environment variable:</p> <pre><code>export FIFTYONE_MEDIA_CACHE_SIZE_BYTES=-1  # disable media cache\n</code></pre> <p>From there, you can configure your database URI and any necessary cloud storage credentials via runtime environment variables as you normally would, eg:</p> <pre><code>export FIFTYONE_DATABASE_URI=mongodb://...\n</code></pre>"},{"location":"teams/data_lens/","title":"Data Lens \u00b6","text":"<p>Available in FiftyOne Teams v2.2+</p> <p>Data Lens is a feature built into the FiftyOne Teams App which allows you to use FiftyOne to explore and import samples from external data sources.</p> <p>Whether your data resides in a database like PostgreSQL or a data lake like Databricks or BigQuery, Data Lens provides a way to search your data sources, visualize sample data, and import into FiftyOne for further analysis.</p> <p></p>"},{"location":"teams/data_lens/#how-it-works","title":"How it works \u00b6","text":"<ol> <li>Define your search experience</li> </ol> <p>Tailor the interactions with your data source to exactly what you need. Data Lens provides a flexible framework which allows for extensive customization, allowing you to hone in on the questions that are most critical to your workflow. See Integrating with Data Lens to learn more.</p> <ol> <li>Connect your data source</li> </ol> <p>Provide configuration to Data Lens to connect to your data source. Once connected, you can start searching for samples directly from FiftyOne. See Connecting a data source to learn more.</p> <ol> <li>Interact with your data</li> </ol> <p>View your samples using Data Lens\u2019 rich preview capabilities. Refine your search criteria to find samples of interest, then import your samples into a FiftyOne dataset for further analysis. See Exploring samples to learn more.</p>"},{"location":"teams/data_lens/#connecting-a-data-source","title":"Connecting a data source \u00b6","text":"<p>A data source represents anywhere that you store your data outside of FiftyOne. This could be a local SQL database, a hosted data lake, or an internal data platform. To connect to your data source, you\u2019ll first implement a simple operator which FiftyOne can use to communicate with your data source.</p> <p>Once your operator is defined, you can navigate to the \u201cData sources\u201d tab by clicking on the tab header or by clicking on \u201cConnect to a data source\u201d from the \u201cHome\u201d tab.</p> <p></p> <p>Add a new data source by clicking on \u201cAdd data source\u201d.</p> <p>Enter a useful name for your data source and provide the URI for your operator. The URI should have the format <code>&lt;your-plugin-name&gt;/&lt;your-operator-name&gt;</code>.</p> <p></p> <p>Click \u201cConnect\u201d once you\u2019re finished to save your configuration.</p> <p></p> <p>If you need to update your Data Lens configuration, simply click the action menu icon and select \u201cEdit\u201d. Similarly, you can delete a Data Lens configuration by clicking the action menu icon and selecting \u201cDelete\u201d.</p> <p>That\u2019s it. Now you\u2019re ready to explore your data source. You can head over to the \u201cQuery data\u201d tab and start interacting with your data.</p> <p>Note</p> <p>Don\u2019t have a Data Lens operator yet? When you\u2019re ready to get started, you can follow our detailed integration guide.</p>"},{"location":"teams/data_lens/#exploring-samples","title":"Exploring samples \u00b6","text":"<p>Once you\u2019ve connected to a data source, you can open the \u201cQuery data\u201d tab to start working with your data.</p> <p>In this tab, you can select from your connected data sources using the \u201cSelect a data source\u201d dropdown.</p> <p>Below this dropdown, you can parameterize your search using the available query parameters. These parameters are unique to each connected data source, allowing you to tailor your search experience to exactly what you need. Selecting a new data source will automatically update the query parameters to match those expected by your data source.</p> <p></p> <p>After you enter your query parameters, you can click the \u201cPreview data\u201d button at the bottom of the page to fetch samples which match your query parameters. These samples will be displayed in the preview panel, along with any features associated with the sample like labels or bounding boxes.</p> <p></p> <p>You can use the zoom slider to control the size of the samples, and you can modify the number of preview samples shown by changing the \u201cNumber of preview samples\u201d input value and clicking \u201cPreview data\u201d again.</p> <p>If you want to change your search, simply reopen the query parameters panel and modify your inputs. Clicking \u201cPreview data\u201d will fetch new samples matching the current query parameters.</p> <p>If you want to import your samples into FiftyOne for further analysis, you can import your samples to a dataset.</p>"},{"location":"teams/data_lens/#importing-samples-to-fiftyone","title":"Importing samples to FiftyOne \u00b6","text":"<p>After generating a preview in Data Lens, you can click on the \u201cImport data\u201d button to open the import dialog.</p> <p></p> <p>Imports can be limited to a specific number of samples, or you can import all samples matching your query parameters.</p> <p>The \u201cSkip existing samples\u201d checkbox allows you to configure the behavior for merging samples into a dataset. If checked, samples with a <code>filepath</code> which is already present in the dataset will be skipped. If left unchecked, all samples will be added to the dataset.</p> <p>Note</p> <p>If you elect to skip existing samples, this will also deduplicate samples within the data being imported.</p> <p>After configuring the size/behavior of your import, select a destination dataset for the samples. This can be an existing dataset, or you can choose to create a new dataset.</p> <p>You can optionally specify tags to append to the <code>tags</code> field of each imported sample.</p> <p>When you click import, you will have the option to either execute immediately or to schedule this import for asynchronous execution.</p> <p></p> <p>If you are importing a small number of samples, then immediate execution may be appropriate. However, for most cases it is recommended to schedule the import, as this will result in more consistent and performant execution.</p> <p>Note</p> <p>Scheduled imports use the delegated operations framework to execute asynchronously on your connected compute cluster!</p> <p>After selecting your execution preference, you will be able to monitor the status of your import through the information provided by the import panel.</p> <p>In the case of immediate execution, you will be presented with an option to view your samples once the import is complete. Clicking on this button will open your destination dataset containing your imported samples.</p> <p></p> <p>In the case of scheduled execution, you will be presented with an option to visit the Runs page.</p> <p></p> <p>From the Runs page, you can track the status of your import.</p> <p></p> <p>Once your samples are imported, you will be able to leverage the full capabilities of FiftyOne to analyze and curate your data, and you can continue to use Data Lens to augment your datasets.</p> <p></p>"},{"location":"teams/data_lens/#integrating-with-data-lens","title":"Integrating with Data Lens \u00b6","text":"<p>Data Lens makes use of FiftyOne\u2019s powerful plugins framework to allow you to tailor your experience to meet the needs of your data. As part of the plugin framework, you are able to create custom operators, which are self-contained Python classes that provide custom functionality to FiftyOne.</p> <p>Data Lens defines an operator interface which makes it easy to connect to your data sources. We\u2019ll walk through an example of creating your first Data Lens operator.</p>"},{"location":"teams/data_lens/#setting-up-your-operator","title":"Setting up your operator \u00b6","text":"<p>To assist with Data Lens integration, we can use the <code>DataLensOperator</code> base class provided with the Teams SDK. This base class handles the implementation for the operator\u2019s <code>execute()</code> method, and defines a single abstract method that we\u2019ll implement.</p> <pre><code># my_plugin/__init__.py\nfrom typing import Generator\n\nimport fiftyone.operators as foo\nfrom fiftyone.operators.data_lens import (\n    DataLensOperator,\n    DataLensSearchRequest,\n    DataLensSearchResponse\n)\n\nclass MyCustomDataLensOperator(DataLensOperator):\n    \"\"\"Custom operator which integrates with Data Lens.\"\"\"\n\n    @property\n    def config(self) -&gt; foo.OperatorConfig:\n        return foo.OperatorConfig(\n            name=\"my_custom_data_lens_operator\",\n            label=\"My custom Data Lens operator\",\n            unlisted=True,\n            execute_as_generator=True,\n        )\n\n    def handle_lens_search_request(\n        self,\n        request: DataLensSearchRequest,\n        ctx: foo.ExecutionContext\n    ) -&gt; Generator[DataLensSearchResponse, None, None]\n        # We'll implement our logic here\n        pass\n</code></pre> <p>Let\u2019s take a look at what we have so far.</p> <pre><code>class MyCustomDataLensOperator(DataLensOperator):\n</code></pre> <p>Our operator extends the <code>DataLensOperator</code> provided by the Teams SDK. This base class defines the abstract <code>handle_lens_search_request()</code> method, which we will need to implement.</p> <pre><code>@property\ndef config(self) -&gt; foo.OperatorConfig:\n    return foo.OperatorConfig(\n        # This is the name of your operator. FiftyOne will canonically\n        # refer to your operator as &lt;your-plugin&gt;/&lt;your-operator&gt;.\n        name=\"my_custom_data_lens_operator\",\n\n        # This is a human-friendly label for your operator.\n        label=\"My custom Data Lens operator\",\n\n        # Setting unlisted to True prevents your operator from appearing\n        # in lists of general-purpose operators, as this operator is not\n        # intended to be directly executed.\n        unlisted=True,\n\n        # For compatibility with the DataLensOperator base class, we\n        # instruct FiftyOne to execute our operator as a generator.\n        execute_as_generator=True,\n    )\n</code></pre> <p>The <code>config</code> property is part of the standard operator interface and provides configuration options for your operator.</p> <pre><code>def handle_lens_search_request(\n    self,\n    request: DataLensSearchRequest,\n    ctx: foo.ExecutionContext\n) -&gt; Generator[DataLensSearchResponse, None, None]\n    pass\n</code></pre> <p>The <code>handle_lens_search_request()</code> method provides us with two arguments: a <code>DataLensSearchRequest</code> instance, and the current operator execution context.</p> <p>The <code>DataLensSearchRequest</code> is generated by the Data Lens framework and provides information about the Data Lens user\u2019s query. The request object has the following properties:</p> <ul> <li> <p><code>request.search_params</code>: a dict containing the search parameters provided by the Data Lens user.</p> </li> <li> <p><code>request.batch_size</code>: a number indicating the maximum number of samples to return in a single batch.</p> </li> <li> <p><code>request.max_results</code>: a number indicating the maximum number of samples to return across all batches.</p> </li> </ul> <p>Note</p> <p>The Data Lens framework will automatically truncate responses to adhere to <code>request.max_results</code>. Any sample data beyond this limit will be discarded.</p> <p>The <code>ctx</code> argument provides access to a range of useful capabilities which you can leverage in your operator, including things like providing secrets to your operator.</p> <p>Using these inputs, we are expected to return a generator which yields <code>DataLensSearchResponse</code> objects. To start, we\u2019ll create some synthetic data to better understand the interaction between Data Lens and our operator. We\u2019ll look at a more realistic example later on.</p> <p>Note</p> <p>Why a generator? Generators provide a convenient approach for long-lived, lazy-fetching connections that are common in databases and data lakes. While Data Lens does support operators which do not execute as generators, we recommend using a generator for ease of integration.</p>"},{"location":"teams/data_lens/#generating-search-responses","title":"Generating search responses \u00b6","text":"<p>To adhere to the Data Lens interface, we need to yield <code>DataLensSearchResponse</code> objects from our operator. A <code>DataLensSearchResponse</code> is comprised of the following fields:</p> <ul> <li> <p><code>response.result_count</code>: a number indicating the number of samples being returned in this response.</p> </li> <li> <p><code>response.query_result</code>: a list of dicts containing serialized <code>Sample</code> data, e.g. obtained via <code>to_dict()</code>.</p> </li> </ul> <p>Note</p> <p>Data Lens expects sample data to adhere to the <code>Sample</code> format, which is easy to achieve by using the FiftyOne SDK to create your sample data, as shown below.</p> <p>To see how Data Lens works, let\u2019s yield a response with a single synthetic sample.</p> <pre><code>def handle_lens_search_request(\n    self,\n    request: DataLensSearchRequest,\n    ctx: foo.ExecutionContext\n) -&gt; Generator[DataLensSearchResponse, None, None]\n    # We'll use a placeholder image for our synthetic data\n    image_url = \"https://placehold.co/150x150\"\n\n    # Create a sample using the SDK\n    synthetic_sample = fo.Sample(filepath=image_url)\n\n    # Convert our samples to dicts\n    samples = [synthetic_sample.to_dict()]\n\n    # We'll ignore any inputs for now and yield a single response\n    yield DataLensSearchResponse(\n        result_count=len(samples),\n        query_result=samples\n    )\n</code></pre> <p>Let\u2019s see what this looks like in Data Lens.</p> <p>After adding the operator as a data source, we can navigate to the \u201cQuery data\u201d tab to interact with the operator. When we click the preview button, the Data Lens framework invokes our operator to retrieve sample data. Our operator yields a single sample, and we see that sample shown in the preview.</p> <p></p> <p>Let\u2019s modify our operator to incorporate the <code>request.batch_size</code> property.</p> <pre><code>def handle_lens_search_request(\n    self,\n    request: DataLensSearchRequest,\n    ctx: foo.ExecutionContext\n) -&gt; Generator[DataLensSearchResponse, None, None]\n    samples = []\n\n    # Generate number of samples equal to request.batch_size\n    for i in range(request.batch_size):\n        samples.append(\n            fo.Sample(\n                # We'll modify our synthetic data to include the\n                # sample's index as the image text.\n                filepath=f\"https://placehold.co/150x150?text={i + 1}\"\n            ).to_dict()\n        )\n\n    # Still yielding a single response\n    yield DataLensSearchResponse(\n        result_count=len(samples),\n        query_result=samples\n    )\n</code></pre> <p>Now if we re-run our preview, we see that we get a number of samples equal to the \u201cNumber of preview samples\u201d input.</p> <p></p> <p>If we modify that number and regenerate the preview, we can see that the number of samples remains in sync. For preview functionality, Data Lens fetches sample data in a single batch, so we can expect these values to be the same.</p>"},{"location":"teams/data_lens/#working-with-user-provided-data","title":"Working with user-provided data \u00b6","text":"<p>Let\u2019s now look at how Data Lens users are able to interact with our operator. Data Lens is designed to enable users to quickly explore samples of interest, and a key component is providing users a way to control the behavior of our operator.</p> <p>To achieve this, we simply need to define the possible inputs to our operator in the <code>resolve_input()</code> method.</p> <pre><code>def resolve_input(self):\n    # We define our inputs as an object.\n    # We'll add specific fields to this object which represent a single input.\n    inputs = types.Object()\n\n    # Add a string field named \"sample_text\"\n    inputs.str(\"sample_text\", label=\"Sample text\", description=\"Text to render in samples\")\n\n    return types.Property(inputs)\n</code></pre> <p>Note</p> <p>For more information on operator inputs, see the plugin documentation.</p> <p>With this method implemented, Data Lens will construct a form allowing users to define any or all of these inputs.</p> <p></p> <p>We can then use this data to change the behavior of our operator. Let\u2019s add logic to integrate <code>sample_text</code> into our operator.</p> <pre><code>def handle_lens_search_request(\n    self,\n    request: DataLensSearchRequest,\n    ctx: foo.ExecutionContext\n) -&gt; Generator[DataLensSearchResponse, None, None]\n    # Retrieve our \"sample_text\" input from request.search_params.\n    # These parameter names should match those used in resolve_input().\n    sample_text = request.search_params.get(\"sample_text\", \"\")\n\n    samples = []\n\n    # Create a sample for each character in our input text\n    for char in sample_text:\n        samples.append(\n            fo.Sample(\n                filepath=f\"https://placehold.co/150x150?text={char}\"\n            ).to_dict()\n        )\n\n        # Yield batches when we have enough samples\n        if len(samples) == request.batch_size:\n            yield DataLensSearchResponse(\n                result_count=len(samples),\n                query_result=samples\n            )\n\n            # Reset our batch\n            samples = []\n\n    # We've generated all our samples, but might be in the middle of a batch\n    if len(samples) &gt; 0:\n        yield DataLensSearchResponse(\n            result_count=len(samples),\n            query_result=samples\n        )\n\n    # Now we're done :)\n</code></pre> <p>Now when we run our preview, we can see that the text we provide as input is reflected in the samples returned by our operator. Modifying the text and regenerating the preview yields the expected result.</p> <p></p> <p>There are a couple things to note about the changes we made here.</p> <ul> <li> <p>Inputs can be specified with <code>required=True</code>, in which case Data Lens will ensure that the user provides a value for that input. If an input is not explicitly required, then we should be sure to handle the case where it is not present.</p> </li> <li> <p>In most real scenarios, our operator will be processing more samples than fit in a single batch. (This is even true here, where there is no upper bound on our input length). As such, our operator should respect the <code>request.batch_size</code> parameter and yield batches of samples as they are available.</p> </li> </ul> <p>Note</p> <p>This example is meant to illustrate how users can interact with our operator. For a more realistic view into how inputs can tailor our search experience, see our example integration with Databricks.</p>"},{"location":"teams/data_lens/#differences-in-preview-and-import","title":"Differences in preview and import \u00b6","text":"<p>While the examples here are focused on preview functionality, the Data Lens framework invokes your operator in the same way to achieve both preview and import functionality. The <code>request.batch_size</code> and <code>request.max_results</code> parameters can be used to optimize your data retrieval, but preview and import should otherwise be treated as functionally equivalent.</p>"},{"location":"teams/data_lens/#example-data-source-connectors","title":"Example data source connectors \u00b6","text":"<p>This section provides example Data Lens connectors for various popular data sources.</p>"},{"location":"teams/data_lens/#databricks","title":"Databricks \u00b6","text":"<p>Below is an example of a Data Lens connector for Databricks. This example uses a schema consistent with the Berkeley DeepDrive dataset format.</p> <pre><code>import json\nimport time\nfrom typing import Generator\n\nimport fiftyone as fo\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.sql import (\n    StatementResponse, StatementState, StatementParameterListItem\n)\nfrom fiftyone import operators as foo\nfrom fiftyone.operators import types\nfrom fiftyone.operators.data_lens import (\n    DataLensOperator, DataLensSearchRequest, DataLensSearchResponse\n)\n\nclass DatabricksConnector(DataLensOperator):\n    \"\"\"Data Lens operator which retrieves samples from Databricks.\"\"\"\n\n    @property\n    def config(self) -&gt; foo.OperatorConfig:\n        return foo.OperatorConfig(\n            name=\"databricks_connector\",\n            label=\"Databricks Connector\",\n            unlisted=True,\n            execute_as_generator=True,\n        )\n\n    def resolve_input(self, ctx: foo.ExecutionContext):\n        inputs = types.Object()\n\n        # Times of day\n        inputs.bool(\n            \"daytime\",\n            label=\"Day\",\n            description=\"Show daytime samples\",\n            default=True,\n        )\n        inputs.bool(\n            \"night\",\n            label=\"Night\",\n            description=\"Show night samples\",\n            default=True,\n        )\n        inputs.bool(\n            \"dawn/dusk\",\n            label=\"Dawn / Dusk\",\n            description=\"Show dawn/dusk samples\",\n            default=True,\n        )\n\n        # Weather\n        inputs.bool(\n            \"clear\",\n            label=\"Clear weather\",\n            description=\"Show samples with clear weather\",\n            default=True,\n        )\n        inputs.bool(\n            \"rainy\",\n            label=\"Rainy weather\",\n            description=\"Show samples with rainy weather\",\n            default=True,\n        )\n\n        # Detection label\n        inputs.str(\n            \"detection_label\",\n            label=\"Detection label\",\n            description=\"Filter samples by detection label\",\n        )\n\n        return types.Property(inputs)\n\n    def handle_lens_search_request(\n            self,\n            request: DataLensSearchRequest,\n            ctx: foo.ExecutionContext\n    ) -&gt; Generator[DataLensSearchResponse, None, None]:\n        handler = DatabricksHandler()\n        for response in handler.handle_request(request, ctx):\n            yield response\n\nclass DatabricksHandler:\n    \"\"\"Handler for interacting with Databricks tables.\"\"\"\n\n    def __init__(self):\n        self.client = None\n        self.warehouse_id = None\n\n    def handle_request(\n            self,\n            request: DataLensSearchRequest,\n            ctx: foo.ExecutionContext\n    ) -&gt; Generator[DataLensSearchResponse, None, None]:\n\n        # Initialize the client\n        self._init_client(ctx)\n\n        # Iterate over samples\n        sample_buffer = []\n        for sample in self._iter_data(request):\n            sample_buffer.append(self._transform_sample(sample))\n\n            # Yield batches of data as they are available\n            if len(sample_buffer) == request.batch_size:\n                yield DataLensSearchResponse(\n                    result_count=len(sample_buffer),\n                    query_result=sample_buffer,\n                )\n\n                sample_buffer = []\n\n        # Yield final batch if it's non-empty\n        if len(sample_buffer) &gt; 0:\n            yield DataLensSearchResponse(\n                result_count=len(sample_buffer),\n                query_result=sample_buffer,\n            )\n\n        # No more samples.\n\n    def _init_client(self, ctx: foo.ExecutionContext):\n        \"\"\"Prepare the Databricks client for query execution.\"\"\"\n\n        # Initialize the Databricks client using credentials provided via `ctx.secret`\n        self.client = WorkspaceClient(\n            host=ctx.secret(\"DATABRICKS_HOST\"),\n            account_id=ctx.secret(\"DATABRICKS_ACCOUNT_ID\"),\n            client_id=ctx.secret(\"DATABRICKS_CLIENT_ID\"),\n            client_secret=ctx.secret(\"DATABRICKS_CLIENT_SECRET\"),\n        )\n\n        # Start a SQL warehouse instance to execute our query\n        self.warehouse_id = self._start_warehouse()\n        if self.warehouse_id is None:\n            raise ValueError(\"No available warehouse\")\n\n    def _start_warehouse(self) -&gt; str:\n        \"\"\"Start a SQL warehouse and return its ID.\"\"\"\n\n        last_warehouse_id = None\n\n        # If any warehouses are already running, use the first available\n        for warehouse in self.client.warehouses.list():\n            last_warehouse_id = warehouse.id\n            if warehouse.health.status is not None:\n                return warehouse.id\n\n        # Otherwise, manually start the last available warehouse\n        if last_warehouse_id is not None:\n            self.client.warehouses.start(last_warehouse_id)\n\n        return last_warehouse_id\n\n    def _iter_data(self, request: DataLensSearchRequest) -&gt; Generator[dict, None, None]:\n        \"\"\"Iterate over sample data retrieved from Databricks.\"\"\"\n\n        # Filter samples based on selected times of day\n        enabled_times_of_day = tuple([\\\n            f'\"{tod}\"'\\\n            for tod in [\"daytime\", \"night\", \"dawn/dusk\"]\\\n            if request.search_params.get(tod, False)\\\n        ])\n\n        # Filter samples based on selected weather\n        enabled_weather = tuple([\\\n            f'\"{weather}\"'\\\n            for weather in [\"clear\", \"rainy\"]\\\n            if request.search_params.get(weather, False)\\\n        ])\n\n        # Build Databricks query\n        query = f\"\"\"\n            SELECT * FROM datasets.bdd.det_train samples\n            WHERE\n                samples.attributes.timeofday IN ({\", \".join(enabled_times_of_day)})\n            AND samples.attributes.weather IN ({\", \".join(enabled_weather)})\n        \"\"\"\n\n        query_parameters = []\n\n        # Filter samples based on detection label if provided\n        if request.search_params.get(\"detection_label\") not in (None, \"\"):\n            query += f\"\"\"\n            AND samples.name IN (\n                SELECT DISTINCT(labels.name)\n                FROM datasets.bdd.det_train_labels labels\n                WHERE labels.category = :detection_label\n            )\n            \"\"\"\n\n            query_parameters.append(\n                StatementParameterListItem(\n                    \"detection_label\",\n                    value=request.search_params.get(\"detection_label\")\n                )\n            )\n\n        # Execute query asynchronously;\n        #   we'll get a statement_id that we can use to poll for results\n        statement_response = self.client.statement_execution.execute_statement(\n            query,\n            self.warehouse_id,\n            catalog=\"datasets\",\n            parameters=query_parameters,\n            row_limit=request.max_results,\n            wait_timeout=\"0s\"\n        )\n\n        # Poll on our statement until it's no longer in an active state\n        while (\n                statement_response.status.state in\n                (StatementState.PENDING, StatementState.RUNNING)\n        ):\n            statement_response = self.client.statement_execution.get_statement(\n                statement_response.statement_id\n            )\n\n            time.sleep(2.5)\n\n        # Process the first batch of data\n        json_result = self._response_to_dicts(statement_response)\n\n        for element in json_result:\n            yield element\n\n        # Databricks paginates samples using \"chunks\"; iterate over chunks until next is None\n        while statement_response.result.next_chunk_index is not None:\n            statement_response = self.client.statement_execution.get_statement_result_chunk_n(\n                statement_response.statement_id,\n                statement_response.result.next_chunk_index\n            )\n\n            # Process the next batch of data\n            json_result = self._response_to_dicts(statement_response)\n\n            for element in json_result:\n                yield element\n\n    def _transform_sample(self, sample: dict) -&gt; dict:\n        \"\"\"Transform a dict of raw Databricks data into a FiftyOne Sample dict.\"\"\"\n\n        return fo.Sample(\n            filepath=f\"cloud://bucket/{sample.get('name')}\",\n            detections=self._build_detections(sample),\n        ).to_dict()\n\n    def _build_detections(self, sample: dict) -&gt; fo.Detections:\n        # Images are a known, static size\n        image_width = 1280\n        image_height = 720\n\n        # Extract detection labels and pre-process bounding boxes\n        labels_list = json.loads(sample[\"labels\"])\n        for label_data in labels_list:\n            if \"box2d\" in label_data:\n                label_data[\"box2d\"] = {\n                    k: float(v)\n                    for k, v in label_data[\"box2d\"].items()\n                }\n\n        return fo.Detections(\n            detections=[\\\n                fo.Detection(\\\n                    label=label_data[\"category\"],\\\n                    # FiftyOne expects bounding boxes to be of the form\\\n                    #   [x, y, width, height]\\\n                    # where values are normalized to the image's dimensions.\\\n                    #\\\n                    # Our source data is of the form\\\n                    #   {x1, y1, x2, y2}\\\n                    # where values are in absolute pixels.\\\n                    bounding_box=[\\\n                        label_data[\"box2d\"][\"x1\"] / image_width,\\\n                        label_data[\"box2d\"][\"y1\"] / image_height,\\\n                        (label_data[\"box2d\"][\"x2\"] - label_data[\"box2d\"][\"x1\"]) / image_width,\\\n                        (label_data[\"box2d\"][\"y2\"] - label_data[\"box2d\"][\"y1\"]) / image_height\\\n                    ]\\\n                )\\\n                for label_data in labels_list\\\n                if \"box2d\" in label_data\\\n            ]\n        )\n\n    def _response_to_dicts(self, response: StatementResponse) -&gt; list[dict]:\n        # Check for response errors before processing\n        self._check_for_error(response)\n\n        # Extract column names from response\n        columns = response.manifest.schema.columns\n        column_names = [column.name for column in columns]\n\n        # Extract data from response\n        data = response.result.data_array or []\n\n        # Each element in data is a list of raw column values.\n        # Remap ([col1, col2, ..., colN], [val1, val2, ..., valN]) tuples\n        #   to {col1: val1, col2: val2, ..., colN: valN} dicts\n        return [\\\n            {\\\n                key: value\\\n                for key, value in zip(column_names, datum)\\\n            }\\\n            for datum in data\\\n        ]\n\n    def _check_for_error(self, response: StatementResponse):\n        if response is None:\n            raise ValueError(\"received null response from databricks\")\n\n        if response.status is not None:\n            if response.status.error is not None:\n                raise ValueError(\"databricks error: ({0}) {1}\".format(\n                    response.status.error.error_code,\n                    response.status.error.message\n                ))\n\n            if response.status.state in (\n                    StatementState.CLOSED,\n                    StatementState.FAILED,\n                    StatementState.CANCELED,\n            ):\n                raise ValueError(\n                    f\"databricks error: response state = {response.status.state}\"\n                )\n</code></pre>"},{"location":"teams/data_lens/#google-bigquery","title":"Google BigQuery \u00b6","text":"<p>Below is an example of a Data Lens connector for BigQuery:</p> <pre><code>import fiftyone.operators as foo\nimport fiftyone.operators.types as types\nfrom fiftyone.operators.data_lens import (\n    DataLensOperator,\n    DataLensSearchRequest,\n    DataLensSearchResponse\n)\n\nfrom google.cloud import bigquery\n\nclass BigQueryConnector(DataLensOperator):\n    @property\n    def config(self):\n        return foo.OperatorConfig(\n            name=\"bq_connector\",\n            label=\"BigQuery Connector\",\n            unlisted=True,\n            execute_as_generator=True,\n        )\n\n    def resolve_input(self, ctx):\n        inputs = types.Object()\n\n        # We'll enable searching on detection labels\n        inputs.str(\n            \"detection_label\",\n            label=\"Detection label\",\n            description=\"Enter a label to find samples with a matching detection\",\n            required=True,\n        )\n\n        return types.Property(inputs)\n\n    def handle_lens_search_request(\n        self,\n        request: DataLensSearchRequest,\n        ctx: foo.ExecutionContext,\n    ) -&gt; Generator[DataLensSearchResponse, None, None]:\n        handler = BigQueryHandler()\n        for batch in handler.handle_request(request, ctx):\n            yield batch\n\nclass BigQueryHandler:\n    def handle_request(\n        self,\n        request: DataLensSearchRequest,\n        ctx: foo.ExecutionContext,\n    ) -&gt; Generator[DataLensSearchResponse, None, None]:\n        # Create our client.\n        # If needed, we can use secrets from `ctx.secrets` to provide credentials\n        #  or other secure configuration required to interact with our data source.\n        client = bigquery.Client()\n\n        try:\n            # Retrieve our Data Lens search parameters\n            detection_label = request.search_params.get(\"detection_label\", \"\")\n\n            # Construct our query\n            query = \"\"\"\n                    SELECT\n                        media_path, tags, detections, keypoints\n                    FROM `my_dataset.samples_json`,\n                    UNNEST(JSON_QUERY_ARRAY(detections)) as detection\n                    WHERE JSON_VALUE(detection.label) = @detection_label\n                \"\"\"\n\n            # Submit our query to BigQuery\n            job_config = bigquery.QueryJobConfig(\n                query_parameters=[\\\n                    bigquery.ScalarQueryParameter(\\\n                        \"detection_label\",\\\n                        \"STRING\",\\\n                        detection_label\\\n                    )\\\n                ]\n            )\n            query_job = client.query(query, job_config=job_config)\n\n            # Wait for results\n            rows = query_job.result(\n                # BigQuery will handle pagination automatically, but\n                # we can optimize its behavior by synchronizing with\n                # the parameters provided by Data Lens\n                page_size=request.batch_size,\n                max_results=request.max_results\n            )\n\n            samples = []\n\n            # Iterate over data from BigQuery\n            for row in rows:\n\n                # Transform sample data from BigQuery format to FiftyOne\n                samples.append(self.convert_to_sample(row))\n\n                # Yield next batch when we have enough samples\n                if len(samples) == request.batch_size:\n                    yield DataLensSearchResponse(\n                        result_count=len(samples),\n                        query_result=samples\n                    )\n\n                    # Reset our batch\n                    samples = []\n\n            # We've run out of rows, but might have a partial batch\n            if len(samples) &gt; 0:\n                yield DataLensSearchResponse(\n                    result_count=len(samples),\n                    query_result=samples\n                )\n\n            # Our generator is now exhausted\n\n        finally:\n            # Clean up our client on exit\n            client.close()\n</code></pre> <p>Let\u2019s take a look at a few parts in detail.</p> <pre><code># Create our client\nclient = bigquery.Client()\n</code></pre> <p>In practice, you\u2019ll likely need to use secrets to securely provide credentials to connect to your data source.</p> <pre><code># Retrieve our Data Lens search parameters\ndetection_label = request.search_params.get(\"detection_label\", \"\")\n\n# Construct our query\nquery = \"\"\"\n        SELECT\n            media_path, tags, detections, keypoints\n        FROM `my_dataset.samples_json`,\n        UNNEST(JSON_QUERY_ARRAY(detections)) as detection\n        WHERE JSON_VALUE(detection.label) = @detection_label\n    \"\"\"\n</code></pre> <p>Here we\u2019re using our user-provided input parameters to tailor our query to only the samples of interest. This logic can be as simple or complex as needed to match our use case.</p> <pre><code># Wait for results\nrows = query_job.result(\n    # BigQuery will handle pagination automatically, but\n    # we can optimize its behavior by synchronizing with\n    # the parameters provided by Data Lens\n    page_size=request.batch_size,\n    max_results=request.max_results\n)\n</code></pre> <p>Here we\u2019re using <code>request.batch_size</code> and <code>request.max_results</code> to help BigQuery align its performance with our use case. In cases where <code>request.max_results</code> is smaller than our universe of samples (such as during preview or small imports), we can prevent fetching more data than we need, improving both query performance and operational cost.</p> <pre><code># Transform sample data from BigQuery format to FiftyOne\nsamples.append(self.convert_to_sample(row))\n</code></pre> <p>Here we are converting our sample data from its storage format to a FiftyOne <code>Sample</code>. This is where we\u2019ll add features to our samples, such as labels.</p> <p>As we can see from this example, we can make our Data Lens search experience as powerful as it needs to be. We can leverage internal libraries and services, hosted solutions, and tooling that meets the specific needs of our data. We can expose flexible but precise controls to users to allow them to find exactly the data that\u2019s needed.</p>"},{"location":"teams/data_lens/#snippet-dynamic-user-inputs","title":"Snippet: Dynamic user inputs \u00b6","text":"<p>As the volume and complexity of your data grows, you may want to expose many options to Data Lens users, but doing so all at once can be overwhelming for the user. In this example, we\u2019ll look at how we can use dynamic operators to conditionally expose configuration options to Data Lens users.</p> <pre><code>class MyOperator(DataLensOperator):\n    @property\n    def config(self) -&gt; foo.OperatorConfig:\n        return OperatorConfig(\n            name=\"my_operator\",\n            label=\"My operator\",\n            dynamic=True,\n        )\n</code></pre> <p>By setting <code>dynamic=True</code> in our operator config, our operator will be able to customize the options shown to a Data Lens user based on the current state. Let\u2019s use this to optionally show an \u201cadvanced options\u201d section in our query parameters.</p> <pre><code>def resolve_input(self, ctx: foo.ExecutionContext):\n    inputs = types.Object()\n\n    inputs.str(\"some_param\", label=\"Parameter value\")\n    inputs.str(\"other_param\", label=\"Other value\")\n\n    inputs.bool(\"show_advanced\", label=\"Show advanced options\", default=False)\n\n    # Since this is a dynamic operator,\n    #   we can use `ctx.params` to conditionally show options\n    if ctx.params.get(\"show_advanced\") is True:\n        # In this example, we'll optionally show configuration which allows a user\n        #   to remap selected sample fields to another name.\n        # This could be used to enable users to import samples into datasets with\n        #   varying schemas.\n        remappable_fields = (\"field_a\", \"field_b\")\n        for field_name in remappable_fields:\n            inputs.str(f\"{field_name}_remap\", label=f\"Remap {field_name} to another name\")\n\n    return types.Property(inputs)\n</code></pre> <p>Our operator\u2019s <code>resolve_input</code> method will be called each time <code>ctx.params</code> changes, which allows us to create an experience that is tailored to the Data Lens user\u2019s behavior. In this example, we\u2019re optionally displaying advanced configuration that allows a user to remap sample fields. Applying this remapping might look something like this.</p> <pre><code>def _remap_sample_fields(self, sample: dict, request: DataLensSearchRequest):\n    remappable_fields = (\"field_a\", \"field_b\")\n    for field_name in remappable_fields:\n        remapped_field_name = request.search_params.get(f\"{field_name}_remap\")\n        if remapped_field_name not in (None, \"\"):\n            sample[remapped_field_name] = sample[field_name]\n            del sample[field_name]\n</code></pre> <p>Of course, dynamic operators can be used for much more than this. Search experiences can be broadened or narrowed to allow for both breadth and depth within your connected data sources.</p> <p>As an example, suppose a user is searching for detections of \u201ctraffic light\u201d in an autonomous driving dataset. A dynamic operator can be used to expose additional search options that are specific to traffic lights, such as being able to select samples with only red, yellow, or green lights. In this way, dynamic operators provide a simple mechanism for developing intuitive and context-sensitive search experiences for Data Lens users.</p>"},{"location":"teams/data_quality/","title":"Data Quality \u00b6","text":"<p>Available in FiftyOne Teams v2.2+</p> <p>The Data Quality panel is a builtin feature of the FiftyOne Teams App that automatically scans your dataset for common quality issues and helps you explore and take action to resolve them.</p>"},{"location":"teams/data_quality/#data-quality-panel","title":"Data Quality panel \u00b6","text":"<p>You can open the Data Quality panel by clicking the \u201c+\u201d icon next to the Samples tab.</p> <p>The panel\u2019s home page shows a list of the available issue types and their current analysis/review status:</p> <ul> <li> <p>Brightness: scans for images that are unusually bright or dim</p> </li> <li> <p>Blurriness: scans for images that are abnormally blurry or sharp</p> </li> <li> <p>Aspect Ratio: scans for images that have extreme aspect ratios</p> </li> <li> <p>Entropy: scans for images that have unusually small or large entropy</p> </li> <li> <p>Near Duplicates: leverages embeddings to scan for near-duplicate samples in your dataset</p> </li> <li> <p>Exact Duplicates: uses filehashes to scan your dataset for duplicate data with either the same or different filenames</p> </li> </ul> <p>Click on the right arrow of an issue type\u2019s card to open its expanded view.</p> <p></p>"},{"location":"teams/data_quality/#scanning-for-issues","title":"Scanning for issues \u00b6","text":"<p>If you have not yet scanned a dataset for a given issue type, you\u2019ll see a landing page like this:</p> <p></p> <p>Clicking the \u201cScan Dataset\u201d button presents two choices for execution:</p> <p></p> <p>Note</p> <p>The \u201cExecute\u201d option is only for testing. In this mode, computation is performend synchronously and will timeout if it does not complete within a few minutes.</p> <p>Choose \u201cSchedule\u201d for all production data, which schedules the scan for delegated execution on your compute cluster.</p> <p>While a scan is in-progress, you\u2019ll see a status page like this:</p> <p></p> <p>Click the link in the notification to navigate to the dataset\u2019s Runs page where you can monitor the status of the task.</p>"},{"location":"teams/data_quality/#analyzing-scan-results","title":"Analyzing scan results \u00b6","text":"<p>Once an issue scan is complete, its card will update to display an interactive histogram that you can use to analyze the findings:</p> <p></p> <p>Note</p> <p>When analyzing issue scan results, we recommend using the split screen icon to the right of the Samples panel tab to arrange the Samples panel and Data Quality panel side-by-side, as shown above.</p> <p>Each issue type\u2019s results are stored under a dedicated field of the dataset, from which the displayed histograms are generated:</p> <ul> <li> <p>Brightness: the brightness of each image is stored in a <code>brightness</code> field of the sample</p> </li> <li> <p>Blurriness: the blurriness of each image is stored in a <code>blurriness</code> field of the sample</p> </li> <li> <p>Aspect Ratio: the aspect ratio of each image is stored in an <code>aspect_ratio</code> field of the sample</p> </li> <li> <p>Entropy: the entropy of each image is stored in an <code>entropy</code> field of the sample</p> </li> <li> <p>Near Duplicates: the nearest neighbor distance of each sample is stored in a <code>nearest_neighbor</code> field of the sample</p> </li> <li> <p>Exact Duplicates: the filehash of each image is stored in a <code>filehash</code> field of the sample</p> </li> </ul> <p>Each issue type comes with a default threshold range that highlights potential issues in your dataset. If issues are identified, the number of potential issues will be displayed in the top-left corner of the Data Quality panel and the Samples panel will automatically update to show the corresponding samples in the grid.</p> <p>You can also use the threshold slider to manually explore different threshold ranges. When you release the slider, the Samples panel will automatically update to show the corresponding samples:</p> <p></p> <p>If you find a better threshold for a dataset, you can save it via the \u201cSave Threshold\u201d option under the settings menu. You can use \u201cReset Threshold\u201d to the revert to the default threshold at any time.</p> <p>Once you\u2019ve reviewed the potential issues in the grid, you can use the \u201cAdd Tags\u201d button to take action on them. Clicking the button will display a modal like this:</p> <p></p> <p>Note</p> <p>If you\u2019ve selected samples in the grid, only those samples will be tagged. Otherwise, tags will be added to all samples in your current view (i.e., all potential issues).</p> <p>You can use the \u201csample tags\u201d filter in the App\u2019s sidebar to retrieve, review, and act on all samples that you\u2019ve previously tagged.</p> <p>The review status indicator in the top-right corner of the panel indicates whether an issue type is currently \u201cIn Review\u201d or \u201cReviewed\u201d. You can click on it at any time to toggle the review status.</p> <p>If you navigate away from an issue type that is currently \u201cIn Review\u201d, you\u2019ll be prompted to indicate whether or not you\u2019d like to mark the issue type as \u201cReviewed\u201d:</p> <p></p>"},{"location":"teams/data_quality/#updating-a-scan","title":"Updating a scan \u00b6","text":"<p>The Data Quality panel gracefully adapts to changes in your datasets after scans have been performed.</p> <p>If you delete samples from a dataset, the histograms of any existing scans will automatically be updated to reflect the new distribution.</p> <p>If you add new samples to a dataset or clear some existing field values associated with a scan (e.g., <code>brightness</code> field values for brightness scans), the panel will automatically detect the presence of unscanned samples and will display contextual information from the home page and analysis page:</p> <p></p> <p>To update an existing scan, open the issue type and click the \u201cScan New Samples\u201d button in the bottom-right corner of the analysis page. This will open a modal that provides additional context and prompts you to initiate the new samples scan:</p> <p></p>"},{"location":"teams/data_quality/#deleting-a-scan","title":"Deleting a scan \u00b6","text":"<p>You can delete an issue scan by simply deleting the corresponding field from the dataset (e.g., <code>brightness</code> for brightness scans).</p> <p>Note</p> <p>Did you know? You can delete sample fields from the App using the <code>delete_sample_field</code> operator available via the Operator browser.</p>"},{"location":"teams/dataset_versioning/","title":"Dataset Versioning \u00b6","text":"<p>FiftyOne Teams provides native support for versioning your datasets!</p> <p>Dataset Versioning allows you to capture the state of your dataset in time so that it can be referenced in the future. This enables workflows like recalling particular important events in the dataset\u2019s lifecycle (model trained, annotation added, etc) as well as helping to prevent accidental data loss.</p> <p>FiftyOne Teams: Dataset Versioning - YouTube</p> <p>Voxel51</p> <p>1.43K subscribers</p> <p>FiftyOne Teams: Dataset Versioning</p> <p>Voxel51</p> <p>Search</p> <p>Info</p> <p>Shopping</p> <p>Tap to unmute</p> <p>If playback doesn't begin shortly, try restarting your device.</p> <p>You're signed out</p> <p>Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.</p> <p>CancelConfirm</p> <p>Share</p> <p>Include playlist</p> <p>An error occurred while retrieving sharing information. Please try again later.</p> <p>Watch later</p> <p>Share</p> <p>Copy link</p> <p>Watch on</p> <p>0:00</p> <p>/ \u2022Live</p> <p>\u2022</p> <p>Watch on YouTube</p>"},{"location":"teams/dataset_versioning/#overview","title":"Overview \u00b6","text":"<p>Dataset Versioning in FiftyOne Teams is implemented as a linear sequence of read-only Snapshots. In other words, creating a new Snapshot creates a permanent record of the dataset\u2019s contents that can be loaded and viewed at any time in the future, but not directly edited. Conversely, the current version of a dataset is called its HEAD (think git). If you have not explicitly loaded a Snapshot, you are viewing its HEAD, and you can make additions, updates, and deletions to the dataset\u2019s contents as you normally would (provided you have sufficient permissions).</p> <p>Snapshots record all aspects of your data stored within FiftyOne, including dataset-level information, schema, samples, frames, brain runs, and evaluations. However, Snapshots exclude any information stored in external services, such as media stored in cloud buckets or embeddings stored in an external vector database, which are assumed to be immutable. If you need to update the image for a sample in a dataset, for example, update the sample\u2019s filepath\u2014-which is tracked by snapshots\u2014-rather than updating the media in cloud storage in-place\u2014-which would not be tracked by snapshots. This design allows dataset snapshots to be as lightweight and versatile as possible.</p> <p>Dataset Versioning has been built with an extensible architecture so that different versioning backends can be swapped in. Each backend may have different tradeoffs in terms of performance, storage, and deployment needs, so users should be able to choose the best fit for their needs. In addition, many users may already have a versioning solution external to FiftyOne Teams, and the goal is to support integration around those use cases as well.</p> <p>Currently, only the internal duplication backend is available, but further improvements and implementing additional backend choices are on the roadmap.</p> <p>Warning</p> <p>Dataset Versioning is not a replacement for database backups. We strongly encourage the use of regular data backups and good storage maintenance processes.</p>"},{"location":"teams/dataset_versioning/#snapshots","title":"Snapshots \u00b6","text":"<p>Dataset Versioning in FiftyOne Teams is implemented as a linear history of Snapshots. A Snapshot captures the state of a dataset at a particular point in time as an immutable object. Compare this concept to creating commits and tags in a single branch of a version control system such as git or svn; a Snapshot is a commit and tag (including readable name, description, creator) all in one.</p> <p>The current working version of the dataset (called the HEAD) can be edited by anyone with appropriate permissions, as normal. Since Snapshots include a commit-like operation, they can only be created on the dataset HEAD.</p>"},{"location":"teams/dataset_versioning/#snapshot-states","title":"Snapshot states \u00b6","text":"<p>Snapshots can be in a few different states of existence depending on deployment choices and user actions.</p> <p>Materialized Snapshot \u00b6</p> <p>A Snapshot whose state and contents are entirely materialized in the MongoDB database. The Snapshot is \u201cready to go\u201d and be loaded instantly for analysis and visualization.</p> <p>Archived Snapshot \u00b6</p> <p>A materialized Snapshot that has been archived to cold storage to free up working space in the MongoDB instance. The Snapshot cannot be loaded by users until it is re-materialized into MongoDB. Since it is stored in its materialized form already though, an archived Snapshot can be re-materialized easily, at merely the cost of network transfer and MongoDB write latencies. See here for more.</p> <p>Virtual Snapshot \u00b6</p> <p>A Snapshot whose state and contents are stored by the pluggable backend versioning implementation in whatever way it chooses. In order to be loaded by FiftyOne Teams users, the Snapshot must be materialized into its workable form in MongoDB. This is done through a combination of the overarching versioning infrastructure and the specific versioning backend.</p> <p>For a given Snapshot, the virtual form always exists. It may also be materialized, archived, or both (in the case that an archived Snapshot has been re-materialized but kept in cold storage also).</p> <p>Note</p> <p>With the internal duplication backend there is no distinction between materialized and virtual Snapshots since by definition the implementation uses materialized Snapshots as its method of storage.</p>"},{"location":"teams/dataset_versioning/#snapshot-archival","title":"Snapshot archival \u00b6","text":"<p>Snapshot your datasets easier knowing your database won\u2019t be overrun!</p> <p>If your snapshots are important for historical significance but aren\u2019t used very often, then you can consider archiving snapshots. This is especially helpful with the internal duplication backend where creating snapshots causes database storage to grow quickly!</p> <p>When a snapshot is archived, all of its contents are stored in an archive in the configured cold storage location: either a mounted filesystem or cloud storage folder (using your deployment\u2019s cloud credentials).</p> <p>Note</p> <p>Snapshots must be unarchived in order to browse them in the UI or load them with the SDK.</p>"},{"location":"teams/dataset_versioning/#automatic-archival","title":"Automatic archival \u00b6","text":"<p>If snapshot archival is enabled, snapshots will automatically be archived to make room for newer snapshots as necessary. This can be triggered when a snapshot is created or unarchived, which would then put the number of snapshots in the database above one of the configured limits.</p> <p>If the total materialized snapshots limit is exceeded, then the snapshot that was least-recently loaded will be automatically archived.</p> <p>If the materialized snapshots per-dataset limit is exceeded, then the snapshot within the dataset that was least-recently loaded will be archived.</p> <p>Note</p> <p>Some snapshots will not be chosen for automatic archival, even if they would otherwise qualify based on their last load time: the most recent snapshot for each dataset, and those that have been loaded within the configured age requirement.</p> <p>If no snapshot can be automatically archived then the triggering event will report an error and fail. This can be fixed by deleting snapshots, manually archiving snapshots, or changing deployment configuration values.</p>"},{"location":"teams/dataset_versioning/#manual-archival","title":"Manual archival \u00b6","text":"<p>Users with Can Manage permissions to a dataset can manually archive snapshots via the UI or management SDK.</p>"},{"location":"teams/dataset_versioning/#unarchival","title":"Unarchival \u00b6","text":"<p>While a snapshot is archived, you cannot browse it in the UI or load with the SDK.</p> <p>To enable browsing or loading again, the snapshot can be unarchived via the UI or management SDK.</p>"},{"location":"teams/dataset_versioning/#usage-notes","title":"Usage notes \u00b6","text":"<p>Note</p> <p>If the most recent snapshot is archived then the latest changes in HEAD cannot be calculated and may be reported as unknown.</p> <p>Note</p> <p>If a snapshot is deleted, the change summary for the following snapshot must be recomputed against the previous snapshot. However, if either of these snapshots are currently archived then the change summary cannot be recomputed and may be reported as unknown.</p>"},{"location":"teams/dataset_versioning/#snapshot-permissions","title":"Snapshot permissions \u00b6","text":"<p>The table below shows the dataset permissions required to perform different Snapshot-related operations:</p> Snapshot Operation User Permissions on Dataset Can View Browse Snapshot in App \u2705 Load Snapshot in SDK \u2705 Create Snapshot Delete Snapshot Archive Snapshot Unarchive Snapshot \u2705 Revert dataset to Snapshot Edit Snapshot\u2019s contents \u274c"},{"location":"teams/dataset_versioning/#using-snapshots","title":"Using snapshots \u00b6","text":"<p>In contrast to a dataset\u2019s HEAD, Snapshots are read-only. When viewing in the App, the UI is similar to interacting with a HEAD dataset, but users will not be able to make any edits to the objects. Similarly, when using the FiftyOne SDK, users will not be able to perform any operation that would trigger a modification to the stored dataset.</p>"},{"location":"teams/dataset_versioning/#list-snapshots","title":"List snapshots \u00b6","text":""},{"location":"teams/dataset_versioning/#teams-ui","title":"Teams UI \u00b6","text":"<p>To access the Snapshot history and management page, click the \u201cHistory tab\u201d on a dataset\u2019s main page.</p> <p></p> <p>On this page you can see a listing of the Snapshot history for the dataset. Each row contains information about a single Snapshot.</p> <p></p>"},{"location":"teams/dataset_versioning/#sdk","title":"SDK \u00b6","text":"<p>You can also list Snapshot names for a dataset using the <code>list_snapshots()</code> method from the Management SDK.</p> <pre><code>import fiftyone.management as fom\n\ndataset_name = \"quickstart\"\nfom.list_snapshots(dataset_name)\n</code></pre> <p>Then you can get more detailed information on a single Snapshot using the <code>get_snapshot_info()</code> method.</p> <pre><code>import fiftyone.management as fom\n\ndataset = \"quickstart\"\nsnapshot_name = \"v0.1\"\n\nfom.get_snapshot_info(dataset, snapshot_name)\n</code></pre>"},{"location":"teams/dataset_versioning/#loading-snapshots","title":"Loading snapshots \u00b6","text":"<p>Any user with Can View permissions to a dataset can view and load its snapshots via the Teams UI or the SDK.</p>"},{"location":"teams/dataset_versioning/#teams-ui_1","title":"Teams UI \u00b6","text":"<p>From the dataset\u2019s History tab, click the \u201cBrowse\u201d button next to a Snapshot in the snapshot list to load the Snapshot in the UI.</p> <p></p> <p>This will open the Snapshot in the normal dataset samples UI with all your favorite FiftyOne visualization tools at your fingertips! However, all dataset-modification features such as tagging have been removed.</p> <p>We can also link directly to this Snapshot page by copying the URL from the address bar or from the \u201cShare Dataset\u201d page which opens from the \u201cShare\u201d button. For the above Snapshot, it would look like this:</p> <pre><code>https://&lt;your-teams-url&gt;/datasets/roadscene-vehicle-detection/samples?snapshot=new+snapshot\n</code></pre> <p>One other difference from the normal page is the Snapshot banner which gives information about the Snapshot being viewed, and other quick-click operations. Clicking the name line drops down a list of the Snapshots where the current one is highlighted. Clicking on a Snapshot in the dropdown will navigate to the browse page for that Snapshot.</p> <p></p> <p>On the right side of the banner, clicking the \u201cBack to the latest version\u201d button will take you back to the samples page for the dataset HEAD. You can also do this by clicking the \u201cSamples\u201d tab. There is also a convenient dropdown from the 3-dot (kebab) menu which gives various management functions for the current Snapshot.</p> <p></p>"},{"location":"teams/dataset_versioning/#sdk_1","title":"SDK \u00b6","text":"<p>Snapshots can also be loaded via the FiftyOne SDK <code>load_dataset()</code> method. The following snippet will load an existing Snapshot of a dataset. It can then be interacted with as if it is a normal dataset, except for any operations that would cause modifications.</p> <pre><code>import fiftyone as fo\n\ndataset_name = \"quickstart\"\nexisting_snapshot_name = \"v1\"\n\nsnapshot = fo.load_dataset(dataset_name, snapshot=existing_snapshot_name)\nprint(snapshot)\n</code></pre>"},{"location":"teams/dataset_versioning/#snapshot-management","title":"Snapshot management \u00b6","text":"<p>The following sections describe how to create and use snapshots.</p>"},{"location":"teams/dataset_versioning/#creating-a-snapshot","title":"Creating a snapshot \u00b6","text":"<p>Users with Can Manage permissions to a dataset can create Snapshots through the Teams UI or the Management SDK.</p> <p>Note</p> <p>Snapshots can only be created from the HEAD of the dataset.</p>"},{"location":"teams/dataset_versioning/#teams-ui_2","title":"Teams UI \u00b6","text":"<p>At the top of the History tab for a dataset is the Create snapshot panel. This panel shows the number of changes that have happened between the last Snapshot and the current state of the dataset.</p> <p>Note</p> <p>The latest changes summary is not continuously updated; click the \u201cRefresh\u201d button to recompute these values.</p> <p></p> <p>To create a Snapshot, provide a unique name and an optional description, then click the \u201cSave new snapshot\u201d button.</p> <p>Note</p> <p>Depending on the versioning backend used, deployment options chosen, and the size of the dataset, this may take some time.</p> <p></p> <p>After creation, the new Snapshot will show up in the list!</p> <p></p>"},{"location":"teams/dataset_versioning/#sdk_2","title":"SDK \u00b6","text":"<p>You can also create Snapshots via the Management SDK.</p> <p>To get the latest changes summary as in the Create snapshot panel, use <code>get_dataset_latest_changes_summary()</code>.</p> <pre><code>import fiftyone.management as fom\n\nfom.get_dataset_latest_changes_summary(dataset.name)\n</code></pre> <p>To recalculate the latest changes summary as in the Refresh button in that panel, use <code>calculate_dataset_latest_changes_summary()</code>.</p> <pre><code>import fiftyone.management as fom\n\nold = fom.calculate_dataset_latest_changes_summary(dataset.name)\nassert old == fom.get_dataset_latest_changes_summary(dataset.name)\n\ndataset.delete_samples(dataset.take(5))\n\n# Cached summary hasn't been updated\nassert old == fom.get_dataset_latest_changes_summary(dataset.name)\n\nnew = fom.calculate_dataset_latest_changes_summary(dataset.name)\nassert new.updated_at &gt; changes.updated_at\n</code></pre> <p>To create a new Snapshot, use the <code>create_snapshot()</code> method.</p> <pre><code>import fiftyone.management as fom\n\ndataset_name = \"quickstart\"\nsnapshot_name = \"v0.1\"\ndescription = \"Version 0.1 in which I have made many awesome changes!\"\nsnapshot = fom.create_snapshot(dataset_name, snapshot_name, description)\n</code></pre>"},{"location":"teams/dataset_versioning/#deleting-a-snapshot","title":"Deleting a snapshot \u00b6","text":"<p>Users with Can Manage permissions to a dataset can delete snapshots through the Teams UI or the Management SDK.</p> <p>If the Snapshot is the most recent, the latest (HEAD) sample changes summary is not automatically recalculated. See this section to see how to recalculate these now-stale values.</p> <p>If the Snapshot is not the most recent, the sample change summary for the following Snapshot will be automatically recalculated based on the previous Snapshot.</p> <p>Warning</p> <p>Deleting a Snapshot cannot be undone!</p>"},{"location":"teams/dataset_versioning/#teams-ui_3","title":"Teams UI \u00b6","text":"<p>To delete a Snapshot via the App, open the 3-dot (kebab) menu for the Snapshot. In the menu, click \u201cDelete snapshot\u201d. This will bring up a confirmation dialog to prevent accidental deletions.</p> <p></p>"},{"location":"teams/dataset_versioning/#sdk_3","title":"SDK \u00b6","text":"<p>You can also use the <code>delete_snapshot()</code> method in the Management SDK.</p> <pre><code>import fiftyone.management as fom\n\ndataset = \"quickstart\"\nsnapshot_name = \"v0.1\"\nfom.delete_snapshot(dataset, snapshot_name)\n</code></pre>"},{"location":"teams/dataset_versioning/#rollback-dataset-to-snapshot","title":"Rollback dataset to snapshot \u00b6","text":"<p>In case unwanted edits have been added to the dataset HEAD, FiftyOne provides the ability for dataset Managers to roll the dataset back (revert) to the state of a given Snapshot.</p> <p>Warning</p> <p>This is a destructive operation! Rolling back to a Snapshot discards all changes between the selected Snapshot and the current working version of the dataset, including all newer Snapshots.</p>"},{"location":"teams/dataset_versioning/#teams-ui_4","title":"Teams UI \u00b6","text":"<p>To revert a dataset to a Snapshot\u2019s state, click the 3-dot (kebab) menu in the History tab for the Snapshot you want to rollback to and select \u201cRollback to this snapshot\u201d. This will bring up a confirmation dialog to prevent accidental deletions.</p> <p></p>"},{"location":"teams/dataset_versioning/#sdk_4","title":"SDK \u00b6","text":"<p>You can also use the <code>revert_dataset_to_snapshot()</code> method in the Management SDK.</p> <pre><code>import fiftyone.management as fom\n\ndataset = \"quickstart\"\nsnapshot_name = \"v0.1\"\ndescription = \"Initial dataset snapshot\"\nfom.create_snapshot(dataset, snapshot_name, description)\n\n# Oops we deleted everything!\ndataset.delete_samples(dataset.values(\"id\"))\n\n# Phew!\nfom.revert_dataset_to_snapshot(dataset.name, snapshot_name)\ndataset.reload()\n\nassert len(dataset) &gt; 0\n</code></pre>"},{"location":"teams/dataset_versioning/#archive-snapshot","title":"Archive snapshot \u00b6","text":"<p>Users with Can Manage permissions to a dataset can manually archive snapshots to the configured cold storage location via the UI or the Management SDK.</p> <p>Note</p> <p>Users cannot browse archived snapshots via the UI or load them via the SDK. The snapshot must first be unarchived.</p>"},{"location":"teams/dataset_versioning/#teams-ui_5","title":"Teams UI \u00b6","text":"<p>To manually archive a snapshot, click the 3-dot (kebab) menu in the History tab for a snapshot you want to archive and select \u201cArchive snapshot\u201d. This will begin the archival process and the browse button will be replaced with an \u201cArchiving\u201d spinner\u201d:</p> <p></p>"},{"location":"teams/dataset_versioning/#sdk_5","title":"SDK \u00b6","text":"<p>You can also use the <code>archive_snapshot()</code> method in the Management SDK:</p> <pre><code>import fiftyone as fo\nimport fiftyone.management as fom\n\nsnapshot_name = \"v0.1\"\n\n# We don't use this regularly, archive it!\nfom.archive_snapshot(dataset.name, snapshot_name)\n\nfo.load_dataset(dataset.name, snapshot_name) # throws error, can't load!\n</code></pre>"},{"location":"teams/dataset_versioning/#unarchive-snapshot","title":"Unarchive snapshot \u00b6","text":"<p>To make an archived snapshot browsable again, users with Can Manage permissions to the dataset can unarchive it via the UI or Management SDK.</p>"},{"location":"teams/dataset_versioning/#teams-ui_6","title":"Teams UI \u00b6","text":"<p>To unarchive a snapshot, click the \u201cUnarchive\u201d button in the History tab for a snapshot you want to unarchive. This will begin the unarchival process and the archive button will be replaced with an \u201cUnarchiving\u201d spinner:</p> <p></p>"},{"location":"teams/dataset_versioning/#sdk_6","title":"SDK \u00b6","text":"<p>You can also use the <code>unarchive_snapshot()</code> method in the Management SDK:</p> <pre><code>import fiftyone as fo\nimport fiftyone.management as fom\n\nsnapshot_name = \"v0.1\"\ndescription = \"Initial dataset snapshot\"\n\n# We don't use this regularly, archive it!\nfom.archive_snapshot(dataset.name, snapshot_name)\nfo.load_dataset(dataset.name, snapshot_name) # throws error, can't load!\n\n# Oops we need it now, unarchive it!\nfom.unarchive_snapshot(dataset.name, snapshot_name)\nfo.load_dataset(dataset.name, snapshot_name) # works now!\n</code></pre>"},{"location":"teams/dataset_versioning/#pluggable-backends","title":"Pluggable backends \u00b6","text":"<p>Dataset versioning was built with an extensible architecture to support different versioning backend implementations being built and swapped in to better suit the users\u2019 needs and technology preferences. In the future, this section will contain information and discussion about each of these available backends, including their strengths/limitations and configuration options.</p> <p>For the initial release in FiftyOne Teams v1.4.0, however, there is only one backend choice described below. Additional backends may be implemented in the future, but for now, releasing dataset versioning with the first iteration was prioritized so that users can begin to see value and provide feedback as soon as possible.</p>"},{"location":"teams/dataset_versioning/#internal-duplication-backend","title":"Internal duplication backend \u00b6","text":"<p>This backend is similar to cloning a dataset; Snapshots are stored in the same MongoDB database as the original dataset.</p> <p>Creating a Snapshot with this backend is similar to cloning a dataset in terms of performance and storage needs.</p> <p>Creating a Snapshot should take roughly the same amount of time as cloning the dataset, and so is proportional to the size of the dataset being versioned.</p> <p>At this time, Snapshots are stored in the same database as the original dataset.</p> <p>These requirements should be taken into consideration when using Snapshots and when determining values for the max number of Snapshots allowed.</p>"},{"location":"teams/dataset_versioning/#time-and-space","title":"Time and space \u00b6","text":"<p>Time</p> <p>The create Snapshot operation takes time proportional to cloning the dataset. This backend is the most performant when creating a Snapshot then immediately loading it for use; while other backends would have to store the virtual Snapshot and then materialize it, this one simply does one big intra-MongoDB clone.</p> <p>Additionally, change summary calculation can be slow.</p> <p>Note</p> <p>In v1.4.0, calculating number of samples modified in particular can cause slowdown with larger datasets. This value is not computed for datasets larger than 200 thousand samples.</p> <p>Space</p> <p>The amount of storage required scales with the number of Snapshots created, not the volume of changes. Since it is stored in the same database as normal datasets, creating too many Snapshots without the ability to archive them could fill up the database.</p>"},{"location":"teams/dataset_versioning/#strengths","title":"Strengths \u00b6","text":"\u2705 Simple \u2705 Uses existing MongoDB; no extra deployment components \u2705 Browsing/loading is fast because the Snapshots are always materialized \u2705 For a create-then-load workflow, it has the lowest overhead cost ofany backend since materialized and virtual forms are one and the same"},{"location":"teams/dataset_versioning/#limitations","title":"Limitations \u00b6","text":"\u274c Creating a Snapshot takes time proportional to clone dataset \u274c Calculating sample change summaries is less efficient \u274c Storage is highly duplicative"},{"location":"teams/dataset_versioning/#configuration","title":"Configuration \u00b6","text":"<p>There are no unique configuration options for this backend.</p>"},{"location":"teams/dataset_versioning/#usage-considerations","title":"Usage considerations \u00b6","text":""},{"location":"teams/dataset_versioning/#best-practices","title":"Best practices \u00b6","text":"<p>As this feature matures, we will have better recommendations for best practices. For now given the limited starting options in the initial iteration, we have the following advice:</p> <ul> <li> <p>Use snapshots on smaller datasets if possible.</p> </li> <li> <p>Since space is at a premium, limit creation of snapshots to marking milestone events which you want to revisit or restore later.</p> </li> <li> <p>Delete old snapshots you don\u2019t need anymore.</p> </li> <li> <p>Set the versioning configuration to the highest your deployment can comfortably support, to better enable user workflows without breaking the (MongoDB) bank.</p> </li> </ul>"},{"location":"teams/dataset_versioning/#configuration_1","title":"Configuration \u00b6","text":"<p>Since Snapshots impact the storage needs of FiftyOne Teams, some guard rails have been put in place to control the maximum amount of Snapshots that can be created. If a threshold has been exceeded while a user attempts to create a new Snapshot, they will receive an error informing them that it may be time to remove old Snapshots.</p> <p>The configurations allowed are described in the table below. Adjusting these defaults should be done in consideration with the needs of the team and the storage requirements necessary.</p> Config name Environment variable Default | Description Maximum total Snapshots <code>FIFTYONE_SNAPSHOTS_MAX_IN_DB</code> 100 Maximum Snapshots per-dataset <code>FIFTYONE_SNAPSHOTS_MAX_PER_DATASET</code> 20 Snapshot Archive Path <code>FIFTYONE_SNAPSHOTS_ARCHIVE_PATH</code> <code>None</code> Automatic Archival Min Age <code>FIFTYONE_SNAPSHOTS_MIN_LAST_LOADED_SEC</code> 86400"},{"location":"teams/dataset_versioning/#roadmap","title":"Roadmap \u00b6","text":"<p>The following are some items that are on the roadmap for future iterations of the dataset versioning system. Keep an eye out for future FiftyOne Teams versions for these additional features!</p> <p>Near term</p> <ul> <li>Optimize diff computation for larger datasets (over 200k samples) and add support for modification summaries for these datasets</li> </ul> <p>Longer term</p> <ul> <li> <p>Further optimize existing versioning system</p> </li> <li> <p>Support external versioning backends</p> </li> <li> <p>Searching Snapshots</p> </li> <li> <p>Content-aware Snapshot change summaries</p> </li> </ul> <p>Exploratory</p> <ul> <li> <p>Visualization of Snapshot diffs</p> </li> <li> <p>Implement a branch-and-merge model</p> </li> <li> <p>Deep integrations with versioning backend tools to version FiftyOne datasets alongside your models and media</p> </li> </ul>"},{"location":"teams/installation/","title":"FiftyOne Teams Installation \u00b6","text":"<p>FiftyOne Teams deployments come with a centralized FiftyOne Teams App and database that allows your entire team to collaborate securely on the same datasets. FiftyOne Teams is deployed entirely into your environment, either on-premises or in a private cloud. Your data never leaves your environment.</p> <p>FiftyOne Teams can be deployed on a wide variety of infrastructure solutions, including Kubernetes and Docker.</p> <p>Note</p> <p>Detailed instructions for the initial FiftyOne Teams deployment, along with all necessary components, are made available by your Voxel51 CS engineer during the onboarding process.</p>"},{"location":"teams/installation/#python-sdk","title":"Python SDK \u00b6","text":"<p>While the FiftyOne Teams App allows for countless new App-centric workflows, any existing Python-based workflows that you\u2019ve fallen in love with in the open-source version of FiftyOne are still directly applicable!</p> <p>FiftyOne Teams requires an updated Python SDK, which is a wrapper around the open-source FiftyOne package that adds new functionality like support for cloud-backed media.</p> <p>You can find the installation instructions under the \u201cInstall FiftyOne\u201d section of the Teams App by clicking on your user icon in the upper right corner:</p> <p></p> <p>There you\u2019ll see instructions for installing a <code>fiftyone</code> package from the private PyPI server as shown below:</p> <pre><code>pip install --index-url https://${TOKEN}@pypi.fiftyone.ai fiftyone\n</code></pre> <p>Note</p> <p>See Installation with Poetry if you use <code>poetry</code> instead of <code>pip</code>.</p> <p>Note</p> <p>The Teams Python package is named <code>fiftyone</code> and has the same module structure as fiftyone, so any existing scripts you built using open source will continue to run after you upgrade!</p>"},{"location":"teams/installation/#next-steps","title":"Next Steps \u00b6","text":"<p>After installing the Teams Python SDK in your virtual environment, you\u2019ll need to configure two things:</p> <ul> <li> <p>Your team\u2019s API connection or MongoDB connection</p> </li> <li> <p>The cloud credentials to access your cloud-backed media</p> </li> </ul> <p>That\u2019s it! Any operations you perform will be stored in a centralized location and will be available to all users with access to the same datasets in the Teams App or their Python workflows.</p>"},{"location":"teams/installation/#installation-with-poetry","title":"Installation with Poetry \u00b6","text":"<p>If you are using poetry to install your dependencies rather than <code>pip</code>, you will need to follow instructions in the docs for installing from a private repository. The two key points are specifying the additional private source and declaring that the <code>fiftyone</code> module should be found there and not the default PyPI location.</p>"},{"location":"teams/installation/#add-source","title":"Add source \u00b6","text":"<p>In poetry v1.5, it is recommended to use an explicit package source.</p> <pre><code>poetry source add --priority=explicit fiftyone-teams https://pypi.fiftyone.ai/simple/\n</code></pre> <p>Prior to v1.5, you should use the deprecated secondary package source.</p> <pre><code>poetry source add --secondary fiftyone-teams https://pypi.fiftyone.ai/simple/\n</code></pre>"},{"location":"teams/installation/#configure-credentials","title":"Configure credentials \u00b6","text":"<pre><code>poetry config http-basic.fiftyone-teams ${TOKEN} \"\"\n</code></pre> <p>Alternatively, you can specify the credentials in environment variables.</p> <pre><code>export POETRY_HTTP_BASIC_FIFTYONE_TEAMS_USERNAME=\"${TOKEN}\"\nexport POETRY_HTTP_BASIC_FIFTYONE_TEAMS_PASSWORD=\"\"\n</code></pre> <p>If you have trouble configuring the credentials, see more in the poetry docs here.</p>"},{"location":"teams/installation/#add-fiftyone-dependency","title":"Add fiftyone dependency \u00b6","text":"<p>Replace <code>X.Y.Z</code> with the proper version</p> <pre><code>poetry add --source fiftyone-teams fiftyone==X.Y.Z\n</code></pre> <p>Note</p> <p>Due to an unresolved misalignment with <code>poetry</code> and a FiftyOne dependency, <code>kaleido</code>, you must add it to your own dependencies as well:</p> <pre><code>poetry add kaleido==0.2.1\n</code></pre> <p>You should then see snippets in the <code>pyproject.toml</code> file like the following (the <code>priority</code> line will be different for <code>poetry&lt;v1.5</code>):</p> <pre><code>[[tool.poetry.source]]\nname = \"fiftyone-teams\"\nurl = \"https://pypi.fiftyone.ai\"\npriority = \"explicit\"\n</code></pre> <pre><code>[tool.poetry.dependencies]\nfiftyone = {version = \"X.Y.Z\", source = \"fiftyone-teams}\n</code></pre>"},{"location":"teams/installation/#cloud-credentials","title":"Cloud credentials \u00b6","text":"<p>In order to utilize cloud-backed media functionality of FiftyOne Teams, at least one cloud source must be configured with proper credentials. Below are instructions for configuring each supported cloud provider for local SDK use or directly to the Teams containers. An admin can also configure\\ credentials for use by all app users.</p>"},{"location":"teams/installation/#cross-origin-resource-sharing-cors","title":"Cross-origin resource sharing (CORS) \u00b6","text":"<p>If your datasets include cloud-backed point clouds or segmentation maps, you may need to configure cross-origin resource sharing (CORS) for your cloud buckets. Details are provided below for each cloud platform.</p>"},{"location":"teams/installation/#browser-caching","title":"Browser caching \u00b6","text":"<p>If your datasets include cloud-backed media, we strongly recommend configuring your data sources to allow for built in browser caching. This will cache signed URL responses so you don\u2019t need to reload assets from your cloud storage between sessions. Details are provided below for each cloud platform.</p>"},{"location":"teams/installation/#amazon-s3","title":"Amazon S3 \u00b6","text":"<p>To work with FiftyOne datasets whose media are stored in Amazon S3, you simply need to provide AWS credentials to your Teams client with read access to the relevant objects and buckets.</p> <p>You can do this in any of the following ways:</p> <p>1. Configure/provide AWS credentials in any format supported by the boto3 library. For example, here are two of the supported methods:</p> <pre><code># Access key\nexport AWS_ACCESS_KEY_ID=...\nexport AWS_SECRET_ACCESS_KEY=...\nexport AWS_SESSION_TOKEN=... # if applicable\nexport AWS_DEFAULT_REGION=...\n</code></pre> <pre><code># Web identity provider\nexport AWS_ROLE_ARN=...\nexport AWS_WEB_IDENTITY_TOKEN_FILE=...\nexport AWS_ROLE_SESSION_NAME... #if applicable\nexport AWS_DEFAULT_REGION=...\n</code></pre> <p>2. Provide AWS credentials on a per-session basis by setting one of the following sets of environment variables to point to your AWS credentials on disk:</p> <pre><code># AWS config file\nexport AWS_CONFIG_FILE=\"/path/to/aws-config.ini\"\nexport AWS_PROFILE=default  # optional\n</code></pre> <pre><code># Shared credentials file\nexport AWS_SHARED_CREDENTIALS_FILE=\"/path/to/aws-credentials.ini\"\nexport AWS_PROFILE=default  # optional\n</code></pre> <p>In the above, the config file should use this syntax and the shared credentials file should use this syntax.</p> <p>Note</p> <p>FiftyOne Teams requires either the <code>s3:ListBucket</code> or <code>s3:GetBucketLocation</code> permission in order to access objects in S3 buckets.</p> <p>If you wish to use multi-account credentials, your credentials must have the <code>s3:ListBucket</code> permission, as <code>s3:GetBucketLocation</code> does not support this.</p> <p>3. Permanently register AWS credentials on a particular machine by adding the following keys to your media cache config:</p> <pre><code>{\n    \"aws_config_file\": \"/path/to/aws-config.ini\",\n    \"aws_profile\": \"default\"  # optional\n}\n</code></pre> <p>If you need to configure CORS on your AWS buckets, here is an example configuration:</p> <pre><code>[\\\n    {\\\n        \"origin\": [\"https://fiftyone-teams-deployment.yourcompany.com\"],\\\n        \"method\": [\"GET\", \"HEAD\"],\\\n        \"responseHeader\": [\"*\"],\\\n        \"maxAgeSeconds\": 86400\\\n    }\\\n]\n</code></pre> <p>If you would like to take advantage of browser caching you can specify cache-control headers on S3 objects. By default S3 does not provide cache-control headers so it will be up to your browser\u2019s heuristics engine to determine how long to cache the object.</p>"},{"location":"teams/installation/#google-cloud-storage","title":"Google Cloud Storage \u00b6","text":"<p>To work with FiftyOne datasets whose media are stored in Google Cloud Storage, you simply need to provide credentials to your Teams client with read access to the relevant objects and buckets.</p> <p>You can do this in any of the following ways:</p> <p>1. Configure application default credentials in a manner supported by Google Cloud, such as:</p> <ul> <li> <p>Using the gcloud CLI</p> </li> <li> <p>Attaching a service account to your Google Cloud resource</p> </li> </ul> <p>2. Provide GCS credentials on a per-session basis by setting the following environment variables to point to your GCS credentials on disk:</p> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/gcp-credentials.json\"\n</code></pre> <p>3. Permanently register GCS credentials on a particular machine by adding the following keys to your media cache config:</p> <pre><code>{\n    \"google_application_credentials\": \"/path/to/gcp-credentials.json\"\n}\n</code></pre> <p>In the above, the credentials file can contain any format supported by google.auth.load_credentials_from_file(), which includes a service account key, stored authorized user credentials, or external account credentials.</p> <p>If you need to configure CORS on your GCP buckets, here is an example configuration:</p> <pre><code>[\\\n    {\\\n        \"origin\": [\"https://fiftyone-teams-deployment.yourcompany.com\"],\\\n        \"method\": [\"GET\", \"HEAD\"],\\\n        \"responseHeader\": [\"*\"],\\\n        \"maxAgeSeconds\": 3600\\\n    }\\\n]\n</code></pre> <p>If you would like to take advantage of browser caching you can specify cache-control headers on GCP content. By default GCP sets the max-age=0 seconds meaning no caching will occur.</p>"},{"location":"teams/installation/#microsoft-azure","title":"Microsoft Azure \u00b6","text":"<p>To work with FiftyOne datasets whose media are stored in Azure Storage, you simply need to provide Azure credentials to your Teams client with read access to the relevant objects and containers.</p> <p>You can do this in any of the following ways:</p> <p>1. Provide your Azure credentials in any manner recognized by azure.identity.DefaultAzureCredential</p> <p>2. Provide your Azure credentials on a per-session basis by setting any group of environment variables shown below:</p> <pre><code># Option 1\nexport AZURE_STORAGE_CONNECTION_STRING=...\nexport AZURE_ALIAS=...  # optional\n</code></pre> <pre><code># Option 2\nexport AZURE_STORAGE_ACCOUNT=...\nexport AZURE_STORAGE_KEY=...\nexport AZURE_ALIAS=...  # optional\n</code></pre> <pre><code># Option 3\nexport AZURE_STORAGE_ACCOUNT=...\nexport AZURE_CLIENT_ID=...\nexport AZURE_CLIENT_SECRET=...\nexport AZURE_TENANT_ID=...\nexport AZURE_ALIAS=...  # optional\n</code></pre> <p>3. Provide Azure credentials on a per-session basis by setting the following environment variables to point to your Azure credentials on disk:</p> <pre><code>export AZURE_CREDENTIALS_FILE=/path/to/azure-credentials.ini\nexport AZURE_PROFILE=default  # optional\n</code></pre> <p>4. Permanently register Azure credentials on a particular machine by adding the following keys to your media cache config:</p> <pre><code>{\n    \"azure_credentials_file\": \"/path/to/azure-credentials.ini\",\n    \"azure_profile\": \"default\"  # optional\n}\n</code></pre> <p>In the options above, the <code>.ini</code> file should have syntax similar to one of the following:</p> <pre><code>[default]\nconn_str = ...\nalias = ...  # optional\n</code></pre> <pre><code>[default]\naccount_name = ...\naccount_key = ...\nalias = ...  # optional\n</code></pre> <pre><code>[default]\naccount_name = ...\nclient_id = ...\nsecret = ...\ntenant = ...\nalias = ...  # optional\n</code></pre> <p>When populating samples with Azure Storage filepaths, you can either specify paths by their full URL:</p> <pre><code>filepath = \"https://${account_name}.blob.core.windows.net/container/path/to/object.ext\"\n\n# For example\nfilepath = \"https://voxel51.blob.core.windows.net/test-container/image.jpg\"\n</code></pre> <p>or, if you have defined an alias in your config, you may instead prefix the alias:</p> <pre><code>filepath = \"${alias}://container/path/to/object.ext\"\n\n# For example\nfilepath = \"az://test-container/image.jpg\"\n</code></pre> <p>Note</p> <p>If you use a custom Azure domain, you can provide it by setting the <code>AZURE_STORAGE_ACCOUNT_URL</code> environment variable or by including the <code>account_url</code> key in your credentials <code>.ini</code> file.</p> <p>If you would like to take advantage of browser caching you can specify cache-control headers on Azure blobs. By default Azure does not provide cache-control headers so it will be up to your browser\u2019s heuristics engine to determine how long to cache the object.</p>"},{"location":"teams/installation/#minio","title":"MinIO \u00b6","text":"<p>To work with FiftyOne datasets whose media are stored in MinIO, you simply need to provide the credentials to your Teams client with read access to the relevant objects and buckets.</p> <p>You can do this in any of the following ways:</p> <p>1. Provide your MinIO credentials on a per-session basis by setting the individual environment variables shown below:</p> <pre><code>export MINIO_ACCESS_KEY=...\nexport MINIO_SECRET_ACCESS_KEY=...\nexport MINIO_ENDPOINT_URL=...\nexport MINIO_ALIAS=...  # optional\nexport MINIO_REGION=...  # if applicable\n</code></pre> <p>2. Provide MinIO credentials on a per-session basis by setting the following environment variables to point to your MinIO credentials on disk:</p> <pre><code>export MINIO_CONFIG_FILE=/path/to/minio-config.ini\nexport MINIO_PROFILE=default  # optional\n</code></pre> <p>3. Permanently register MinIO credentials on a particular machine by adding the following keys to your media cache config:</p> <pre><code>{\n    \"minio_config_file\": \"/path/to/minio-config.ini\",\n    \"minio_profile\": \"default\"  # optional\n}\n</code></pre> <p>In the options above, the <code>.ini</code> file should have syntax similar the following:</p> <pre><code>[default]\naccess_key = ...\nsecret_access_key = ...\nendpoint_url = ...\nalias = ...  # optional\nregion = ...  # if applicable\n</code></pre> <p>When populating samples with MinIO filepaths, you can either specify paths by prefixing your MinIO endpoint URL:</p> <pre><code>filepath = \"${endpoint_url}/bucket/path/to/object.ext\"\n\n# For example\nfilepath = \"https://voxel51.min.io/test-bucket/image.jpg\"\n</code></pre> <p>or, if you have defined an alias in your config, you may instead prefix the alias:</p> <pre><code>filepath = \"${alias}://bucket/path/to/object.ext\"\n\n# For example\nfilepath = \"minio://test-bucket/image.jpg\"\n</code></pre> <p>If you would like to take advantage of browser caching you can specify cache-control headers on MinIO content using the metadata field of the put_object API. By default Minio does not provide cache-control headers so it will be up to your browser\u2019s heuristics engine to determine how long to cache the object.</p>"},{"location":"teams/installation/#extra-client-arguments","title":"Extra client arguments \u00b6","text":"<p>Configuring credentials following the instructions above is almost always sufficient for FiftyOne Teams to properly utilize them. In rare cases where the cloud provider client needs non-default configuration, you can add extra client kwargs via the media cache config:</p> <pre><code>{\n    \"extra_client_kwargs\": {\n        \"azure\": {\"extra_kwarg\": \"value\"},\n        \"gcs\": {\"extra_kwarg\": \"value\"},\n        \"minio\": {\"extra_kwarg\": \"value\"},\n        \"s3\": {\"extra_kwarg\": \"value\"}\n    }\n}\n</code></pre> <p>Provider names and the class that extra kwargs are passed to:</p> <ul> <li>azure: <code>azure.identity.DefaultAzureCredential</code></li> <li>gcs: <code>google.cloud.storage.Client</code></li> <li>minio: <code>botocore.config.Config</code></li> <li>s3: <code>botocore.config.Config</code></li> </ul>"},{"location":"teams/installation/#cloud-storage-page","title":"Cloud storage page \u00b6","text":"<p>Admins can also configure cloud credentials via the Settings &gt; Cloud storage page.</p> <p>Credentials configured via this page are stored (encrypted) in the Teams database, rather than needing to be configured through environment variables in your Teams deployment.</p> <p>Note</p> <p>Any credentials configured via environment variables in your deployment will not be displayed in this page.</p> <p>To upload a new credential, click the <code>Add credential</code> button:</p> <p></p> <p>This will open a modal that you can use to add a credential for any of the available providers:</p> <p></p> <p>After the appropriate files or fields are populated, click <code>Save credential</code> to store the (encrypted) credential.</p> <p>As depicted in the screenshot above, a credential can optionally be restricted to a specific list of bucket(s):</p> <ul> <li> <p>If one or more buckets are provided, the credentials are bucket-specific credentials that will only be used to read/write media within the specified bucket(s)</p> </li> <li> <p>If no buckets are provided, the credentials are default credentials that will be used whenever trying to read/write any media for the provider that does not belong to a bucket with bucket-specific credentials</p> </li> </ul> <p>Note</p> <p>Bucket-specific credentials are useful in situations where you cannot or do not wish to provide a single set of credentials to cover all buckets that your team plans to use within a given cloud storage provider.</p> <p>When providing bucket-specific credentials, you may either provide bucket names like <code>my-bucket</code>, or you can provide fully-qualified buckets like <code>s3://my-bucket</code> and <code>https://voxel51.blob.core.windows.net/my-container</code>.</p> <p>Alternatively, credentials can be updated programmatically with the <code>add_cloud_credentials()</code> method in the Management SDK.</p> <p>Any cloud credentials uploaded via this method will automatically be used by the Teams UI when any user attempts to load media associated with the appropriate provider or specific bucket.</p> <p>Note</p> <p>By default, Teams servers refresh their credentials every 120 seconds, so you may need to wait up to two minutes after modifying your credentials via this page in order for the changes to take effect.</p> <p>Note</p> <p>Users cannot access stored credentials directly, either via the Teams UI or by using the Teams SDK locally. The credentials are only decrypted and used internally by the Teams servers.</p>"},{"location":"teams/management_sdk/","title":"Teams Management SDK \u00b6","text":"<p>One of FiftyOne\u2019s core design principles is that you should be able to do everything programmatically if you want.</p> <p>To this end, the <code>fiftyone.management</code> module provides Teams-specific methods for managing users, invitations, dataset permissions, plugins, API keys, and more.</p> <p>Note</p> <p>You must use an API connection (not a direct MongoDB connection) in order to use Management SDK methods.</p>"},{"location":"teams/management_sdk/#api-reference","title":"API reference \u00b6","text":""},{"location":"teams/management_sdk/#connections","title":"Connections \u00b6","text":""},{"location":"teams/management_sdk/#api-keys","title":"API keys \u00b6","text":""},{"location":"teams/management_sdk/#cloud-credentials","title":"Cloud credentials \u00b6","text":""},{"location":"teams/management_sdk/#dataset-permissions","title":"Dataset permissions \u00b6","text":""},{"location":"teams/management_sdk/#organization-settings","title":"Organization settings \u00b6","text":""},{"location":"teams/management_sdk/#plugin-management","title":"Plugin management \u00b6","text":""},{"location":"teams/management_sdk/#snapshots","title":"Snapshots \u00b6","text":""},{"location":"teams/management_sdk/#user-management","title":"User management \u00b6","text":""},{"location":"teams/management_sdk/#group-management","title":"Group management \u00b6","text":""},{"location":"teams/migrations/","title":"Migrations \u00b6","text":"<p>This page describes how to migrate between FiftyOne Teams versions, both for admins migrating the core Teams App infrastructure and individual users who need to install a new version of the Teams Python SDK.</p> <p>Refer to this section to see how to migrate existing datasets from open source to Teams.</p>"},{"location":"teams/migrations/#upgrading-your-python-sdk","title":"Upgrading your Python SDK \u00b6","text":"<p>Users can upgrade their FiftyOne Teams Python client to the latest version as follows:</p> <pre><code>pip install --index-url https://${TOKEN}@pypi.fiftyone.ai \u2013-upgrade fiftyone\n</code></pre> <p>A specific FiftyOne Teams client version can be installed like so:</p> <pre><code>pip install --index-url https://${TOKEN}@pypi.fiftyone.ai fiftyone==${VERSION}\n</code></pre> <p>Note</p> <p>You can find your <code>TOKEN</code> by logging into the FiftyOne Teams App and clicking on the account icon in the upper right.</p>"},{"location":"teams/migrations/#upgrading-your-deployment","title":"Upgrading your deployment \u00b6","text":"<p>The basic admin workflow for upgrading a FiftyOne Teams deployment is:</p> <ul> <li> <p>Upgrade all automated services and individual user workflows that use the Teams Python SDK to an appropriate SDK version</p> </li> <li> <p>Upgrade your core Teams App infrastructure (via Kubernetes, Docker, etc)</p> </li> <li> <p>Upgrade your database\u2019s version, as described below</p> </li> </ul> <p>Note</p> <p>Contact your Voxel51 CS Engineer for all relevant upgrade information, including compatible SDK versions, deployment assets, and upgrade assistance.</p> <p>New FiftyOne Teams versions occasionally introduce data model changes that require database migrations when upgrading your deployment.</p> <p>Admins can check a deployment\u2019s current version via the Python SDK as shown below:</p> <pre><code>$ fiftyone migrate --info\nFiftyOne Teams version: 0.7.1\nFiftyOne compatibility version: 0.15.1\nDatabase version: 0.15.1\n\n...\n</code></pre> <p>Note</p> <p>Individual datasets have versions as well. They are lazily upgraded the first time they are loaded under a new database version. Often there is no migration required, but there could be.</p> <p>Unlike open source FiftyOne, a Teams database is not automatically upgraded when a user connects to the database with a newer Python client version. Instead, an admin must manually upgrade your Teams database by installing the newest version of the Teams SDK locally, assuming admin privileges, and running the command shown below:</p> <pre><code>export FIFTYONE_DATABASE_ADMIN=true\n\n# Option 1: update the database version only (datasets lazily migrated on load)\nfiftyone migrate\n\n# Option 2: migrate the database and all datasets\nfiftyone migrate --all\n</code></pre> <p>Note</p> <p>Once the database is upgraded, all users must upgrade their Python SDK to a compatible version. Any connections from incompatible Python clients will be refused and an informative error message will be displayed.</p>"},{"location":"teams/migrations/#downgrading-your-deployment","title":"Downgrading your deployment \u00b6","text":"<p>Admins can also downgrade their FiftyOne Teams deployment to an older version if necessary.</p> <p>The steps are the same as when upgrading, except that you\u2019ll need to know the appropriate database version to migrate down to. Each version of Teams corresponds to a version of open source FiftyOne called its \u201copen source compatibility version\u201d, and this versioning system is used to set the database version.</p> <p>For example, you can downgrade to Teams v0.10 like so:</p> <pre><code>OS_COMPAT_VERSION=0.18.0  # OS compatibility version for Teams v0.10.0\n\nexport FIFTYONE_DATABASE_ADMIN=true\nfiftyone migrate --all -v ${OS_COMPAT_VERSION}\n</code></pre> <p>Note</p> <p>The above command must be run with the newer SDK version installed.</p> <p>Note</p> <p>Contact your Voxel51 CS engineer if you need to know the open source compatibility version for a particular Teams version that you wish to downgrade to.</p>"},{"location":"teams/migrations/#migrating-datasets-to-teams","title":"Migrating datasets to Teams \u00b6","text":"<p>Any datasets that you have created via open source FiftyOne can be migrated to your Teams deployment by exporting them in FiftyOneDataset format:</p> <pre><code># Open source SDK\nimport fiftyone as fo\n\ndataset = fo.load_dataset(...)\n\ndataset.export(\n    export_dir=\"/tmp/dataset\",\n    dataset_type=fo.types.FiftyOneDataset,\n    export_media=False,\n)\n</code></pre> <p>and then re-importing them with the Teams SDK connected to your Teams deployment:</p> <pre><code># Teams SDK\nimport fiftyone as fo\n\ndataset = fo.Dataset.from_dir(\n    dataset_dir=\"/tmp/dataset\",\n    dataset_type=fo.types.FiftyOneDataset,\n    persistent=True,\n)\n</code></pre> <p>Note that you\u2019ll need to update any local filepaths to cloud paths in order to use the dataset in Teams.</p> <p>If you need to upload the local media to the cloud, the Teams SDK provides a builtin utility for this:</p> <pre><code>import fiftyone.core.storage as fos\n\nfos.upload_media(\n    dataset,\n    \"s3://path/for/media\",\n    update_filepaths=True,\n    progress=True,\n)\n</code></pre> <p>Note</p> <p>By default, the above method only uploads the media in the <code>filepath</code> field of your samples. If your dataset contains other media fields (e.g. thumbnails, segmentations, or heatmaps) simply run the above command multiple times, using the <code>media_field</code> argument to specify the appropriate fields to upload.</p> <p>If any media fields use the same filenames as other fields, be sure to provide different <code>remote_dir</code> paths each time you call the above method to avoid overwriting existing media.</p> <p>If the files already exist in cloud buckets, you can manually update the filepaths on the dataset:</p> <pre><code>cloud_paths = []\nfor filepath in dataset.values(\"filepath\"):\n    cloud_path = get_cloud_path(filepath)  # your function\n    cloud_paths.append(cloud_path)\n\ndataset.set_values(\"filepath\", cloud_paths)\n</code></pre> <p>When you\u2019re finished, delete the local export of the dataset:</p> <pre><code>shutil.rmtree(\"/tmp/dataset\")\n</code></pre>"},{"location":"teams/overview/","title":"FiftyOne Teams Overview \u00b6","text":"<p>FiftyOne Teams is purpose-built to integrate into your existing ML workflows, including annotation, evaluation, model training, and deployment.</p> <p>Note</p> <p>Learn more about FiftyOne Teams and contact us to try it!</p>"},{"location":"teams/overview/#fiftyone-vs-fiftyone-teams","title":"FiftyOne vs FiftyOne Teams \u00b6","text":"<p>Here\u2019s a high-level overview of the capabilities that FiftyOne Teams brings:</p> FiftyOne Teams FiftyOne Curate Datasets Evaluate Models Find Mistakes Visualize Embeddings Deployment Multi-user, on-premise,private/public cloud Local, Single user Dataset Management User Permissions Dataset Permissions Dataset Versioning SSO Enterprise Support Discord Community Licensing Unlimited data, flexibleuser-based licensing <p>Apache 2.0</p>"},{"location":"teams/overview/#backwards-compatibility","title":"Backwards compatibility \u00b6","text":"<p>FiftyOne Teams is fully backwards compatible with open-source FiftyOne. This means that all of your pre-existing FiftyOne workflows should be usable without modification.</p> <p>For example, you can continue running all of the workflows listed below as you would with open source FiftyOne:</p> Application Workflows Data ingestion Loading data into FiftyOne Data curation Using the FiftyOne AppCreating views into datasetsEmbedding-based dataset analysisVisual similarity and dataset uniqueness Annotation Using the annotation API Model training and evaluation Exporting data for model trainingAdding model predictions to FiftyOneEvaluating models in FiftyOneUsing interactive plots to explore results"},{"location":"teams/overview/#system-architecture","title":"System architecture \u00b6","text":"<p>FiftyOne Teams is implemented as a set of interoperable services, as described in the figure below.</p> <p></p> <p>FiftyOne Teams is strictly a software offering. All relevant hardware is owned and managed by your organization, whether on-premises or in your virtual private cloud.</p> <p>Teams database services</p> <p>The primary storage location for all of the FiftyOne Teams datasets and related metadata (excluding media files) for your organization.</p> <p>Teams web service</p> <p>An always-on front-end from which you can visually access the datasets in your FiftyOne Teams deployment. Web-based access is the standard entrypoint for non-technical users who need point-and-click access to dataset curation and related features, as well as basic workflows for technical users. Most dataset curation and model analysis work by engineers happens via client installations.</p> <p>Teams API authentication</p> <p>Technical users connecting to FiftyOne Teams via Python or Jupyter notebooks use token-based authentication to make authorized connections to the centralized database storing your Team\u2019s dataset metadata.</p> <p>Python/notebook users (your organization)</p> <p>Similar to FiftyOne, technical users can install the FiftyOne Teams client in their working environment(s). These clients are configured to use the centralized database service and will additionally serve their own App instances (like open source FiftyOne) so that engineers can work locally, remotely, and in Jupyter notebooks.</p> <p>Web users (your organization)</p> <p>FiftyOne Teams provides an always-on login portal at <code>https://&lt;your-org&gt;.fiftyone.ai</code> that users can login to from any browser for web-only workflows.</p> <p>Data lake (your organization)</p> <p>FiftyOne Teams does not require duplication or control over how your source media files are stored. Instead, FiftyOne Teams stores references (e.g., cloud object URLs or network storage paths) to the media in your datasets, thereby minimizing storage costs and providing you the flexibility to provision your object storage as you see fit. FiftyOne Teams has full support for cloud, network, and local media storage.</p> <p>User authentication (your organization)</p> <p>FiftyOne Teams can be configured to work with your organization\u2019s authentication and authorization systems, enabling you to manage access to FiftyOne Teams using your existing OAuth stack. FiftyOne Teams supports SAML 2.0 and OAuth 2.0.</p>"},{"location":"teams/overview/#security-considerations","title":"Security considerations \u00b6","text":"<p>FiftyOne Teams relies on your organization\u2019s existing security infrastructure. No user accounts are created specifically for FiftyOne Teams; we integrate directly with your OAuth system.</p> <p>Usage of the FiftyOne Teams client by technical users of your organization is also secure. All database access is managed by the central authentication service, and self-hosted App instances can be configured to only accept connections from known servers (e.g., only localhost connections). In remote client workflows, users are instructed how to configure ssh tunneling to securely access self-hosted App instances.</p> <p>No outside network access is required to operate FiftyOne Teams. Voxel51 only requests the ability to (a) access the system logs for usage tracking and auditing purposes, and (b) access the system at the customer\u2019s request to provide technical support. We are flexible in the mechanisms used to accomplish these goals.</p>"},{"location":"teams/pluggable_auth/","title":"Pluggable Authentication \u00b6","text":"<p>FiftyOne Teams v1.6.0 introduces Pluggable Authentication that provides the Central Authentication Service (CAS). CAS is a self-contained authentication system with two modes ( <code>legacy</code> and <code>internal</code>). Legacy mode uses Auth0. Internal mode eliminates the Auth0 external dependency and may run in environments without egress to the internet. CAS provides a UI, REST API, and JavaScript (JS) Hook mechanism to manage FiftyOne Teams user data and authentication.</p>"},{"location":"teams/pluggable_auth/#fiftyone-authentication-modes","title":"FiftyOne Authentication Modes \u00b6","text":"<p>The setting <code>FIFTYONE_AUTH_MODE</code> specifies the authentication mode <code>legacy</code> or <code>internal</code>.</p>"},{"location":"teams/pluggable_auth/#legacy-mode","title":"Legacy Mode \u00b6","text":"<p>In legacy mode, FiftyOne Teams uses Auth0 for user authentication and authorization. This mode requires an external connection to Auth0 endpoints. User data is eventually consistent (where changes are reflected across FiftyOneTeams eventually). Auth0 contains the configuration for identity providers and the persistence of user data. Auth0 supports multiple providers (including SAML). For the supported IdPs, see Auth0 Enterprise Identity Providers.</p>"},{"location":"teams/pluggable_auth/#internal-mode","title":"Internal Mode \u00b6","text":"<p>In internal mode, FiftyOne Teams the CAS replaces Auth0. FiftyOne Teams will not require network egress to external services. User data is immediately consistent (where changes are reflected across FiftyOne Teams instantly). Directory data is immediately written to MongoDB, and organizations have the autonomy to manage their Identity Provider Configuration. Internal mode supports OpenID Connect (OIDC) and OAuth2.</p> <p>NOTE: SAML is not supported in internal mode</p>"},{"location":"teams/pluggable_auth/#super-admin-ui","title":"Super Admin UI \u00b6","text":"<p>The Super Admin UI contains FiftyOne Teams deployment wide configurations. When logging into FiftyOne Teams as an admin, you are in the context of an organization. Settings are scoped by organization and only apply to that organization. The Super Admin UI allows you to administer all organizations and global configurations (Identity Providers, Session timeouts, and JS hooks).</p> <p>NOTE: In v1.6.0, the Super Admin UI is only available in internal mode</p> <p></p> <p>To login to this application navigate to <code>https://&lt;YOUR_FIFTYONE_TEAMS_URL&gt;/cas/configurations</code>. In the top right of the screen, and provide the <code>FIFTYONE_AUTH_SECRET</code> to login.</p> <p>NOTE: The value of <code>FIFTYONE_AUTH_SECRET</code> should be set prior to installation or upgrade.</p>"},{"location":"teams/pluggable_auth/#new-user-invitations","title":"New User Invitations \u00b6","text":"<p>As of FiftyOne Teams 2.1.0, onboarding new users can be done via invitation links. To do so, \u201cEnable invitation\u201d must be toggled on in the Organizations section of the Super Admin UI.</p> <p></p> <p>This allows creating invitation links in internal mode that can be manually sent to users. When those users click the links, they will be added to the Organization and prompted to log in.</p> <p>As of FiftyOne Teams 2.2.0, these invitation links can be automatically sent via an email through a configured SMTP server. This provides similar functionality to legacy mode email invitations, but without the need for Auth0 or any other external connections beyond the SMTP server itself.</p> <p>NOTE: Unless the deployment is a Managed Deployment, users must provide their own SMTP server to use in internal mode. This new functionality provides a way to authenticate and communicate with your existing mail server.</p> <p>To do so, \u201cSend email invitations\u201d must be toggled on in the Organizations section of the Super Admin UI</p> <p></p> <p>To configure your SMTP connection, navigate to the SMTP section of the Super Admin UI</p> <p></p> <p>A notification at the top of the menu will inform if an SMTP configuration is already saved. Currently, only one SMTP configuration can be saved per Organization.</p> <p>Select the appropriate type of authentication for your SMTP server and fill out the associated fields. When you save the configuration, FiftyOne Teams will do a preliminary check to ensure that the SMTP server is reachable at the host provided. The configuration will not save otherwise.</p> <p>Additionally, users can enter a valid email address and click Send Test Email to test the connection.</p> <p></p> <p>NOTE: Emails and an SMTP connection are not required to use invitations. Invitation links can still be generated and manually distributed without an SMTP configuration.</p>"},{"location":"teams/pluggable_auth/#identity-providers-idp","title":"Identity Providers (IdP) \u00b6","text":"<p>In internal mode, use the CAS REST API or Super Admin UI to configure FiftyOne teams to authenticate users via OIDC or OAuth2 compatible Identity Providers. Below is an example configuration for KeyCloak as an Identity Provider.</p> <pre><code>{\n    \"id\": \"keycloak-example\",\n    \"wellKnown\": \"https://keycloak.dev.acme.ai/auth/realms/acme/.well-known/openid-configuration\",\n    \"name\": \"KeyCloak\",\n    \"type\": \"oauth\",\n    \"authorization\": {\n        \"url\": \"https://keycloak.acme.ai/auth/realms/acme/protocol/openid-connect/auth\",\n        \"params\": {\n            \"scope\": \"openid email profile\"\n        }\n    },\n    \"clientId\": \"...\",\n    \"clientSecret\": \"...\",\n    \"issuer\": \"https://keycloak.dev.acme.ai/auth/realms/acme\",\n    \"token\": {\n        \"url\": \"https://keycloak.dev.acme.ai/auth/realms/acme/protocol/openid-connect/token\"\n    },\n    \"userinfo\": {\n        \"url\": \"https://keycloak.dev.acme.ai/auth/realms/acme/protocol/openid-connect/userinfo\"\n    },\n    \"idToken\": true,\n    \"style\": {\n        \"logoDark\": \"https://images.com/keycloak.svg\",\n        \"bg\": \"#47abc6\",\n        \"text\": \"#fff\"\n    },\n    \"profile\": \"(e,l)=&gt;(console.log({profile:e,tokens:l}),{id:e.sub,name:e.name,email:e.email,image:null})\",\n    \"allowDangerousEmailAccountLinking\": true\n}\n</code></pre>"},{"location":"teams/pluggable_auth/#getting-started-with-internal-mode","title":"Getting Started with Internal Mode \u00b6","text":"<p>These steps are only required to run FiftyOne Teams in internal mode. Please skip when using Auth0 in legacy mode.</p> <ol> <li> <p>Configure your Identity Provider</p> </li> <li> <p>Login to the SuperUser UI by navigating to <code>https://&lt;YOUR_FIFTYONE_TEAMS_URL&gt;/cas/configurations</code> and in the top right, and provide the <code>FIFTYONE_AUTH_SECRET</code> to login.</p> </li> <li> <p>Create an Admin</p> </li> <li> <p>Click on the \u201cAdmins\u201d tab</p> </li> <li> <p>Click \u201cAdd admin\u201d in the bottom left</p> </li> <li> <p>Set the name and email address (as it appears in your Identity Provider)       and click \u201cAdd\u201d</p> </li> <li> <p>Add your Identity Provider</p> </li> <li> <p>Click on the \u201cIdentity Providers\u201d tab at the top of the screen and click       \u201cAdd provider\u201d</p> </li> <li> <p>Fill out the \u201cAdd identity provider\u201d</p> <ol> <li>You can also click \u201cSwitch to advanced editor\u201d to provide the full      configuration as a JSON object</li> <li>In the \u201cProfile callback\u201d field, set the mapping that your Identity   Provider expects</li> <li>Login with the admin user</li> </ol> </li> <li> <p>Navigate to <code>https://&lt;YOUR_FIFTYONE_TEAMS_URL&gt;/datasets</code></p> </li> <li> <p>You should see the login screen for your newly configured authentication       provider</p> </li> <li> <p>Before logging in, set the admin user (in step 5). Otherwise, you will       need to remove this user from the database and try again.</p> </li> <li> <p>Click the login button and provide the credentials of the Admin user       (set in step 3)</p> </li> <li> <p>Click on the icon in the top right corner then click \u201cSettings\u201d</p> </li> <li> <p>Click \u201cUsers\u201d on the left side</p> </li> <li> <p>Validate the user is listed as an admin</p> </li> </ol>"},{"location":"teams/pluggable_auth/#syncing-with-3rd-party-directories-open-directory-ldap-and-active-directory","title":"Syncing with 3rd Party Directories (Open Directory, LDAP, and Active Directory) \u00b6","text":"<p>Below is an example of how to use JavaScript hooks to sync FiftyOne Teams with a corporate directory (such as Open Directory, LDAP, or Active Directory) via an intermediary REST API or Identity Provider. The recommended setup is with OAuth/OIDC claims, however the example below illustrates a more intricate integration.</p> <p>This example specifically addresses a scenario in which additional actions are performed during the <code>signIn</code> trigger. This demonstrates how hooks can extend beyond simple authentication to interact with external APIs and internal services for complex user management and group assignment tasks. Here\u2019s a breakdown of the example:</p> <pre><code>// Example JavaScript hook implementation\nasync function Hook(context) {\n    const { params, services, trigger } = context;\n    const { user } = params;\n\n    switch (trigger) {\n        // Callback: user is trying to sign in\n        case \"signIn\":\n            // returning false will prevent user from signing in\n            try {\n                const groups = await getGroups();\n                for (const group of groups) {\n                    await addUserToGroup(group.name);\n                }\n            } catch (error) {\n                console.error(error);\n            }\n            return true;\n    }\n\n    async function getGroups() {\n        const { account } = params;\n        const response = await services.util.http.get(\n            `https://fiftyone.ai/list_groups/${user.id}`,\n            { headers: { Authorization: `Bearer ${account?.access_token}` } }\n        );\n        return response.json();\n    }\n\n    async function addUserToGroup(groupName) {\n        const orgId = \"my_org_id\";\n        // Retrieve all existing groups\n        const groups = await services.directory.groups.listGroups(orgId);\n        // Find an existing group by name\n        let group = groups.find((group) =&gt; {\n            return group.name === groupName;\n        });\n        // If group does not exist, create a new group\n        if (!group) {\n            group = await services.directory.groups.createGroup(\n                orgId,\n                groupName\n            );\n        }\n\n        if (!group.userIds.includes(user.id)) {\n            // Add signed-in user to the group and update the group\n            const updatedGroupName = undefined;\n            const updatedGroupDescription = undefined;\n            const updatedGroupUserIds = [...group.userIds, user.id];\n            const accessorId = undefined;\n            await services.directory.groups.updateGroup(\n                orgId,\n                group.id,\n                accessorId,\n                updatedGroupName,\n                updatedGroupDescription,\n                updatedGroupUserIds\n            );\n        }\n    }\n}\n</code></pre>"},{"location":"teams/pluggable_auth/#context-object","title":"Context Object \u00b6","text":"<p>The context object provides information about the current operation, including parameters like the user\u2019s details, and services (services) that offer utility functions and access to directory operations.</p>"},{"location":"teams/pluggable_auth/#external-api-integration","title":"External API Integration \u00b6","text":"<ul> <li> <p><code>getGroups</code>: This function calls an external API to retrieve a list of groups to which the signing-in user should be added. It utilizes the <code>services.util.http.get</code> method for making the HTTP request, demonstrating how external services can be queried within the hook.</p> </li> <li> <p><code>addUserToGroup</code>: For each group retrieved from the external API, this function checks if the group exists in the organization\u2019s directory. If a group does not exist, it is created, and then the user is added to it. This process involves querying and modifying the organization\u2019s group directory, illustrating the hook\u2019s capability to perform complex operations like dynamic group management based on external data.</p> </li> </ul>"},{"location":"teams/pluggable_auth/#error-handling","title":"Error Handling \u00b6","text":"<ul> <li>The try-catch block around the external API call and group manipulation logic ensures that errors do not prevent the user from signing in but are properly logged</li> </ul>"},{"location":"teams/pluggable_auth/#summary","title":"Summary \u00b6","text":"<p>This hook example demonstrates a pattern for extending authentication flows in CAS with custom logic. By integrating with an external API to fetch group information and manipulating the organization\u2019s group memberships accordingly, it showcases the flexibility and extensibility of hooks in supporting complex, real-world authentication and authorization scenarios.</p>"},{"location":"teams/pluggable_auth/#rest-api","title":"REST API \u00b6","text":"<p>You can view the REST API Documentation by logging into the Super Admin UI (see above) or by directly visiting <code>https://&lt;YOUR_FIFTYONE_TEAMS_URL&gt;/cas/api-doc</code></p>"},{"location":"teams/pluggable_auth/#configuration","title":"Configuration \u00b6","text":"Setting Type Default Description <code>authentication_providers</code> <code>Array</code> <code>[]</code> A list of definitions of OIDC and/or OAuth providers. <code>authentication_provider.profile</code> <code>String</code> (parseable to JS function) <code>null</code> When provided this function is called to map the external user_info to the internal fiftyone user/account. <code>session_ttl</code> <code>Number</code> <code>300</code> Time in seconds for sessions to live after which users will be forced to log out. Must be greater than 120 seconds to support refreshing of user session using refresh token. <code>js_hook_enabled</code> <code>Boolean</code> <code>true</code> When set to False, configured JavaScript hooks will not be invoked. <code>js_hook</code> <code>String</code> (parseable to a single JS function) <code>null</code> JavaScript hook which is invoked on several CAS events described in JS Hooks section below <code>skipSignInPage</code> <code>Boolean</code> <code>false</code> When set to True, sign in page with identity provider choices will be skipped when there is only one identity provider is configured"},{"location":"teams/pluggable_auth/#javascript-hooks","title":"JavaScript Hooks \u00b6","text":"<p>This documentation outlines the JavaScript hook implementation for the Central Authentication Service (CAS). As a CAS superuser, you are able to define JavaScript functions that integrate with various authentication flows within CAS, customizing the authentication processes.</p>"},{"location":"teams/pluggable_auth/#overview","title":"Overview \u00b6","text":"<p>JavaScript hooks allow superusers to programmatically influence authentication flows, including sign-in, sign-up, JWT handling and customization, redirection, and session management. This document describes the available hooks, their triggers, expected return types, and contextual information provided to each hook.</p>"},{"location":"teams/pluggable_auth/#example-javascript-hook","title":"Example JavaScript Hook \u00b6","text":"<pre><code>// Example JavaScript hook implementation\nasync function Hook(context) {\n    const { params, services, trigger } = context;\n\n    switch (trigger) {\n        // user is trying to sign in\n        case \"signIn\":\n            // returning false will prevent user from signing in\n            return true;\n\n        // JWT token is created for user session\n        case \"jwt\":\n            // custom payload returned will be merged with default payload\n            return {};\n\n        // user has logged out\n        case \"logout\":\n            // returning or throwing here does not affect the sign out flow\n            break;\n\n        // user is being redirected on sign in our sign out\n        case \"redirect\":\n            // user will be redirected to this URL on sign in our sign out\n            return \"/settings/accounts\";\n    }\n}\n</code></pre>"},{"location":"teams/pluggable_auth/#actionable-triggers","title":"Actionable Triggers \u00b6","text":"Trigger Description Return Type <code>signIn</code> Invoked when a user signs in. If the hook returns false or error is thrown, sign-in will be prevented. Boolean <code>signUp</code> Invoked when a new user signs in for the very first time. If the hook returns false or an error is thrown, sign-in will be prevented and a user/account will not be created. <code>jwt</code> Invoked when JWT is created (on signIn, signUp, refresh token). The returned object will override payload of default JWT payload. If an error is thrown, the session will be expired and user will be redirected to sign-in Object | undefined <code>redirect</code> Invoked post signIn or signOut. The user will be redirected to the URL/Path returned from the hook. String (URL) <code>session</code> Invoked when a request for a session (on signIn, signUp, refresh token) is received. <p>Event-Only Triggers</p> Trigger Description <code>signOut</code> Invoked when a user signs out. <code>createUser</code> Invoked when the adapter is asked to create a user. <code>linkAccount</code> Invoked when an account is linked to a user. <p>JavaScript Hooks Contextual Parameters</p> Parameter Description Available in Triggers <code>token</code> The payload of a JWT token. <code>signIn, signUp, jwt, session, signOut, linkAccount, createUser</code> <code>user</code> The signed-in user object. <code>signIn, signUp, jwt, linkAccount</code> <code>account</code> The account from an identity provider. <code>signIn, signUp, jwt, linkAccount</code> <code>profile</code> The profile from an identity provider. <code>signIn, signUp, jwt, linkAccount</code> <code>isNewUser</code> True if a user is signing in for the first time. <code>jwt</code> <code>trigger</code> Specifies the current trigger event. <code>jwt</code> <code>Session</code> The session object. <code>session</code> <code>services</code> Provides access to various services. all <p>Services</p> Service Description <code>services.util.http</code> Provides <code>get</code>, <code>post</code>, <code>put</code>, and <code>delete</code> functions for making <code>HTTP</code> requests from a JS Hook. <code>services.userContext</code> Object containing information about the user performing the current action. <code>services.directory</code> <code>services.directory.users</code> The <code>UserService</code> - providing methods for interacting with the directory of users. <code>services.directory.groups</code> The <code>GroupsService</code>- providing methods for interacting with the directory of groups. <code>services.config</code> The <code>ConfigService</code>- providing methods for reading and writing the <code>AuthenticationConfig</code>. <code>services.util</code> <code>services.directory.orgs</code> The <code>OrgsService</code>- providing methods for interacting with the directory of organizations. <code>services.webhookService</code> Experimental <code>process.env['MY_ENV_VAR']</code> Syntax for reading environment variables in a JS Hook."},{"location":"teams/query_performance/","title":"Query Performance \u00b6","text":"<p>Available in FiftyOne Teams v2.2+</p> <p>Query Performance is a builtin feature of the FiftyOne Teams App that leverages database indexes to optimize your queries on large-scale datasets.</p>"},{"location":"teams/query_performance/#optimizing-query-performance","title":"Optimizing Query Performance \u00b6","text":"<p>The App\u2019s sidebar is optimized to leverage database indexes whenever possible.</p> <p>Fields that are indexed are indicated by lightning bolt icons next to their field/attribute names:</p> <p></p> <p>The above GIF shows Query Performance in action on the train split of the BDD100K dataset with an index on the <code>detections.detections.label</code> field.</p> <p>Note</p> <p>When filtering by multiple fields, queries will be more efficient when your first filter is on an indexed field.</p> <p>If you perform a filter that could benefit from an index and the query takes longer than a few seconds, you\u2019ll see a toast notification that nudges you to take the appropriate action to optimize the query:</p> <p></p> <p>Clicking \u201cCreate Index\u201d will open the Query Performance panel with a preconfigured recommendation of an index or summary field to create.</p> <p>Note</p> <p>Clicking \u201cDismiss\u201d will prevent this notification from appearing for the remainder of your current App session.</p>"},{"location":"teams/query_performance/#query-performance-panel","title":"Query Performance panel \u00b6","text":"<p>You can open the Query Performance panel manually either by clicking the \u201c+\u201d icon next to the Samples tab or by clicking the yellow lightning bolt in the top-right of the sidbar:</p> <p></p> <p>The first time you open the Query Performance panel, you\u2019ll see a welcome page:</p> <p></p> <p>After you\u2019ve created at least one custom index or summary field for a dataset, you\u2019ll instead see a list of the indexes and summary fields that exist on the dataset:</p> <p></p>"},{"location":"teams/query_performance/#creating-indexes","title":"Creating indexes \u00b6","text":"<p>You can create a new index at any time by clicking the <code>Create Index</code> button in the top-right of the panel:</p> <p></p> <p>When you click \u201cExecute\u201d, the index will be initiated and you\u2019ll see \u201cIn progress\u201d in the panel\u2019s summary table.</p> <p>After the index creation has finished, the field that you indexed will have a lightning bolt icon in the sidebar, and you should notice that expanding the field\u2019s filter widget and performing queries on it will be noticably faster.</p> <p>Warning</p> <p>For large datasets, index creation can have a significant impact on the performance of the database while the index is under construction.</p> <p>We recommend indexing only the specific fields that you wish to perform initial filters on, and we recommend consulting with your deployment admin before creating multiple indexes simultaneously.</p> <p>You can also create and manage custom indexes via the SDK.</p>"},{"location":"teams/query_performance/#creating-summary-fields","title":"Creating summary fields \u00b6","text":"<p>The Query Performance panel also allows you to create summary fields, which are sample-level fields that allow you to efficiently perform queries on large datasets where directly querying the underlying field is prohibitively slow due to the number of objects/frames in the field.</p> <p>For example, summary fields can help you query video datasets to find samples that contain specific classes of interest, eg <code>person</code>, in at least one frame.</p> <p>You can create a new summary field at any time by clicking the <code>Create Index</code> button in the top-right of the panel and selecting the \u201cSummary field\u201d type in the model:</p> <p></p> <p>Warning</p> <p>For large datasets, creating summary fields can take a few minutes.</p> <p>You can also create and manage summary fields via the SDK.</p>"},{"location":"teams/query_performance/#updating-summary-fields","title":"Updating summary fields \u00b6","text":"<p>Since a summary field is derived from the contents of another field, it must be updated whenever there have been modifications to its source field.</p> <p>Click the update icon in the actions column of any summary field to open a modal that will provide guidance on whether to update the summary field to reflect recent dataset changes.</p>"},{"location":"teams/query_performance/#deleting-indexessummaries","title":"Deleting indexes/summaries \u00b6","text":"<p>You can delete a custom index or summary field by clicking its trash can icon in the actions column of the panel.</p>"},{"location":"teams/query_performance/#disabling-query-performance","title":"Disabling Query Performance \u00b6","text":"<p>Query Performance is enabled by default for all datasets. This is generally the recommended setting for all large datasets to ensure that queries are performant.</p> <p>However, in certain circumstances you may prefer to disable Query Performance, which enables the App\u2019s sidebar to show additional information such as label/value counts that are useful but more expensive to compute.</p> <p>You can enable/disable Query Performance for a particular dataset for its lifetime (in your current browser) via the gear icon in the Samples panel\u2019s actions row:</p> <p></p> <p>You can also enable/disable Query Performance via the status button in the upper right corner of the Query Performance panel:</p> <p></p> <p>Deployment admins can also configure the global behavior of Query Performance via the following environment variables:</p> <pre><code># Disable Query Performance by default for all new datasets\nFIFTYONE_APP_DEFAULT_QUERY_PERFORMANCE=false\n</code></pre> <pre><code># Completely disable Query Performance for all users\nFIFTYONE_APP_ENABLE_QUERY_PERFORMANCE=false\n</code></pre>"},{"location":"teams/roles_and_permissions/","title":"Roles and permissions \u00b6","text":"<p>FiftyOne Teams is built for collaboration, with the goal of making it as easy as possible for engineers, data scientists, and stakeholders to work together to build high quality datasets and computer vision models.</p> <p>Accordingly, FiftyOne Teams gives you the flexibility to configure user roles, user groups and fine-grained permissions so that you can safely and securely collaborate both inside and outside your organization at all stages of your workflows.</p> <p>This page introduces the basic roles and permissions available in FiftyOne Teams.</p>"},{"location":"teams/roles_and_permissions/#roles","title":"Roles \u00b6","text":"<p>FiftyOne Teams has four user roles: Admin, Member, Collaborator, and Guest.</p> <p>Admins can access user management features by clicking on their account icon in the upper-right of the FiftyOne Teams App and navigating to the \u201cSettings &gt; Team &gt; Users\u201d page:</p> <p></p> <p>Admins can invite new users by clicking on \u201cInvite people\u201d, as shown below. Invited users will receive an email invitation with a link to accept the invitation.</p> <p>Note</p> <p>Invited users may login using any identity provider that has been enabled on your deployment. If you need more information about configuring IdPs or increasing your user quotas, contact your Voxel51 CS engineer.</p> <p></p>"},{"location":"teams/roles_and_permissions/#admin","title":"Admin \u00b6","text":"<p>Admins have full access to all of an organization\u2019s datasets and can create, edit, and delete any dataset.</p> <p>Admins can also invite or remove users from the organization and change any other user\u2019s roles, including promoting/demoting users to admins.</p>"},{"location":"teams/roles_and_permissions/#member","title":"Member \u00b6","text":"<p>Members can create new datasets and can be granted any desired level of permission on existing datasets. Members may also have a default access level to datasets that use this feature.</p> <p>Members do not have the ability to see or manage an organization\u2019s users.</p>"},{"location":"teams/roles_and_permissions/#collaborator","title":"Collaborator \u00b6","text":"<p>Collaborators only have access to datasets to which they have been specifically granted access (a dataset\u2019s default access level does not apply to Collaborators), and they may only be granted Can view, Can tag or Can edit access to datasets.</p> <p>Collaborators cannot create new datasets, clone existing datasets, or view other users of the deployment. Collaborators may export datasets to which they\u2019ve been granted access.</p>"},{"location":"teams/roles_and_permissions/#guest","title":"Guest \u00b6","text":"<p>Guests only have access to datasets to which they have been specifically granted access (a dataset\u2019s default access level does not apply to Guests), and they may only be granted Can view access to datasets.</p> <p>Guests cannot create new datasets, clone existing datasets, export datasets, or view other users of the deployment.</p>"},{"location":"teams/roles_and_permissions/#groups","title":"Groups \u00b6","text":"<p>User groups in FiftyOne Teams allow organization admins to manage a collection of users as a single entity. This simplifies the process of assigning permissions to multiple users, making it more efficient to control access to datasets.</p> <p>Admins can manage groups through the \u201cSettings &gt; Team &gt; Groups\u201d page. Each group can be given specific dataset access permissions, which apply to all users within the group. Collaborators\u2019 and guests\u2019 access to the dataset is limited by the maximum dataset access level of the role.</p> <p></p> <p>Admins can create a new group by clicking on \u201cCreate group\u201d and then adding existing users to the group by clicking on \u201cAdd users\u201d.</p> <p></p> <p>Note</p> <p>Non-existing users cannot be directly added to a group. Users must be invited and accept the invitation before they can be added to a group.</p>"},{"location":"teams/roles_and_permissions/#permissions","title":"Permissions \u00b6","text":"<p>Admins and users with the Can manage permission on a dataset can configure a dataset\u2019s permissions under the dataset\u2019s Manage tab in the FiftyOne Teams App.</p> <p>In FiftyOne Teams, dataset permissions for a user are determined by both the access they receive from their groups\u2019 permissions and individual permissions assigned to them.</p> <p>A user\u2019s permissions on a dataset is the maximum of their permissions from the following sources:</p> <ul> <li> <p>Admins implicitly have full access to all datasets</p> </li> <li> <p>Members have the dataset\u2019s default access level</p> </li> <li> <p>Users may be granted specific access to the dataset</p> </li> <li> <p>Users may be members of one or more groups, each of which may have specific access to the dataset</p> </li> </ul> <p>Note</p> <p>User role determines the highest level of access that a user can be granted to a dataset. For example, a user with Guest role can be added to a group with Can edit permission to a dataset, but this user will have Can view permission instead of Can edit permission of the dataset, because Guest role only allows Can view permission to datasets.</p>"},{"location":"teams/roles_and_permissions/#default-access","title":"Default access \u00b6","text":"<p>All datasets have a default access level, which defines a minimum permission level that all Members have on the dataset.</p> <p>A dataset\u2019s default access level can be set to No access, Can view, Can tag, Can edit, or Can manage as shown below:</p> <p></p> <p>Note</p> <p>Default access level only applies to Members. Guests and Collaborators must be granted specific access to datasets.</p>"},{"location":"teams/roles_and_permissions/#specific-access","title":"Specific access \u00b6","text":"<p>Authorized users can grant specific access to a dataset using the \u201cPeople and groups with access\u201d section shown below.</p> <p>To give access to an existing user or group, simply click \u201cShare\u201d button on the top right. A list of users with access to the dataset is shown. Click \u201cAdd User\u201d or \u201cAdd Group\u201d to grant access to a new user or group.</p> <p></p> <p>The following permissions are available to each user role:</p> <ul> <li> <p>Groups may be granted Can view, Can tag, Can edit, or Can manage permissions</p> </li> <li> <p>Members may be granted Can view, Can tag, Can edit, or Can manage permissions</p> </li> <li> <p>Collaborators may be granted Can view, Can tag, or Can edit permissions</p> </li> <li> <p>Guests may be granted Can view permissions</p> </li> </ul> <p>Note</p> <p>Authorized users can use the \u201cGrant access\u201d workflow to give Can view , Can tag, or Can edit access to a dataset to an email address that is not yet user of a FiftyOne Teams deployment.</p> <p>When the invitation is accepted, the user will become a Guest if the Can view permission is chosen or a Collaborator if a higher permission is chosen, and an Admin can upgrade this user to another role if desired via the Team Settings page.</p>"},{"location":"teams/roles_and_permissions/#no-access","title":"No access \u00b6","text":"<p>If a user has no access to a dataset, the dataset will not appear in the user\u2019s search results or show on their dataset listing page. Any direct links to this dataset that the user attempts to open will show a 404 page.</p>"},{"location":"teams/roles_and_permissions/#can-view","title":"Can view \u00b6","text":"<p>A user with Can view permissions on a dataset can find the dataset from their dataset listing page.</p> <p>Users with Can view permissions cannot modify the dataset in any way, for example by adding or removing samples, tags, annotation runs, brain runs, etc.</p> <p>Note</p> <p>Members (but not Guests or Collaborators) with Can view access to a dataset may clone the dataset.</p>"},{"location":"teams/roles_and_permissions/#can-tag","title":"Can tag \u00b6","text":"<p>A user with Can tag permissions on a dataset can find the dataset from their dataset listing page.</p> <p>Users with Can tag permissions can modify sample/label tags but cannot modify the dataset in any other way.</p>"},{"location":"teams/roles_and_permissions/#can-edit","title":"Can edit \u00b6","text":"<p>A user with Can edit permissions on a dataset has all permissions from Can view and, in addition, can modify the dataset, including:</p> <ul> <li> <p>Adding, editing, and deleting samples</p> </li> <li> <p>Adding, editing, and deleting tags</p> </li> <li> <p>Adding and deleting annotation runs, brain runs, etc.</p> </li> </ul> <p>Note</p> <p>Deleting a dataset requires the Can manage permission.</p>"},{"location":"teams/roles_and_permissions/#can-manage","title":"Can manage \u00b6","text":"<p>A user with Can manage permissions on a dataset has all permissions from Can view, Can tag and Can edit and, in addition, can delete the dataset and configure the permissions on the dataset of other users.</p> <p>Remember that all admins can implicitly access and manage all datasets created on your team\u2019s deployment.</p> <p>Note</p> <p>Any member who creates a dataset (including cloning an existing dataset or view) will be granted Can manage permissions on the new dataset.</p>"},{"location":"teams/roles_and_permissions/#roles-page","title":"Roles page \u00b6","text":"<p>Admins can review the actions and permissions available to each user role by navigating to the \u201cSettings &gt; Security &gt; Roles\u201d page:</p> <p></p>"},{"location":"teams/secrets/","title":"FiftyOne Teams Secrets \u00b6","text":"<p>FiftyOne Teams provides a Secrets interface for storing sensitive information such as API tokens and login credentials in a secure manner for use by your Team\u2019s plugins.</p> <p>Managing secrets through the Teams App is a straightforward and secure way to configure connections to and integrations with external services and APIs such as GCP, OpenAI, CVAT, etc without the need to change the configuration or environment variables of your FiftyOne Teams containers and restarting them. Instead, you can simply add or remove secrets using the Teams UI and they will immediately be available to any plugins that require them.</p>"},{"location":"teams/secrets/#adding-secrets","title":"Adding secrets \u00b6","text":"<p>Admins can add, configure, and remove secrets in the FiftyOne Teams App by navigating to the Secrets Management page under Settings &gt; Secrets:</p> <p></p> <p>When you tap on the \u201cAdd secret\u201d button, you will see that a secret is comprised of a key, value, and optional description:</p> <p></p> <p>Secret keys must be upper snake case strings like <code>MY_SECRET_KEY</code>.</p> <p>Secret values are stored encrypted in the database and are only available to and decrypted at runtime by internal services that have access to your encryption key.</p> <p>Note</p> <p>Only Admins have access to the Secrets page. However, once added, any App component or plugin requiring secret values can access them via the Secrets interface.</p>"},{"location":"teams/secrets/#using-secrets","title":"Using secrets \u00b6","text":"<p>In order to access secrets, plugins must declare the secrets that they may use by adding them to the plugin\u2019s <code>fiftyone.yml</code> file.</p> <p>For example, the @voxel51/annotation plugin declares the following secrets:</p> <pre><code>secrets:\n  - FIFTYONE_CVAT_URL\n  - FIFTYONE_CVAT_USERNAME\n  - FIFTYONE_CVAT_PASSWORD\n  - FIFTYONE_CVAT_EMAIL\n  - FIFTYONE_LABELBOX_URL\n  - FIFTYONE_LABELBOX_API_KEY\n  - FIFTYONE_LABELSTUDIO_URL\n  - FIFTYONE_LABELSTUDIO_API_KEY\n</code></pre> <p>At runtime, the plugin\u2019s execution context will automatically be hydrated with any available secrets that are declared by the plugin. Operators access these secrets via the <code>ctx.secrets</code> dict:</p> <pre><code>def execute(self, ctx):\n    url = ctx.secrets[\"FIFTYONE_CVAT_URL\"]\n    username = ctx.secrets[\"FIFTYONE_CVAT_USERNAME\"]\n    password = ctx.secrets[\"FIFTYONE_CVAT_PASSWORD\"]\n    email = ctx.secrets[\"FIFTYONE_CVAT_EMAIL\"]\n</code></pre> <p>The <code>ctx.secrets</code> dict will also be automatically populated with the values of any environment variables whose name matches a secret key declared by a plugin. Therefore, a plugin written using the above pattern can run in all of the following environments with no code changes:</p> <ul> <li> <p>A FiftyOne Teams deployment that uses the Secrets interface</p> </li> <li> <p>A FiftyOne Teams deployment that injects secrets directly as environment variables</p> </li> <li> <p>A locally launched App via the Teams SDK</p> </li> <li> <p>Open source FiftyOne</p> </li> </ul>"},{"location":"teams/teams_app/","title":"FiftyOne Teams App \u00b6","text":"<p>The FiftyOne Teams App allows you to visualize, browse, and interact with your individual datasets like you can with the FiftyOne App, but with expanded features for organizing, permissionsing, versioning, and sharing your team\u2019s datasets, all from a centralized web portal.</p> <p>This page provides a brief overview of some features available only in the FiftyOne Teams App.</p>"},{"location":"teams/teams_app/#the-homepage","title":"The homepage \u00b6","text":"<p>When you login to the FiftyOne Teams App, you\u2019ll land on the homepage pictured below.</p> <p>In the top bar of this page, on the left side, the gray number next to \u201cAll datasets\u201d indicates the total number of datasets that you have access to. If there are more than 20 datasets, you can use the \u201cPrevious\u201d and \u201cNext\u201d buttons at the bottom of the page to see different batches of datasets.</p> <p>Note</p> <p>You can return to the homepage from any page of the Teams App by clicking on the Voxel51 logo in the upper left corner.</p> <p></p>"},{"location":"teams/teams_app/#pinned-datasets","title":"Pinned datasets \u00b6","text":"<p>You can pin datasets for easy access by hovering over the dataset\u2019s name in the main table and clicking the pin icon.</p> <p>The \u201cYour pinned datasets\u201d widget on the right-hand side of the hompage shows your pinned datasets at a glance and allows you to quickly open one by clicking on its name. Pinned datasets are listed in reverse chronological order (most recently pinned on top).</p> <p>To unpin a dataset, click the pin icon next to the dataset name in the \u201cYour pinned datasets\u201d widget or the pin next to the dataset\u2019s name in the main table.</p> <p></p>"},{"location":"teams/teams_app/#sorting-datasets","title":"Sorting datasets \u00b6","text":"<p>You can use the drop-down menu in the upper left of the main table to sort your datasets by various criteria, including size, creation date, recently used, and alphabetically by name:</p> <p></p>"},{"location":"teams/teams_app/#filtering-datasets","title":"Filtering datasets \u00b6","text":"<p>You can use the search bar (with the magnifying glass icon) in the upper right corner of the dataset table to filter datasets by name, tags, and media type:</p> <p></p> <p>By default, datasets that match across any supported field are returned, but you can narrow the search to specific fields by selecting the relevant option in the search dropdown:</p> <p></p>"},{"location":"teams/teams_app/#creating-datasets","title":"Creating datasets \u00b6","text":"<p>To create a new dataset, click on the \u201cNew dataset\u201d button in the upper right corner of the homepage. A pop-up will appear allowing you to choose a name, description, and tags for the dataset:</p> <ul> <li> <p>Name: as you\u2019re typing a name for your dataset, a URL will appear below denoting the address at which the dataset will be accessible. If the name or URL is not available, you will be prompted to try another name.</p> </li> <li> <p>Description: an optional free text description that you can use to store relevant information about your dataset.</p> </li> <li> <p>Tags: an optional list of tag(s) for your dataset. For example, you may want to record the media type, task type, project name, or other pertinent information. To add a tag, type it in the text bar. If you have previously used a tag, it will automatically appear in a dropdown and you can select it. To add a new tag, type tab or comma.</p> </li> </ul> <p>Note</p> <p>A dataset\u2019s name, description, and tags can be edited later from the dataset\u2019s Manage tab.</p> <p></p> <p>Note</p> <p>What next? Use the Teams Python SDK to upload new samples, labels, and metadata to your dataset. A common approach is to automate this process via cloud functions.</p>"},{"location":"teams/teams_app/#using-a-dataset","title":"Using a dataset \u00b6","text":"<p>Click on a dataset from the homepage to open the dataset\u2019s \u201cSamples\u201d tab.</p> <p>From the Samples tab you can visualize, tag, filter, and explore your dataset just as you would via the FiftyOne App.</p> <p></p> <p>Note</p> <p>Did you know? You can also navigate directly to a dataset of interest by pasting its URL into your browser\u2019s URL bar.</p>"},{"location":"teams/teams_app/#managing-a-dataset","title":"Managing a dataset \u00b6","text":"<p>The FiftyOne Teams App provides a number of options for managing existing datasets, as described below.</p> <p>You can access these options from the Samples tab by clicking on the \u201cManage\u201d tab in the upper left corner of the page.</p> <p>You can also directly navigate to this page from the homepage by clicking the three dots on the right hand side of a row of the dataset listing table and selecting \u201cEdit dataset\u201d.</p> <p>Note</p> <p>Did you know? You can also use the Teams SDK to programmatically, create, edit, and delete datasets.</p>"},{"location":"teams/teams_app/#basic-info","title":"Basic info \u00b6","text":"<p>The \u201cBasic info\u201d tab is accessible to all users with Can view access to the dataset.</p> <p>Users with Can manage permissions on the dataset can edit the name, description, and tags of a dataset from this page.</p> <p>Additionally, members can create a copy of the dataset by clicking on the \u201cClone this dataset\u201d button.</p> <p></p>"},{"location":"teams/teams_app/#access","title":"Access \u00b6","text":"<p>The \u201cAccess\u201d tab is only accessible to users with Can manage permissions on the dataset.</p> <p>From this tab, users can add, remove, edit, or invite users to the dataset. Refer to this page for more information about the available dataset-level permissions that you can grant.</p> <p></p>"},{"location":"teams/teams_app/#danger-zone","title":"Danger zone \u00b6","text":"<p>The \u201cDanger zone\u201d tab is only accessible to users with Can manage permissions on the dataset.</p> <p>From this tab, you can select \u201cDelete entire dataset\u201d to permanently delete a dataset from your Teams deployment. You must type the dataset\u2019s full name in the modal to confirm this action.</p> <p></p> <p>Warning</p> <p>Deleting a dataset is permanent!</p>"},{"location":"teams/teams_plugins/","title":"FiftyOne Teams Plugins \u00b6","text":"<p>FiftyOne Teams provides native support for installing and running FiftyOne plugins, which offers powerful opportunities to extend and customize the functionality of your Teams deployment to suit your needs.</p> <p>Note</p> <p>What can you do with plugins? Check out delegated operations to see some quick examples, then check out the FiftyOne plugins repository for a growing collection of prebuilt plugins that you can add to your Teams deployment!</p>"},{"location":"teams/teams_plugins/#plugins-page","title":"Plugins page \u00b6","text":"<p>Admins can use the plugins page to upload, manage, and configure permissions for plugins that are made available to users of your Teams deployment.</p> <p>Admins can access the plugins page under Settings &gt; Plugins. It displays a list of all installed plugins and their operators, as well as the enablement and permissions of each.</p> <p></p>"},{"location":"teams/teams_plugins/#installing-a-plugin","title":"Installing a plugin \u00b6","text":"<p>Admins can install plugins via the Teams UI or Management SDK.</p> <p>Note</p> <p>A plugin is a directory (or ZIP of it) that contains a top-level <code>fiftyone.yml</code> file.</p>"},{"location":"teams/teams_plugins/#teams-ui","title":"Teams UI \u00b6","text":"<p>To install a plugin, click the \u201cInstall plugin\u201d button on the plugins page.</p> <p></p> <p>Then upload or drag and drop the plugin contents as a ZIP file and click install.</p> <p></p> <p>You should then see a success message and the newly installed plugin listed on the plugins page.</p> <p></p>"},{"location":"teams/teams_plugins/#sdk","title":"SDK \u00b6","text":"<p>Admins can also use the <code>upload_plugin()</code> method from the Management SDK:</p> <pre><code>import fiftyone.management as fom\n\n# You can pass the directory or an already zipped version of it\nfom.upload_plugin(\"/path/to/plugin_dir\")\n</code></pre>"},{"location":"teams/teams_plugins/#upgrading-a-plugin","title":"Upgrading a plugin \u00b6","text":"<p>Admins can upgrade plugins at any time through the Teams UI or Management SDK.</p>"},{"location":"teams/teams_plugins/#teams-ui_1","title":"Teams UI \u00b6","text":"<p>To upgrade a plugin, click the plugin\u2019s dropdown and select \u201cUpgrade plugin\u201d.</p> <p></p> <p>Then upload or drag and drop the upgraded plugin as a ZIP file and click upgrade.</p> <p></p> <p>Note</p> <p>If the <code>name</code> attribute within the uploaded plugin\u2019s <code>fiftyone.yml</code> file doesn\u2019t match the existing plugin, a new plugin will be created. Simply delete the old one.</p> <p>You should then see a success message and the updated information about the plugin on the plugins page.</p> <p></p>"},{"location":"teams/teams_plugins/#sdk_1","title":"SDK \u00b6","text":"<p>Admins can also use the <code>upload_plugin()</code> method from the Management SDK with the <code>overwrite=True</code> option:</p> <pre><code>import fiftyone.management as fom\n\n# You can pass the directory or an already zipped version of it\nfom.upload_plugin(\"/path/to/plugin_dir\", overwrite=True)\n</code></pre>"},{"location":"teams/teams_plugins/#uninstalling-a-plugin","title":"Uninstalling a plugin \u00b6","text":"<p>Admins can uninstall plugins at any time through the Teams UI or Management SDK.</p> <p>Note</p> <p>Did you know? You can enable/disable plugins rather than permanently uninstalling them.</p>"},{"location":"teams/teams_plugins/#teams-ui_2","title":"Teams UI \u00b6","text":"<p>To uninstall a plugin, click the plugin\u2019s dropdown and select \u201cUninstall plugin\u201d.</p> <p></p>"},{"location":"teams/teams_plugins/#sdk_2","title":"SDK \u00b6","text":"<p>Admins can also use the <code>delete_plugin()</code> method from the Management SDK:</p> <pre><code>import fiftyone.management as fom\n\nfom.delete_plugin(plugin_name)\n</code></pre>"},{"location":"teams/teams_plugins/#enablingdisabling-plugins","title":"Enabling/disabling plugins \u00b6","text":""},{"location":"teams/teams_plugins/#teams-ui_3","title":"Teams UI \u00b6","text":"<p>When plugins are first installed into Teams, they are enabled by default, along with any operators they contain.</p> <p>Admins can enable/disable a plugin and all of its operators by toggling the enabled/disabled switch.</p> <p></p> <p>Admins can also disable/enable specific operators within an (enabled) plugin by clicking on the plugin\u2019s operators link.</p> <p></p> <p>and then toggling the enabled/disabled switch for each operator as necessary.</p> <p></p>"},{"location":"teams/teams_plugins/#sdk_3","title":"SDK \u00b6","text":"<p>Admins can also use the <code>set_plugin_enabled()</code> and <code>set_plugin_operator_enabled()</code> methods from the management SDK:</p> <pre><code>import fiftyone.management as fom\n\n# Disable a plugin\nfom.set_plugin_enabled(plugin_name, False)\n\n# Disable a particular operator\nfom.set_plugin_operator_enabled(plugin_name, operator_name, False)\n</code></pre>"},{"location":"teams/teams_plugins/#plugin-permissions","title":"Plugin permissions \u00b6","text":"<p>Admins can optionally configure access to plugins and individual operators within them via any combination of the permissions described below:</p> Permission Description Minimum Role The minimum role a user must have to execute the operation. Minimum Dataset Permission The minimum dataset permission a user must have to perform the operationon a particular dataset."},{"location":"teams/teams_plugins/#teams-ui_4","title":"Teams UI \u00b6","text":"<p>To configure the permissions for an operator, first click on the plugin\u2019s operators link.</p> <p></p> <p>Then change the dropdown for the operator to reflect the desired permission level.</p> <p> </p>"},{"location":"teams/teams_plugins/#sdk_4","title":"SDK \u00b6","text":"<p>Admins can also use the <code>set_plugin_operator_permissions()</code> method from the Management SDK:</p> <pre><code>import fiftyone.management as fom\n\n# Set minimum role permission only\nfom.set_plugin_operator_enabled(\n    plugin_name,\n    operator_name,\n    minimum_role=fom.MEMBER,\n)\n\n# Set minimum dataset permission only\nfom.set_plugin_operator_enabled(\n    plugin_name,\n    operator_name,\n    minimum_dataset_permission=fom.EDIT,\n)\n\n# Set both minimum role and minimum dataset permissions\nfom.set_plugin_operator_enabled(\n    plugin_name,\n    operator_name,\n    minimum_role=fom.EDIT,\n    minimum_dataset_permission=fom.EDIT,\n)\n</code></pre>"},{"location":"teams/teams_plugins/#default-permissions","title":"Default permissions \u00b6","text":"<p>When new plugins are installed, any operators they contain are initialized with the default permissions for your deployment.</p> <p>By default, the initial permissions are:</p> Permission Default Minimum Role Member Minimum Dataset Permission Edit"},{"location":"teams/teams_plugins/#teams-ui_5","title":"Teams UI \u00b6","text":"<p>Default operator permissions can be configured by navigating to the page at Settings &gt; Security and looking under the Plugins header. Click the dropdown for the permission you want to change and select the new value.</p> <p></p>"},{"location":"teams/teams_plugins/#sdk_5","title":"SDK \u00b6","text":"<p>Admins can also use the <code>set_organization_settings()</code> method from the Management SDK:</p> <pre><code>import fiftyone.management as fom\n\nfom.set_organization_settings(\n    default_operator_minimum_role=fom.MEMBER,\n    default_operator_minimum_dataset_permission=fom.EDIT,\n)\n</code></pre>"},{"location":"teams/teams_plugins/#delegated-operations","title":"Delegated operations \u00b6","text":"<p>Delegated operations are a powerful feature of FiftyOne\u2019s plugin framework that allows users to schedule tasks from within the App that are executed in the background on a connected compute cluster.</p> <p>With FiftyOne Teams, your team can upload and permission custom operations that your users can execute from the Teams App, all of which run against a central orchestrator configured by your admins.</p> <p>Why is this awesome? Your AI stack needs a flexible data-centric component that enables you to organize and compute on your data. With delegated operations, FiftyOne Teams becomes both a dataset management/visualization tool and a workflow automation tool that defines how your data-centric workflows like ingestion, curation, and evaluation are performed. In short, think of FiftyOne Teams as the single source of truth on which you co-develop your data and models together.</p> <p>What can delegated operations do for you? Get started by installing any of these plugins available in the FiftyOne Plugins repository:</p> @voxel51/annotation \u270f\ufe0f Utilities for integrating FiftyOne with annotation tools @voxel51/brain \ud83e\udde0 Utilities for working with the FiftyOne Brain @voxel51/evaluation \u2705 Utilities for evaluating models with FiftyOne @voxel51/io \ud83d\udcc1 A collection of import/export utilities @voxel51/indexes \ud83d\udcc8 Utilities working with FiftyOne database indexes @voxel51/utils \u2692\ufe0f Call your favorite SDK utilities from the App @voxel51/voxelgpt \ud83e\udd16 An AI assistant that can query visual datasets, search the FiftyOne docs, and answer general computer vision questions @voxel51/zoo \ud83c\udf0e Download datasets and run inference with models from the FiftyOne Zoo, all without leaving the App <p>For example, wish you could import data from within the App? With the @voxel51/io, plugin you can!</p> <p></p> <p>Want to send data for annotation from within the App? Sure thing, just install the @voxel51/annotation plugin:</p> <p></p> <p>Have model predictions on your dataset that you want to evaluate? The @voxel51/evaluation plugin makes it easy:</p> <p></p> <p>Need to compute embedding for your dataset so you can visualize them in the Embeddings panel? Kick off the task with the @voxel51/brain plugin and proceed with other work while the execution happens in the background:</p> <p></p> <p>When you choose delegated execution in the App, these tasks are automatically scheduled for execution on your connected orchestrator and you can continue with other work. Meanwhile, all datasets have a Runs tab in the App where you can browse a history of all delegated operations that have been run on the dataset and their status.</p>"},{"location":"teams/teams_plugins/#configuring-your-orchestrators","title":"Configuring your orchestrator(s) \u00b6","text":"<p>FiftyOne Teams offers a builtin orchestrator that is configured as part of your team\u2019s deployment with a default level of compute capacity.</p> <p>It is also possible to connect your FiftyOne Teams deployment to an externally managed workflow orchestration tool ( Airflow, Flyte, Spark, etc).</p> <p>Note</p> <p>Contact your Voxel51 support team to scale your deployment\u2019s compute capacity or if you\u2019d like to use an external orchestrator.</p>"},{"location":"teams/teams_plugins/#managing-delegated-operations","title":"Managing delegated operations \u00b6","text":"<p>Every dataset in FiftyOne Teams has a Runs page that allows users with access to monitor and explore delegated operations scheduled against that dataset.</p> <p>All scheduled operations are maintained in a queue and will be automatically executed as resources are available on the targeted orchestrator.</p> <p>Note</p> <p>The Runs page only tracks operations that are scheduled for delegated execution, not operations that are executed immediately in the App.</p>"},{"location":"teams/teams_plugins/#runs-page","title":"Runs page \u00b6","text":"<p>The Runs page is accessible to all users with Can view access to the dataset.</p> <p>You can access the Runs page by clicking on the \u201cRuns\u201d tab from the Samples tab.</p> <p>Once you are on the Runs page, you will see a table with the list of all operators scheduled by any user of your organization on the dataset. You can sort, search and filter runs listed to refine the list as you like:</p> <p></p>"},{"location":"teams/teams_plugins/#sorting","title":"Sorting \u00b6","text":"<p>By default, the runs table is sorted by recency, but you can use the dropdown menu in the upper left of table to sort by other fields like update time or the name of the operator:</p> <p></p>"},{"location":"teams/teams_plugins/#filtering","title":"Filtering \u00b6","text":"<p>You can also filter the runs table to see a subset of runs.</p> <p>Use the \u201cMy runs\u201d radio button to see only the runs that you scheduled:</p> <p></p> <p>You can further refine the list of runs using the status dropdown to select one or more status you would like to filter by:</p> <p></p>"},{"location":"teams/teams_plugins/#searching","title":"Searching \u00b6","text":"<p>You can also use the search functionality to filter the list of runs by keyword. As you type your query in the search box, the list of runs will be updated to show only the runs matching your query:</p> <p></p> <p>Note</p> <p>Search is case-sensitive and you can currently only search by operator name, not label. For example, searches will not match against Demo: Export to GCP in the image above.</p>"},{"location":"teams/teams_plugins/#re-running","title":"Re-running \u00b6","text":"<p>From the Runs page, you can trigger a re-run of any listed run by clicking the three-dots to open actions menu and then clicking \u201cRe-run\u201d:</p> <p></p>"},{"location":"teams/teams_plugins/#pinning","title":"Pinning \u00b6","text":"<p>Pinned runs are displayed to the right of the runs table. By default, five pinned runs will be displayed. However, if there are more than five pinned runs, you will see a button to expand the list.</p> <p>To pin a run, hover over its row in the runs table and click the pin icon that appears beside the operator label:</p> <p></p> <p>Note</p> <p>Pinned runs are stored at the dataset-level and will be visible to all users with access to the dataset.</p>"},{"location":"teams/teams_plugins/#renaming","title":"Renaming \u00b6","text":"<p>When delegating an operator multiple times on the same dataset, you may wish to give the runs custom labels so that you can easily identify each run later.</p> <p>To edit the label of an operator run, move your mouse cursor over the label of interest and click the pencil button as indicated by \u201c1\u201d below. This will present an input field indicated by \u201c2\u201d where you can update label to text of your choice. Once you are ready to apply changes, click the save button indicated by \u201c3\u201d.</p> <p></p>"},{"location":"teams/teams_plugins/#mark-as-failed","title":"Mark as failed \u00b6","text":"<p>If a delegated operation run terminates unexpectedly without reporting failure, you can manually mark it as failed from the Runs page.</p> <p>To mark a run as failed, click the three dots indicated by \u201c1\u201d. Then, in the menu, click \u201cMark as failed\u201d as indicated by \u201c2\u201d. The run status will be updated and will now display as failed.</p> <p></p> <p>Note</p> <p>If the delegated operation is, in fact, still in progress in your orchestrator, marking the run as failed will not terminate the execution of operation.</p>"},{"location":"teams/teams_plugins/#monitoring-progress","title":"Monitoring progress \u00b6","text":"<p>Delegated operations can optionally report their progress during execution.</p> <p>If a progress is available for a run, it will be displayed in the Runs table as indicated by \u201c2\u201d. By default, the progress of running operations is automatically refreshed. You can disable auto-refresh of running operations by toggling the auto refresh setting indicated by \u201c1\u201d.</p> <p></p> <p>Note</p> <p>Only the progress of running operations is automatically refreshed.</p>"},{"location":"teams/teams_plugins/#run-page","title":"Run page \u00b6","text":"<p>The Run page allows you to see information about a specific run such as inputs, outputs, and errors.</p> <p>You can visit the Run page for a run by clicking on a run in the runs table, the Pinned runs, or Recent runs widgets.</p>"},{"location":"teams/teams_plugins/#input","title":"Input \u00b6","text":"<p>The Input tab on the Run page lets you see the input parameters that were provided when the run was scheduled:</p> <p></p> <p>Raw input</p> <p>By default, a rendered version (similar to what is displayed when invoking an operator) of input parameters is displayed. However, you can switch to raw view by clicking the \u201cShow raw\u201d toggle button:</p> <p></p>"},{"location":"teams/teams_plugins/#output","title":"Output \u00b6","text":"<p>The Output tab on the Run page lets you see the preview of the result of a completed run:</p> <p>Note</p> <p>Output tab is only available for completed run.</p> <p></p>"},{"location":"teams/teams_plugins/#errors","title":"Errors \u00b6","text":"<p>The Errors tab on the Run page will appear if the run failed and lets you see the errors that occurred:</p> <p></p>"},{"location":"teams/teams_plugins/#view","title":"View \u00b6","text":"<p>The View tab on the Run page lets you see the dataset view on which the run was scheduled:</p> <p></p>"},{"location":"tutorials/","title":"FiftyOne Tutorials \u00b6","text":"<p>Each tutorial below is a curated demonstration of how FiftyOne can help refine your datasets and turn your good models into great models.</p>"},{"location":"tutorials/#pandas-style-queries-in-fiftyone","title":"pandas-style queries in FiftyOne","text":"<p>Translate your pandas knowledge to FiftyOne. This tutorial gives a side-by-side comparison of performing common operations in pandas and FiftyOne.</p> <p>Filtering,Dataset-Evaluation</p> <p></p>"},{"location":"tutorials/#evaluating-object-detections","title":"Evaluating object detections","text":"<p>Aggregate statistics aren't sufficient for object detection. This tutorial shows how to use FiftyOne to perform powerful evaluation workflows on your detector.</p> <p>Model-Evaluation</p> <p></p>"},{"location":"tutorials/#evaluating-a-classifier","title":"Evaluating a classifier","text":"<p>Evaluation made easy. This tutorial walks through an end-to-end example of fine-tuning a classifier and understanding its failure modes using FiftyOne.</p> <p>Model-Evaluation</p> <p></p>"},{"location":"tutorials/#using-image-embeddings","title":"Using image embeddings","text":"<p>Visualize your data in new ways. This tutorial shows how to use FiftyOne's powerful embeddings visualization capabilities to improve your image datasets.</p> <p>Visualization,Brain,Embeddings</p> <p></p>"},{"location":"tutorials/#annotating-with-cvat","title":"Annotating with CVAT","text":"<p>So you've loaded and explored your data in FiftyOne... but now what? See how to send it off to CVAT for annotation in just one line of code.</p> <p>Annotation,Dataset-Evaluation</p> <p></p>"},{"location":"tutorials/#annotating-with-labelbox","title":"Annotating with Labelbox","text":"<p>Unlock the power of the Labelbox platform. See how you can get your FiftyOne datasets annotated with just one line of code.</p> <p>Annotation,Dataset-Evaluation</p> <p></p>"},{"location":"tutorials/#training-with-detectron2","title":"Training with Detectron2","text":"<p>Put your FiftyOne datasets to work and learn how to train and evaluate Detectron2 models directly on your data.</p> <p>Model-Training,Model-Evaluation</p> <p></p>"},{"location":"tutorials/#downloading-and-evaluating-open-images","title":"Downloading and evaluating Open Images","text":"<p>Expand your data lake and evaluate your object detection models with Google's Open Images dataset and evaluation protocol, all natively within FiftyOne.</p> <p>Dataset-Evaluation,Model-Evaluation,Dataset-Zoo</p> <p></p>"},{"location":"tutorials/#exploring-image-uniqueness","title":"Exploring image uniqueness","text":"<p>Your models need diverse data. This tutorial shows how FiftyOne can remove near-duplicate images and recommend unique samples for model training.</p> <p>Dataset-Evaluation,Brain</p> <p></p>"},{"location":"tutorials/#finding-classification-mistakes","title":"Finding classification mistakes","text":"<p>Better models start with better data. This tutorial shows how FiftyOne can automatically find label mistakes in your classification datasets.</p> <p>Dataset-Evaluation,Brain</p> <p></p>"},{"location":"tutorials/#finding-detection-mistakes","title":"Finding detection mistakes","text":"<p>How good are your ground truth objects? Use the FiftyOne Brain's mistakenness feature to find annotation errors in your object detections.</p> <p>Dataset-Evaluation,Brain</p> <p></p>"},{"location":"tutorials/#nearest-neighbor-embeddings-classification-with-qdrant","title":"Nearest Neighbor Embeddings Classification with Qdrant","text":"<p>Easily pre-annotate your FiftyOne datasets using approximate nearest neighbors search on embeddings with Qdrant.</p> <p>Dataset-Evaluation,Model-Evaluation,Embeddings</p> <p></p>"},{"location":"tutorials/#fine-tuning-yolov8-model-predictions","title":"Fine-tuning YOLOv8 model predictions","text":"<p>Visualize and evaluate YOLOv8 model predictions before fine-tuning for your custom use case.</p> <p>Model-Evaluation</p> <p></p>"},{"location":"tutorials/#build-3d-point-cloud-datasets-with-point-e","title":"Build 3D point cloud datasets with Point-E","text":"<p>Lidar is expensive. This tutorial shows how FiftyOne can help you construct high quality 3D point cloud datasets using Point-E point cloud models.</p> <p>Dataset-Curation,Filtering,Visualization</p> <p></p>"},{"location":"tutorials/#monocular-depth-estimation-with-hugging-face","title":"Monocular Depth Estimation with Hugging Face","text":"<p>Metrics for monocular depth estimation can be deceiving. Run MDE models on your data and visualize their predictions with FiftyOne.</p> <p>Model-Evaluation,Visualization</p> <p></p>"},{"location":"tutorials/#visualizing-data-with-dimensionality-reduction","title":"Visualizing Data with Dimensionality Reduction","text":"<p>Compare and contrast dimensionality reduction techniques for visualizing your data in FiftyOne.</p> <p>Brain,Visualization</p> <p></p>"},{"location":"tutorials/#zero-shot-image-classification","title":"Zero-Shot Image Classification","text":"<p>Run and evaluate zero-shot image classification models with OpenCLIP, Hugging Face Transformers, and FiftyOne.</p> <p>Filtering,Model-Evaluation,Model-Zoo</p> <p></p>"},{"location":"tutorials/#augmenting-datasets-with-albumentations","title":"Augmenting Datasets with Albumentations","text":"<p>Learn how to apply and test out different augmentations on your datasets using FiftyOne and Albumentations.</p> <p>App,Dataset-Curation,Visualization</p> <p></p>"},{"location":"tutorials/#clustering-images-with-embeddings","title":"Clustering Images with Embeddings","text":"<p>Use embeddings to cluster images in your dataset and visualize the results in FiftyOne.</p> <p>App,Brain,Dataset-Curation,Embeddings,Visualization</p> <p></p>"},{"location":"tutorials/#small-object-detection-with-sahi","title":"Small Object Detection with SAHI","text":"<p>Detect small objects in your images with Slicing-Aided Hyper-Inference (SAHI) and FiftyOne.</p> <p>Model-Evaluation,Model-Zoo</p> <p></p>"},{"location":"tutorials/#anomaly-detection-with-anomalib","title":"Anomaly Detection with Anomalib","text":"<p>Detect anomalies in your images with Anomalib and FiftyOne.</p> <p>Embeddings,Model-Evaluation,Model-Training,Visualization</p> <p></p> <p>Note</p> <p>Check out the fiftyone-examples repository for more examples of using FiftyOne!</p>"},{"location":"tutorials/anomaly_detection/","title":"Anomaly Detection with FiftyOne and Anomalib","text":"<p>Anomaly detection (AD) is a crucial task in mission-critical applications such as fraud detection, network security, and medical diagnosis. Anomaly detection on visual data like images, videos, and satellite imagery, is a particularly challenging task due to the high dimensionality of the data and the complexity of the underlying patterns. Yet visual anomaly detection is essential for detecting defects in manufacturing, identifying suspicious activity in surveillance footage, and detecting abnormalities in medical images.</p> <p>In this walkthrough, you'll learn how to perform anomaly detection on visual data using FiftyOne and Anomalib from the OpenVINO\u2122 toolkit. We'll use the MVTec AD dataset for demonstration, which contains images of various objects with anomalies like scratches, dents, and holes.</p> <p>The notebook covers the following:</p> <ul> <li>What is visual anomaly detection?</li> <li>Loading the MVTec AD dataset in FiftyOne</li> <li>Training an anomaly detection model with Anomalib</li> <li>Evaluating anomaly detection models in FiftyOne</li> </ul> <p>Make sure you are running this in a virtual environment with <code>python=3.10</code>.</p> <pre>conda create -n anomalib_env python=3.10; conda activate anomalib_env\n</pre> <p>Anomalib requires Python 3.10, so make sure you have the correct version installed.</p> <p>After this, install Anomalib and its dependencies. If you're running this in a colab notebook, the installation might take a few minutes, but local installation should be faster.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -U torchvision einops FrEIA timm open_clip_torch imgaug lightning kornia openvino git+https://github.com/openvinotoolkit/anomalib.git\n</pre> !pip install -U torchvision einops FrEIA timm open_clip_torch imgaug lightning kornia openvino git+https://github.com/openvinotoolkit/anomalib.git <p>Install Anomalib from source, per the instructions in the Anomalib README</p> <p>If you don't have it already installed, install FiftyOne. Make sure your version is <code>fiftyone&gt;=0.23.8</code> so we can use the Hugging Face Hub integration to load the MVTec AD dataset:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -U fiftyone\n</pre> !pip install -U fiftyone <p>Just a few more packages to install, and we're ready to go. Now you can see why we recommend using a virtual environment for this project!</p> <ul> <li><code>huggingface_hub</code> for loading the MVTec AD dataset</li> <li><code>clip</code> for computing image embeddings</li> <li><code>umap-learn</code> for dimensionality reduction</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!pip install -U huggingface_hub umap-learn git+https://github.com/openai/CLIP.git\n</pre> !pip install -U huggingface_hub umap-learn git+https://github.com/openai/CLIP.git <p>Now let's import all of the relevant modules we will need from FiftyOne:</p> In\u00a0[1]: Copied! <pre>import fiftyone as fo # base library and app\nimport fiftyone.brain as fob # ML methods\nimport fiftyone.zoo as foz # zoo datasets and models\nfrom fiftyone import ViewField as F # helper for defining views\nimport fiftyone.utils.huggingface as fouh # Hugging Face integration\n</pre> import fiftyone as fo # base library and app import fiftyone.brain as fob # ML methods import fiftyone.zoo as foz # zoo datasets and models from fiftyone import ViewField as F # helper for defining views import fiftyone.utils.huggingface as fouh # Hugging Face integration <p>And load the MVTec AD dataset from the Hugging Face Hub:</p> In\u00a0[\u00a0]: Copied! <pre>dataset = fouh.load_from_hub(\"Voxel51/mvtec-ad\", persistent=True, overwrite=True)\n</pre> dataset = fouh.load_from_hub(\"Voxel51/mvtec-ad\", persistent=True, overwrite=True) <p>\ud83d\udca1 It is also possible to load the MVTec AD data directly from Anomalib:</p> <pre>from anomalib.data import MVTec\ndatamodule = MVTec()\n</pre> <p>But this way we have all of the metadata and annotations in FiftyOne, which is useful for visualization and evaluation.</p> <p>Before moving on, let's take a look at the dataset in the FiftyOne App:</p> In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) <p></p> <p>The dataset has 5354 images across 12 object categories. Each category has \"good\" images and \"anomalous\" images with defects like scratches, dents, and holes. Each of the anomalous samples also has a mask which localizes the defective regions of the image.</p> <p>The defect labels differ across categories, which is typical in real-world anomaly detection scenarios. In typical anomaly detection scenarios, you train a different model for each category. Here we'll go through the process for one category, and you can apply the same steps to other categories.</p> <p>One more thing to note is that the dataset is split into training and test sets. The training set contains only \"good\" images, while the test set contains both \"good\" and \"anomalous\" images.</p> <p>Before we train a model, let's dig into the dataset a bit more. We can get a feel for the structure and patterns hidden in our data by computing image embeddings and visualizing them in a lower-dimensional space. First, we'll compute embeddings for all the images in the dataset using the CLIP model:</p> In\u00a0[\u00a0]: Copied! <pre>model = foz.load_zoo_model(\n    \"clip-vit-base32-torch\"\n)  # load the CLIP model from the zoo\n\n# Compute embeddings for the dataset\ndataset.compute_embeddings(\n    model=model, embeddings_field=\"clip_embeddings\", batch_size=64\n)\n\n# Dimensionality reduction using UMAP on the embeddings\nfob.compute_visualization(\n    dataset, embeddings=\"clip_embeddings\", method=\"umap\", brain_key=\"clip_vis\"\n)\n</pre> model = foz.load_zoo_model(     \"clip-vit-base32-torch\" )  # load the CLIP model from the zoo  # Compute embeddings for the dataset dataset.compute_embeddings(     model=model, embeddings_field=\"clip_embeddings\", batch_size=64 )  # Dimensionality reduction using UMAP on the embeddings fob.compute_visualization(     dataset, embeddings=\"clip_embeddings\", method=\"umap\", brain_key=\"clip_vis\" ) <p>Refresh the FiftyOne App, click the \"+\" tab, and select \"Embeddings\". Choose \"all_clip_vis\" from the dropdown menu. You'll see a scatter plot of the image embeddings in a 2D space, where each point corresponds to a sample in the dataset. Using the color-by dropdown, notice how the embeddings cluster based on the object category. This is because CLIP encodes semantic information about the images. Also note that within a category, CLIP embeddings don't cluster based on the defect type.</p> <p></p> <p>If instead we embed our images using a traditional computer vision model like ResNet, we also see some clustering within a category based on the defect type. However, as we established earlier, we will not have access to defect labels during inference. Instead, we'll use an unsupervised anomaly detection model.</p> In\u00a0[\u00a0]: Copied! <pre>model = foz.load_zoo_model(\n    \"resnet50-imagenet-torch\"\n)  # load the ResNet50 model from the zoo\n\n# Compute embeddings for the dataset \u2014 this might take a while on a CPU\ndataset.compute_embeddings(model=model, embeddings_field=\"resnet50_embeddings\")\n\n# Dimensionality reduction using UMAP on the embeddings\nfob.compute_visualization(\n    dataset,\n    embeddings=\"resnet50_embeddings\",\n    method=\"umap\",\n    brain_key=\"resnet50_vis\",\n)\n</pre> model = foz.load_zoo_model(     \"resnet50-imagenet-torch\" )  # load the ResNet50 model from the zoo  # Compute embeddings for the dataset \u2014 this might take a while on a CPU dataset.compute_embeddings(model=model, embeddings_field=\"resnet50_embeddings\")  # Dimensionality reduction using UMAP on the embeddings fob.compute_visualization(     dataset,     embeddings=\"resnet50_embeddings\",     method=\"umap\",     brain_key=\"resnet50_vis\", ) <p></p> <p>\ud83d\udca1 For deep dives into embeddings and dimensionality reduction, check out our tutorials:</p> <ul> <li>Using Image Embeddings</li> <li>Visualizing Data with Dimensionality Reduction Techniques</li> </ul> <p>Now that we have a sense of the dataset, we're ready to train an anomaly detection model using Anomalib.</p> <p>Task: Anomalib supports classification, detection, and segmentation tasks for images. We'll focus on segmentation, where the model predicts whether each pixel in the image is anomalous or not, creating a mask that localizes the defect.</p> <p>Model: Anomalib supports a variety of anomaly detection algorithms, including Deep Feature Kernel Density Estimation (DFKDE), FastFlow, and Reverse Distillation. For a complete list of supported algorithms, check out Anomalib's reference guide. For this walkthrough, we'll use two algorithms:</p> <ul> <li>PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and Localization</li> <li>PatchCore: Towards Total Recall in Industrial Anomaly Detection</li> </ul> <p>Preprocessing: For this walkthrough, we will resize the images to 256x256 pixels before training the model. Adding this as a transform via Torchvision's <code>Resize</code> class lets us resize the images on-the-fly during training and inference.</p> <p>Import the necessary modules from Anomalib and helper modules for processing images and paths:</p> In\u00a0[15]: Copied! <pre>import numpy as np\nimport os\nfrom pathlib import Path\nfrom PIL import Image\nfrom torchvision.transforms.v2 import Resize\n</pre> import numpy as np import os from pathlib import Path from PIL import Image from torchvision.transforms.v2 import Resize In\u00a0[\u00a0]: Copied! <pre>from anomalib import TaskType\nfrom anomalib.data.image.folder import Folder\nfrom anomalib.deploy import ExportType, OpenVINOInferencer\nfrom anomalib.engine import Engine\nfrom anomalib.models import Padim, Patchcore\n</pre> from anomalib import TaskType from anomalib.data.image.folder import Folder from anomalib.deploy import ExportType, OpenVINOInferencer from anomalib.engine import Engine from anomalib.models import Padim, Patchcore <p>Now define some constants to use throughout the notebook.</p> <ul> <li><code>OBJECT</code>: The object category we'll focus on. For this walkthrough, we'll use \"bottle\". If you want to loop over categories, you can get the list of categories from the dataset with <code>dataset.distinct(\"category.label\")</code>.</li> <li><code>ROOT_DIR</code>: The root directory where Anomalib will look for images and masks. Our data is already stored on disk, so we will just symlink files to the directory Anomalib expects.</li> <li><code>TASK</code>: The task we're performing. We'll use \"segmentation\" for this walkthrough.</li> <li><code>IMAGE_SIZE</code>: The size to resize images to before training the model. We'll use $256$ x $256$ pixels.</li> </ul> In\u00a0[17]: Copied! <pre>OBJECT = \"bottle\" ## object to train on\nROOT_DIR = Path(\"/tmp/mvtec_ad\") ## root directory to store data for anomalib\nTASK = TaskType.SEGMENTATION ## task type for the model\nIMAGE_SIZE = (256, 256) ## preprocess image size for uniformity\n</pre> OBJECT = \"bottle\" ## object to train on ROOT_DIR = Path(\"/tmp/mvtec_ad\") ## root directory to store data for anomalib TASK = TaskType.SEGMENTATION ## task type for the model IMAGE_SIZE = (256, 256) ## preprocess image size for uniformity <p>For a given object type (category), the <code>create_datamodule()</code> function below creates an Anomalib  <code>DataModule</code> object. This will get passed into our engine's <code>fit()</code> method to train the model, and will be used to instantiate dataloaders for training and validation.</p> <p>The code might look complex, so let's break down what's going on:</p> <ul> <li>We create subsets of our data containing only the \"good\" training images and \"anomalous\" images for validation.</li> <li>We symlink the images and masks to the directory Anomalib expects.</li> <li>We instantiate and setup a datamodule from Anomalib's <code>Folder</code>, which is the general-purpose class for custom datasets.</li> </ul> <p>\ud83d\udca1 It is also possible to create a torch <code>DataLoader</code> from scratch and pass it to the engine's <code>fit()</code> method. This gives you more control over the data loading process. This is left as an exercise for the reader \ud83d\ude09.</p> In\u00a0[21]: Copied! <pre>def create_datamodule(object_type, transform=None):\n    ## Build transform\n    if transform is None:\n        transform = Resize(IMAGE_SIZE, antialias=True)\n\n    normal_data = dataset.match(F(\"category.label\") == object_type).match(\n        F(\"split\") == \"train\"\n    )\n    abnormal_data = (\n        dataset.match(F(\"category.label\") == object_type)\n        .match(F(\"split\") == \"test\")\n        .match(F(\"defect.label\") != \"good\")\n    )\n\n    normal_dir = Path(ROOT_DIR) / object_type / \"normal\"\n    abnormal_dir = ROOT_DIR / object_type / \"abnormal\"\n    mask_dir = ROOT_DIR / object_type / \"mask\"\n\n    # create directories if they do not exist\n    os.makedirs(normal_dir, exist_ok=True)\n    os.makedirs(abnormal_dir, exist_ok=True)\n    os.makedirs(mask_dir, exist_ok=True)\n\n    if not os.path.exists(str(normal_dir)):\n        normal_data.export(\n            export_dir=str(normal_dir),\n            dataset_type=fo.types.ImageDirectory,\n            export_media=\"symlink\",\n        )\n\n    for sample in abnormal_data.iter_samples():\n        base_filename = sample.filename\n        dir_name = os.path.dirname(sample.filepath).split(\"/\")[-1]\n        new_filename = f\"{dir_name}_{base_filename}\"\n        if not os.path.exists(str(abnormal_dir / new_filename)):\n            os.symlink(sample.filepath, str(abnormal_dir / new_filename))\n\n        if not os.path.exists(str(mask_dir / new_filename)):\n            os.symlink(sample.defect_mask.mask_path, str(mask_dir / new_filename))\n\n    datamodule = Folder(\n        name=object_type,\n        root=ROOT_DIR,\n        normal_dir=normal_dir,\n        abnormal_dir=abnormal_dir,\n        mask_dir=mask_dir,\n        task=TASK,\n        transform=transform\n    )\n    datamodule.setup()\n    return datamodule\n</pre> def create_datamodule(object_type, transform=None):     ## Build transform     if transform is None:         transform = Resize(IMAGE_SIZE, antialias=True)      normal_data = dataset.match(F(\"category.label\") == object_type).match(         F(\"split\") == \"train\"     )     abnormal_data = (         dataset.match(F(\"category.label\") == object_type)         .match(F(\"split\") == \"test\")         .match(F(\"defect.label\") != \"good\")     )      normal_dir = Path(ROOT_DIR) / object_type / \"normal\"     abnormal_dir = ROOT_DIR / object_type / \"abnormal\"     mask_dir = ROOT_DIR / object_type / \"mask\"      # create directories if they do not exist     os.makedirs(normal_dir, exist_ok=True)     os.makedirs(abnormal_dir, exist_ok=True)     os.makedirs(mask_dir, exist_ok=True)      if not os.path.exists(str(normal_dir)):         normal_data.export(             export_dir=str(normal_dir),             dataset_type=fo.types.ImageDirectory,             export_media=\"symlink\",         )      for sample in abnormal_data.iter_samples():         base_filename = sample.filename         dir_name = os.path.dirname(sample.filepath).split(\"/\")[-1]         new_filename = f\"{dir_name}_{base_filename}\"         if not os.path.exists(str(abnormal_dir / new_filename)):             os.symlink(sample.filepath, str(abnormal_dir / new_filename))          if not os.path.exists(str(mask_dir / new_filename)):             os.symlink(sample.defect_mask.mask_path, str(mask_dir / new_filename))      datamodule = Folder(         name=object_type,         root=ROOT_DIR,         normal_dir=normal_dir,         abnormal_dir=abnormal_dir,         mask_dir=mask_dir,         task=TASK,         transform=transform     )     datamodule.setup()     return datamodule <p>Now we can put it all together. The <code>train_and_export_model()</code> function below trains an anomaly detection model using Anomalib's <code>Engine</code> class, exports the model to OpenVINO, and returns the model \"inferencer\" object. The inferencer object is used to make predictions on new images.</p> In\u00a0[19]: Copied! <pre>def train_and_export_model(object_type, model, transform=None):\n    engine = Engine(task=TASK)\n    datamodule = create_datamodule(object_type, transform=transform)\n    engine.fit(model=model, datamodule=datamodule)\n\n    engine.export(\n        model=model,\n        export_type=ExportType.OPENVINO,\n    )\n    output_path = Path(engine.trainer.default_root_dir)\n\n\n    openvino_model_path = output_path / \"weights\" / \"openvino\" / \"model.bin\"\n    metadata = output_path / \"weights\" / \"openvino\" / \"metadata.json\"\n\n    inferencer = OpenVINOInferencer(\n        path=openvino_model_path,\n        metadata=metadata,\n        device=\"CPU\",\n    )\n    return inferencer\n</pre> def train_and_export_model(object_type, model, transform=None):     engine = Engine(task=TASK)     datamodule = create_datamodule(object_type, transform=transform)     engine.fit(model=model, datamodule=datamodule)      engine.export(         model=model,         export_type=ExportType.OPENVINO,     )     output_path = Path(engine.trainer.default_root_dir)       openvino_model_path = output_path / \"weights\" / \"openvino\" / \"model.bin\"     metadata = output_path / \"weights\" / \"openvino\" / \"metadata.json\"      inferencer = OpenVINOInferencer(         path=openvino_model_path,         metadata=metadata,         device=\"CPU\",     )     return inferencer <p>Let's try this with <code>PaDiM</code> first. The training process should take less than a minute:</p> In\u00a0[\u00a0]: Copied! <pre>model = Padim()\n\ninferencer = train_and_export_model(OBJECT, model)\n</pre> model = Padim()  inferencer = train_and_export_model(OBJECT, model) <p>And just like that, we have an anomaly detection model trained on the \"bottle\" category. Let's run our inferencer on a single image and inspect the results:</p> In\u00a0[23]: Copied! <pre>## get the test split of the dataset\ntest_split = dataset.match(F(\"category.label\") == OBJECT).match(\n    F(\"split\") == \"test\"\n)\n\n## get the first sample from the test split\ntest_image = Image.open(test_split.first().filepath)\n\noutput = inferencer.predict(image=test_image)\nprint(output)\n</pre> ## get the test split of the dataset test_split = dataset.match(F(\"category.label\") == OBJECT).match(     F(\"split\") == \"test\" )  ## get the first sample from the test split test_image = Image.open(test_split.first().filepath)  output = inferencer.predict(image=test_image) print(output) <pre>ImageResult(image=[[[255 255 255]\n  [255 255 255]\n  [255 255 255]\n  ...\n  [255 255 255]\n  [255 255 255]\n  [255 255 255]]\n\n [[255 255 255]\n  [255 255 255]\n  [255 255 255]\n  ...\n  [255 255 255]\n  [255 255 255]\n  [255 255 255]]\n\n [[255 255 255]\n  [255 255 255]\n  [255 255 255]\n  ...\n  [255 255 255]\n  [255 255 255]\n  [255 255 255]]\n\n ...\n\n [[255 255 255]\n  [255 255 255]\n  [255 255 255]\n  ...\n  [255 255 255]\n  [255 255 255]\n  [255 255 255]]\n\n [[255 255 255]\n  [255 255 255]\n  [255 255 255]\n  ...\n  [255 255 255]\n  [255 255 255]\n  [255 255 255]]\n\n [[255 255 255]\n  [255 255 255]\n  [255 255 255]\n  ...\n  [255 255 255]\n  [255 255 255]\n  [255 255 255]]], pred_score=0.7751642969087686, pred_label=1, anomaly_map=[[0.32784402 0.32784402 0.32784414 ... 0.3314721  0.33147204 0.33147204]\n [0.32784402 0.32784402 0.32784414 ... 0.3314721  0.33147204 0.33147204]\n [0.32784408 0.32784408 0.3278442  ... 0.33147222 0.33147216 0.33147216]\n ...\n [0.32959    0.32959    0.32959005 ... 0.3336093  0.3336093  0.3336093 ]\n [0.3295899  0.3295899  0.32958996 ... 0.33360928 0.33360928 0.33360928]\n [0.3295899  0.3295899  0.32958996 ... 0.33360928 0.33360928 0.33360928]], gt_mask=None, gt_boxes=None, pred_boxes=None, box_labels=None, pred_mask=[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]], heat_map=[[[153 235 255]\n  [153 235 255]\n  [153 235 255]\n  ...\n  [153 236 255]\n  [153 236 255]\n  [153 236 255]]\n\n [[153 235 255]\n  [153 235 255]\n  [153 235 255]\n  ...\n  [153 236 255]\n  [153 236 255]\n  [153 236 255]]\n\n [[153 235 255]\n  [153 235 255]\n  [153 235 255]\n  ...\n  [153 236 255]\n  [153 236 255]\n  [153 236 255]]\n\n ...\n\n [[153 236 255]\n  [153 236 255]\n  [153 236 255]\n  ...\n  [153 238 255]\n  [153 238 255]\n  [153 238 255]]\n\n [[153 236 255]\n  [153 236 255]\n  [153 236 255]\n  ...\n  [153 238 255]\n  [153 238 255]\n  [153 238 255]]\n\n [[153 236 255]\n  [153 236 255]\n  [153 236 255]\n  ...\n  [153 238 255]\n  [153 238 255]\n  [153 238 255]]], segmentations=[[[255 255 255]\n  [255 255 255]\n  [255 255 255]\n  ...\n  [255 255 255]\n  [255 255 255]\n  [255 255 255]]\n\n [[255 255 255]\n  [255 255 255]\n  [255 255 255]\n  ...\n  [255 255 255]\n  [255 255 255]\n  [255 255 255]]\n\n [[255 255 255]\n  [255 255 255]\n  [255 255 255]\n  ...\n  [255 255 255]\n  [255 255 255]\n  [255 255 255]]\n\n ...\n\n [[255 255 255]\n  [255 255 255]\n  [255 255 255]\n  ...\n  [255 255 255]\n  [255 255 255]\n  [255 255 255]]\n\n [[255 255 255]\n  [255 255 255]\n  [255 255 255]\n  ...\n  [255 255 255]\n  [255 255 255]\n  [255 255 255]]\n\n [[255 255 255]\n  [255 255 255]\n  [255 255 255]\n  ...\n  [255 255 255]\n  [255 255 255]\n  [255 255 255]]])\n</pre> <p>The output contains a scalar anomaly score <code>pred_score</code>, a <code>pred_mask</code> denoting the predicted anomalous regions, and a heatmap <code>anomaly_map</code> showing the anomaly scores for each pixel. This is all valuable information for understanding the model's predictions. The <code>run_inference()</code> function below will take a FiftyOne sample collection (e.g. our test set) as input, along with the inferencer object, and a key for storing the results in the samples. It will run the model on each sample in the collection and store the results. The <code>threshold</code> argument acts as a cutoff for the anomaly score. If the score is above the threshold, the sample is considered anomalous. In this example, we'll use a threshold of $0.5$, but you can experiment with different values.</p> In\u00a0[24]: Copied! <pre>def run_inference(sample_collection, inferencer, key, threshold=0.5):\n    for sample in sample_collection.iter_samples(autosave=True, progress=True):\n        output = inferencer.predict(image=Image.open(sample.filepath))\n        \n        conf = output.pred_score\n        anomaly = \"normal\" if conf &lt; threshold else \"anomaly\"\n\n        sample[f\"pred_anomaly_score_{key}\"] = conf\n        sample[f\"pred_anomaly_{key}\"] = fo.Classification(label=anomaly)\n        sample[f\"pred_anomaly_map_{key}\"] = fo.Heatmap(map=output.anomaly_map)\n        sample[f\"pred_defect_mask_{key}\"] = fo.Segmentation(mask=output.pred_mask)\n</pre> def run_inference(sample_collection, inferencer, key, threshold=0.5):     for sample in sample_collection.iter_samples(autosave=True, progress=True):         output = inferencer.predict(image=Image.open(sample.filepath))                  conf = output.pred_score         anomaly = \"normal\" if conf &lt; threshold else \"anomaly\"          sample[f\"pred_anomaly_score_{key}\"] = conf         sample[f\"pred_anomaly_{key}\"] = fo.Classification(label=anomaly)         sample[f\"pred_anomaly_map_{key}\"] = fo.Heatmap(map=output.anomaly_map)         sample[f\"pred_defect_mask_{key}\"] = fo.Segmentation(mask=output.pred_mask) In\u00a0[\u00a0]: Copied! <pre>run_inference(test_split, inferencer, \"padim\")\n</pre> run_inference(test_split, inferencer, \"padim\") <p>Let's visualize these results in the FiftyOne App:</p> In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(view=test_split)\n</pre> session = fo.launch_app(view=test_split) <p></p> <p>We have an anomaly detection model, but how do we know if it's any good? For one, we can evaluate the model using metrics like precision, recall, and F1 score. FiftyOne's Evaluation API makes this easy. We are going to evaluate the full-image classification performance of the model, as well as the segmentation performance.</p> <p>We need to prepare our data for evaluation. First, we need to add null masks for the \"normal\" images to ensure the evaluation is fair:</p> In\u00a0[\u00a0]: Copied! <pre>for sample in test_split.iter_samples(autosave=True, progress=True):\n    if sample[\"defect\"].label == \"good\":\n        sample[\"defect_mask\"] = fo.Segmentation(\n            mask=np.zeros_like(sample[\"pred_defect_mask_padim\"].mask)\n        )\n</pre> for sample in test_split.iter_samples(autosave=True, progress=True):     if sample[\"defect\"].label == \"good\":         sample[\"defect_mask\"] = fo.Segmentation(             mask=np.zeros_like(sample[\"pred_defect_mask_padim\"].mask)         ) <p>We also need to ensure consistency in naming/labels between ground truth and predictions. We'll rename all of our \"good\" images to \"normal\" and every type of anomaly to \"anomaly\":</p> In\u00a0[39]: Copied! <pre>old_labels = test_split.distinct(\"defect.label\")\nlabel_map = {label:\"anomaly\" for label in old_labels if label != \"good\"}\nlabel_map[\"good\"] = \"normal\"\nmapped_view = test_split.map_labels(\"defect\", label_map)\n</pre> old_labels = test_split.distinct(\"defect.label\") label_map = {label:\"anomaly\" for label in old_labels if label != \"good\"} label_map[\"good\"] = \"normal\" mapped_view = test_split.map_labels(\"defect\", label_map) In\u00a0[40]: Copied! <pre>session.view = mapped_view.view()\n</pre> session.view = mapped_view.view() <p></p> <p>For classification, we'll use binary evaluation, with \"normal\" as the negative class and \"anomaly\" as the positive class:</p> In\u00a0[41]: Copied! <pre>eval_classif_padim = mapped_view.evaluate_classifications(\n    \"pred_anomaly_padim\",\n    gt_field=\"defect\",\n    eval_key=\"eval_classif_padim\",\n    method=\"binary\",\n    classes=[\"normal\", \"anomaly\"],\n)\n</pre> eval_classif_padim = mapped_view.evaluate_classifications(     \"pred_anomaly_padim\",     gt_field=\"defect\",     eval_key=\"eval_classif_padim\",     method=\"binary\",     classes=[\"normal\", \"anomaly\"], ) In\u00a0[42]: Copied! <pre>eval_classif_padim.print_report()\n</pre> eval_classif_padim.print_report() <pre>              precision    recall  f1-score   support\n\n      normal       0.95      0.90      0.92        20\n     anomaly       0.97      0.98      0.98        63\n\n    accuracy                           0.96        83\n   macro avg       0.96      0.94      0.95        83\nweighted avg       0.96      0.96      0.96        83\n\n</pre> <p>The model performs quite well on the classification task!</p> <p>If we go back over to the app and sort by anomaly score, we can see that certain types of anomalies tend to have higher scores than others. In this example, <code>contamination</code> instances tend to have either very high or very low scores relative to <code>broken_small</code> and <code>broken_large</code>. When we put this model in production we might be more likely to miss certain types of anomalies. Other types of models, or ensembles of models, might be more robust to this!</p> <p>For segmentation evaluation, we will only be interested in pixel values of $0$ (normal) and $255$ (anomaly), so we will filter our report for these \"classes\":</p> In\u00a0[\u00a0]: Copied! <pre>eval_seg_padim = mapped_view.evaluate_segmentations(\n    \"pred_defect_mask_padim\",\n    gt_field=\"defect_mask\",\n    eval_key=\"eval_seg_padim\",\n)\n</pre> eval_seg_padim = mapped_view.evaluate_segmentations(     \"pred_defect_mask_padim\",     gt_field=\"defect_mask\",     eval_key=\"eval_seg_padim\", ) In\u00a0[44]: Copied! <pre>eval_seg_padim.print_report(classes=[0, 255])\n</pre> eval_seg_padim.print_report(classes=[0, 255]) <pre>              precision    recall  f1-score   support\n\n           0       0.99      0.96      0.98 63343269.0\n         255       0.60      0.89      0.72 3886731.0\n\n   micro avg       0.96      0.96      0.96 67230000.0\n   macro avg       0.80      0.93      0.85 67230000.0\nweighted avg       0.97      0.96      0.96 67230000.0\n\n</pre> <p>Just because anomaly detection is unsupervised doesn't mean we can't compare models and choose the best one for our use case. We can train multiple models on the same data and compare their performance using metrics like F1 score, precision, and recall. We can also compare the models visually by inspecting the masks and heatmaps they generate.</p> <p>Let's repeat the training process for the <code>PatchCore</code> model and compare the two models:</p> In\u00a0[\u00a0]: Copied! <pre>## Train Patchcore model and run inference\n\nmodel = Patchcore()\n\n## This will take a little longer to train, but should still be &lt; 5 minutes\ninferencer = train_and_export_model(OBJECT, model)\n\nrun_inference(mapped_view, inferencer, \"patchcore\")\n</pre> ## Train Patchcore model and run inference  model = Patchcore()  ## This will take a little longer to train, but should still be &lt; 5 minutes inferencer = train_and_export_model(OBJECT, model)  run_inference(mapped_view, inferencer, \"patchcore\") In\u00a0[46]: Copied! <pre>## Evaluate Patchcore model on classification task\neval_classif_patchcore = mapped_view.evaluate_classifications(\n    \"pred_anomaly_patchcore\",\n    gt_field=\"defect\",\n    eval_key=\"eval_classif_patchcore\",\n    method=\"binary\",\n    classes=[\"normal\", \"anomaly\"],\n)\n\neval_classif_patchcore.print_report()\n</pre> ## Evaluate Patchcore model on classification task eval_classif_patchcore = mapped_view.evaluate_classifications(     \"pred_anomaly_patchcore\",     gt_field=\"defect\",     eval_key=\"eval_classif_patchcore\",     method=\"binary\",     classes=[\"normal\", \"anomaly\"], )  eval_classif_patchcore.print_report() <pre>              precision    recall  f1-score   support\n\n      normal       0.95      1.00      0.98        20\n     anomaly       1.00      0.98      0.99        63\n\n    accuracy                           0.99        83\n   macro avg       0.98      0.99      0.98        83\nweighted avg       0.99      0.99      0.99        83\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>eval_seg_patchcore = mapped_view.match(\n    F(\"defect.label\") == \"anomaly\"\n).evaluate_segmentations(\n    \"pred_defect_mask_patchcore\",\n    gt_field=\"defect_mask\",\n    eval_key=\"eval_seg_patchcore\",\n)\n</pre> eval_seg_patchcore = mapped_view.match(     F(\"defect.label\") == \"anomaly\" ).evaluate_segmentations(     \"pred_defect_mask_patchcore\",     gt_field=\"defect_mask\",     eval_key=\"eval_seg_patchcore\", ) In\u00a0[48]: Copied! <pre>eval_seg_patchcore.print_report(classes=[0, 255])\n</pre> eval_seg_patchcore.print_report(classes=[0, 255]) <pre>              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97 47143269.0\n         255       0.60      0.85      0.70 3886731.0\n\n   micro avg       0.95      0.95      0.95 51030000.0\n   macro avg       0.80      0.90      0.84 51030000.0\nweighted avg       0.96      0.95      0.95 51030000.0\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>session.view = mapped_view.shuffle().view()\n</pre> session.view = mapped_view.shuffle().view() <p></p> <p>The metrics back up what we see in the app: PatchCore has much higher recall for the \"anomaly\" class, but lower precision. This means it's more likely to catch anomalies, but also more likely to make false positive predictions. After all, PatchCore is designed for \"total recall\" in industrial anomaly detection.</p> <p>Looking at the heatmaps, we can also see what types of anomalies each model is better at detecting. An ensemble of the two models might be more robust to different types of anomalies.</p> <p>Beyond the anomaly detection algorithm itself, there are many other knobs we can turn to improve the performance of our model. These include:</p> <ul> <li>Backbone: The architecture of the model used for feature extraction</li> <li>Algorithm hyperparameters: Parameters specific to the anomaly detection algorithm. For PatchCore, this includes <code>coreset_sampling_ratio</code> and <code>num_neighbors</code>.</li> <li>Data augmentation: Techniques to artificially increase the size of the training set and improve the model's generalization.</li> </ul> <p>This section briefly illustrates the role of data augmentation techniques in anomaly detection. We'll use FiftyOne's Albumentations plugin to test and visualize transformations on \"good\" samples from the dataset and then apply the same transformations (with <code>torchvision.transforms</code>) to the training images.</p> <p>The goal is to increase the diversity of the training set without changing the images so much that we verge into the \"anomalous\" territory.</p> <p>First, let's install Albumentations and download the plugin:</p> In\u00a0[\u00a0]: Copied! <pre>pip install -U albumentations\n</pre> pip install -U albumentations In\u00a0[\u00a0]: Copied! <pre>!fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin\n</pre> !fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin <p>Refresh the FiftyOne App and hit the backtick key on your keyboard to open the operators list. Select <code>\"augment_with_albumentations\"</code> from the dropdown menu. Try applying transformations like <code>GaussianBlur</code>, and <code>ColorJitter</code>, and see how they affect the images. Use the <code>\"view_last_albumentations_run\"</code> operator to see the augmentations generated by the last run. Play around with the kernel size for blurring, and the brightness, contrast, and saturation values for color jittering. Depending on the object category, it might also make sense to apply 90 degree rotations, horizontal flips, and other transformations!</p> <p></p> <p>Once you're happy with the transformations, use the <code>\"get_last_albumentations_run_info\"</code> operator to see the transformations applied and their parameters. You can then use these with <code>torchvision.transforms</code> to augment the training images.</p> In\u00a0[49]: Copied! <pre>from torchvision.transforms.v2 import GaussianBlur, ColorJitter, Compose\n</pre> from torchvision.transforms.v2 import GaussianBlur, ColorJitter, Compose In\u00a0[50]: Copied! <pre>transform = Compose([\n    Resize(IMAGE_SIZE, antialias=True),\n    GaussianBlur(kernel_size=3),\n    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n])\n</pre> transform = Compose([     Resize(IMAGE_SIZE, antialias=True),     GaussianBlur(kernel_size=3),     ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), ]) In\u00a0[\u00a0]: Copied! <pre>model = Patchcore()\naugmented_inferencer = train_and_export_model(\n    OBJECT, model, transform=transform\n)\n\nrun_inference(mapped_view, augmented_inferencer, \"patchcore_augmented\")\n\nsession.view = mapped_view.view()\n</pre> model = Patchcore() augmented_inferencer = train_and_export_model(     OBJECT, model, transform=transform )  run_inference(mapped_view, augmented_inferencer, \"patchcore_augmented\")  session.view = mapped_view.view() <p>This is just a starting point for data augmentation. You can experiment with different transformations and parameters to see what works best for your dataset.</p> <p>In this walkthrough, we learned how to perform anomaly detection on visual data using FiftyOne and Anomalib. We trained two anomaly detection models, PaDiM and PatchCore, on the MVTec AD dataset and evaluated their performance using metrics like precision, recall, and F1 score. We also visualized the models' predictions using heatmaps and masks. Finally, we tested data augmentation techniques to improve the models' generalization.</p> <p>If you want to dive deeper into unsupervised learning, check out these tutorials:</p> <ul> <li>Visualizing Data with Dimensionality Reduction Techniques</li> <li>Clustering Images with Embeddings</li> </ul> In\u00a0[52]: Copied! <pre>session.view = mapped_view.view()\n</pre> session.view = mapped_view.view()"},{"location":"tutorials/anomaly_detection/#anomaly-detection-with-fiftyone-and-anomalib","title":"Anomaly Detection with FiftyOne and Anomalib\u00b6","text":""},{"location":"tutorials/anomaly_detection/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/anomaly_detection/#install-dependencies","title":"Install dependencies\u00b6","text":""},{"location":"tutorials/anomaly_detection/#load-and-visualize-the-mvtec-ad-dataset","title":"Load and Visualize the MVTec AD dataset\u00b6","text":""},{"location":"tutorials/anomaly_detection/#train-an-anomaly-detection-model","title":"Train an Anomaly Detection Model\u00b6","text":""},{"location":"tutorials/anomaly_detection/#evaluate-anomaly-detection-models","title":"Evaluate Anomaly Detection Models\u00b6","text":""},{"location":"tutorials/anomaly_detection/#compare-anomaly-detection-models","title":"Compare Anomaly Detection Models\u00b6","text":""},{"location":"tutorials/anomaly_detection/#test-data-augmentation-techniques","title":"Test Data Augmentation Techniques\u00b6","text":""},{"location":"tutorials/anomaly_detection/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/classification_mistakes/","title":"Finding Classification Mistakes with FiftyOne","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>We'll also need <code>torch</code> and <code>torchvision</code> installed:</p> In\u00a0[1]: Copied! <pre>!pip install torch torchvision\n</pre> !pip install torch torchvision <p>In this tutorial, we'll use a pretrained CIFAR-10 PyTorch model (a ResNet-50) from the web:</p> In\u00a0[\u00a0]: Copied! <pre># Download the software\n!git clone --depth 1 --branch v2.1 https://github.com/huyvnphan/PyTorch_CIFAR10.git\n\n# Download the pretrained model (90MB)\n!eta gdrive download --public \\\n    1dGfpeFK_QG0kV-U6QDHMX2EOGXPqaNzu \\\n    PyTorch_CIFAR10/cifar10_models/state_dicts/resnet50.pt\n</pre> # Download the software !git clone --depth 1 --branch v2.1 https://github.com/huyvnphan/PyTorch_CIFAR10.git  # Download the pretrained model (90MB) !eta gdrive download --public \\     1dGfpeFK_QG0kV-U6QDHMX2EOGXPqaNzu \\     PyTorch_CIFAR10/cifar10_models/state_dicts/resnet50.pt In\u00a0[\u00a0]: Copied! <pre>import random\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Load the CIFAR-10 test split\n# Downloads the dataset from the web if necessary\ndataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\n\n# Get the CIFAR-10 classes list\nclasses = dataset.default_classes\n\n# Artificially corrupt 10% of the labels\n_num_mistakes = int(0.1 * len(dataset))\nfor sample in dataset.take(_num_mistakes):\n    mistake = random.randint(0, 9)\n    while classes[mistake] == sample.ground_truth.label:\n        mistake = random.randint(0, 9)\n\n    sample.tags.append(\"mistake\")\n    sample.ground_truth = fo.Classification(label=classes[mistake])\n    sample.save()\n</pre> import random  import fiftyone as fo import fiftyone.zoo as foz  # Load the CIFAR-10 test split # Downloads the dataset from the web if necessary dataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\")  # Get the CIFAR-10 classes list classes = dataset.default_classes  # Artificially corrupt 10% of the labels _num_mistakes = int(0.1 * len(dataset)) for sample in dataset.take(_num_mistakes):     mistake = random.randint(0, 9)     while classes[mistake] == sample.ground_truth.label:         mistake = random.randint(0, 9)      sample.tags.append(\"mistake\")     sample.ground_truth = fo.Classification(label=classes[mistake])     sample.save() <p>Let's print some information about the dataset to verify the operation that we performed:</p> In\u00a0[3]: Copied! <pre># Verify that the `mistake` tag is now in the dataset's schema\nprint(dataset)\n</pre> # Verify that the `mistake` tag is now in the dataset's schema print(dataset) <pre>Name:           cifar10-test\nMedia type:     image\nNum samples:    10000\nPersistent:     False\nTags:           ['mistake', 'test']\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n</pre> In\u00a0[3]: Copied! <pre># Count the number of samples with the `mistake` tag\nnum_mistakes = len(dataset.match_tags(\"mistake\"))\nprint(\"%d ground truth labels are now mistakes\" % num_mistakes)\n</pre> # Count the number of samples with the `mistake` tag num_mistakes = len(dataset.match_tags(\"mistake\")) print(\"%d ground truth labels are now mistakes\" % num_mistakes) <pre>1000 ground truth labels are now mistakes\n</pre> In\u00a0[4]: Copied! <pre>import sys\n\nimport numpy as np\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader\n\nimport fiftyone.utils.torch as fout\n\nsys.path.insert(1, \"PyTorch_CIFAR10\")\nfrom cifar10_models import resnet50\n\n\ndef make_cifar10_data_loader(image_paths, sample_ids, batch_size):\n    mean = [0.4914, 0.4822, 0.4465]\n    std = [0.2023, 0.1994, 0.2010]\n    transforms = torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize(mean, std),\n        ]\n    )\n    dataset = fout.TorchImageDataset(\n        image_paths, sample_ids=sample_ids, transform=transforms\n    )\n    return DataLoader(dataset, batch_size=batch_size, num_workers=4)\n\n\ndef predict(model, imgs):\n    logits = model(imgs).detach().cpu().numpy()\n    predictions = np.argmax(logits, axis=1)\n    odds = np.exp(logits)\n    confidences = np.max(odds, axis=1) / np.sum(odds, axis=1)\n    return predictions, confidences, logits\n\n\n#\n# Load a model\n#\n# Model performance numbers are available at:\n#   https://github.com/huyvnphan/PyTorch_CIFAR10\n#\n\nmodel = resnet50(pretrained=True)\nmodel_name = \"resnet50\"\n\n#\n# Extract a few images to process\n# (some of these will have been manipulated above)\n#\n\nnum_samples = 1000\nbatch_size = 20\nview = dataset.take(num_samples)\nimage_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view.iter_samples()])\ndata_loader = make_cifar10_data_loader(image_paths, sample_ids, batch_size)\n\n#\n# Perform prediction and store results in dataset\n#\n\nwith fo.ProgressBar() as pb:\n    for imgs, sample_ids in pb(data_loader):\n        predictions, _, logits_ = predict(model, imgs)\n\n        # Add predictions to your FiftyOne dataset\n        for sample_id, prediction, logits in zip(sample_ids, predictions, logits_):\n            sample = dataset[sample_id]\n            sample.tags.append(\"processed\")\n            sample[model_name] = fo.Classification(\n                label=classes[prediction], logits=logits,\n            )\n            sample.save()\n</pre> import sys  import numpy as np import torch import torchvision from torch.utils.data import DataLoader  import fiftyone.utils.torch as fout  sys.path.insert(1, \"PyTorch_CIFAR10\") from cifar10_models import resnet50   def make_cifar10_data_loader(image_paths, sample_ids, batch_size):     mean = [0.4914, 0.4822, 0.4465]     std = [0.2023, 0.1994, 0.2010]     transforms = torchvision.transforms.Compose(         [             torchvision.transforms.ToTensor(),             torchvision.transforms.Normalize(mean, std),         ]     )     dataset = fout.TorchImageDataset(         image_paths, sample_ids=sample_ids, transform=transforms     )     return DataLoader(dataset, batch_size=batch_size, num_workers=4)   def predict(model, imgs):     logits = model(imgs).detach().cpu().numpy()     predictions = np.argmax(logits, axis=1)     odds = np.exp(logits)     confidences = np.max(odds, axis=1) / np.sum(odds, axis=1)     return predictions, confidences, logits   # # Load a model # # Model performance numbers are available at: #   https://github.com/huyvnphan/PyTorch_CIFAR10 #  model = resnet50(pretrained=True) model_name = \"resnet50\"  # # Extract a few images to process # (some of these will have been manipulated above) #  num_samples = 1000 batch_size = 20 view = dataset.take(num_samples) image_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view.iter_samples()]) data_loader = make_cifar10_data_loader(image_paths, sample_ids, batch_size)  # # Perform prediction and store results in dataset #  with fo.ProgressBar() as pb:     for imgs, sample_ids in pb(data_loader):         predictions, _, logits_ = predict(model, imgs)          # Add predictions to your FiftyOne dataset         for sample_id, prediction, logits in zip(sample_ids, predictions, logits_):             sample = dataset[sample_id]             sample.tags.append(\"processed\")             sample[model_name] = fo.Classification(                 label=classes[prediction], logits=logits,             )             sample.save() <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [11.0s elapsed, 0s remaining, 4.7 samples/s]      \n</pre> <p>Let's print some information about the predictions that were generated and how many of them correspond to samples whose ground truth labels were corrupted:</p> In\u00a0[5]: Copied! <pre># Count the number of samples with the `processed` tag\nnum_processed = len(dataset.match_tags(\"processed\"))\n\n# Count the number of samples with both `processed` and `mistake` tags\nnum_corrupted = len(dataset.match_tags(\"processed\").match_tags(\"mistake\"))\n\nprint(\"Added predictions to %d samples\" % num_processed)\nprint(\"%d of these samples have label mistakes\" % num_corrupted)\n</pre> # Count the number of samples with the `processed` tag num_processed = len(dataset.match_tags(\"processed\"))  # Count the number of samples with both `processed` and `mistake` tags num_corrupted = len(dataset.match_tags(\"processed\").match_tags(\"mistake\"))  print(\"Added predictions to %d samples\" % num_processed) print(\"%d of these samples have label mistakes\" % num_corrupted) <pre>Added predictions to 1000 samples\n86 of these samples have label mistakes\n</pre> In\u00a0[6]: Copied! <pre>import fiftyone.brain as fob\n\n# Get samples for which we added predictions\nh_view = dataset.match_tags(\"processed\")\n\n# Compute mistakenness\nfob.compute_mistakenness(h_view, model_name, label_field=\"ground_truth\", use_logits=True)\n</pre> import fiftyone.brain as fob  # Get samples for which we added predictions h_view = dataset.match_tags(\"processed\")  # Compute mistakenness fob.compute_mistakenness(h_view, model_name, label_field=\"ground_truth\", use_logits=True) <pre>Computing mistakenness...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [2.4s elapsed, 0s remaining, 446.1 samples/s]      \nMistakenness computation complete\n</pre> <p>The above method added <code>mistakenness</code> field to all samples for which we added predictions. We can easily sort by likelihood of mistakenness from code:</p> In\u00a0[7]: Copied! <pre># Sort by likelihood of mistake (most likely first)\nmistake_view = (dataset\n    .match_tags(\"processed\")\n    .sort_by(\"mistakenness\", reverse=True)\n)\n\n# Print some information about the view\nprint(mistake_view)\n</pre> # Sort by likelihood of mistake (most likely first) mistake_view = (dataset     .match_tags(\"processed\")     .sort_by(\"mistakenness\", reverse=True) )  # Print some information about the view print(mistake_view) <pre>Dataset:        cifar10-test\nMedia type:     image\nNum samples:    1000\nTags:           ['mistake', 'processed', 'test']\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n    resnet50:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n    mistakenness: fiftyone.core.fields.FloatField\nView stages:\n    1. MatchTags(tags=['processed'])\n    2. SortBy(field_or_expr='mistakenness', reverse=True)\n</pre> In\u00a0[8]: Copied! <pre># Inspect the first few samples\nprint(mistake_view.head())\n</pre> # Inspect the first few samples print(mistake_view.head()) <pre>[&lt;SampleView: {\n    'id': '6064c24201257d68b7b046d7',\n    'media_type': 'image',\n    'filepath': '/home/ben/fiftyone/cifar10/test/data/001326.jpg',\n    'tags': BaseList(['test', 'mistake', 'processed']),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '6064c24c01257d68b7b0b34c',\n        'tags': BaseList([]),\n        'label': 'ship',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n    'resnet50': &lt;Classification: {\n        'id': '6064c26e01257d68b7b0be64',\n        'tags': BaseList([]),\n        'label': 'deer',\n        'confidence': None,\n        'logits': array([-0.925419  , -1.2076195 , -0.37321544, -0.2750331 ,  6.723097  ,\n               -0.44599843, -0.7555994 , -0.43585306, -1.1593063 , -1.1450499 ],\n              dtype=float32),\n    }&gt;,\n    'mistakenness': 0.9778614850560818,\n}&gt;, &lt;SampleView: {\n    'id': '6064c24201257d68b7b04977',\n    'media_type': 'image',\n    'filepath': '/home/ben/fiftyone/cifar10/test/data/001550.jpg',\n    'tags': BaseList(['test', 'mistake', 'processed']),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '6064c24c01257d68b7b0b20e',\n        'tags': BaseList([]),\n        'label': 'deer',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n    'resnet50': &lt;Classification: {\n        'id': '6064c26701257d68b7b0b94a',\n        'tags': BaseList([]),\n        'label': 'automobile',\n        'confidence': None,\n        'logits': array([-0.6696544 ,  6.331352  , -0.90380824, -0.8609426 , -0.97413117,\n               -0.8693008 , -0.8035213 , -0.9215686 , -0.48488098,  0.15646096],\n              dtype=float32),\n    }&gt;,\n    'mistakenness': 0.967886808991774,\n}&gt;, &lt;SampleView: {\n    'id': '6064c24401257d68b7b060c9',\n    'media_type': 'image',\n    'filepath': '/home/ben/fiftyone/cifar10/test/data/003540.jpg',\n    'tags': BaseList(['test', 'processed']),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '6064c24401257d68b7b060c8',\n        'tags': BaseList([]),\n        'label': 'cat',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n    'resnet50': &lt;Classification: {\n        'id': '6064c26e01257d68b7b0bdfe',\n        'tags': BaseList([]),\n        'label': 'ship',\n        'confidence': None,\n        'logits': array([ 0.74897313, -0.7627302 , -0.79189354, -0.78844124, -1.0206403 ,\n               -1.0742921 , -0.9762771 , -1.0660601 ,  6.3457403 , -0.6143737 ],\n              dtype=float32),\n    }&gt;,\n    'mistakenness': 0.9653186284617471,\n}&gt;]\n</pre> <p>Let's open the FiftyOne App to visually inspect the results:</p> In\u00a0[9]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) Activate In\u00a0[10]: Copied! <pre># Show only the samples that were processed\nsession.view = dataset.match_tags(\"processed\")\n</pre> # Show only the samples that were processed session.view = dataset.match_tags(\"processed\") Activate In\u00a0[15]: Copied! <pre># Show only the samples for which we added label mistakes\nsession.view = dataset.match_tags(\"mistake\")\n</pre> # Show only the samples for which we added label mistakes session.view = dataset.match_tags(\"mistake\") Activate In\u00a0[18]: Copied! <pre># Show the samples we processed in rank order by the mistakenness\nsession.view = mistake_view\n</pre> # Show the samples we processed in rank order by the mistakenness session.view = mistake_view Activate <p>In a real world scenario, we would then take the ground truth classifications that are likely mistakes and send them off to our annotation provider of choice as annotations to be reviewed. FiftyOne currently offers integrations for both Labelbox and Scale.</p> In\u00a0[19]: Copied! <pre>session.freeze() # screenshot the active App for sharing\n</pre> session.freeze() # screenshot the active App for sharing"},{"location":"tutorials/classification_mistakes/#finding-classification-mistakes-with-fiftyone","title":"Finding Classification Mistakes with FiftyOne\u00b6","text":"<p>Annotations mistakes create an artificial ceiling on the performance of your models. However, finding these mistakes by hand is at least as arduous as the original annotation work! Enter FiftyOne.</p> <p>In this tutorial, we explore how FiftyOne can be used to help you find mistakes in your classification annotations. To detect mistakes in detection datasets, check out this tutorial.</p> <p>We'll cover the following concepts:</p> <ul> <li>Loading your existing dataset into FiftyOne</li> <li>Adding model predictions to your dataset</li> <li>Computing insights into your dataset relating to possible label mistakes</li> <li>Visualizing mistakes in the FiftyOne App</li> </ul> <p>So, what's the takeaway?</p> <p>FiftyOne can help you find and correct label mistakes in your datasets, enabling you to curate higher quality datasets and, ultimately, train better models!</p>"},{"location":"tutorials/classification_mistakes/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"tutorials/classification_mistakes/#manipulating-the-data","title":"Manipulating the data\u00b6","text":"<p>For this walkthrough, we will artificially perturb an existing dataset with mistakes on the labels. Of course, in your normal workflow, you would not add labeling mistakes; this is only for the sake of the walkthrough.</p> <p>The code block below loads the test split of the CIFAR-10 dataset into FiftyOne and randomly breaks 10% (1000 samples) of the labels:</p>"},{"location":"tutorials/classification_mistakes/#add-predictions-to-the-dataset","title":"Add predictions to the dataset\u00b6","text":"<p>Using an off-the-shelf model, let's now add predictions to the dataset, which are necessary for us to deduce some understanding of the possible label mistakes.</p> <p>The code block below adds model predictions to another randomly chosen 10% (1000 samples) of the dataset:</p>"},{"location":"tutorials/classification_mistakes/#find-the-mistakes","title":"Find the mistakes\u00b6","text":"<p>Now we can run a method from FiftyOne that estimates the mistakenness of the ground samples for which we generated predictions:</p>"},{"location":"tutorials/clustering/","title":"Clustering Images with Embeddings","text":"<p>Clustering is an essential unsupervised learning technique that can help you discover hidden patterns in your data. This walkthrough, you'll learn how to bring structure your visual data using Scikit-learn and FiftyOne!</p> <p>It covers the following:</p> <ul> <li>What is clustering?</li> <li>Generating features to cluster images</li> <li>Clustering images using the FiftyOne Clustering Plugin</li> <li>Keeping track of clustering runs in the FiftyOne App</li> <li>Assigning labels to clusters using GPT-4V</li> </ul> <p>Imagine you have a ton of Lego blocks of all shapes and sizes spread out on the floor. It\u2019s time to put the legos away, and you realize you don\u2019t have a large enough bin to store all of them. Luckily, you find four smaller bins that can each hold roughly the same number of pieces. You could dump a random assortment of Legos in each bin and call it a day. But then, the next time you went to find a specific piece, you\u2019d have quite the time digging around for it.</p> <p>Instead, you have a better idea: putting similar pieces in the same bin would save you a lot of time and trouble later. But what criterion are you going to use to put Legos in bins? Are you going to assign bins for different colors? Or put all the square pieces in one bin and the circular pieces in another? It really depends on what Legos you have! This, in a nutshell, is clustering.</p> <p>More formally, clustering, or cluster analysis, is a set of techniques for grouping data points. Clustering algorithms take in a bunch of objects, and spit out assignments for each object. Unlike classification, however, clustering does not start with a list of classes to categorize the objects, forcing objects to fall into preset buckets. Rather, clustering attempts to discover the buckets given the data. In other words, clustering is about uncovering structure in data, not predicting labels in a preexisting structure.</p> <p>This last point merits repeating: clustering is not about predicting labels. Unlike classification, detection, and segmentation tasks, there are no ground truth labels for clustering tasks. We call algorithms like this unsupervised, contrasting with supervised and self-supervised learning tasks.</p> <p>To hammer it home, clustering is training-free. A clustering algorithm will take in features of your data points (the objects) and use those features to split your objects into groups. When successful, those groups highlight unique characteristics, giving you a view into the structure of your data.</p> <p>\ud83d\udca1 This means that clustering is an extremely powerful tool for exploring your data\u2014especially when your data is unlabeled!</p> <p>If you\u2019ve been paying close attention, you may have noticed the distinction subtly drawn between clustering and clustering algorithms. This is because clustering is an umbrella term encompassing various techniques!</p> <p>Clustering algorithms come in a few flavors, distinguished by the criterion they use to assign cluster membership. A few of the most common flavors of clustering are:</p> <p>Centroid-based clustering: for example, techniques like K-means and Mean Shift clustering. These methods try to find central points by which to define each cluster, called centroids, which seek to maximize some notion of coherence between points within a cluster. This flavor of clustering scales well to large datasets but is sensitive to outliers and random initialization. Often, multiple runs are performed, and the best one is chosen. You may find that techniques like K-means struggle with high-dimensional data \u2014 \u201cthe curse of dimensionality\u201d \u2014 and can better uncover structure when paired with dimensionality reduction techniques like uniform manifold approximation &amp; projection (UMAP). We\u2019ll explain how to pair the two below.</p> <p>Density-based clustering: techniques like DBSCAN, HDBSCAN, and OPTICS select clusters based on how sparsely or densely populated the feature space is. Conceptually, these algorithms treat high-density regions as clusters, breaking the clusters off when the points become sufficiently spread out in feature space. Simple density-based techniques like DBSCAN can have difficulty working with high-dimensional data, where data may not be densely colocated. However, more sophisticated techniques like HDBSCAN can overcome some of these limitations and uncover remarkable structure from high dimensional features.</p> <p>Hierarchical clustering: These techniques seek to either:</p> <ol> <li>Construct clusters by starting with individual points and iteratively combining clusters into larger composites or</li> <li>Deconstruct clusters, starting with all objects in one cluster and iteratively diving clusters into smaller components.</li> </ol> <p>Constructive techniques like Agglomerative Clustering become computationally expensive as the dataset grows, but performance can be quite impressive for small-to-medium datasets and low-dimensional features.</p> <p>\ud83d\udcda For a comprehensive discussion on 10+ of the most commonly used clustering algorithms, check out this intuitive, well-written guide from Scikit-learn!</p> <p>For the Lego bricks we started this discussion with, the features (length, width, height, curvature, etc.) are independent entities we can view as columns in a data table. After normalizing this data so that no one feature dominates the others, we could pass a row of numerical values as a feature vector into our clustering algorithm for each Lego block. Historically, clustering has had many applications like this, operating on lightly preprocessed numerical values from data tables or time series.</p> <p>Unstructured data like images don\u2019t fit quite as nicely into this framework for a few simple reasons:</p> <ol> <li>Images can vary in size (aspect ratio and resolution)</li> <li>Raw pixel values can be very noisy</li> <li>Correlations between pixels can be highly nonlinear</li> </ol> <p>If we were to go through the trouble of reshaping and standardizing all of our image sizes, normalizing pixel values, denoising, and flattening the multidimensional arrays into \u201cfeature vectors\u201d, treating these processed pixel arrays as features would put a tremendous amount of stress on the unsupervised clustering algorithm to uncover structure. This can work for simple datasets like MNIST, but it is often not an option in practice.</p> <p>Fortunately, we have powerful nonlinear function approximation tools called deep neural networks! Restricting our attention to the image domain, we have models like CLIP and DINOv2 whose output is a meaningful representation of the input data, and we have models trained for specific tasks like image classification, from which we typically take the outputs of the second to last layer of the network. There are also variational autoencoder (VAE) networks, from which it is common to take the representation at the middle layer!</p> <p>\ud83d\udca1Different models have different architectures, and were trained on different datasets and towards different tasks. All of these elements inform the types of features a model learns. Do your homework \ud83d\udcda:)</p> <p>With all that background out of the way, let\u2019s turn theory into practice and learn how to use clustering to structure our unstructured data. We\u2019ll be leveraging two open-source machine learning libraries: scikit-learn, which comes pre-packaged with implementations of most common clustering algorithms, and fiftyone, which streamlines the management and visualization of unstructured data:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -U scikit-learn fiftyone\n</pre> !pip install -U scikit-learn fiftyone <p>The FiftyOne Clustering Plugin makes our lives even easier.  It provides the connective tissue between scikit-learn\u2019s clustering algorithms and our images and wraps all of this in a simple UI within the FiftyOne App. We can install the plugin from the CLI:</p> In\u00a0[\u00a0]: Copied! <pre>!fiftyone plugins download https://github.com/jacobmarks/clustering-plugin\n</pre> !fiftyone plugins download https://github.com/jacobmarks/clustering-plugin <p>We will also need two more libraries: OpenAI\u2019s CLIP GitHub repo, enabling us to generate image features with the CLIP model, and the umap-learn library, which will let us apply a dimensionality reduction technique called Uniform Manifold Approximation and Projection (UMAP) to those features to visualize them in 2D:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install umap-learn git+https://github.com/openai/CLIP.git\n</pre> !pip install umap-learn git+https://github.com/openai/CLIP.git <p>Note that neither of these two libraries is strictly necessary \u2014 you could generate features with any model from the FiftyOne Model Zoo that exposes embeddings, and can perform dimensionality reduction with alternative techniques like PCA or tSNE.</p> <p>Once you have all of the necessary libraries installed, in a Python process, import the relevant FiftyOne modules, and load a dataset from the FiftyOne Dataset Zoo (or your data if you\u2019d like!). For this walkthrough, we\u2019ll be using the validation split (5,000 samples) from the MS COCO dataset:</p> In\u00a0[\u00a0]: Copied! <pre>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n\n# load dataset from the zoo, rename, and persist to database\ndataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\")\n# delete labels to simulate starting with unlabeled data\ndataset.select_fields().keep_fields()\ndataset.name = \"clustering-demo\"\ndataset.persistent = True\n\n# launch the app to visualize the dataset\nsession = fo.launch_app(dataset)\n</pre> import fiftyone as fo import fiftyone.brain as fob import fiftyone.zoo as foz from fiftyone import ViewField as F  # load dataset from the zoo, rename, and persist to database dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\") # delete labels to simulate starting with unlabeled data dataset.select_fields().keep_fields() dataset.name = \"clustering-demo\" dataset.persistent = True  # launch the app to visualize the dataset session = fo.launch_app(dataset) <p>If you\u2019re working in a Jupyter Notebook, you can pass auto=False and then open a tab in your browser to wherever <code>session.url</code> is pointing (typically \u200b\u200bhttp://localhost:5151/) to see the app in its full glory.</p> <p></p> <p>Now that we have our data, we must generate the features we will use to cluster. For this walkthrough, we will look at two different features: the 512-dimensional vectors generated by our CLIP Vision Transformer and the two-dimensional vectors generated by running these high-dimensional vectors through a UMAP dimensionality reduction routine.</p> <p>To run dimensionality reduction on a FiftyOne sample collection, we will use the FiftyOne Brain\u2019s <code>compute_visualization()</code> function, which supports UMAP, PCA, and tSNE via the method keyword argument. We could generate the CLIP embeddings using our dataset\u2019s <code>compute_embeddings()</code> method and then explicitly pass this into our dimensionality reduction routine. But instead, we can kill two birds with one stone by implicitly telling <code>compute_visualization()</code> to compute embeddings using CLIP and store these embeddings in a field <code>\u201dclip_embeddings\u201d</code>, then use these to get 2D representations:</p> In\u00a0[\u00a0]: Copied! <pre>res = fob.compute_visualization(\n    dataset, \n    model=\"clip-vit-base32-torch\", \n    embeddings=\"clip_embeddings\", \n    method=\"umap\", \n    brain_key=\"clip_vis\", \n    batch_size=10\n)\ndataset.set_values(\"clip_umap\", res.current_points)\n</pre> res = fob.compute_visualization(     dataset,      model=\"clip-vit-base32-torch\",      embeddings=\"clip_embeddings\",      method=\"umap\",      brain_key=\"clip_vis\",      batch_size=10 ) dataset.set_values(\"clip_umap\", res.current_points) <p>The <code>brain_key</code> argument allows us to access these results by name, either programmatically or in the FiftyOne App moving forward. The last line takes the array of 2D vectors we generated and stores them in a new field <code>\u201dclip_umap\u201d</code> on our dataset.</p> <p>Refreshing the app and opening an Embeddings Panel, we should see a 2D representation of our dataset, where each point in the plot corresponds to a single image:</p> <p></p> <p>With our feature vectors in hand, we can use the FiftyOne Clustering Plugin to bring structure to our data. In the FiftyOne App, press the backtick key on your keyboard and type <code>compute_clusters</code>. Click on the entry in the dropdown to open the clustering modal.</p> <p></p> <p>Enter a <code>run_key</code> (similar to the <code>brain_key</code> above) to access the clustering run's results. As you do so, watch the input form dynamically update. At this point, you have two key decisions to make: what features to cluster on and which clustering algorithm to employ!</p> <p>Select <code>\u201dkmeans\u201d</code> as your clustering method and <code>\u201dclip_umap\u201d</code> as your feature vectors. Set the number of clusters to 20, using the default values for all other parameters. Hit enter and let the clustering algorithm run. It should only take a few seconds.</p> <p>Once the computation finishes, notice the new field on your samples containing string representations of integers, which signify which cluster a given sample was assigned to. You can filter on these values directly and view one cluster at a time in the sample grid:</p> <p></p> <p>What is even more interesting is coloring by these cluster labels in our embeddings plot:</p> <p></p> <p>Visualizing your clusters like this allows you to sanity check the clustering routine and can provide an intuitive view into the structure of your data. In this example, we can see a cluster of teddy bears which is fairly well separated from the rest of our data. This clustering routine also uncovered a boundary between farm animals and more exotic animals like elephants and zebras.</p> <p>Now, create a new clustering run, increasing the number of clusters to 30 (don\u2019t forget to color the embeddings in this new field). Depending on a bit of randomness (all of the routine\u2019s initializations are random), there\u2019s a strong chance that elephants and zebras will now occupy their own clusters.</p> <p>Returning to the initial set of clusters, let\u2019s dig into one final area in the embeddings plot. Notice how a few images of people playing soccer got lumped into a cluster of primarily tennis images. This is because we passed 2D dimensionality reduced vectors into our clustering routine rather than the embedding vectors themselves. While 2D projections are helpful for visualization, and techniques like UMAP are fairly good at retaining structure, relative distances are not exactly preserved, and some information is lost. Suppose we instead pass our CLIP embeddings directly into our clustering computation with the same hyperparameters. In that case, these soccer images are assigned to the same cluster as the rest of the soccer images, along with other field sports like frisbee and baseball:</p> <p></p> <p>\ud83d\udca1 The key takeaway is that high-dimensional features are not better than low-dimensional ones or vice versa. Every choice comes with a trade-off. This is why you should experiment with different techniques, hyperparameters, and features.</p> <p>To make this even more apparent, let\u2019s use HDBSCAN as our clustering algorithm, which does not allow us to specify the number of clusters, replacing this with parameters like <code>min_cluster_size</code> and <code>max_cluster_size</code> along with criteria on which to merge clusters. We\u2019ll use our CLIP embeddings as features, and as a rough starting point, we\u2019ll say we only want clusters between 10 and 300 elements. If the cluster is too large, it may not be helpful; if it is too small, it may pick up on noise rather than signal. The specific values are, of course, dataset-dependent!</p> <p>When we color by our cluster labels, the results look a bit messy. However, when we look at the images for each cluster individually, we see that we identified some very interesting collections of samples in our dataset.</p> <p></p> <p>Note that for HDBSCAN, label <code>-1</code> is given to all background images. These images are not merged into any of the final clusters.</p> <p>As you test out various combinations of features, clustering techniques, and hyperparameters, you may want to keep track of what \u201cconfiguration\u201d you used to generate a specific set of clusters. Fortunately, the FiftyOne Clustering Plugin handles all of this for you, using custom runs. The plugin exposes an operator <code>get_clustering_run_info</code>, which lets you select a run by run_key and view a nicely formatted printout of all of the run\u2019s parameters in the app:</p> <p></p> <p>You can also access this information programmatically by passing the <code>run_key</code> to the dataset\u2019s <code>get_run_info()</code> method!</p> <p>Until now, our clusters have only had numbers, which we have used as a glorified housekeeping device. However, if we cluster for some specific characteristic in our dataset, we should be able to identify that and use it to label our samples loosely. Naively, we could go through our clusters individually, select and visualize just the images in a given cluster, and try to tag the cluster ourselves.</p> <p>Or\u2026we could use a multimodal large language model to do this for us! The FiftyOne Clustering Plugin provides this functionality, leveraging GPT-4V's multimodal understanding capabilities to give each cluster a conceptual label.</p> <p>To use this functionality, you must have an OpenAI API key environment variable (creating an account if necessary), which you can set as follows:</p> In\u00a0[\u00a0]: Copied! <pre>!export OPENAI_API_KEY=sk-...\n</pre> !export OPENAI_API_KEY=sk-... <p>This functionality is provided via the <code>label_clusters_with_gpt4v</code> operator, which randomly selects five images from each cluster, feeds them into GPT-4V with a task-specific prompt, and processes the results.</p> <p>Depending on the number of clusters you have (GPT-4V can be slow, and this scales linearly in the number of clusters), you may want to delegate execution of the operation by checking the box in the operator\u2019s modal and then launch the job from the command line with:</p> <pre>fiftyone delegated launch\n</pre> <p></p> <p>In this walkthrough, we covered how to combine deep neural networks with popular clustering algorithms to bring structure to unstructured data using scikit-learn and FiftyOne. Along the way, we saw that the feature vectors, the algorithm, and the hyperparameter you choose can greatly impact the final results of clustering computations, both in terms of what the clusters select for and how well they identify structure in your data.</p> <p>Once you have run these clustering routines on your data, a few key questions arise:</p> <ol> <li>How do I quantitatively compare and contrast these clustering runs?</li> <li>How do I synthesize the insights from multiple clustering runs to better understand my data?</li> <li>How do I leverage these insights to train better models?</li> </ol> <p>Answering these questions will help you reap the rewards of clustering.</p> <p>If you want to dive deeper into the world of clustering, here are a few avenues that you may want to explore:</p> <ul> <li>Choice of embedding model: We used CLIP, a semantic foundation model for this walkthrough. See how things change when you use other semantic models from Hugging Face\u2019s Transformers library, or OpenCLIP. Now see how the picture changes when you use a \u201cpixels-and-patches\u201d computer vision model like ResNet50, or a self\u2013supervised model like DINOv2.</li> <li>Clustering Hyperparameters: We barely touched the number of clusters in this walkthrough. Your results may vary as you increase or decrease this number. For some techniques, like k-means clustering, there are heuristics you can use to estimate the optimal number of clusters. Don\u2019t stop there; experiment with other hyperparameters as well!</li> <li>Concept Modeling Techniques: the built-in concept modeling technique in this walkthrough uses GPT-4V and some light prompting to identify each cluster's core concept. This is but one way to approach an open-ended problem. Try using image captioning and topic modeling, or create your own technique!</li> </ul>"},{"location":"tutorials/clustering/#clustering-images-with-embeddings","title":"Clustering Images with Embeddings\u00b6","text":""},{"location":"tutorials/clustering/#what-is-clustering","title":"What is Clustering?\u00b6","text":""},{"location":"tutorials/clustering/#the-building-blocks-of-clustering","title":"The Building Blocks of Clustering\u00b6","text":""},{"location":"tutorials/clustering/#how-clustering-works","title":"How Clustering Works\u00b6","text":""},{"location":"tutorials/clustering/#what-features-do-i-cluster-on","title":"What Features Do I Cluster On?\u00b6","text":""},{"location":"tutorials/clustering/#clustering-images-with-fiftyone-and-scikit-learn","title":"Clustering Images with FiftyOne and Scikit-learn\u00b6","text":""},{"location":"tutorials/clustering/#setup-and-installation","title":"Setup and Installation\u00b6","text":""},{"location":"tutorials/clustering/#generating-features","title":"Generating Features\u00b6","text":""},{"location":"tutorials/clustering/#computing-and-visualizing-clusters","title":"Computing and Visualizing Clusters\u00b6","text":""},{"location":"tutorials/clustering/#keeping-track-of-clustering-runs","title":"Keeping Track of Clustering Runs\u00b6","text":""},{"location":"tutorials/clustering/#labeling-clusters-with-gpt-4v","title":"Labeling Clusters with GPT-4V\u00b6","text":""},{"location":"tutorials/clustering/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"tutorials/cvat_annotation/","title":"Annotating Datasets with CVAT","text":"<p>So, what's the takeaway?</p> <p>FiftyOne makes it incredibly easy to explore datasets, understand them, and discover ways to improve them. This walkthrough covers the imporant next step: using CVAT to take action to both annotate datasets and correct existing label deficiencies that you've identified in your datasets.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>In order to use CVAT, you must create an account on a CVAT server.</p> <p>By default, FiftyOne uses app.cvat.ai. So if you haven't already, go to app.cvat.ai and create an account now.</p> <p>Another option is to set up CVAT locally and then configure FiftyOne to use your self-hosted server. A primary benefit of setting up CVAT locally is that you are limited to 10 tasks and 500MB of data with app.cvat.ai.</p> <p>In any case, FiftyOne will need to connect to your CVAT account. The easiest way to configure your CVAT login credentials is to store them in environment variables:</p> In\u00a0[\u00a0]: Copied! <pre>!export FIFTYONE_CVAT_USERNAME=&lt;YOUR_USERNAME&gt;\n!export FIFTYONE_CVAT_PASSWORD=&lt;YOUR_PASSWORD&gt;\n!export FIFTYONE_CVAT_EMAIL=&lt;YOUR_EMAIL&gt;  # if applicable\n</pre> !export FIFTYONE_CVAT_USERNAME= !export FIFTYONE_CVAT_PASSWORD= !export FIFTYONE_CVAT_EMAIL=  # if applicable <p>There are also other ways to configure your login credentials if you prefer.</p> <p>In this tutorial we\u2019ll use the UMAP technique for visualizing samples in a 2D scatterplot. We'll also optionally run an object detection model that requires TensorFlow:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install umap-learn tensorflow\n</pre> !pip install umap-learn tensorflow  <p>For most machine learning projects, the first step is to collect a suitable dataset for the task. For computer vision projects specifically, this will generally result in thousands of images or videos that have been gathered from internet sources like Flickr or captured by new footage from a data acquisition team.</p> <p>With collections containing thousands or millions of samples, the cost to annotate every single sample can be astronomical. It thus makes sense to ensure that only the most useful and relevant data is being sent to annotation. One metric for how \"useful\" data is in training a model is how unique the example is with respect to the rest of the dataset. Multiple similar examples will not provide the model with as much new information to learn as visually unique examples.</p> <p>FiftyOne provides a visual similarity capability that we'll use in this tutorial to select some unique images to annotate.</p> <p>We begin by loading a set of unlabeled images into FiftyOne. This can be done in just one line of code. For example, if you are using your own data you can run the following:</p> In\u00a0[1]: Copied! <pre># Example\nimport fiftyone as fo\n\ndataset_dir = \"/path/to/raw/data\"\nname = \"my_dataset\"\n\ndataset = fo.Dataset.from_dir(\n    dataset_dir=dataset_dir,\n    dataset_type=fo.types.ImageDirectory,\n    name=name,\n)\n</pre> # Example import fiftyone as fo  dataset_dir = \"/path/to/raw/data\" name = \"my_dataset\"  dataset = fo.Dataset.from_dir(     dataset_dir=dataset_dir,     dataset_type=fo.types.ImageDirectory,     name=name, ) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 0/0 [3.5ms elapsed, ? remaining, ? samples/s] \n</pre> <p>However, in this walkthrough, we'll download some images from the Open Images V6 dataset using the built-in FiftyOne Dataset Zoo.</p> In\u00a0[2]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"open-images-v6\",\n    split=\"validation\",\n    label_types=[],\n    max_samples=200,\n)\n</pre> import fiftyone as fo import fiftyone.zoo as foz  dataset = foz.load_zoo_dataset(     \"open-images-v6\",     split=\"validation\",     label_types=[],     max_samples=200, ) <pre>Downloading split 'validation' to '/home/voxel51/fiftyone/open-images-v6/validation' if necessary\nNecessary images already downloaded\nExisting download of split 'validation' is sufficient\nLoading 'open-images-v6' split 'validation'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [72.4ms elapsed, 0s remaining, 2.8K samples/s]   \nDataset 'open-images-v6-validation-200' created\n</pre> <p>Now let's make the dataset persistent so that we can access it in future Python sessions.</p> In\u00a0[\u00a0]: Copied! <pre>dataset.persistent = True\n</pre> dataset.persistent = True <p>Now that the data is loaded, let's visualize it in the FiftyOne App:</p> In\u00a0[3]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) Activate In\u00a0[4]: Copied! <pre>session.freeze() # screen shot the App for this example\n</pre> session.freeze() # screen shot the App for this example <p>Now let's run the compute_similarity() method on the dataset in order to index all samples in the dataset by their visual similarity.</p> <p>Once this is done, we can then use the index to find the most unique samples:</p> In\u00a0[5]: Copied! <pre>import fiftyone.brain as fob\n\nresults = fob.compute_similarity(dataset, brain_key=\"img_sim\")\nresults.find_unique(100)\n</pre> import fiftyone.brain as fob  results = fob.compute_similarity(dataset, brain_key=\"img_sim\") results.find_unique(100) <pre>Computing embeddings...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [2.2m elapsed, 0s remaining, 1.7 samples/s]      \nGenerating index...\nIndex complete\nComputing unique samples...\nthreshold: 1.000000, kept: 200, target: 100\nthreshold: 2.000000, kept: 200, target: 100\nthreshold: 4.000000, kept: 200, target: 100\nthreshold: 8.000000, kept: 122, target: 100\nthreshold: 16.000000, kept: 7, target: 100\nthreshold: 12.000000, kept: 26, target: 100\nthreshold: 10.000000, kept: 66, target: 100\nthreshold: 9.000000, kept: 81, target: 100\nthreshold: 8.500000, kept: 98, target: 100\nthreshold: 8.250000, kept: 107, target: 100\nthreshold: 8.375000, kept: 103, target: 100\nthreshold: 8.437500, kept: 102, target: 100\nthreshold: 8.468750, kept: 100, target: 100\nUniqueness computation complete\n</pre> <p>We can also visualize the exact samples that were selected.</p> In\u00a0[6]: Copied! <pre>vis_results = fob.compute_visualization(dataset, brain_key=\"img_vis\")\n\nplot = results.visualize_unique(visualization=vis_results)\nplot.show()\n</pre> vis_results = fob.compute_visualization(dataset, brain_key=\"img_vis\")  plot = results.visualize_unique(visualization=vis_results) plot.show() <pre>Computing embeddings...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [2.1m elapsed, 0s remaining, 1.7 samples/s]      \nGenerating visualization...\nUMAP(dens_frac=0.0, dens_lambda=0.0, verbose=True)\nConstruct fuzzy simplicial set\nFri Aug 20 13:59:55 2021 Finding Nearest Neighbors\nFri Aug 20 13:59:58 2021 Finished Nearest Neighbor Search\nFri Aug 20 14:00:01 2021 Construct embedding\n\tcompleted  0  /  500 epochs\n\tcompleted  50  /  500 epochs\n\tcompleted  100  /  500 epochs\n\tcompleted  150  /  500 epochs\n\tcompleted  200  /  500 epochs\n\tcompleted  250  /  500 epochs\n\tcompleted  300  /  500 epochs\n\tcompleted  350  /  500 epochs\n\tcompleted  400  /  500 epochs\n\tcompleted  450  /  500 epochs\nFri Aug 20 14:00:05 2021 Finished embedding\n</pre> <pre></pre> <p>This plot can be interacted with by attaching it to your <code>session</code> object, allowing you to select points in the plot and visualize them in the App.</p> <p>Note: Interactive plots currently only work in Jupyter notebook environments.</p> In\u00a0[8]: Copied! <pre>session.plots.attach(plot, name=\"unique\")\nsession.show()\n</pre> session.plots.attach(plot, name=\"unique\") session.show() Activate <p>Now let's create a DatasetView into the dataset containing only the unique samples that were selected and visualize them.</p> In\u00a0[9]: Copied! <pre>unique_view = dataset.select(results.unique_ids)\nsession.view = unique_view\n</pre> unique_view = dataset.select(results.unique_ids) session.view = unique_view Activate In\u00a0[10]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>Having reduced the number of samples that need to be annotated, the time and cost of annotating this dataset have also been reduced.</p> <p>Whether you are annotating the data yourself or have a team of annotators, the workflow of uploading data from FiftyOne to CVAT is the same. The annotate() method on a collection of samples lets you specify the name, type, and classes for the labels you are annotating.</p> <p>For example, let's annotate instance segmentation masks for the classes \"person\", \"vehicle\", and \"animal\".</p> <p>We'll only include a few samples to be annotated in our view for brevity. To create annotation jobs in CVAT for these samples, we simply call annotate() passing in a unique name for this annotation run and the relevant label schema information for the annotation task. Since we'll be annotating these samples ourselves, we pass <code>launch_editor=True</code> to automatically launch a browser window with the CVAT editor open once the data has been loaded.</p> In\u00a0[10]: Copied! <pre># Randomly select 5 samples to load to CVAT\nunique_5_view = unique_view.take(5)\n\n# A unique identifer for this run\nanno_key = \"segs_run\"\n\n# Upload the samples and launch CVAT\nanno_results = unique_5_view.annotate(\n    anno_key,\n    label_field=\"segmentations\",\n    label_type=\"instances\",\n    classes=[\"person\", \"vehicle\", \"animal\"],\n    launch_editor=True,\n)\n</pre> # Randomly select 5 samples to load to CVAT unique_5_view = unique_view.take(5)  # A unique identifer for this run anno_key = \"segs_run\"  # Upload the samples and launch CVAT anno_results = unique_5_view.annotate(     anno_key,     label_field=\"segmentations\",     label_type=\"instances\",     classes=[\"person\", \"vehicle\", \"animal\"],     launch_editor=True, ) <pre>Uploading samples to CVAT...\nComputing image metadata...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [151.6ms elapsed, 0s remaining, 33.0 samples/s] \nUpload complete\nLaunching editor at 'https://app.cvat.ai/tasks/386/jobs/441'...\n</pre> <p>The <code>CVATAnnotationResults</code> object that was returned can be used to get the current status of the tasks that were created:</p> In\u00a0[12]: Copied! <pre>anno_results.print_status()\n</pre> anno_results.print_status() <pre>\nStatus for label field 'segmentations':\n\n\tTask 386 (FiftyOne_open-images-v6-validation-200_segmentations):\n\t\tStatus: annotation\n\t\tAssignee: None\n\t\tLast updated: 2021-08-20T21:22:37.928988Z\n\t\tURL: https://app.cvat.ai/tasks/386\n\n\t\tJob 441:\n\t\t\tStatus: annotation\n\t\t\tAssignee: None\n\t\t\tReviewer: None\n\n</pre> <p></p> <p>Once the annotation is complete and saved in CVAT, we can download the annotations and automatically update our dataset by calling load_annotations().</p> <p>Since accounts on app.cvat.ai only allow for 10 tasks, we'll set <code>cleanup=True</code> so that the CVAT tasks are automatically deleted after the annotations are loaded.</p> In\u00a0[3]: Copied! <pre>unique_5_view.load_annotations(\"segs_run\", cleanup=True)\n\nsession.view = unique_5_view\n</pre> unique_5_view.load_annotations(\"segs_run\", cleanup=True)  session.view = unique_5_view <pre>Downloading labels from CVAT...\nDownload complete\nAdding labels for 'segmentations'...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [76.5ms elapsed, 0s remaining, 65.4 samples/s] \n</pre> Activate In\u00a0[14]: Copied! <pre>session.freeze() # screen shot the App for this example\n</pre> session.freeze() # screen shot the App for this example <p>If you want to upload the dataset to a team of annotators, you can provide the CVAT usernames of the annotators and reviewers that will be assigned round-robin style. The <code>segment_size</code> parameter is used to define the maximum number of images in each job in your CVAT task.</p> In\u00a0[24]: Copied! <pre>anno_key = \"full_annot\"\nunique_view.annotate(\n    anno_key,\n    label_field=\"ground_truth_cls\",\n    label_type=\"classifications\",\n    classes=[\"person\", \"vehicle\", \"animal\"],\n    segment_size=25,\n    job_reviewers=[\"user1\", \"user2\"],\n    job_assignees=[\"user3\", \"user4\"],\n    task_assignee=\"user5\",\n)\n</pre> anno_key = \"full_annot\" unique_view.annotate(     anno_key,     label_field=\"ground_truth_cls\",     label_type=\"classifications\",     classes=[\"person\", \"vehicle\", \"animal\"],     segment_size=25,     job_reviewers=[\"user1\", \"user2\"],     job_assignees=[\"user3\", \"user4\"],     task_assignee=\"user5\", ) <pre>Uploading samples to CVAT...\nUpload complete\n</pre> Out[24]: <pre></pre> <p></p> <p>For larger datasets, the annotation process may take some time. The <code>anno_key</code> that we provided stores the relevant information about this annotation run on the dataset itself. When the annotations are ready to be imported back into FiftyOne, we can easily do so.</p> In\u00a0[\u00a0]: Copied! <pre>dataset.name = \"example_dataset\"\n</pre> dataset.name = \"example_dataset\" <p>In practice, annotation tasks can take awhile. But don't worry, you can always load your annotations back onto your dataset in the future in a new Python session:</p> In\u00a0[\u00a0]: Copied! <pre>import fiftyone as fo\n\ndataset = fo.load_dataset(\"example_dataset\")\n\nanno_key = \"full_annot\"\ndataset.load_annotations(anno_key)\n\nview = dataset.load_annotation_view(anno_key)\nsession = fo.launch_app(view=view)\n</pre> import fiftyone as fo  dataset = fo.load_dataset(\"example_dataset\")  anno_key = \"full_annot\" dataset.load_annotations(anno_key)  view = dataset.load_annotation_view(anno_key) session = fo.launch_app(view=view) <p>Creating new labels to exactly match your schema can require a lot of specifics in terms of the classes, attributes, and types that are used. This section outlines how different parameters can be used to specify an annotation run.</p> <p></p> <p>The attribute annotation formats available for CVAT are:</p> <ul> <li><code>text</code>: a free-form text box. In this case, default is optional and values is unused</li> <li><code>select</code>: a selection dropdown. In this case, values is required and default is optional</li> <li><code>radio</code>: a radio button list UI. In this case, values is required and default is optional</li> <li><code>checkbox</code>: a boolean checkbox UI. In this case, default is optional and values is unused</li> <li><code>occluded</code>: CVAT\u2019s builtin occlusion toggle icon. This widget type can only be specified for at most one attribute, which must be a boolean</li> </ul> <p>When you are annotating existing label fields, the <code>attributes</code> parameter can take additional values:</p> <ul> <li><code>True</code> (default): export all custom attributes observed on the existing labels, using their observed values to determine the appropriate UI type and possible values, if applicable</li> <li><code>False</code>: do not include any custom attributes in the export</li> <li>a list of custom attributes to include in the export</li> <li>a full dictionary syntax described above</li> </ul> <p>Note that only scalar-valued label attributes are supported. Other attribute types like lists, dictionaries, and arrays will be omitted.</p> <p>For example, the following will create <code>attr1</code> with text-box input and <code>attr2</code> with drop-down selection of the given values:</p> In\u00a0[24]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nattributes = {\n    \"attr1\": {\n        \"type\": \"text\",\n    },\n    \"attr2\": {\n        \"type\": \"select\",\n        \"values\": [\"val1\", \"val2\"],\n        \"default\": \"val1\",\n    }\n}\n\nrandom_view = dataset.take(1, seed=51)\n\nanno_key = \"random_attrs\"\nrandom_view.annotate(\n    anno_key,\n    label_field=\"ground_truth\",\n    attributes=attributes,\n    launch_editor=True,\n)\n</pre> import fiftyone as fo import fiftyone.zoo as foz  dataset = foz.load_zoo_dataset(\"quickstart\")  attributes = {     \"attr1\": {         \"type\": \"text\",     },     \"attr2\": {         \"type\": \"select\",         \"values\": [\"val1\", \"val2\"],         \"default\": \"val1\",     } }  random_view = dataset.take(1, seed=51)  anno_key = \"random_attrs\" random_view.annotate(     anno_key,     label_field=\"ground_truth\",     attributes=attributes,     launch_editor=True, ) <pre>Uploading samples to CVAT...\nComputing image metadata...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [141.7ms elapsed, 0s remaining, 7.1 samples/s] \nSamples uploaded successfully\nLaunching editor for label field 'ground_truth' of type detections at https://app.cvat.ai/tasks/349/jobs/393\n</pre> <p>In CVAT, <code>attr1</code> and <code>attr2</code> are now available for annotation on every new and existing label.</p> <p></p> <p>These annotations can now be loaded back into FiftyOne with load_annotations().</p> In\u00a0[26]: Copied! <pre>random_view.load_annotations(\"random_attrs\")\n</pre> random_view.load_annotations(\"random_attrs\") In\u00a0[27]: Copied! <pre>session.view = random_view\n</pre> session.view = random_view Activate In\u00a0[28]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>Note: When uploading existing labels to CVAT, the id of the labels in FiftyOne are stored in a <code>label_id</code> attribute of the CVAT shapes. <code>label_id</code> is the single-source of provenance for a label. If this attribute is modified or deleted in CVAT, then FiftyOne will not be able to merge the annotation with its existing Label instance when the annotations are loaded back into FiftyOne. Instead, the existing label will be deleted and a new Label will be created. This can result in data loss if you sent only a subset of the label\u2019s attributes to CVAT.</p> In\u00a0[39]: Copied! <pre>label_schema = {\n    \"ground_truth\": {},\n    \"gt_segmentations\": {\n        \"type\": \"instances\",\n        \"classes\": [\"person\", \"vehicle\", \"food\"],\n        \"attributes\": {\n            \"iscrowd\": {\n                \"type\": \"radio\",\n                \"values\": [True, False],\n                \"default\": False,\n            }\n        }\n    }\n}\n\nrandom_view = dataset.take(1, seed=52)\n\nanno_key = \"random_segs\"\nrandom_view.annotate(anno_key, label_schema=label_schema, launch_editor=True)\n</pre> label_schema = {     \"ground_truth\": {},     \"gt_segmentations\": {         \"type\": \"instances\",         \"classes\": [\"person\", \"vehicle\", \"food\"],         \"attributes\": {             \"iscrowd\": {                 \"type\": \"radio\",                 \"values\": [True, False],                 \"default\": False,             }         }     } }  random_view = dataset.take(1, seed=52)  anno_key = \"random_segs\" random_view.annotate(anno_key, label_schema=label_schema, launch_editor=True) <pre>Uploading samples to CVAT...\nSamples uploaded successfully\nLaunching editor for label field 'ground_truth' of type instances at https://app.cvat.ai/tasks/354/jobs/398\n</pre> <p></p> In\u00a0[40]: Copied! <pre>random_view.load_annotations(\"random_segs\")\n</pre> random_view.load_annotations(\"random_segs\") In\u00a0[41]: Copied! <pre>session.view = random_view\n</pre> session.view = random_view Activate In\u00a0[42]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>Note: CVAT annotation schemas do not have a notion of label fields. Therefore, if you define an annotation schema that involves the same class label in multiple fields, the name of the label field will be appended to the class in CVAT in order to distinguish the class labels.</p> <p>In many projects, a dataset already exists and is being used to train models. In such cases, the best use of time is likely to improve the quality of the dataset, which often provides greater performance gains than similar effort into optimizing the model architecture.</p> <p>FiftyOne provides a powerful API and App workflows to identify the samples/annotations that need to be updated, and the tight integration with CVAT allows you to take the necessary actions to improve your dataset's quality.</p> <p>To demonstrate this workflow, let's load a subset of the COCO object detection dataset from the FiftyOne Dataset Zoo.</p> In\u00a0[54]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    max_samples=200,\n)\n</pre> import fiftyone as fo import fiftyone.zoo as foz  dataset = foz.load_zoo_dataset(     \"coco-2017\",     split=\"validation\",     max_samples=200, ) <pre>Downloading split 'validation' to '/home/voxel51/fiftyone/coco-2017/validation' if necessary\nFound annotations at '/home/voxel51/fiftyone/coco-2017/raw/instances_val2017.json'\nSufficient images already downloaded\nExisting download of split 'validation' is sufficient\nLoading 'coco-2017' split 'validation'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [1.4s elapsed, 0s remaining, 147.7 samples/s]         \nDataset 'coco-2017-validation-200' created\n</pre> <p>If your machine has a GPU, then you can run the following code to generate predictions on the dataset with an object detection model from the FiftyOne Model Zoo. Note that you can also easily load your own model predictions onto your FiftyOne dataset.</p> In\u00a0[48]: Copied! <pre>model = foz.load_zoo_model(\"faster-rcnn-resnet50-coco-tf\")\ndataset.apply_model(model, \"predictions\")\n</pre> model = foz.load_zoo_model(\"faster-rcnn-resnet50-coco-tf\") dataset.apply_model(model, \"predictions\") <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 250/250 [3.6m elapsed, 0s remaining, 1.2 samples/s]      \n</pre> <p>For CPU only machines, let's save time and just load the <code>quickstart</code> dataset from the dataset zoo, which contains a small subset of COCO with precomputed predictions.</p> In\u00a0[1]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n</pre> import fiftyone as fo import fiftyone.zoo as foz  dataset = foz.load_zoo_dataset(\"quickstart\") <pre>Dataset already downloaded\nLoading 'quickstart'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [4.3s elapsed, 0s remaining, 38.8 samples/s]      \nDataset 'quickstart' created\n</pre> <p>Let's visualize these model predictions.</p> In\u00a0[60]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) Activate In\u00a0[31]: Copied! <pre>session.freeze() # screen shot the App for this example\n</pre> session.freeze() # screen shot the App for this example <p>In order to find specific cases of how this model performed, we can evaluate the object detections. When running evaluation, we can provide an <code>eval_key</code>, which will cause per-label TP/FP/FN evaluation results to be stored on our samples for future use.</p> In\u00a0[5]: Copied! <pre>results = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval\",\n    compute_mAP=True,\n)\nprint(results.mAP())\n</pre> results = dataset.evaluate_detections(     \"predictions\",     gt_field=\"ground_truth\",     eval_key=\"eval\",     compute_mAP=True, ) print(results.mAP()) <pre>Evaluating detections...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [6.5s elapsed, 0s remaining, 22.3 samples/s]       \nPerforming IoU sweep...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [12.5s elapsed, 0s remaining, 15.3 samples/s]      \n0.3957238101325776\n</pre> <p>Using the query language of FiftyOne, we can construct different views into the dataset. Specifically, let's find the samples where the model was confident in its prediction but the prediction was labeled incorrect (false positive). This indicates that the model may have been correct but the ground truth annotation was incorrect.</p> In\u00a0[61]: Copied! <pre>from fiftyone import ViewField as F\n\nview = dataset.filter_labels(\n    \"predictions\",\n    (F(\"confidence\") &gt; 0.85) &amp; (F(\"eval\") == \"fp\")\n).sort_by(\n    F(\"predictions.detections\").length(),\n    reverse=True,\n)\n\nsession.view = view\n</pre> from fiftyone import ViewField as F  view = dataset.filter_labels(     \"predictions\",     (F(\"confidence\") &gt; 0.85) &amp; (F(\"eval\") == \"fp\") ).sort_by(     F(\"predictions.detections\").length(),     reverse=True, )  session.view = view Activate In\u00a0[63]: Copied! <pre>session.freeze() # screen shot the App for this example\n</pre> session.freeze() # screen shot the App for this example <p>Browsing through some of these results, we can see a pattern emerge. The COCO dataset annotations have an <code>iscrowd</code> attribute that indicates if a bounding box contains a crowd of multiple objects or just a single instance of the object. In many of the situations where the model was incorrect, the <code>iscrowd</code> attribute is incorrectly annotated or missing entirely.</p> <p>We can tag some of these samples for annotation by clicking selecting the relevant samples and clicking the tag button.</p> In\u00a0[64]: Copied! <pre>session.show()\n</pre> session.show() Activate In\u00a0[65]: Copied! <pre>session.freeze() # screen shot the App for this example\n</pre> session.freeze() # screen shot the App for this example In\u00a0[2]: Copied! <pre>tagged_view = dataset.match_tags(\"requires_annotation\")\n</pre> tagged_view = dataset.match_tags(\"requires_annotation\") <p>We can now use the <code>annotate()</code> method to upload these samples and labels to CVAT for re-annotation.</p> <p>The following code creates a new task in your account in CVAT containing only the samples with the <code>requires_annotation</code> tag.</p> In\u00a0[4]: Copied! <pre>anno_key = \"tagged_anno\"\ntagged_view.annotate(anno_key, label_field=\"ground_truth\", launch_editor=True)\n</pre> anno_key = \"tagged_anno\" tagged_view.annotate(anno_key, label_field=\"ground_truth\", launch_editor=True) <pre>Uploading samples to CVAT...\nUpload complete\nLaunching editor at 'https://app.cvat.ai/tasks/383/jobs/434'...\n</pre> <p>In CVAT, click on the box whose <code>iscrowd</code> attribute we want to modify. On the attributes sidebar, enter a value of <code>1</code> for the attribute:</p> <p></p> <p>After updating the relevant annotations in all of the samples, make sure to hit the save button in CVAT. Now that the re-annotation is complete, let's load the updated labels back into FiftyOne and clean up the tasks that were created in CVAT.</p> In\u00a0[10]: Copied! <pre>tagged_view.load_annotations(\"tagged_anno\", cleanup=True)\nsession.view = tagged_view\n</pre> tagged_view.load_annotations(\"tagged_anno\", cleanup=True) session.view = tagged_view <pre>Downloading labels from CVAT...\nDownload complete\nMerging labels for 'ground_truth'...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [11.1ms elapsed, 0s remaining, 89.9 samples/s] \n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [352.8ms elapsed, 0s remaining, 2.8 samples/s] \n</pre> Activate In\u00a0[11]: Copied! <pre>session.freeze() # screen shot the App for this example\n</pre> session.freeze() # screen shot the App for this example <p>As we can see, the ground truth labels on the dataset have been updated. Let's evaluate the same model again on these updated labels.</p> In\u00a0[12]: Copied! <pre>results = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval\",\n    compute_mAP=True,\n)\nprint(results.mAP())\n</pre> results = dataset.evaluate_detections(     \"predictions\",     gt_field=\"ground_truth\",     eval_key=\"eval\",     compute_mAP=True, ) print(results.mAP()) <pre>Evaluating detections...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [6.1s elapsed, 0s remaining, 28.2 samples/s]      \nPerforming IoU sweep...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [15.3s elapsed, 0s remaining, 13.2 samples/s]      \n0.3984999388520894\n</pre> <p>The mAP of the model has improved from 39.57% to 39.85% just by updating a single label in one sample!</p> <p>In practice, the next step is to spend more time exploring the dataset in FiftyOne finding more annotation mistakes and sending them to a team of annotators in CVAT for review and re-annotation. Specific users can be assigned to annotate or review the tasks that are created through this API.</p> In\u00a0[\u00a0]: Copied! <pre>anno_key = \"all_mistakes\"\ntagged_view.annotate(\n    anno_key,\n    label_field=\"ground_truth\", \n    segment_size=25,\n    task_assignee=\"user1\",\n    job_assignees=[\"user2\", \"user3\"],\n    job_reviewers=\"user4\",\n)\n</pre> anno_key = \"all_mistakes\" tagged_view.annotate(     anno_key,     label_field=\"ground_truth\",      segment_size=25,     task_assignee=\"user1\",     job_assignees=[\"user2\", \"user3\"],     job_reviewers=\"user4\", ) <p>This workflow has shown how to improve the validation split of a dataset and the subsequent performance of a model on that split. However, we never actually retrained the model at this point!</p> <p>In practice, a complete workflow might include performing K-fold cross-validation on the dataset. This is where the complete dataset is split into K equal parts where each is treated as the validation split and the model is retrained K times on each combination of the remaining splits. Following the above steps on each of these K validation splits will then result in the entire dataset having been iterated over and improved. Finally, the model should then be retrained on the entirety of this newly updated dataset.</p> <p>Let's upload and modify some labels from the <code>quickstart-video</code> dataset. The workflow in FiftyOne looks identical to the image-based workflow. The only difference is needing to navigate to each video task separately inside of CVAT.</p> In\u00a0[43]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n</pre> import fiftyone as fo import fiftyone.zoo as foz In\u00a0[44]: Copied! <pre>dataset = foz.load_zoo_dataset(\"quickstart-video\")\n</pre> dataset = foz.load_zoo_dataset(\"quickstart-video\") <pre>Dataset already downloaded\nLoading 'quickstart-video'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [5.0s elapsed, 0s remaining, 1.9 samples/s]      \nDataset 'quickstart-video' created\n</pre> In\u00a0[45]: Copied! <pre>random_view = dataset.take(2, seed=51)\n</pre> random_view = dataset.take(2, seed=51) In\u00a0[50]: Copied! <pre>anno_key = \"vid_anno\"\nrandom_view.annotate(anno_key, label_field=\"frames.detections\", launch_editor=True)\n</pre> anno_key = \"vid_anno\" random_view.annotate(anno_key, label_field=\"frames.detections\", launch_editor=True) <pre>Uploading samples to CVAT...\nComputing video metadata...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [296.8ms elapsed, 0s remaining, 3.4 samples/s] \nComputing video metadata...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [141.0ms elapsed, 0s remaining, 7.1 samples/s] \nSamples uploaded successfully\nLaunching editor for label field 'frames.detections' of type detections at https://app.cvat.ai/tasks/356/jobs/400\n</pre> <p>In CVAT, you can press the play button to watch the video and annotations.</p> <p>In the attributes, the tracks that are shown provide some additional options over standard labels. For example, the first square icon with a line coming out of it indicates that the object is \"outside\" of the frame and should not be loaded into FiftyOne for that frame.</p> <p></p> <p>Let's save our annotation work and then load the labels back into FiftyOne.</p> In\u00a0[\u00a0]: Copied! <pre>random_view.load_annotations(\"vid_anno\", cleanup=True)\n</pre> random_view.load_annotations(\"vid_anno\", cleanup=True) In\u00a0[5]: Copied! <pre>session.view = random_view\n</pre> session.view = random_view Activate In\u00a0[6]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>Building high-quality datasets is the best way to produce high-performing models, and FiftyOne and CVAT are two open-source tools that you can use to curate and annotate datasets from scratch, as well as explore and re-annotate existing datasets to improve them.</p> <p>Thanks to the tight integration between FiftyOne and CVAT, the power of one of the most popular open-source annotation tools is just one command away when working with your datasets in FiftyOne.</p>"},{"location":"tutorials/cvat_annotation/#annotating-datasets-with-cvat","title":"Annotating Datasets with CVAT\u00b6","text":"<p>FiftyOne and CVAT are two leading open-source tools, each tackling different parts of the dataset curation and improvement workflows.</p> <p>The tight integration between FiftyOne and CVAT allows you to curate and explore datasets in FiftyOne and then send off samples or existing labels for annotation in CVAT with just one line of code.</p> <p>This walkthrough covers:</p> <ul> <li>Selecting subsets and annotating unlabeled image datasets with CVAT</li> <li>Improving datasets and fixing annotation mistakes with CVAT</li> <li>Annotating videos with CVAT</li> </ul>"},{"location":"tutorials/cvat_annotation/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"tutorials/cvat_annotation/#unlabeled-dataset-annotation","title":"Unlabeled dataset annotation\u00b6","text":""},{"location":"tutorials/cvat_annotation/#loading-data","title":"Loading data\u00b6","text":""},{"location":"tutorials/cvat_annotation/#finding-unique-samples","title":"Finding unique samples\u00b6","text":""},{"location":"tutorials/cvat_annotation/#annotating-samples-in-cvat","title":"Annotating samples in CVAT\u00b6","text":""},{"location":"tutorials/cvat_annotation/#additional-annotation-parameters","title":"Additional annotation parameters\u00b6","text":""},{"location":"tutorials/cvat_annotation/#single-label-fields","title":"Single label fields\u00b6","text":"<p>If only one new label field is being created, then the <code>label_field</code>, <code>label_type</code>, <code>classes</code>, <code>attributes</code> and <code>mask_targets</code> arguments to annotate() can be used to specify how to construct the label schema for CVAT.</p> <ul> <li><code>label_field</code>: the string name of the field to create</li> <li><code>label_type</code>: the type of labels that will be created. The supported values are [\"classification\", \"classifications\", \"detections\", \"instances\", \"segmentations\", \"keypoints\", \"polylines\", \"polygons\", \"scalar\"]</li> <li><code>classes</code>: a list of class strings available to label</li> <li><code>attributes</code>: an optional list or dictionary containing specifications for attributes to annotate on every label</li> </ul>"},{"location":"tutorials/cvat_annotation/#label-types","title":"Label types\u00b6","text":"<p>There are a few things to note about label types:</p> <ul> <li>\"classification\" creates a single <code>Classification</code> label from the first CVAT tag on every sample while \"classifications\" creates a <code>Classifications</code> label storing all CVAT tags on each sample</li> <li>\"instance\" segmentations are downloaded alongside \"detections\" since they are stored on the <code>mask</code> field in the FiftyOne <code>Detection</code> label type. Both bounding boxes and segmentation masks can be uploaded from existing <code>Detections</code> fields if the label type is left blank</li> <li>\"scalar\" does not accept attributes and optionally takes classes</li> </ul> <p>Expanding on the point about scalar fields, this integrations allows for the creation and editing of integer, float, boolean, and string scalar fields on samples. In CVAT, they are annotated using the <code>tag</code> tool. Since scalar fields are not labels, no <code>attributes</code> are supported. If a list of <code>classes</code> is given, then the <code>tag</code> tool in CVAT lets you select from a dropdown of the given classes. If <code>classes</code> is not given, then the <code>tag</code> tool will require you to type in the scalar value as an attribute in CVAT.</p>"},{"location":"tutorials/cvat_annotation/#attributes","title":"Attributes\u00b6","text":"<p>Non-scalar label types support per-label attributes that can be modified and created with CVAT. For existing label fields, these attributes will be parsed automatically if possible to determine the type of attribute annotation format to create.</p> <p>For new attributes, they can be specified either as a list of strings of attribute names in which case each attribute is annotated with a text box input in CVAT. Alternatively, a dictionary can be passed in with more concrete specifications of the type of formatting to use for the attributes.</p>"},{"location":"tutorials/cvat_annotation/#label-schema","title":"Label schema\u00b6","text":"<p>In order to annotate multiple label fields at once, or just to specify the field name, label type, classes and attributes in one structure, the <code>label_schema</code> argument to annotate() can be used. This argument accepts a dictionary keyed by label field names that contain a dictionary setting up the type, classes and attributes as follows:</p> <pre>label_schema = {\n    \"new_field\": {\n        \"type\": \"detections\",\n        \"classes\": [\"class_1\", \"class_2\"],\n        \"attributes\": {\n            \"attr1\": {\n                \"type\": \"radio\",\n                \"values\": [\"val1\", \"val2\"],\n                \"default\": \"val1\",\n            }\n        }\n    },\n    \"existing_field\": {},\n}\n</pre> <p>For existing fields, the dictionary entry can be left blank and the relevant information will be parsed. For new fields, the type and classes are required (except for \"scalar\" type fields) and attributes are optional just like before.</p>"},{"location":"tutorials/cvat_annotation/#dataset-improvement","title":"Dataset improvement\u00b6","text":""},{"location":"tutorials/cvat_annotation/#annotating-videos-in-cvat","title":"Annotating videos in CVAT\u00b6","text":"<p>Videos are handled slightly differently by CVAT. Each task is only able to contain a single video, so if multiple video samples are uploaded at once via a call to <code>annotate()</code>, separate tasks will be created for each video.</p> <p>CVAT primarily allows for per-frame annotations of objects and classifications, so you should prepend <code>\"frames.\"</code> to field names to indicate that you are working with frame-level, not sample-level, fields.</p> <p>All CVAT label types except <code>tags</code> provide an option to annotate tracks in videos, which captures the identity of a single object as it moves through the video. When you import video tracks into FiftyOne, the <code>index</code> attribute of each label will contain the integer number of its track, and any labels that are keyframes will have their <code>keyframe=True</code> attribute set.</p>"},{"location":"tutorials/cvat_annotation/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/data_augmentation/","title":"Augmenting Datasets with Albumentations","text":"<p>Traditionally, data augmentation) is performed on-the-fly during training. This is great... if you know exactly what augmentations you want to apply to your dataset.</p> <p>However, if you're just getting started with a new dataset, you may not know what augmentations are appropriate for your data. Should you include rotations? How much blurring is reasonable? Does it make sense to employ random cropping? These questions just scratch the surface.</p> <p>When effective, data augmentation can significantly boost model performance by reducing overfitting and turning a small set of collected data into a much larger and more diverse data moat. But when left unchecked, data augmentation transformations can completely confuse your model, burying the original high quality data in a glut of digital garbage.</p> <p>In this walkthrough, you'll learn how to apply data augmentation to your dataset using the Albumentations library, and how to ensure those augmentations are appropriate for your data.</p> <p>It covers the following:</p> <ul> <li>What is data augmentation?</li> <li>The perils of blind data augmentation</li> <li>Testing transformations on your dataset</li> </ul>  Illustration of data augmentation applied to a single natural image  <p>Broadly speaking, data augmentation is any process that involves increasing the size of the training set by modifying the original data. Typically, data augmentation is used to fill expected gaps in the original data and reduce overfitting to the specific data that you were able to collect and curate. It is also a handy technique for mitigating class imbalance: by augmenting the number of samples for underrepresented classes, you can restore balance to the training distribution. Often, augmentations can account for 90% of the training data, or even more.</p> <p>In the context of computer vision, modifications can be made via geometric transformations like rotations and reflections, transformations which blur or add noise, or transformations that simulate different lighting conditions, such as changing the brightness, contrast, or saturation. For most of these augmentations, if you just need to transform the raw images, then torchvision\u2019s <code>transforms</code> module is a great solution. If you want to take your labels (bounding boxes, masks, and keypoints) along for the ride, then you\u2019ll need a purpose-built image augmentation library, such as Albumentations, imgaug, or Augmentor.</p> <p>There are also more sophisticated image augmentation techniques for changing the scenery, background, and weather conditions in images. If you\u2019re interested in this level of control over your augmentations, check out Kopikat and Stability AI\u2019s Sky Replacer.</p> <p>While data augmentation itself does not include completely synthetic data generation, augmentation is often used in conjunction with synthetic data generation approaches. Synthetic data from NVIDIA Omniverse can be orders of magnitude cheaper than collecting similar data in the field \u2014 combining this with (computationally inexpensive) data augmentation can lead to still further cost savings!</p> <p>When applied irresponsibly, data augmentations can degrade model performance and have severe real\u2013world consequences. Some transformations clearly push beyond the bounds of the desired data distribution \u2014 too much blurring makes an image unrecognizable; vertically flipping a portrait (leading to an upside down face) is clearly undesirable for most use cases.</p> <p>But the damage done by blindly boosting dataset size can be far more subtle and pernicious. Let\u2019s look at two examples, to make this explicit.</p> <p>Suppose you\u2019re working for a wildlife conservation organization, using computer vision to count the number of blue-throated macaws. At last count, there were only around 350 in the wild, so you only have a few images to start with, and you want to augment your data.</p>  Blue-throated macaw. Image courtesy of wikimedia commons  <p>Your field cameras take pretty high-resolution images, so you augment the data by randomly cropping 600x600 patches from your original images. When you randomly crop, some of the resulting augmentations look  like this:</p>  600x600 pixel random crops of the image above  <p>But there\u2019s a problem. If you use this to train your model, the model might incorrectly tag blue-and-gold macaws \u2014 which are far more abundant (more than 10,000 in the wild), share an overlapping geographic range and apart from their head look pretty similar. This might significantly throw off your population estimates.</p>  Blue and yellow macaw. Image courtesy of Ketian Chen  <p>To hammer this idea  home, let\u2019s look at another example. Suppose you\u2019re building a model to detect pneumonia from chest X-rays. Typically, pneumonia shows up in these images as an abnormally opaque region within the chest, so teaching a neural net to diagnose it should be possible, but you only have hundreds of images \u2014 far too few to train your desired model.</p> <p>One of the augmentations you are interested in performing is changing the contrast in the images. Each lab from which you are receiving data sends you X-ray images with different amounts of contrast, so it seems reasonable to turn each image into a set of images across the spectrum of contrast.</p> <p>But there\u2019s a problem here too. Turning the contrast up or down may be viable for some images. However, too high of a contrast can also change the perceived diagnosis. Consider the image on the left side below, of a non-pneumatic patient from the ChestX-ray14 dataset. Now look at the image on the right, after the contrast has been increased and a region made substantially more opaque. If we retained the training label from the left, we would be telling the model that images like this are non-pneumatic. This could potentially cause confusion and result in false negative diagnoses.</p>  Left: Lung without pneumonia (image from ChestX-ray14 dataset). Right: Contrast-heightened augmentation of left image  <p>The examples highlighted in the last section may not apply in your use case, but there are countless ways that augmentations can make a mess out of high quality data. Albumentations has 80+ transformations, many of which give you multiple control knobs to turn. And these transformations can be composed, altogether amounting to a massive space of possible augmentations.</p> <p>To ensure that your augmentations are reasonable, in domain, and add diversity to your dataset, it is absolutely essential that you test out your transformations before including them in a training loop.</p> <p>Fortunately, the Albumentations plugin for FiftyOne allows you to do just this! In particular, you can:</p> <ul> <li>Apply Albumentations transformations</li> <li>View samples generated by last augmentation</li> <li>Save augmentations to the dataset, and</li> <li>Save transformations you found useful</li> </ul> <p>The augmentation transforms not only the raw image, but also any Object Detections, Keypoints, Instance Segmentations, Semantic Segmentations, and Heatmap labels on the transformed samples.</p> <p>To get started, first make sure you have FiftyOne and Albumentations installed:</p> <pre>pip install -U fiftyone albumentations\n</pre> <p>Then download the Albumentations plugin with FiftyOne\u2019s plugin CLI syntax:</p> <pre>fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin\n</pre> <p>For this walkthrough, we\u2019ll pretend that our goal is to train a vision model for an autonomous vehicle application, but we are starting from just a handful of labeled images. In particular, we\u2019ll take just the first 10 images from the KITTI dataset, which contains left stereo images from road scenes.</p> In\u00a0[2]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"kitti\", split=\"train\", max_samples=10)\nsession = fo.launch_app(dataset)\n</pre> import fiftyone as fo import fiftyone.zoo as foz  dataset = foz.load_zoo_dataset(\"kitti\", split=\"train\", max_samples=10) session = fo.launch_app(dataset)  Subset of 10 images from the train split of the KITTI dataset, visualized in the FiftyOne App <p>To make things more fun \u2014 and to show that this plugin allows you to experiment with all different types of labels \u2014 let\u2019s add some pose estimation keypoints with Ultralytics, and some relative depth maps with Hugging Face\u2019s Transformers library:</p> <pre>pip install -U transformers ultralytics\n</pre> In\u00a0[\u00a0]: Copied! <pre>## Add depth maps\nfrom transformers import AutoModelForDepthEstimation\ndepth_model = AutoModelForDepthEstimation.from_pretrained(\n    \"Intel/dpt-large\"\n)\ndataset.apply_model(depth_model, \"depth\")\n\n\n## Add keypoints\nfrom ultralytics import YOLO\npose_model = YOLO('yolov8x-pose.pt')\ndataset.apply_model(pose_model, \"keypoints\")\n\nsession = fo.launch_app(dataset)\n</pre> ## Add depth maps from transformers import AutoModelForDepthEstimation depth_model = AutoModelForDepthEstimation.from_pretrained(     \"Intel/dpt-large\" ) dataset.apply_model(depth_model, \"depth\")   ## Add keypoints from ultralytics import YOLO pose_model = YOLO('yolov8x-pose.pt') dataset.apply_model(pose_model, \"keypoints\")  session = fo.launch_app(dataset) <p></p> <p>Pressing the backtick \u201c`\u201d key on the keyboard, and typing \u201caugment\u201d in. Press the <code>augment_with_albumentations</code> option. This is an operator in the FiftyOne Plugin system, and by interacting with the UI-based input form, we will be able to specify what transform we want to apply.</p> <p>Let\u2019s try a simple example of randomly cropping boxes out of each image. To do so, we will use the <code>RandomCropFromBorders</code> transform from Albumentations:</p> <p></p> <p>Notice how as we interact with the input form, the contents dynamically change. In this case, when we select the transformation we want to apply, we are greeted with input items for each argument taken by that transform function. This is made possible through the use of Python\u2019s <code>inspect</code> module \u2014 each argument is processed (input and output) in semi-automated fashion by utilizing the docstrings and function signatures of Albumentations\u2019 transformations.</p> <p>Also notice that we chose to generate just one augmentation per sample from this transform \u2014 hence going from 10 to 20 samples. For transformations which involve randomness, it can be helpful to generate multiple augmentations to investigate the broader range of possible generations.</p> <p>If we wanted to isolate the samples we just generated, as opposed to viewing them in line with the original samples, we could do so by invoking the <code>view_last_albumentations_run operator</code>:</p> <p></p> <p>If we want to keep them, then we can save the augmentations to the dataset with the <code>save_albumentations_augmentations</code> operator. Otherwise, they will be treated as temporary \u2014 for the purposes of experimentation \u2014 and deleted when you next generate augmentations.</p> <p>Perhaps even more importantly, running <code>get_last_albumentations_run_info</code> will display for us a formatted compilation of all of the parameters used to construct the prior transformation and generate these augmentations:</p> <p></p> <p>If we are satisfied with this transformation and the hyperparameters employed, we can save it, either for composition with other transforms in our exploration, or to use in your inference pipelines:</p> <p></p> <p>In production-grade inference pipelines, augmentations are often generated by composing multiple augmentation transformations to each base sample. For instance, you might apply a random brightness shift, followed by a random crop, and finally some sort of blur. Let\u2019s see this in action:</p> <p></p> <p>This is of course just one combination, and yet even this indicates that perhaps if we want to combine cropping with brightness changes, we should be intentional about the minimum size of the cropped region or the maximum amount of darkening we add. And this will all depend on the particular application!</p> <p>Whether you\u2019re building a low-latency embedded vision model for real-time detection or you\u2019re building the next state of the art multimodal foundation model, it almost goes without saying that data augmentation is an essential ingredient in the training process. Yet far too often, we treat data augmentation as a black-box component and heuristically determine what transformations to apply.</p> <p>But if you\u2019re optimizing your model architecture, and painstakingly pouring over your ground truth data to ensure the highest quality, there\u2019s no reason not to take the same care with your data augmentation. I hope this post hammers home the importance of understanding what transformations you are applying, and gives you the tools you need to start treating data augmentation like data curation!</p>"},{"location":"tutorials/data_augmentation/#augmenting-datasets-with-albumentations","title":"Augmenting Datasets with Albumentations\u00b6","text":""},{"location":"tutorials/data_augmentation/#what-is-data-augmentation","title":"What is Data Augmentation?\u00b6","text":""},{"location":"tutorials/data_augmentation/#the-perils-of-blind-data-augmentation","title":"The Perils of Blind Data Augmentation\u00b6","text":""},{"location":"tutorials/data_augmentation/#testing-transformations-with-albumentations-and-fiftyone","title":"Testing Transformations with Albumentations and FiftyOne\u00b6","text":""},{"location":"tutorials/data_augmentation/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/data_augmentation/#creating-augmentations","title":"Creating Augmentations\u00b6","text":""},{"location":"tutorials/data_augmentation/#isolating-the-augmented-samples","title":"Isolating the Augmented Samples\u00b6","text":""},{"location":"tutorials/data_augmentation/#inspecting-the-generating-transformation","title":"Inspecting the Generating Transformation\u00b6","text":""},{"location":"tutorials/data_augmentation/#composing-transformations","title":"Composing Transformations\u00b6","text":""},{"location":"tutorials/data_augmentation/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/detection_mistakes/","title":"Finding Detection Mistakes with FiftyOne","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>In order to compute mistakenness, your dataset needs to have two detections fields, one with your ground truth annotations and one with your model predictions.</p> <p>In this example, we'll load the quickstart dataset from the FiftyOne Dataset Zoo, which has ground truth annotations and predictions from a PyTorch Faster-RCNN model for a few samples from the COCO dataset.</p> In\u00a0[3]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n</pre> import fiftyone as fo import fiftyone.zoo as foz  dataset = foz.load_zoo_dataset(\"quickstart\") <pre>Dataset already downloaded\nLoading 'quickstart'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [2.0s elapsed, 0s remaining, 99.8 samples/s]          \nDataset 'quickstart' created\n</pre> In\u00a0[4]: Copied! <pre>print(dataset)\n</pre> print(dataset) <pre>Name:        quickstart\nMedia type:  image\nNum samples: 200\nPersistent:  False\nTags:        []\nSample fields:\n    id:           fiftyone.core.fields.ObjectIdField\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:   fiftyone.core.fields.FloatField\n    predictions:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n</pre> In\u00a0[5]: Copied! <pre># Print a sample ground truth detection\nsample = dataset.first()\nprint(sample.predictions.detections[0])\n</pre> # Print a sample ground truth detection sample = dataset.first() print(sample.predictions.detections[0]) <pre>&lt;Detection: {\n    'id': '5f452c60ef00e6374aad9394',\n    'attributes': {},\n    'tags': [],\n    'label': 'bird',\n    'bounding_box': [\n        0.22192673683166503,\n        0.06093006531397502,\n        0.4808845520019531,\n        0.8937615712483724,\n    ],\n    'mask': None,\n    'confidence': 0.9750854969024658,\n    'index': None,\n}&gt;\n</pre> <p>Let's start by visualizing the dataset in the FiftyOne App:</p> In\u00a0[\u00a0]: Copied! <pre># Open the dataset in the App\nsession = fo.launch_app(dataset)\n</pre> # Open the dataset in the App session = fo.launch_app(dataset) <p></p> <p>When working with FiftyOne datasets that contain a field with <code>Detections</code>, you can create a patches view both through Python and directly in the FiftyOne App to view each detection as a separate sample.</p> In\u00a0[8]: Copied! <pre>patches_view = dataset.to_patches(\"ground_truth\")\nprint(patches_view)\n</pre> patches_view = dataset.to_patches(\"ground_truth\") print(patches_view) <pre>Dataset:     quickstart\nMedia type:  image\nNum patches: 1232\nPatch fields:\n    id:           fiftyone.core.fields.ObjectIdField\n    sample_id:    fiftyone.core.fields.ObjectIdField\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detection)\nView stages:\n    1. ToPatches(field='ground_truth', config=None)\n</pre> <p>Let's open the App and click the patches button, then select <code>ground_truth</code> to create the same view that we created above.</p> In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) <p></p> In\u00a0[15]: Copied! <pre>import fiftyone.brain as fob\n\n# Compute mistakenness of annotations in `ground_truth` field using \n# predictions from `predictions` field as point of reference\nfob.compute_mistakenness(dataset, \"predictions\", label_field=\"ground_truth\")\n</pre> import fiftyone.brain as fob  # Compute mistakenness of annotations in `ground_truth` field using  # predictions from `predictions` field as point of reference fob.compute_mistakenness(dataset, \"predictions\", label_field=\"ground_truth\") <pre>Evaluating detections...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [13.8s elapsed, 0s remaining, 9.9 samples/s]       \nComputing mistakenness...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [2.6s elapsed, 0s remaining, 52.2 samples/s]       \nMistakenness computation complete\n</pre> <p>The above method populates a number of fields on the samples of our dataset as well as the ground truth and predicted objects:</p> <p>New ground truth object attributes (in <code>ground_truth</code> field):</p> <ul> <li><code>mistakenness</code> (float): A measure of the likelihood that a ground truth object's label is incorrect</li> <li><code>mistakenness_loc</code>: A measure of the likelihood that a ground truth object's localization (bounding box) is inaccurate</li> <li><code>possible_spurious</code>: Ground truth objects that were not matched with a predicted object and are deemed to be likely spurious annotations will have this attribute set to True</li> </ul> <p>New predicted object attributes (in <code>predictions</code> field):</p> <ul> <li><code>possible_missing</code>: If a highly confident prediction with no matching ground truth object is encountered, this attribute is set to True to indicate that it is a likely missing ground truth annotation</li> </ul> <p>Sample-level fields:</p> <ul> <li><code>mistakenness</code>: The maximum mistakenness of the ground truth objects in each sample</li> <li><code>possible_spurious</code>: The number of possible spurious ground truth objects in each sample</li> <li><code>possible_missing</code>: The number of possible missing ground truth objects in each sample</li> </ul> In\u00a0[20]: Copied! <pre>from fiftyone import ViewField as F\n\n# Sort by likelihood of mistake (most likely first)\nmistake_view = dataset.sort_by(\"mistakenness\", reverse=True)\n\n# Print some information about the view\nprint(mistake_view)\n</pre> from fiftyone import ViewField as F  # Sort by likelihood of mistake (most likely first) mistake_view = dataset.sort_by(\"mistakenness\", reverse=True)  # Print some information about the view print(mistake_view) <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 200\nSample fields:\n    id:                fiftyone.core.fields.ObjectIdField\n    filepath:          fiftyone.core.fields.StringField\n    tags:              fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:        fiftyone.core.fields.FloatField\n    predictions:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    mistakenness:      fiftyone.core.fields.FloatField\n    possible_missing:  fiftyone.core.fields.IntField\n    possible_spurious: fiftyone.core.fields.IntField\nView stages:\n    1. SortBy(field_or_expr='mistakenness', reverse=True, create_index=True)\n</pre> In\u00a0[7]: Copied! <pre># Inspect some samples and detections\n# This is the first detection of the first sample\nprint(mistake_view.first().ground_truth.detections[0])\n</pre> # Inspect some samples and detections # This is the first detection of the first sample print(mistake_view.first().ground_truth.detections[0]) <pre>&lt;Detection: {\n    'id': '5f452487ef00e6374aad2744',\n    'attributes': BaseDict({}),\n    'tags': BaseList([]),\n    'label': 'tv',\n    'bounding_box': BaseList([\n        0.002746666666666667,\n        0.36082,\n        0.24466666666666667,\n        0.3732,\n    ]),\n    'mask': None,\n    'confidence': None,\n    'index': None,\n    'area': 16273.3536,\n    'iscrowd': 0.0,\n    'mistakenness': 0.005771428346633911,\n    'mistakenness_loc': 0.16955941131917984,\n}&gt;\n</pre> <p>Let's use the App to visually inspect the results:</p> In\u00a0[\u00a0]: Copied! <pre># Show the samples we processed in rank order by the mistakenness\nsession.view = mistake_view\n</pre> # Show the samples we processed in rank order by the mistakenness session.view = mistake_view <p></p> <p>Another useful query is to find all objects that have a high mistakenness, lets say &gt; 0.95:</p> In\u00a0[\u00a0]: Copied! <pre>from fiftyone import ViewField as F\n\nsession.view = dataset.filter_labels(\"ground_truth\", F(\"mistakenness\") &gt; 0.95)\n</pre> from fiftyone import ViewField as F  session.view = dataset.filter_labels(\"ground_truth\", F(\"mistakenness\") &gt; 0.95) <p></p> <p>Looking through the results, we can see that many of these images have a bunch of predictions which actually look like they are correct, but no ground truth annotations. This is a common mistake in object detection datasets, where the annotator may have missed some objects in the image. On the other hand, there are some detections which are mislabeled, like the <code>cow</code> in the fifth image above which is predicted to be a horse.</p> <p>We can use a similar workflow to look at objects that may be localized poorly:</p> In\u00a0[\u00a0]: Copied! <pre>session.view = dataset.filter_labels(\"ground_truth\", F(\"mistakenness_loc\") &gt; 0.85)\n</pre> session.view = dataset.filter_labels(\"ground_truth\", F(\"mistakenness_loc\") &gt; 0.85) <p></p> <p>In some of these examples, like the image of people on the beach, there is not necessarily highly mistaken localization, there are just a bunch of small, relatively overlapping objects. In other examples, such as the handbag in the second instance and the skis in the third instance, the localization is clearly off.</p> <p>The <code>possible_missing</code> field can also be useful to sort by to find instances of incorrect annotations.</p> <p>Similarly, <code>possible_spurious</code> can be used to find objects that the model detected that may have been missed by annotators.</p> In\u00a0[\u00a0]: Copied! <pre>session.view = dataset.match(F(\"possible_missing\") &gt; 0)\n</pre> session.view = dataset.match(F(\"possible_missing\") &gt; 0) <p></p> <p>An example that showed up from this search is shown above. There is an <code>apple</code> that was not annotated that the model detected.</p> In\u00a0[14]: Copied! <pre># A dataset can be filtered to only contain labels with certain tags\n# Helpful for isolating labels with issues and sending off to an annotation provider\nmissing_ground_truth = dataset.select_labels(tags=\"missing\")\n</pre> # A dataset can be filtered to only contain labels with certain tags # Helpful for isolating labels with issues and sending off to an annotation provider missing_ground_truth = dataset.select_labels(tags=\"missing\") <p>REMEMBER: Since you are using model predictions to guide the mistakenness process, the better your model, the more accurate the mistakenness suggestions. Additionally, using logits of confidence scores will also provide better results.</p> <p>We used Faster-RCNN in this example which is quite a few years old. Using EfficientDet D7 provided much better results. For example, it was easily able to find this <code>snowboard</code> labeled as <code>skis</code>:</p> <p></p> In\u00a0[13]: Copied! <pre>session.freeze() # screenshot the active App for sharing\n</pre> session.freeze() # screenshot the active App for sharing"},{"location":"tutorials/detection_mistakes/#finding-detection-mistakes-with-fiftyone","title":"Finding Detection Mistakes with FiftyOne\u00b6","text":"<p>Annotations mistakes create an artificial ceiling on the performance of your models. However, finding these mistakes by hand is at least as arduous as the original annotation work! Enter FiftyOne.</p> <p>In this tutorial, we explore how FiftyOne can be used to help you find mistakes in your object detection annotations. To detect mistakes in classification datasets, check out this tutorial.</p> <p>We'll cover the following concepts:</p> <ul> <li>Loading your existing dataset into FiftyOne</li> <li>Adding model predictions to your dataset</li> <li>Computing insights into your dataset relating to possible label mistakes</li> <li>Visualizing mistakes in the FiftyOne App</li> </ul> <p>So, what's the takeaway?</p> <p>FiftyOne can help you find and correct label mistakes in your datasets, enabling you to curate higher quality datasets and, ultimately, train better models!</p>"},{"location":"tutorials/detection_mistakes/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"tutorials/detection_mistakes/#compute-mistakenness","title":"Compute mistakenness\u00b6","text":"<p>Now we're ready to assess the mistakenness of the ground truth detections.</p> <p>We can do so by running the compute_mistakenness() method from the FiftyOne Brain:</p>"},{"location":"tutorials/detection_mistakes/#analyzing-the-results","title":"Analyzing the results\u00b6","text":"<p>Let's use FiftyOne to investigate the results.</p> <p>First, let's show the samples with the most likely annotation mistakes:</p>"},{"location":"tutorials/detection_mistakes/#tagging-and-resolution","title":"Tagging and resolution\u00b6","text":"<p>Any label or collection of labels can be tagged at any time in the sample grid or expanded sample view. In the expanded sample view, individual samples can be selected by clicking on them in the media player. We can, for example, tag this <code>apple</code> prediction as <code>missing</code> and any other predictions without an associated ground truth detection.</p> <p>Labels with specific tags can then be selected with select_labels() stage and sent off to assist in improving the annotations with your annotation provided of choice. FiftyOne currently offers integrations for both Labelbox and Scale.</p>"},{"location":"tutorials/detectron2/","title":"Training and Evaluating FiftyOne Datasets with Detectron2","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone In\u00a0[\u00a0]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n</pre> import fiftyone as fo import fiftyone.zoo as foz In\u00a0[\u00a0]: Copied! <pre>!python -m pip install pyyaml==5.1\n\n# Detectron2 has not released pre-built binaries for the latest pytorch (https://github.com/facebookresearch/detectron2/issues/4053)\n# so we install from source instead. This takes a few minutes.\n!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n\n# Install pre-built detectron2 that matches pytorch version, if released:\n# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n#!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/{CUDA_VERSION}/{TORCH_VERSION}/index.html\n</pre> !python -m pip install pyyaml==5.1  # Detectron2 has not released pre-built binaries for the latest pytorch (https://github.com/facebookresearch/detectron2/issues/4053) # so we install from source instead. This takes a few minutes. !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'  # Install pre-built detectron2 that matches pytorch version, if released: # See https://detectron2.readthedocs.io/tutorials/install.html for instructions #!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/{CUDA_VERSION}/{TORCH_VERSION}/index.html In\u00a0[\u00a0]: Copied! <pre>import torch, detectron2\n!nvcc --version\nTORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\nCUDA_VERSION = torch.__version__.split(\"+\")[-1]\nprint(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\nprint(\"detectron2:\", detectron2.__version__)\n</pre> import torch, detectron2 !nvcc --version TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2]) CUDA_VERSION = torch.__version__.split(\"+\")[-1] print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION) print(\"detectron2:\", detectron2.__version__) <pre>nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2020 NVIDIA Corporation\nBuilt on Mon_Oct_12_20:09:46_PDT_2020\nCuda compilation tools, release 11.1, V11.1.105\nBuild cuda_11.1.TC455_06.29190527_0\ntorch:  1.12 ; cuda:  cu113\ndetectron2: 0.6\n</pre> In\u00a0[\u00a0]: Copied! <pre># Setup detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common libraries\nimport numpy as np\nimport os, cv2\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\n</pre> # Setup detectron2 logger import detectron2 from detectron2.utils.logger import setup_logger setup_logger()  # import some common libraries import numpy as np import os, cv2  # import some common detectron2 utilities from detectron2 import model_zoo from detectron2.engine import DefaultPredictor from detectron2.config import get_cfg from detectron2.data import MetadataCatalog, DatasetCatalog <p>In this section, we show how to use a custom FiftyOne Dataset to train a detectron2 model. We'll train a license plate segmentation model from an existing model pre-trained on COCO dataset, available in detectron2's model zoo.</p> <p>Since the COCO dataset doesn't have a \"Vehicle registration plates\" category, we will be using segmentations of license plates from the Open Images v6 dataset in the FiftyOne Dataset Zoo to train the model to recognize this new category.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = foz.load_zoo_dataset(\n    \"open-images-v6\", \n    split=\"validation\", \n    classes=[\"Vehicle registration plate\"], \n    label_types=[\"segmentations\"],\n    label_field=\"segmentations\",\n)\n</pre> dataset = foz.load_zoo_dataset(     \"open-images-v6\",      split=\"validation\",      classes=[\"Vehicle registration plate\"],      label_types=[\"segmentations\"],     label_field=\"segmentations\", ) <p>Specifying a <code>classes</code> when downloading a dataset from the zoo will ensure that only samples with one of the given classes will be present. However, these samples may still contain other labels, so we can use the powerful filtering capability of FiftyOne to easily keep only the \"Vehicle registration plate\" labels. We will also untag these samples as \"validation\" and create our own split out of them.</p> In\u00a0[\u00a0]: Copied! <pre>from fiftyone import ViewField as F\n\n# Remove other classes and existing tags\ndataset.filter_labels(\"segmentations\", F(\"label\") == \"Vehicle registration plate\").save()\ndataset.untag_samples(\"validation\")\n</pre> from fiftyone import ViewField as F  # Remove other classes and existing tags dataset.filter_labels(\"segmentations\", F(\"label\") == \"Vehicle registration plate\").save() dataset.untag_samples(\"validation\") In\u00a0[\u00a0]: Copied! <pre>import fiftyone.utils.random as four\n\nfour.random_split(dataset, {\"train\": 0.8, \"val\": 0.2})\n</pre> import fiftyone.utils.random as four  four.random_split(dataset, {\"train\": 0.8, \"val\": 0.2}) <p>Next we will register the FiftyOne dataset to detectron2, following the detectron2 custom dataset tutorial. Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into detectron2's standard format.</p> <p>Note: In this example, we are specifically parsing the segmentations into bounding boxes and polylines. This function may require tweaks depending on the model being trained and the data it expects.</p> In\u00a0[\u00a0]: Copied! <pre>from detectron2.structures import BoxMode\n\ndef get_fiftyone_dicts(samples):\n    samples.compute_metadata()\n\n    dataset_dicts = []\n    for sample in samples.select_fields([\"id\", \"filepath\", \"metadata\", \"segmentations\"]):\n        height = sample.metadata[\"height\"]\n        width = sample.metadata[\"width\"]\n        record = {}\n        record[\"file_name\"] = sample.filepath\n        record[\"image_id\"] = sample.id\n        record[\"height\"] = height\n        record[\"width\"] = width\n      \n        objs = []\n        for det in sample.segmentations.detections:\n            tlx, tly, w, h = det.bounding_box\n            bbox = [int(tlx*width), int(tly*height), int(w*width), int(h*height)]\n            fo_poly = det.to_polyline()\n            poly = [(x*width, y*height) for x, y in fo_poly.points[0]]\n            poly = [p for x in poly for p in x]\n            obj = {\n                \"bbox\": bbox,\n                \"bbox_mode\": BoxMode.XYWH_ABS,\n                \"segmentation\": [poly],\n                \"category_id\": 0,\n            }\n            objs.append(obj)\n\n        record[\"annotations\"] = objs\n        dataset_dicts.append(record)\n\n    return dataset_dicts\n\nfor d in [\"train\", \"val\"]:\n    view = dataset.match_tags(d)\n    DatasetCatalog.register(\"fiftyone_\" + d, lambda view=view: get_fiftyone_dicts(view))\n    MetadataCatalog.get(\"fiftyone_\" + d).set(thing_classes=[\"vehicle_registration_plate\"])\n\nmetadata = MetadataCatalog.get(\"fiftyone_train\")\n</pre> from detectron2.structures import BoxMode  def get_fiftyone_dicts(samples):     samples.compute_metadata()      dataset_dicts = []     for sample in samples.select_fields([\"id\", \"filepath\", \"metadata\", \"segmentations\"]):         height = sample.metadata[\"height\"]         width = sample.metadata[\"width\"]         record = {}         record[\"file_name\"] = sample.filepath         record[\"image_id\"] = sample.id         record[\"height\"] = height         record[\"width\"] = width                objs = []         for det in sample.segmentations.detections:             tlx, tly, w, h = det.bounding_box             bbox = [int(tlx*width), int(tly*height), int(w*width), int(h*height)]             fo_poly = det.to_polyline()             poly = [(x*width, y*height) for x, y in fo_poly.points[0]]             poly = [p for x in poly for p in x]             obj = {                 \"bbox\": bbox,                 \"bbox_mode\": BoxMode.XYWH_ABS,                 \"segmentation\": [poly],                 \"category_id\": 0,             }             objs.append(obj)          record[\"annotations\"] = objs         dataset_dicts.append(record)      return dataset_dicts  for d in [\"train\", \"val\"]:     view = dataset.match_tags(d)     DatasetCatalog.register(\"fiftyone_\" + d, lambda view=view: get_fiftyone_dicts(view))     MetadataCatalog.get(\"fiftyone_\" + d).set(thing_classes=[\"vehicle_registration_plate\"])  metadata = MetadataCatalog.get(\"fiftyone_train\") <p>To verify the dataset is in correct format, let's visualize the annotations of the training set:</p> In\u00a0[\u00a0]: Copied! <pre>dataset_dicts = get_fiftyone_dicts(dataset.match_tags(\"train\"))\nids = [dd[\"image_id\"] for dd in dataset_dicts]\n\nview = dataset.select(ids)\nsession = fo.launch_app(view)\n</pre> dataset_dicts = get_fiftyone_dicts(dataset.match_tags(\"train\")) ids = [dd[\"image_id\"] for dd in dataset_dicts]  view = dataset.select(ids) session = fo.launch_app(view) <pre></pre> Activate In\u00a0[\u00a0]: Copied! <pre>session.freeze()  # screenshot the App\n</pre> session.freeze()  # screenshot the App In\u00a0[\u00a0]: Copied! <pre>from detectron2.engine import DefaultTrainer\n\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"fiftyone_train\",)\ncfg.DATASETS.TEST = ()\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 2  # This is the real \"batch size\" commonly known to deep learning people\ncfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\ncfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\ncfg.SOLVER.STEPS = []        # do not decay learning rate\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512)\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (Vehicle registration plate). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = DefaultTrainer(cfg) \ntrainer.resume_or_load(resume=False)\ntrainer.train()\n</pre> from detectron2.engine import DefaultTrainer  cfg = get_cfg() cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")) cfg.DATASETS.TRAIN = (\"fiftyone_train\",) cfg.DATASETS.TEST = () cfg.DATALOADER.NUM_WORKERS = 2 cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo cfg.SOLVER.IMS_PER_BATCH = 2  # This is the real \"batch size\" commonly known to deep learning people cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset cfg.SOLVER.STEPS = []        # do not decay learning rate cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512) cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (Vehicle registration plate). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets) # NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.  os.makedirs(cfg.OUTPUT_DIR, exist_ok=True) trainer = DefaultTrainer(cfg)  trainer.resume_or_load(resume=False) trainer.train() In\u00a0[\u00a0]: Copied! <pre># Look at training curves in tensorboard:\n%load_ext tensorboard\n%tensorboard --logdir output\n</pre> # Look at training curves in tensorboard: %load_ext tensorboard %tensorboard --logdir output <p></p> In\u00a0[\u00a0]: Copied! <pre># Inference should use the config with parameters that are used in training\n# cfg now already contains everything we've set previously. We changed it a little bit for inference:\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\npredictor = DefaultPredictor(cfg)\n</pre> # Inference should use the config with parameters that are used in training # cfg now already contains everything we've set previously. We changed it a little bit for inference: cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold predictor = DefaultPredictor(cfg) <p>Then, we generate predictions on each sample in the validation set, and convert the outputs from detectron2 to FiftyOne format, then add them to our FiftyOne dataset.</p> In\u00a0[\u00a0]: Copied! <pre>def detectron_to_fo(outputs, img_w, img_h):\n    # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n    detections = []\n    instances = outputs[\"instances\"].to(\"cpu\")\n    for pred_box, score, c, mask in zip(\n        instances.pred_boxes, instances.scores, instances.pred_classes, instances.pred_masks,\n    ):\n        x1, y1, x2, y2 = pred_box\n        fo_mask = mask.numpy()[int(y1):int(y2), int(x1):int(x2)]\n        bbox = [float(x1)/img_w, float(y1)/img_h, float(x2-x1)/img_w, float(y2-y1)/img_h]\n        detection = fo.Detection(label=\"Vehicle registration plate\", confidence=float(score), bounding_box=bbox, mask=fo_mask)\n        detections.append(detection)\n\n    return fo.Detections(detections=detections)\n</pre> def detectron_to_fo(outputs, img_w, img_h):     # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format     detections = []     instances = outputs[\"instances\"].to(\"cpu\")     for pred_box, score, c, mask in zip(         instances.pred_boxes, instances.scores, instances.pred_classes, instances.pred_masks,     ):         x1, y1, x2, y2 = pred_box         fo_mask = mask.numpy()[int(y1):int(y2), int(x1):int(x2)]         bbox = [float(x1)/img_w, float(y1)/img_h, float(x2-x1)/img_w, float(y2-y1)/img_h]         detection = fo.Detection(label=\"Vehicle registration plate\", confidence=float(score), bounding_box=bbox, mask=fo_mask)         detections.append(detection)      return fo.Detections(detections=detections) In\u00a0[\u00a0]: Copied! <pre>val_view = dataset.match_tags(\"val\")\ndataset_dicts = get_fiftyone_dicts(val_view)\npredictions = {}\nfor d in dataset_dicts:\n    img_w = d[\"width\"]\n    img_h = d[\"height\"]\n    img = cv2.imread(d[\"file_name\"])\n    outputs = predictor(img)\n    detections = detectron_to_fo(outputs, img_w, img_h)\n    predictions[d[\"image_id\"]] = detections\n\ndataset.set_values(\"predictions\", predictions, key_field=\"id\")\n</pre> val_view = dataset.match_tags(\"val\") dataset_dicts = get_fiftyone_dicts(val_view) predictions = {} for d in dataset_dicts:     img_w = d[\"width\"]     img_h = d[\"height\"]     img = cv2.imread(d[\"file_name\"])     outputs = predictor(img)     detections = detectron_to_fo(outputs, img_w, img_h)     predictions[d[\"image_id\"]] = detections  dataset.set_values(\"predictions\", predictions, key_field=\"id\") <pre>Computing image metadata...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [328.7ms elapsed, 0s remaining, 173.4 samples/s]      \n</pre> <p>Let's visualize the predictions and take a look at how the model did. We can click the eye icon next to the \"val\" tag to view all of the validation samples that we ran inference on.</p> In\u00a0[6]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) Activate In\u00a0[8]: Copied! <pre>session.freeze()  # screenshot the App\n</pre> session.freeze()  # screenshot the App <p>From here, we can use the built-in evaluation methods provided by FiftyOne. The <code>evaluate_detections()</code> method can be used to evaluate the instance segmentations using the <code>use_masks=True</code> parameter. We can also use this to compute mAP with the options being the COCO-style (default) or Open Images-style mAP protocol.</p> In\u00a0[\u00a0]: Copied! <pre>results = dataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"segmentations\",\n    eval_key=\"eval\",\n    use_masks=True,\n    compute_mAP=True,\n)\n</pre> results = dataset.evaluate_detections(     \"predictions\",     gt_field=\"segmentations\",     eval_key=\"eval\",     use_masks=True,     compute_mAP=True, ) <pre>Evaluating detections...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 287/287 [1.2s elapsed, 0s remaining, 237.9 samples/s]         \nPerforming IoU sweep...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 287/287 [953.7ms elapsed, 0s remaining, 302.4 samples/s]      \n</pre> <p>We can use this results object to view the mAP, print an evaluation report, plot PR curves, plot confusion matrices, and more.</p> In\u00a0[11]: Copied! <pre>results.mAP()\n</pre> results.mAP() Out[11]: <pre>0.12387340239495186</pre> In\u00a0[12]: Copied! <pre>results.print_report()\n</pre> results.print_report() <pre>                            precision    recall  f1-score   support\n\nVehicle registration plate       0.72      0.18      0.29       292\n\n                 micro avg       0.72      0.18      0.29       292\n                 macro avg       0.72      0.18      0.29       292\n              weighted avg       0.72      0.18      0.29       292\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>results.plot_pr_curves()\n</pre> results.plot_pr_curves() <p></p> <p>From the PR curve we can see that the model is not generating many predictions---resulting in many false negatives---but the predictions that are generated are often fairly accurate.</p> <p>We can also create a view into the dataset looking at high-confidence false positive predictions to understand where the model is going wrong and how to potentially improve it in the future.</p> In\u00a0[\u00a0]: Copied! <pre>from fiftyone import ViewField as F\n\nsession.view = dataset.filter_labels(\"predictions\", (F(\"eval\") == \"fp\") &amp; (F(\"confidence\") &gt; 0.8))\n</pre> from fiftyone import ViewField as F  session.view = dataset.filter_labels(\"predictions\", (F(\"eval\") == \"fp\") &amp; (F(\"confidence\") &gt; 0.8)) Activate In\u00a0[\u00a0]: Copied! <pre>session.freeze()  # screenshot the App\n</pre> session.freeze()  # screenshot the App <p>There are a few samples with false positives like this one that contain plates with characters not from the Latin alphabet indicating we may want to introduce images from a wider range of countries into the training set.</p>"},{"location":"tutorials/detectron2/#training-and-evaluating-fiftyone-datasets-with-detectron2","title":"Training and Evaluating FiftyOne Datasets with Detectron2\u00b6","text":"<p>FiftyOne has all of the building blocks necessary to develop high-quality datasets to train your models, as well as advanced model evaluation capabilities. To make use of these, FiftyOne easily integrates with your existing model training and inference pipelines. In this walktrhough we'll cover how you can use your FiftyOne datasets to train a model with Detectron2, Facebook AI Reasearch's library for detection and segmentation algorithms.</p> <p>This walkthrough is based off of the official Detectron2 tutorial, augmented to load data to and from FiftyOne.</p> <p>Specifically, this walkthrough covers:</p> <ul> <li>Loading a dataset from the FiftyOne Zoo, and splitting it into training/validation</li> <li>Initializing a segmentation model from the detectron2 model zoo</li> <li>Loading ground truth annotations from a FiftyOne dataset into a detectron2 model training pipeline and training the model</li> <li>Loading predictions from a detectron2 model into a FiftyOne dataset</li> <li>Evaluating model predictions in FiftyOne</li> </ul> <p>So, what\u2019s the takeaway?</p> <p>By writing two simple functions, you can integrate FiftyOne into your Detectron2 model training and inference pipelines.</p>"},{"location":"tutorials/detectron2/#setup","title":"Setup\u00b6","text":"<p>To get started, you need to install FiftyOne and detectron2:</p>"},{"location":"tutorials/detectron2/#train-on-a-fiftyone-dataset","title":"Train on a FiftyOne dataset\u00b6","text":""},{"location":"tutorials/detectron2/#prepare-the-dataset","title":"Prepare the dataset\u00b6","text":"<p>For this example, we will just use some of the samples from the official \"validation\" split of the dataset. To improve model performance, we could always add in more data from the official \"train\" split as well but that will take longer to train so we'll just stick to the \"validation\" split for this walkthrough.</p>"},{"location":"tutorials/detectron2/#load-the-model-and-train","title":"Load the model and train!\u00b6","text":"<p>Now, let's fine-tune a COCO-pretrained R50-FPN Mask R-CNN model on the FiftyOne dataset. It takes ~2 minutes to train 300 iterations on a P100 GPU.</p>"},{"location":"tutorials/detectron2/#inference-evaluation-using-the-trained-model","title":"Inference &amp; evaluation using the trained model\u00b6","text":"<p>Now, let's run inference with the trained model on the license plate validation dataset. First, let's create a predictor using the model we just trained:</p>"},{"location":"tutorials/dimension_reduction/","title":"Visualizing Data with Dimensionality Reduction Techniques","text":"<p>In this walkthrough, you'll learn how to run PCA, t-SNE, UMAP, and custom dimensionality reduction techniques on your data in FiftyOne!</p> <p>It covers the following:</p> <ul> <li>Why dimensionality reduction is useful</li> <li>Strengths and weaknesses of different dimensionality reduction techniques</li> <li>Running built-in dimensionality reduction techniques in FiftyOne</li> <li>Running custom dimensionality reduction techniques in FiftyOne</li> </ul> <p>These days, everyone is excited about embeddings \u2014 numeric vectors that represent features of your input data. In computer vision for instance, image embeddings are used in reverse image search applications. And in the context of large language models (LLMs), documents are chunked and embedded (with text embedding models) for retrieval augmented generation (RAG).</p> <p>Embeddings are incredibly powerful, but given their high dimensionality (with lengths typically between 384 and 4096), they can be hard for humans to interpret and inspect. This is where dimensionality reduction techniques come in handy!</p> <p>Dimensionality reduction techniques are quantitative methods for representing information from a higher dimensional space in a lower dimensional space. By squeezing our embeddings into two or three dimensions, we can visualize them to get a more intuitive understanding of the \u201chidden\u201d structure in our data.</p> <p>When we project high dimensional data into a low dimensional space, we implicitly make a trade-off between representational complexity and interpretability. To compress embeddings, dimensionality reduction techniques make assumptions about the underlying data, its distribution, and the relationships between variables.</p> <p>In this post, we will visualize embeddings using four popular dimensionality reduction techniques: PCA, t-SNE, and UMAP. We will give a brief overview of the strengths, weaknesses, and assumptions of each technique. And we will illustrate that both the model used to generate embeddings, and the dimensionality reduction technique play essential roles in shaping the visualization of your data.</p> <p>It is also important to note that dimensionality reduction techniques often have hyperparameters, which can have non-negligible impacts on the results. In this post, I am going to use the default hyperparameters everywhere that choices arise. Feel free to modify as you see fit!</p> <p>For this walkthrough, we will be using the FiftyOne library for data management and visualization. We will use scikit-learn for PCA and t-SNE, and umap-learn for UMAP dimension reduction implementations:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -U fiftyone scikit-learn umap-learn\n</pre> !pip install -U fiftyone scikit-learn umap-learn <p>We will be using the test split of the CIFAR-10 dataset as our testbed, which contains 10,000 images of size 32x32 pixels, spanning 10 image classes. We can load the dataset/split directly from the FiftyOne Dataset Zoo:</p> In\u00a0[\u00a0]: Copied! <pre>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\nsession = fo.launch_app(dataset)\n</pre> import fiftyone as fo import fiftyone.brain as fob import fiftyone.zoo as foz  dataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\") session = fo.launch_app(dataset) <p></p> <p>We will compare and contrast our four dimensionality reduction techniques with two image embedding models: ResNet-101 and CLIP. Whereas ResNet-101 is a more traditional vision model, representing the relationships between pixels and patches in images, CLIP captures more of the semantic content of the images.</p> <p>We can load both from the FiftyOne Model Zoo:</p> In\u00a0[\u00a0]: Copied! <pre>clip = foz.load_zoo_model(\"clip-vit-base32-torch\")\nresnet101 = foz.load_zoo_model(\"resnet101-imagenet-torch\")\n</pre> clip = foz.load_zoo_model(\"clip-vit-base32-torch\") resnet101 = foz.load_zoo_model(\"resnet101-imagenet-torch\") <p>Then generating embeddings for each model amounts to making a single call to the dataset\u2019s <code>compute_embeddings()</code> method:</p> In\u00a0[\u00a0]: Copied! <pre>## compute and store resnet101 embeddings \ndataset.compute_embeddings(\n    resnet101, \n    embeddings_field=\"resnet101_embeddings\"\n)\n\n## compute and store clip embeddings \ndataset.compute_embeddings(\n    clip, \n    embeddings_field=\"clip_embeddings\"\n)\n</pre> ## compute and store resnet101 embeddings  dataset.compute_embeddings(     resnet101,      embeddings_field=\"resnet101_embeddings\" )  ## compute and store clip embeddings  dataset.compute_embeddings(     clip,      embeddings_field=\"clip_embeddings\" ) <p>Before we dive into the details of each dimensionality reduction technique, let\u2019s recap the API for running dimensionality reduction in FiftyOne. The FiftyOne Brain provides a compute_visualization() function that can be used to run dimensionality reduction on your data.</p> <p>The first and only positional argument to this function is a sample collection, which can be either a Dataset or a DatasetView.</p> <p>Beyond that, you need to specify the following three things:</p> <ol> <li>What you want to reduce the dimensionality of.</li> <li>How you want to reduce the dimensionality.</li> <li>Where you want to store the results.</li> </ol> <p>There are multiple ways to specify what you would like dimension-reduced. Here are a few options (but certainly not all of them!):</p> <ul> <li>You can specify the name of the field containing the embeddings you would like to reduce using the <code>embeddings</code> argument. If your embeddings are stored in field \"my_embeddings_field\" on your samples, you would employ the syntax <code>embeddings=\"my_embeddings_field\"</code>. This is useful if you need to reuse the same embeddings for multiple dimensionality reduction techniques, or for other brain methods.</li> <li>You can pass the embeddings in directly using as numpy array, also via the <code>embeddings</code> argument. This is useful if you have already computed your embeddings, and don\u2019t need to store them on your samples.</li> <li>You can specify the model you would like to use to generate embeddings. This can be:<ul> <li>A <code>FiftyOne.Model</code> instance</li> <li>The name (a string) of a model from the model zoo, in which case the model by that will be loaded from the FiftyOne Model Zoo.</li> <li>A Hugging Face Transformers model, in which case the model will be converted to a <code>FiftyOne.Model</code> instance. See the Hugging Face integration docs for more details.</li> </ul> </li> </ul> <p>You can specify the base dimensionality reduction technique to use via the <code>method</code> argument. This can be one of the following strings: <code>pca</code>, <code>tsne</code>, <code>umap</code>, or <code>manual</code>.</p> <p>For <code>pca</code>, <code>tsne</code>, and <code>umap</code>, you can specify the number of dimensions to reduce to via the <code>num_dims</code> argument. Additionally, you can specify hyperparameters for each technique as kwargs. For a complete description of available options, check out the visualization configs: TSNEVisualizationConfig, UMAPVisualizationConfig, and PCAVisualizationConfig.</p> <p>You can use <code>method=\"manual\"</code> if you already have the dimensionality-reduced data, and just want to store it on your samples for visualization purposes.</p> <p>Once you have specified what you want to reduce the dimensionality of, and how you want to do it, you need to specify where you want to store the results. This is done via the <code>brain_key</code> argument. Once you have run the <code>compute_visualization()</code> method, you will be able to select this brain key in the FiftyOne App to visualize the results. You can also use the brain key to access the results programmatically:</p> In\u00a0[\u00a0]: Copied! <pre>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\n\n## Compute PCA visualization\nfob.compute_visualization(\n    dataset,\n    embeddings=\"resnet101\",\n    method=\"pca\",\n    brain_key=\"pca_resnet101\"\n)\n\n## Access results\npca_resnet_results = dataset.load_brain_results(\"pca_resnet101\")\n</pre> import fiftyone as fo import fiftyone.brain as fob import fiftyone.zoo as foz  dataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\")  ## Compute PCA visualization fob.compute_visualization(     dataset,     embeddings=\"resnet101\",     method=\"pca\",     brain_key=\"pca_resnet101\" )  ## Access results pca_resnet_results = dataset.load_brain_results(\"pca_resnet101\") <p>Principal Component Analysis, or PCA, is a dimensionality reduction technique that seeks to preserve as much variance as possible. Intuitively, PCA finds a set of orthogonal axes (principal components) that jointly \u201cexplain\u201d as much of the variation in the data as possible. Mathematically, you can interpret PCA algorithms as effectively performing singular value decompositions and truncating the number of dimensions by eliminating the singular vectors with the smallest eigenvalues.</p> <p>Strengths</p> <ul> <li>Simple, intuitive, and efficient for large datasets!</li> <li>PCA is amenable to new data: If you have precomputed the transformation on an initial set of embeddings, you can apply that transformation to new embeddings and immediately visualize them in the same space.</li> </ul> <p>Limitations</p> <ul> <li>Assumes that the relationships between variables are linear \u2014 an assumption which often does not hold when the inputs are embeddings, which themselves come from highly nonlinear deep neural networks</li> <li>Very susceptible to outliers.</li> </ul> <p>PCA is natively supported by the FiftyOne Brain\u2019s <code>compute_visualization()</code>. To reduce dimensionality for a set of embeddings, we can specify the field the embeddings are stored in, and pass in <code>method=\"pca\"</code>. In the app, we can open up an Embeddings panel to view the results:</p> In\u00a0[\u00a0]: Copied! <pre>## PCA with ResNet101 embeddings\nfob.compute_visualization(\n    dataset, \n    embeddings=\"resnet101_embeddings\", \n    method=\"pca\", \n    brain_key=\"resnet101_pca\"\n)\n\n## PCA with CLIP embeddings\nfob.compute_visualization(\n    dataset, \n    embeddings=\"clip_embeddings\", \n    method=\"pca\", \n    brain_key=\"resnet101_pca\"\n)\n\nsession = fo.launch_app(dataset)\n</pre> ## PCA with ResNet101 embeddings fob.compute_visualization(     dataset,      embeddings=\"resnet101_embeddings\",      method=\"pca\",      brain_key=\"resnet101_pca\" )  ## PCA with CLIP embeddings fob.compute_visualization(     dataset,      embeddings=\"clip_embeddings\",      method=\"pca\",      brain_key=\"resnet101_pca\" )  session = fo.launch_app(dataset) <p></p> <p>We can color by any attribute on our samples \u2014 in this case the ground truth label \u2014 and filter the contents of the sample grid interactively by selecting regions in the embeddings panel.</p> <p></p> <p>For both the CLIP and ResNet-101 embeddings, the PCA plot does seem to very loosely retain information from the embeddings (and the original images). However, when we color by label, there is substantial overlap from one class to another.</p> <p>Restricting the CLIP PCA view to just automobiles, trucks, and ships, we can see that the distributions for all three classes are essentially identical, aside from the ships extending slightly farther out.</p> <p></p> <p>t-Distributed Stochastic Neighbor Embedding, or t-SNE, is a nonlinear dimensionality reduction technique that aims to, roughly speaking, keep neighbors close. More precisely, t-SNE takes the initial, high-dimensional data (in our case embedding vectors) and computes the similarity between inputs. The algorithm then attempts to learn a lower-dimensional representation which preserves as much of the similarity as possible. Mathematically, this learning is achieved by minimizing the Kullback-Leibler divergence between the high-dimensional (fixed) and low-dimensional (trained) distributions.</p> <p>Strengths</p> <ul> <li>t-SNE is nonlinear, making it a much better fit for (embeddings computed on) datasets like MNIST and CIFAR-10.</li> <li>The technique is good at preserving local structure, making it easy to see clustering in data!</li> </ul> <p>Limitations</p> <ul> <li>t-SNE relies on random initialization, so good fits are not guaranteed Still sensitive to outliers</li> <li>Not scalable: for a dataset with n samples, t-SNE takes $\\mathcal{O}(n^2)$ time to run, and requires $\\mathcal{O}(n^2)$ space to operate</li> </ul> <p>Like PCA, t-SNE is natively supported by the FiftyOne Brain\u2019s <code>compute_visualization()</code>, so we can run dimensionality reduction on our embeddings by passing <code>method=\"tsne\"</code>:</p> In\u00a0[\u00a0]: Copied! <pre>## t-SNE with ResNet101 embeddings\nfob.compute_visualization(\n    dataset, \n    embeddings=\"resnet101_embeddings\", \n    method=\"tsne\", \n    brain_key=\"resnet101_tsne\"\n)\n\n## t-SNE with CLIP embeddings\nfob.compute_visualization(\n    dataset, \n    embeddings=\"clip_embeddings\", \n    method=\"tsne\", \n    brain_key=\"resnet101_tsne\"\n)\n\nsession = fo.launch_app(dataset)\n</pre> ## t-SNE with ResNet101 embeddings fob.compute_visualization(     dataset,      embeddings=\"resnet101_embeddings\",      method=\"tsne\",      brain_key=\"resnet101_tsne\" )  ## t-SNE with CLIP embeddings fob.compute_visualization(     dataset,      embeddings=\"clip_embeddings\",      method=\"tsne\",      brain_key=\"resnet101_tsne\" )  session = fo.launch_app(dataset) <p>Looking at the results of t-SNE dimensionality reduction on both ResNet-101 and CLIP embeddings, we can see a lot more separation between the distributions of different classes.</p> <p></p> <p>In both cases, similar classes are still close to each other \u2014 for instance, automobiles and trucks are adjacent \u2014 but we can also mostly distinguish a main cluster for almost every class. In other words, t-SNE does a very good job at capturing local structure, and a decent job at capturing global structure.</p> <p></p> <p>Uniform Manifold Approximation and Projection (UMAP) is a nonlinear dimensionality reduction technique based on the mathematics of topology. I won\u2019t go into the gory details, as there is an excellent visual explanation of the approach here, but in essence, UMAP treats the input data as points lying on a special kind of surface called a manifold (technically here a Riemannian manifold), and tries to learn a lower dimensional representation of the manifold. This explicitly takes global structure into consideration, as opposed to t-SNE, which concerns itself with keeping neighbors close (local structure).</p> <p>Strengths</p> <ul> <li>Preserves both global and local structure</li> <li>Better scaling than t-SNE with dataset size</li> </ul> <p>Limitations</p> <ul> <li>Like t-SNE, UMAP relies on randomness, and is dependent upon hyperparameters</li> <li>UMAP assumes that the manifold is locally connected. This can cause problems if there are a few data points that are very far away from the rest of the data.</li> </ul> <p>Like PCA and t-SNE, UMAP  is natively supported by the FiftyOne Brain\u2019s <code>compute_visualization()</code>, so we can run dimensionality reduction on our embeddings by passing <code>method=\"umap\"</code>:</p> In\u00a0[\u00a0]: Copied! <pre>## UMAP with ResNet101 embeddings\nfob.compute_visualization(\n    dataset, \n    embeddings=\"resnet101_embeddings\", \n    method=\"umap\", \n    brain_key=\"resnet101_umap\"\n)\n\n## UMAP with CLIP embeddings\nfob.compute_visualization(\n    dataset, \n    embeddings=\"clip_embeddings\", \n    method=\"umap\", \n    brain_key=\"resnet101_umap\"\n)\n\nsession = fo.launch_app(dataset)\n</pre> ## UMAP with ResNet101 embeddings fob.compute_visualization(     dataset,      embeddings=\"resnet101_embeddings\",      method=\"umap\",      brain_key=\"resnet101_umap\" )  ## UMAP with CLIP embeddings fob.compute_visualization(     dataset,      embeddings=\"clip_embeddings\",      method=\"umap\",      brain_key=\"resnet101_umap\" )  session = fo.launch_app(dataset) <p></p> <p>For both sets of embeddings, the clusters are a lot more spread out than with t-SNE. For ResNet-101, all of the vehicles (automobile, truck, airplane, ship) are in one mega-cluster \u2014 or two smaller clusters, depending on how you view it \u2014 and all of the animals are in another mega-cluster.</p> <p></p> <p>Interestingly, for the CLIP embeddings, we see that the <code>airplane</code> cluster is situated close to both <code>bird</code> and <code>ship</code>. The <code>car</code> and <code>truck</code> clusters are very close together; and the <code>cat</code> and <code>dog</code> clusters are very close together.</p> <p>Depending on the specific structure of your data, you may find that none of the techniques detailed above provide an intuitive view into your data. Fortunately, there are tons of other techniques you can use. In this section, we\u2019ll show you how to run custom dimensionality reduction techniques with FiftyOne.</p> <p>Like UMAP, Isomap is also a nonlinear manifold learning technique. Isomap is built into scikit-learn, so we can fit our high-dimensional data and generate low-dimensional transformed data points as follows:</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom sklearn.manifold import Isomap\n\n## get embeddings from dataset\nembeddings = np.array(dataset.values(\"resnet101_embeddings\"))\n\n## create and fit\nmanifold_embedding = Isomap(n_components=2)\nz = manifold_embedding.fit_transform(embeddings)\n</pre> import numpy as np from sklearn.manifold import Isomap  ## get embeddings from dataset embeddings = np.array(dataset.values(\"resnet101_embeddings\"))  ## create and fit manifold_embedding = Isomap(n_components=2) z = manifold_embedding.fit_transform(embeddings) <p>We can then create a visualization in FiftyOne by passing <code>method=\u2019manual\u2019</code> into <code>compute_visualization()</code> and providing these lower-dimensional points via the <code>points</code> argument:</p> In\u00a0[\u00a0]: Copied! <pre>fob.compute_visualization(\n    dataset,\n    method='manual',\n    points=z,\n    brain_key='resnet101_isomap'\n)\n</pre> fob.compute_visualization(     dataset,     method='manual',     points=z,     brain_key='resnet101_isomap' ) <p></p> <p>Any dimensionality reduction method supported by scikit-learn can be used in analogous fashion.</p> <p>CompressionVAE uses Variational Autoencoders to deterministically and reversibly transform the high-dimensional data into a lower dimensional space.</p> <p>To run CompressionVAE, clone this forked repo:</p> In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/jacobmarks/CompressionVAE.git\n</pre> !git clone https://github.com/jacobmarks/CompressionVAE.git <p>Then <code>cd</code> into the directory and install the package locally:</p> In\u00a0[\u00a0]: Copied! <pre>!cd CompressionVAE\n!pip install .\n</pre> !cd CompressionVAE !pip install . <p>Embed the input data (embeddings) into a lower-dimensional space, and create a visualization in FiftyOne via the same <code>manual</code> method:</p> In\u00a0[\u00a0]: Copied! <pre>from cvae import cvae\n\nX = np.array(dataset.values(\"clip_embeddings\"))\nembedder = cvae.CompressionVAE(X)\nembedder.train()\nz = embedder.embed(X)\n\nfob.compute_visualization(\n    dataset,\n    method='manual',\n    points=z,\n    brain_key='clip_cvae'\n)\n</pre> from cvae import cvae  X = np.array(dataset.values(\"clip_embeddings\")) embedder = cvae.CompressionVAE(X) embedder.train() z = embedder.embed(X)  fob.compute_visualization(     dataset,     method='manual',     points=z,     brain_key='clip_cvae' ) <p></p> <p>If you find yourself running the same custom dimensionality reduction technique like Isomap or CompressionVAE over and over again, you can create a custom visualization config to make your life easier. This is done by subclassing the VisualizationConfig and Visualization classes.</p> <p>Here's a simple example using Isomap:</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom sklearn.manifold import Isomap\n\nimport fiftyone as fo\nimport fiftyone.brain as fob\n\nclass IsomapVisualizationConfig(fob.VisualizationConfig):\n    def __init__(self, n_neighbors=5, **kwargs):\n        super().__init__(**kwargs)\n        self.n_neighbors = n_neighbors\n\n\nclass IsomapVisualization(fob.Visualization):\n    def fit(self, embeddings):\n        manifold_embedding = Isomap(\n            n_components=2, \n            n_neighbors=self.config.n_neighbors\n        )\n        return manifold_embedding.fit_transform(embeddings)\n</pre> import numpy as np from sklearn.manifold import Isomap  import fiftyone as fo import fiftyone.brain as fob  class IsomapVisualizationConfig(fob.VisualizationConfig):     def __init__(self, n_neighbors=5, **kwargs):         super().__init__(**kwargs)         self.n_neighbors = n_neighbors   class IsomapVisualization(fob.Visualization):     def fit(self, embeddings):         manifold_embedding = Isomap(             n_components=2,              n_neighbors=self.config.n_neighbors         )         return manifold_embedding.fit_transform(embeddings) <p>You can then use this by passing in the new visualization config in for the <code>method</code> argument:</p> In\u00a0[\u00a0]: Copied! <pre>dataset = foz.load_zoo_dataset(\"quickstart\")\n\nmodel = foz.load_zoo_model(\"clip-vit-base32-torch\")\nembeddings = dataset.compute_embeddings(model)\n\nresults2 = fob.compute_visualization(\n    dataset,\n    embeddings=embeddings,\n    method=IsomapVisualizationConfig,\n    brain_key=\"isomap\",\n)\n</pre> dataset = foz.load_zoo_dataset(\"quickstart\")  model = foz.load_zoo_model(\"clip-vit-base32-torch\") embeddings = dataset.compute_embeddings(model)  results2 = fob.compute_visualization(     dataset,     embeddings=embeddings,     method=IsomapVisualizationConfig,     brain_key=\"isomap\", ) <p>All of the dimensionality reduction techniques we have discussed so far have been applied to embeddings computed on entire images. However, you can also apply dimensionality reduction to embeddings computed on object patches. This can be useful if you want to visualize the relationships between objects in your images.</p> <p>To do this, pass in the name of the field containing the object patches you would like to reduce via the <code>patches_field</code> argument.</p> In\u00a0[\u00a0]: Copied! <pre>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\nfob.compute_visualization(\n    dataset,\n    patches_field=\"ground_truth\",\n    method=\"umap\",\n    brain_key=\"gt_umap\"\n)\n</pre> import fiftyone as fo import fiftyone.brain as fob import fiftyone.zoo as foz  dataset = foz.load_zoo_dataset(\"quickstart\")  fob.compute_visualization(     dataset,     patches_field=\"ground_truth\",     method=\"umap\",     brain_key=\"gt_umap\" ) <p>Dimensionality reduction is critical to understanding our data, and our models. But it is important to think of dimensionality reduction not just as a single tool, but rather as a collection of techniques. Each technique has its own advantages; and each method projects certain assumptions onto the data, which may or may not hold for your data. I hope this walkthrough helps you to see your data in a new way!</p>"},{"location":"tutorials/dimension_reduction/#visualizing-data-with-dimensionality-reduction-techniques","title":"Visualizing Data with Dimensionality Reduction Techniques\u00b6","text":""},{"location":"tutorials/dimension_reduction/#why-dimensionality-reduction","title":"Why Dimensionality Reduction?\u00b6","text":""},{"location":"tutorials/dimension_reduction/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/dimension_reduction/#dimensionality-reduction-api-in-fiftyone","title":"Dimensionality Reduction API in FiftyOne\u00b6","text":""},{"location":"tutorials/dimension_reduction/#what-to-reduce-the-dimensionality-of","title":"What to reduce the dimensionality of\u00b6","text":""},{"location":"tutorials/dimension_reduction/#how-to-reduce-the-dimensionality","title":"How to reduce the dimensionality\u00b6","text":""},{"location":"tutorials/dimension_reduction/#where-to-store-the-results","title":"Where to store the results\u00b6","text":""},{"location":"tutorials/dimension_reduction/#dimensionality-reduction-with-pca","title":"Dimensionality Reduction with PCA\u00b6","text":""},{"location":"tutorials/dimension_reduction/#running-pca-on-embeddings","title":"Running PCA on Embeddings\u00b6","text":""},{"location":"tutorials/dimension_reduction/#dimensionality-reduction-with-t-sne","title":"Dimensionality Reduction with t-SNE\u00b6","text":""},{"location":"tutorials/dimension_reduction/#running-t-sne-on-embeddings","title":"Running t-SNE on Embeddings\u00b6","text":""},{"location":"tutorials/dimension_reduction/#dimensionality-reduction-with-umap","title":"Dimensionality Reduction with UMAP\u00b6","text":""},{"location":"tutorials/dimension_reduction/#running-umap-on-embeddings","title":"Running UMAP on Embeddings\u00b6","text":""},{"location":"tutorials/dimension_reduction/#dimensionality-reduction-with-custom-methods","title":"Dimensionality Reduction with Custom Methods\u00b6","text":""},{"location":"tutorials/dimension_reduction/#isomap","title":"Isomap\u00b6","text":""},{"location":"tutorials/dimension_reduction/#compressionvae","title":"CompressionVAE\u00b6","text":""},{"location":"tutorials/dimension_reduction/#advanced-dimensionality-reduction-in-fiftyone","title":"Advanced Dimensionality Reduction in FiftyOne\u00b6","text":""},{"location":"tutorials/dimension_reduction/#registerting-custom-visualization-methods","title":"Registerting Custom Visualization Methods\u00b6","text":""},{"location":"tutorials/dimension_reduction/#dimensionality-reduction-with-object-patches","title":"Dimensionality Reduction with Object Patches\u00b6","text":""},{"location":"tutorials/dimension_reduction/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/evaluate_detections/","title":"Evaluating Object Detections with FiftyOne","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>In this tutorial, we'll use an off-the-shelf Faster R-CNN detection model provided by PyTorch. To use it, you'll need to install <code>torch</code> and <code>torchvision</code>, if necessary.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install torch torchvision\n</pre> !pip install torch torchvision <p>If you wanted to, you could download the pretrained model from the web and load it with <code>torchvision</code>. However, this model is also available via the FiftyOne Model Zoo, which makes our lives much easier!</p> In\u00a0[4]: Copied! <pre>import fiftyone.zoo as foz\nmodel = foz.load_zoo_model('faster-rcnn-resnet50-fpn-coco-torch')\n</pre> import fiftyone.zoo as foz model = foz.load_zoo_model('faster-rcnn-resnet50-fpn-coco-torch') <p>We'll perform our analysis on the validation split of the COCO dataset, which is conveniently available for download via the FiftyOne Dataset Zoo.</p> <p>The snippet below will download the validation split and load it into FiftyOne.</p> In\u00a0[3]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n\ndataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    split=\"validation\",\n    dataset_name=\"evaluate-detections-tutorial\",\n)\ndataset.persistent = True\n</pre> import fiftyone as fo import fiftyone.zoo as foz  dataset = foz.load_zoo_dataset(     \"coco-2017\",     split=\"validation\",     dataset_name=\"evaluate-detections-tutorial\", ) dataset.persistent = True <pre>Downloading split 'validation' to '/Users/jacobmarks/fiftyone/coco-2017/validation' if necessary\nFound annotations at '/Users/jacobmarks/fiftyone/coco-2017/raw/instances_val2017.json'\nImages already downloaded\nExisting download of split 'validation' is sufficient\nLoading 'coco-2017' split 'validation'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [14.7s elapsed, 0s remaining, 360.0 samples/s]      \nDataset 'evaluate-detections-tutorial' created\n</pre> <p>Let's inspect the dataset to see what we downloaded:</p> In\u00a0[5]: Copied! <pre># Print some information about the dataset\nprint(dataset)\n</pre> # Print some information about the dataset print(dataset) <pre>Name:        evaluate-detections-tutorial\nMedia type:  image\nNum samples: 5000\nPersistent:  True\nTags:        []\nSample fields:\n    id:           fiftyone.core.fields.ObjectIdField\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n</pre> In\u00a0[6]: Copied! <pre># Print a ground truth detection\nsample = dataset.first()\nprint(sample.ground_truth.detections[0])\n</pre> # Print a ground truth detection sample = dataset.first() print(sample.ground_truth.detections[0]) <pre>&lt;Detection: {\n    'id': '66047a4705c1282f3e97c5e9',\n    'attributes': {},\n    'tags': [],\n    'label': 'potted plant',\n    'bounding_box': [\n        0.37028125,\n        0.3345305164319249,\n        0.038593749999999996,\n        0.16314553990610328,\n    ],\n    'mask': None,\n    'confidence': None,\n    'index': None,\n    'supercategory': 'furniture',\n    'iscrowd': 0,\n}&gt;\n</pre> <p>Note that the ground truth detections are stored in the <code>ground_truth</code> field of the samples.</p> <p>Before we go further, let's launch the FiftyOne App and use the GUI to explore the dataset visually:</p> In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) <p></p> In\u00a0[9]: Copied! <pre># Choose a random subset of 100 samples to add predictions to\npredictions_view = dataset.take(100, seed=51)\n</pre> # Choose a random subset of 100 samples to add predictions to predictions_view = dataset.take(100, seed=51) In\u00a0[10]: Copied! <pre>predictions_view.apply_model(model, label_field=\"faster_rcnn\")\n</pre> predictions_view.apply_model(model, label_field=\"faster_rcnn\") <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [1.4m elapsed, 0s remaining, 1.3 samples/s]      \n</pre> <p>Let's load <code>predictions_view</code> in the App to visualize the predictions that we added:</p> In\u00a0[11]: Copied! <pre>session.view = predictions_view\n</pre> session.view = predictions_view <p></p> In\u00a0[\u00a0]: Copied! <pre># Resets the session; the entire dataset will now be shown\nsession.view = None\n</pre> # Resets the session; the entire dataset will now be shown session.view = None <p></p> <p>Only the 100 samples in <code>predictions_view</code> have predictions in their <code>faster_rcnn</code> field, so some of the samples we see above do not have predicted boxes.</p> <p>If we want to recover our predictions view, we can do this programmatically via <code>session.view = predictions_view</code>, or we can use the view bar in the App to accomplish the same thing:</p> In\u00a0[\u00a0]: Copied! <pre># Use the view bar to create an `Exists(faster_rcnn, True)` stage\n# Now your view contains only the 100 samples with predictions in `faster_rcnn` field\nsession.show()\n</pre> # Use the view bar to create an `Exists(faster_rcnn, True)` stage # Now your view contains only the 100 samples with predictions in `faster_rcnn` field session.show() <p></p> <p>Each field of the samples are shown as togglable checkboxes on the left sidebar which can be used to control whether ground truth detections or predictions are rendered on the images.</p> <p>You can also click on an image to view the sample in more detail:</p> <p></p> <p></p> <p>Let's reset our session to show our <code>predictions_view</code>:</p> In\u00a0[12]: Copied! <pre>session.view = predictions_view\n</pre> session.view = predictions_view In\u00a0[\u00a0]: Copied! <pre># Click the down caret on the `faster_rcnn` field of Fields Sidebar\n# and apply a confidence threshold\nsession.show()\n</pre> # Click the down caret on the `faster_rcnn` field of Fields Sidebar # and apply a confidence threshold session.show() <p></p> <p>It looks like a confidence threshold of 0.75 is a good choice for our model, but we'll confirm that quantitatively later.</p> In\u00a0[13]: Copied! <pre>from fiftyone import ViewField as F\n\n# Only contains detections with confidence &gt;= 0.75\nhigh_conf_view = predictions_view.filter_labels(\"faster_rcnn\", F(\"confidence\") &gt; 0.75, only_matches=False)\n</pre> from fiftyone import ViewField as F  # Only contains detections with confidence &gt;= 0.75 high_conf_view = predictions_view.filter_labels(\"faster_rcnn\", F(\"confidence\") &gt; 0.75, only_matches=False) <p>Note the <code>only_matches=False</code> argument. When filtering labels, any samples that no longer contain labels would normally be removed from the view. However, this is not desired when performing evaluations since it can skew your results between views. We set <code>only_matches=False</code> so that all samples will be retained, even if some no longer contain labels.</p> In\u00a0[14]: Copied! <pre># Print some information about the view\nprint(high_conf_view)\n</pre> # Print some information about the view print(high_conf_view) <pre>Dataset:     evaluate-detections-tutorial\nMedia type:  image\nNum samples: 100\nSample fields:\n    id:           fiftyone.core.fields.ObjectIdField\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    faster_rcnn:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\nView stages:\n    1. Take(size=100, seed=51)\n    2. FilterLabels(field='faster_rcnn', filter={'$gt': ['$$this.confidence', 0.75]}, only_matches=False, trajectories=False)\n</pre> In\u00a0[15]: Copied! <pre># Print a prediction from the view to verify that its confidence is &gt; 0.75\nsample = high_conf_view.first()\nprint(sample.faster_rcnn.detections[0])\n</pre> # Print a prediction from the view to verify that its confidence is &gt; 0.75 sample = high_conf_view.first() print(sample.faster_rcnn.detections[0]) <pre>&lt;Detection: {\n    'id': '66047b0f05c1282f3e986920',\n    'attributes': {},\n    'tags': [],\n    'label': 'airplane',\n    'bounding_box': [\n        0.5629980087280273,\n        0.7977214296832356,\n        0.03478360176086426,\n        0.1007584484383529,\n    ],\n    'mask': None,\n    'confidence': 0.9952868223190308,\n    'index': None,\n}&gt;\n</pre> <p>Now let's load our view in the App to view the predictions that we programmatically selected:</p> In\u00a0[16]: Copied! <pre># Load high confidence view in the App\nsession.view = high_conf_view\n</pre> # Load high confidence view in the App session.view = high_conf_view <p></p> In\u00a0[\u00a0]: Copied! <pre>session.show()\n</pre> session.show() <p></p> In\u00a0[17]: Copied! <pre># Evaluate the predictions in the `faster_rcnn` field of our `high_conf_view`\n# with respect to the objects in the `ground_truth` field\nresults = high_conf_view.evaluate_detections(\n    \"faster_rcnn\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval\",\n    compute_mAP=True,\n)\n</pre> # Evaluate the predictions in the `faster_rcnn` field of our `high_conf_view` # with respect to the objects in the `ground_truth` field results = high_conf_view.evaluate_detections(     \"faster_rcnn\",     gt_field=\"ground_truth\",     eval_key=\"eval\",     compute_mAP=True, ) <pre>Evaluating detections...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [1.8s elapsed, 0s remaining, 53.7 samples/s]         \nPerforming IoU sweep...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [882.6ms elapsed, 0s remaining, 113.3 samples/s]      \n</pre> In\u00a0[18]: Copied! <pre># Get the 10 most common classes in the dataset\ncounts = dataset.count_values(\"ground_truth.detections.label\")\nclasses_top10 = sorted(counts, key=counts.get, reverse=True)[:10]\n\n# Print a classification report for the top-10 classes\nresults.print_report(classes=classes_top10)\n</pre> # Get the 10 most common classes in the dataset counts = dataset.count_values(\"ground_truth.detections.label\") classes_top10 = sorted(counts, key=counts.get, reverse=True)[:10]  # Print a classification report for the top-10 classes results.print_report(classes=classes_top10) <pre>               precision    recall  f1-score   support\n\n       person       0.89      0.80      0.84       263\n          car       0.72      0.56      0.63        55\n        chair       0.53      0.23      0.32        35\n         book       1.00      0.30      0.47        33\n       bottle       0.60      0.67      0.63         9\n          cup       0.93      0.81      0.87        16\n dining table       0.50      0.62      0.55        13\ntraffic light       0.50      0.46      0.48        13\n         bowl       0.71      0.38      0.50        13\n      handbag       0.50      0.18      0.26        17\n\n    micro avg       0.81      0.64      0.72       467\n    macro avg       0.69      0.50      0.56       467\n weighted avg       0.81      0.64      0.70       467\n\n</pre> <p>We can also compute the mean average-precision (mAP) of our detector:</p> In\u00a0[19]: Copied! <pre>print(results.mAP())\n</pre> print(results.mAP()) <pre>0.3519380509318074\n</pre> <p>Since evaluate_detections() uses the official COCO evaluation protocol, this mAP value will match what <code>pycocotools</code> would report.</p> <p>We can also view some precision-recall (PR) curves for specific classes of our model:</p> <p>Install ipywidgets to view the PR curves:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install 'ipywidgets&gt;=8,&lt;9'\n</pre> !pip install 'ipywidgets&gt;=8,&lt;9' In\u00a0[23]: Copied! <pre>plot = results.plot_pr_curves(classes=[\"person\", \"car\"])\nplot.show()\n</pre> plot = results.plot_pr_curves(classes=[\"person\", \"car\"]) plot.show() <pre>FigureWidget({\n    'data': [{'customdata': array([0.99988198, 0.99976206, 0.99951147, 0.9986501 , 0.89960496, 0.89951644,\n                                   0.89939888, 0.89922802, 0.89912866, 0.89890523, 0.89880363, 0.89853175,\n                                   0.89828404, 0.89820041, 0.8979461 , 0.8976577 , 0.89749938, 0.89728618,\n                                   0.89644278, 0.8958758 , 0.89509976, 0.8889141 , 0.88526444, 0.87643276,\n                                   0.79657969, 0.79617561, 0.79541582, 0.79494855, 0.79427896, 0.79371219,\n                                   0.79325874, 0.79228693, 0.79010382, 0.78802435, 0.78699406, 0.78525752,\n                                   0.78358504, 0.78199387, 0.77871775, 0.7757645 , 0.77280099, 0.76468775,\n                                   0.75996894, 0.75730504, 0.67831692, 0.67567982, 0.67354833, 0.66996306,\n                                   0.66708532, 0.66099035, 0.65253912, 0.64910335, 0.6437327 , 0.6400404 ,\n                                   0.6362098 , 0.55842573, 0.55395973, 0.55032358, 0.54633195, 0.53838573,\n                                   0.5347954 , 0.53086147, 0.5256753 , 0.517045  , 0.51440163, 0.43472316,\n                                   0.42722535, 0.41919324, 0.41490045, 0.40448562, 0.32215623, 0.3179356 ,\n                                   0.31494951, 0.31069795, 0.30823869, 0.23063738, 0.15299265, 0.15200521,\n                                   0.07574205, 0.        , 0.        , 0.        , 0.        , 0.        ,\n                                   0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                                   0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                                   0.        , 0.        , 0.        , 0.        , 0.        ]),\n              'hovertemplate': ('&lt;b&gt;class: %{text}&lt;/b&gt;&lt;br&gt;recal' ... 'customdata:.3f}&lt;extra&gt;&lt;/extra&gt;'),\n              'line': {'color': '#3366CC'},\n              'mode': 'lines',\n              'name': 'person (AP = 0.509)',\n              'text': array(['person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person', 'person', 'person', 'person', 'person',\n                             'person', 'person', 'person'], dtype='&lt;U6'),\n              'type': 'scatter',\n              'uid': '66ce9289-b6aa-4a54-bb82-0bc0460e85d7',\n              'x': array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n                          0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22, 0.23,\n                          0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33, 0.34, 0.35,\n                          0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47,\n                          0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59,\n                          0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71,\n                          0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83,\n                          0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95,\n                          0.96, 0.97, 0.98, 0.99, 1.  ]),\n              'y': array([0.92      , 0.90888889, 0.90284238, 0.89852744, 0.88      , 0.88      ,\n                          0.85833333, 0.84762712, 0.83489985, 0.82650231, 0.82142118, 0.82142118,\n                          0.82142118, 0.82142118, 0.82142118, 0.81653111, 0.81653111, 0.81547709,\n                          0.80838064, 0.80431617, 0.80318612, 0.79497421, 0.7921321 , 0.79025332,\n                          0.75801627, 0.75730504, 0.7545687 , 0.75036704, 0.75036704, 0.74633333,\n                          0.74527706, 0.74300926, 0.73489046, 0.73043   , 0.72891309, 0.72255181,\n                          0.71933957, 0.71507423, 0.70769337, 0.70046499, 0.69900115, 0.69389888,\n                          0.69153571, 0.69054998, 0.63638141, 0.63585231, 0.63517771, 0.63110115,\n                          0.62859029, 0.62577531, 0.62185488, 0.62044935, 0.61628621, 0.61454057,\n                          0.60968873, 0.54427961, 0.54121914, 0.53782453, 0.53578699, 0.5319866 ,\n                          0.52940762, 0.52660187, 0.52270998, 0.51687176, 0.51557335, 0.4406486 ,\n                          0.43746655, 0.43406038, 0.43284696, 0.42778695, 0.34685383, 0.34561677,\n                          0.34448594, 0.3432135 , 0.34190356, 0.25871351, 0.17432327, 0.17403153,\n                          0.08731707, 0.        , 0.        , 0.        , 0.        , 0.        ,\n                          0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                          0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                          0.        , 0.        , 0.        , 0.        , 0.        ])},\n             {'customdata': array([0.9954716 , 0.99384798, 0.89484104, 0.89484104, 0.89227479, 0.89227479,\n                                   0.89119812, 0.89119812, 0.88875514, 0.88875514, 0.88697321, 0.88584394,\n                                   0.88584394, 0.78478823, 0.78478823, 0.78311113, 0.78311113, 0.77853746,\n                                   0.77853746, 0.77533967, 0.77533967, 0.76721376, 0.7543198 , 0.7543198 ,\n                                   0.67268419, 0.67268419, 0.67017357, 0.67017357, 0.66724778, 0.66724778,\n                                   0.66214673, 0.65545037, 0.65545037, 0.65282088, 0.65282088, 0.64337576,\n                                   0.64337576, 0.62889966, 0.62889966, 0.54726801, 0.54726801, 0.53973907,\n                                   0.52120453, 0.52120453, 0.43208969, 0.43208969, 0.34965264, 0.34965264,\n                                   0.33724424, 0.33724424, 0.24307428, 0.07925715, 0.07925715, 0.07671282,\n                                   0.07671282, 0.07531983, 0.07531983, 0.        , 0.        , 0.        ,\n                                   0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                                   0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                                   0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                                   0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                                   0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                                   0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                                   0.        , 0.        , 0.        , 0.        , 0.        ]),\n              'hovertemplate': ('&lt;b&gt;class: %{text}&lt;/b&gt;&lt;br&gt;recal' ... 'customdata:.3f}&lt;extra&gt;&lt;/extra&gt;'),\n              'line': {'color': '#DC3912'},\n              'mode': 'lines',\n              'name': 'car (AP = 0.344)',\n              'text': array(['car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car',\n                             'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car',\n                             'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car',\n                             'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car',\n                             'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car',\n                             'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car',\n                             'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car',\n                             'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car',\n                             'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car',\n                             'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car',\n                             'car'], dtype='&lt;U3'),\n              'type': 'scatter',\n              'uid': '1397aa7c-2de6-4ffb-8933-314c318a25df',\n              'x': array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n                          0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22, 0.23,\n                          0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33, 0.34, 0.35,\n                          0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47,\n                          0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59,\n                          0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71,\n                          0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83,\n                          0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95,\n                          0.96, 0.97, 0.98, 0.99, 1.  ]),\n              'y': array([0.91111111, 0.91111111, 0.9       , 0.9       , 0.88      , 0.88      ,\n                          0.88      , 0.88      , 0.87777778, 0.87777778, 0.86777778, 0.86777778,\n                          0.86777778, 0.79      , 0.79      , 0.79      , 0.79      , 0.75238095,\n                          0.75238095, 0.70827839, 0.70827839, 0.68805199, 0.67094633, 0.67094633,\n                          0.63432971, 0.63432971, 0.63432971, 0.63432971, 0.6331723 , 0.6331723 ,\n                          0.62206119, 0.62122507, 0.62122507, 0.62122507, 0.62122507, 0.61711367,\n                          0.61711367, 0.6049049 , 0.6049049 , 0.54814815, 0.54814815, 0.54597701,\n                          0.52910497, 0.52910497, 0.44904905, 0.44904905, 0.37854406, 0.37854406,\n                          0.3525132 , 0.3525132 , 0.24468468, 0.07837838, 0.07837838, 0.07692308,\n                          0.07692308, 0.07380952, 0.07380952, 0.        , 0.        , 0.        ,\n                          0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                          0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                          0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                          0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                          0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                          0.        , 0.        , 0.        , 0.        , 0.        , 0.        ,\n                          0.        , 0.        , 0.        , 0.        , 0.        ])}],\n    'layout': {'margin': {'b': 0, 'l': 0, 'r': 0, 't': 30},\n               'shapes': [{'line': {'dash': 'dash'}, 'type': 'line', 'x0': 0, 'x1': 1, 'y0': 1, 'y1': 0}],\n               'template': '...',\n               'xaxis': {'constrain': 'domain', 'range': [0, 1], 'title': {'text': 'Recall'}},\n               'yaxis': {'constrain': 'domain',\n                         'range': [0, 1],\n                         'scaleanchor': 'x',\n                         'scaleratio': 1,\n                         'title': {'text': 'Precision'}}}\n})</pre> In\u00a0[24]: Copied! <pre>plot.freeze()  # replaces interactive plot with static image\n</pre> plot.freeze()  # replaces interactive plot with static image In\u00a0[25]: Copied! <pre># Our dataset's schema now contains `eval_*` fields\nprint(dataset)\n</pre> # Our dataset's schema now contains `eval_*` fields print(dataset) <pre>Name:        evaluate-detections-tutorial\nMedia type:  image\nNum samples: 5000\nPersistent:  True\nTags:        []\nSample fields:\n    id:           fiftyone.core.fields.ObjectIdField\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    faster_rcnn:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:      fiftyone.core.fields.IntField\n    eval_fp:      fiftyone.core.fields.IntField\n    eval_fn:      fiftyone.core.fields.IntField\n</pre> <p>The individual predicted and ground truth objects also have fields populated on them describing the results of the matching process:</p> <ul> <li><code>eval</code>: whether the object is a TP/FP/FN</li> <li><code>eval_id</code>: the ID of the matching ground truth/predicted object, if any</li> <li><code>eval_iou</code>: the IoU between the matching objects, if any</li> </ul> In\u00a0[26]: Copied! <pre># Our detections have helpful evaluation data on them\nsample = high_conf_view.first()\nprint(sample.faster_rcnn.detections[0])\n</pre> # Our detections have helpful evaluation data on them sample = high_conf_view.first() print(sample.faster_rcnn.detections[0]) <pre>&lt;Detection: {\n    'id': '66047b0f05c1282f3e986920',\n    'attributes': {},\n    'tags': [],\n    'label': 'airplane',\n    'bounding_box': [\n        0.5629980087280273,\n        0.7977214296832356,\n        0.03478360176086426,\n        0.1007584484383529,\n    ],\n    'mask': None,\n    'confidence': 0.9952868223190308,\n    'index': None,\n    'eval_iou': 0.8995312552391188,\n    'eval_id': '66047a4b05c1282f3e97f285',\n    'eval': 'tp',\n}&gt;\n</pre> <p>These extra fields were added because we provided the <code>eval_key</code> parameter to evaluate_detections(). If we had omitted this parameter, then no information would have been recorded on our samples.</p> <p>Don't worry, if you forget what evaluations you've run, you can retrieve information about the evaluation later:</p> In\u00a0[27]: Copied! <pre>print(dataset.list_evaluations())\n</pre> print(dataset.list_evaluations()) <pre>['eval']\n</pre> In\u00a0[28]: Copied! <pre>print(dataset.get_evaluation_info(\"eval\"))\n</pre> print(dataset.get_evaluation_info(\"eval\")) <pre>{\n    \"key\": \"eval\",\n    \"version\": \"0.24.0\",\n    \"timestamp\": \"2024-03-27T22:10:32.599000\",\n    \"config\": {\n        \"cls\": \"fiftyone.utils.eval.coco.COCOEvaluationConfig\",\n        \"type\": \"detection\",\n        \"method\": \"coco\",\n        \"pred_field\": \"faster_rcnn\",\n        \"gt_field\": \"ground_truth\",\n        \"iou\": 0.5,\n        \"classwise\": true,\n        \"iscrowd\": \"iscrowd\",\n        \"use_masks\": false,\n        \"use_boxes\": false,\n        \"tolerance\": null,\n        \"compute_mAP\": true,\n        \"iou_threshs\": [\n            0.5,\n            0.55,\n            0.6,\n            0.65,\n            0.7,\n            0.75,\n            0.8,\n            0.85,\n            0.9,\n            0.95\n        ],\n        \"max_preds\": 100,\n        \"error_level\": 1\n    }\n}\n</pre> <p>You can even load the view on which you ran an evaluation by calling the load_evaluation_view() method on the parent dataset:</p> In\u00a0[29]: Copied! <pre># Load the view on which we ran the `eval` evaluation\neval_view = dataset.load_evaluation_view(\"eval\")\nprint(eval_view)\n</pre> # Load the view on which we ran the `eval` evaluation eval_view = dataset.load_evaluation_view(\"eval\") print(eval_view) <pre>Dataset:     evaluate-detections-tutorial\nMedia type:  image\nNum samples: 100\nSample fields:\n    id:           fiftyone.core.fields.ObjectIdField\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    faster_rcnn:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:      fiftyone.core.fields.IntField\n    eval_fp:      fiftyone.core.fields.IntField\n    eval_fn:      fiftyone.core.fields.IntField\nView stages:\n    1. Take(size=100, seed=51)\n    2. FilterLabels(field='faster_rcnn', filter={'$gt': ['$$this.confidence', 0.75]}, only_matches=False, trajectories=False)\n</pre> <p>Finally, you can delete an evaluation from a dataset, including any information that was added to your samples, by calling delete_evaluation().</p> In\u00a0[30]: Copied! <pre>eval_patches = dataset.to_evaluation_patches(\"eval\")\nprint(eval_patches)\n</pre> eval_patches = dataset.to_evaluation_patches(\"eval\") print(eval_patches) <pre>Dataset:     evaluate-detections-tutorial\nMedia type:  image\nNum patches: 37747\nPatch fields:\n    id:           fiftyone.core.fields.ObjectIdField\n    sample_id:    fiftyone.core.fields.ObjectIdField\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    faster_rcnn:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    crowd:        fiftyone.core.fields.BooleanField\n    type:         fiftyone.core.fields.StringField\n    iou:          fiftyone.core.fields.FloatField\nView stages:\n    1. ToEvaluationPatches(eval_key='eval', config=None)\n</pre> In\u00a0[31]: Copied! <pre>session.view = high_conf_view\n</pre> session.view = high_conf_view <p></p> <p>Let's use this evaluation view to find individual false positive detections with a confidence of 0.85 or greater.</p> <p></p> In\u00a0[33]: Copied! <pre># Show samples with most true positives\nsession.view = high_conf_view.sort_by(\"eval_tp\", reverse=True)\n</pre> # Show samples with most true positives session.view = high_conf_view.sort_by(\"eval_tp\", reverse=True) <p></p> In\u00a0[34]: Copied! <pre># Show samples with most false positives\nsession.view = high_conf_view.sort_by(\"eval_fp\", reverse=True)\n</pre> # Show samples with most false positives session.view = high_conf_view.sort_by(\"eval_fp\", reverse=True) <p></p> In\u00a0[37]: Copied! <pre># Compute metadata so we can reference image height/width in our view\ndataset.compute_metadata()\n</pre> # Compute metadata so we can reference image height/width in our view dataset.compute_metadata() In\u00a0[38]: Copied! <pre>#\n# Create an expression that will match objects whose bounding boxes have\n# area less than 32^2 pixels\n#\n# Bounding box format is [top-left-x, top-left-y, width, height]\n# with relative coordinates in [0, 1], so we multiply by image\n# dimensions to get pixel area\n#\nbbox_area = (\n    F(\"$metadata.width\") * F(\"bounding_box\")[2] *\n    F(\"$metadata.height\") * F(\"bounding_box\")[3]\n)\nsmall_boxes = bbox_area &lt; 32 ** 2\n\n# Create a view that contains only small (and high confidence) predictions\nsmall_boxes_view = high_conf_view.filter_labels(\"faster_rcnn\", small_boxes)\n\nsession.view = small_boxes_view\n</pre> # # Create an expression that will match objects whose bounding boxes have # area less than 32^2 pixels # # Bounding box format is [top-left-x, top-left-y, width, height] # with relative coordinates in [0, 1], so we multiply by image # dimensions to get pixel area # bbox_area = (     F(\"$metadata.width\") * F(\"bounding_box\")[2] *     F(\"$metadata.height\") * F(\"bounding_box\")[3] ) small_boxes = bbox_area &lt; 32 ** 2  # Create a view that contains only small (and high confidence) predictions small_boxes_view = high_conf_view.filter_labels(\"faster_rcnn\", small_boxes)  session.view = small_boxes_view <p></p> <p>We can always re-run evaluation to see how our detector fairs on only small boxes:</p> In\u00a0[33]: Copied! <pre># Create a view that contains only small GT and predicted boxes\nsmall_boxes_eval_view = (\n    high_conf_view\n    .filter_labels(\"ground_truth\", small_boxes, only_matches=False)\n    .filter_labels(\"faster_rcnn\", small_boxes, only_matches=False)\n)\n\n# Run evaluation\nsmall_boxes_results = small_boxes_eval_view.evaluate_detections(\n    \"faster_rcnn\",\n    gt_field=\"ground_truth\",\n)\n</pre> # Create a view that contains only small GT and predicted boxes small_boxes_eval_view = (     high_conf_view     .filter_labels(\"ground_truth\", small_boxes, only_matches=False)     .filter_labels(\"faster_rcnn\", small_boxes, only_matches=False) )  # Run evaluation small_boxes_results = small_boxes_eval_view.evaluate_detections(     \"faster_rcnn\",     gt_field=\"ground_truth\", ) <pre>Evaluating detections...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 34/34 [339.1ms elapsed, 0s remaining, 100.3 samples/s]     \n</pre> In\u00a0[34]: Copied! <pre># Get the 10 most common small object classes\nsmall_counts = small_boxes_eval_view.count_values(\"ground_truth.detections.label\")\nclasses_top10_small = sorted(small_counts, key=counts.get, reverse=True)[:10]\n\n# Print a classification report for the top-10 small object classes\nsmall_boxes_results.print_report(classes=classes_top10_small)\n</pre> # Get the 10 most common small object classes small_counts = small_boxes_eval_view.count_values(\"ground_truth.detections.label\") classes_top10_small = sorted(small_counts, key=counts.get, reverse=True)[:10]  # Print a classification report for the top-10 small object classes small_boxes_results.print_report(classes=classes_top10_small) <pre>               precision    recall  f1-score   support\n\n       person       0.66      0.44      0.53        80\n          car       0.69      0.43      0.53        21\n        chair       0.00      0.00      0.00         4\n         book       0.00      0.00      0.00        20\n       bottle       0.25      1.00      0.40         1\n          cup       0.00      0.00      0.00         1\n dining table       0.00      0.00      0.00         2\ntraffic light       0.56      0.33      0.42        15\n      handbag       0.00      0.00      0.00         7\n         boat       0.00      0.00      0.00         1\n\n    micro avg       0.61      0.33      0.43       152\n    macro avg       0.22      0.22      0.19       152\n weighted avg       0.50      0.33      0.39       152\n\n</pre> In\u00a0[39]: Copied! <pre># View the `iscrowd` attribute on a ground truth object\nsample = dataset.first()\nprint(sample.ground_truth.detections[0])\n</pre> # View the `iscrowd` attribute on a ground truth object sample = dataset.first() print(sample.ground_truth.detections[0]) <pre>&lt;Detection: {\n    'id': '66047a4705c1282f3e97c5e9',\n    'attributes': {},\n    'tags': [],\n    'label': 'potted plant',\n    'bounding_box': [\n        0.37028125,\n        0.3345305164319249,\n        0.038593749999999996,\n        0.16314553990610328,\n    ],\n    'mask': None,\n    'confidence': None,\n    'index': None,\n    'supercategory': 'furniture',\n    'iscrowd': 0,\n}&gt;\n</pre> <p>Let's create a view that contains only samples with at least one detection for which <code>iscrowd</code> is 1:</p> In\u00a0[40]: Copied! <pre># Create a view that contains only samples for which at least one detection has \n# its iscrowd attribute set to 1\ncrowded_images_view = high_conf_view.match(\n    F(\"ground_truth.detections\").filter(F(\"iscrowd\") == 1).length() &gt; 0\n)\n\nsession.view = crowded_images_view\n</pre> # Create a view that contains only samples for which at least one detection has  # its iscrowd attribute set to 1 crowded_images_view = high_conf_view.match(     F(\"ground_truth.detections\").filter(F(\"iscrowd\") == 1).length() &gt; 0 )  session.view = crowded_images_view <p></p> In\u00a0[41]: Copied! <pre>session.view = crowded_images_view.sort_by(\"eval_fp\", reverse=True)\n</pre> session.view = crowded_images_view.sort_by(\"eval_fp\", reverse=True) <p></p> <p>Let's compare the above view to another view that just sorts by false positive count, regardless of whether the image is crowded:</p> In\u00a0[43]: Copied! <pre>session.view = high_conf_view.sort_by(\"eval_fp\", reverse=True)\n</pre> session.view = high_conf_view.sort_by(\"eval_fp\", reverse=True) <p></p> <p>This was one of the first images in the view. As we can see, while the evaluation is detecting $7$ false positives, all of the model's predictions seem accurate. It is just that the ground truth labels lumped a bunch of orange slices together into one box.</p> <p>See anything interesting?</p> <p>What you find will likely be different because a random subset of samples were chosen. In our case, we find missing ground truth boxes for two of the laptop keyboards, a bottle, and even perhaps a cell phone. The model did not confidently predict many of the boxes in this image, but from a high-level, an example like this makes us consider the consequences of including complex or dense images in datasets. It will likely mean incorrect or incomplete ground truth annotations the annotators are not diligent! And that ultimately leads to confused models, and misinformed evaluations.</p> <p>This conclusion would have been nearly impossible to achieve without visually inspecting the individual samples in the dataset according to the variety of criteria that we considered in this tutorial.</p> In\u00a0[45]: Copied! <pre>session.freeze()  # screenshot the active App for sharing\n</pre> session.freeze()  # screenshot the active App for sharing In\u00a0[44]: Copied! <pre># Tag all highly confident false positives as \"possibly-missing\"\n(\n    high_conf_view\n        .filter_labels(\"faster_rcnn\", F(\"eval\") == \"fp\")\n        .select_fields(\"faster_rcnn\")\n        .tag_labels(\"possibly-missing\")\n)\n</pre> # Tag all highly confident false positives as \"possibly-missing\" (     high_conf_view         .filter_labels(\"faster_rcnn\", F(\"eval\") == \"fp\")         .select_fields(\"faster_rcnn\")         .tag_labels(\"possibly-missing\") ) <p>These tagged labels could then be sent off to our annotation provider of choice for review and addition to the ground truth labels. FiftyOne currently offers integrations for Scale AI, Labelbox, and CVAT.</p> <p>For example, the snippet below exports the tagged labels and their source media to disk in CVAT format:</p> In\u00a0[\u00a0]: Copied! <pre># Export all labels with the `possibly-missing` tag in CVAT format\n(\n    dataset\n        .select_labels(tags=[\"possibly-missing\"])\n        .export(\"/path/for/export\", fo.types.CVATImageDataset)\n)\n</pre> # Export all labels with the `possibly-missing` tag in CVAT format (     dataset         .select_labels(tags=[\"possibly-missing\"])         .export(\"/path/for/export\", fo.types.CVATImageDataset) )"},{"location":"tutorials/evaluate_detections/#evaluating-object-detections-with-fiftyone","title":"Evaluating Object Detections with FiftyOne\u00b6","text":"<p>This walkthrough demonstrates how to use FiftyOne to perform hands-on evaluation of your detection model.</p> <p>It covers the following concepts:</p> <ul> <li>Loading a dataset with ground truth labels into FiftyOne</li> <li>Adding model predictions to your dataset</li> <li>Evaluating your model using FiftyOne's evaluation API</li> <li>Viewing the best and worst performing samples in your dataset</li> </ul> <p>So, what's the takeaway?</p> <p>Aggregate measures of performance like mAP don't give you the full picture of your detection model. In practice, the limiting factor on your model's performance is often data quality issues that you need to see to address. FiftyOne is designed to make it easy to do just that.</p> <p>Running the workflow presented here on your ML projects will help you to understand the current failure modes (edge cases) of your model and how to fix them, including:</p> <ul> <li>Identifying scenarios that require additional training samples in order to boost your model's performance</li> <li>Deciding whether your ground truth annotations have errors/weaknesses that need to be corrected before any subsequent model training will be profitable</li> </ul>"},{"location":"tutorials/evaluate_detections/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"tutorials/evaluate_detections/#add-predictions-to-dataset","title":"Add predictions to dataset\u00b6","text":"<p>Now let's generate some predictions to analyze.</p> <p>Because we loaded the model from the FiftyOne Model Zoo, it is a FiftyOne model object, which means we can apply it directly to our dataset (or any subset thereof) for inference using the sample collection's <code>apply_model()</code> method.</p> <p>The code below performs inference with the Faster R-CNN model on a randomly chosen subset of 100 samples from the dataset and stores the resulting predictions in a <code>faster_rcnn</code> field of the samples.</p>"},{"location":"tutorials/evaluate_detections/#analyzing-detections","title":"Analyzing detections\u00b6","text":"<p>Let's analyze the raw predictions we've added to our dataset in more detail.</p>"},{"location":"tutorials/evaluate_detections/#visualizing-bounding-boxes","title":"Visualizing bounding boxes\u00b6","text":"<p>Let's start by loading the full dataset in the App:</p>"},{"location":"tutorials/evaluate_detections/#selecting-samples-of-interest","title":"Selecting samples of interest\u00b6","text":"<p>You can select images in the App by clicking on the checkbox when hovering over an image. Then, you can create a view that contains only those samples by clicking the orange checkmark with the number of selected samples in the top left corner of the sample grid and clicking <code>Only show selected samples</code>.</p>"},{"location":"tutorials/evaluate_detections/#confidence-thresholding-in-the-app","title":"Confidence thresholding in the App\u00b6","text":"<p>From the App instance above, it looks like our detector is generating some spurious low-quality detections. Let's use the App to interactively filter the predictions by <code>confidence</code> to identify a reasonable confidence threshold for our model:</p>"},{"location":"tutorials/evaluate_detections/#confidence-thresholding-in-python","title":"Confidence thresholding in Python\u00b6","text":"<p>FiftyOne also provides the ability to write expressions that match, filter, and sort detections based on their attributes. See using DatasetViews for full details.</p> <p>For example, we can programmatically generate a view that contains only detections whose <code>confidence</code> is at least <code>0.75</code> as follows:</p>"},{"location":"tutorials/evaluate_detections/#viewing-object-patches","title":"Viewing object patches\u00b6","text":"<p>There are multiple situations where it can be useful to visualize each object separately. For example, if a sample contains dozens of objects overlapping one another or if you want to look specifically for instances of a class of objects.</p> <p>In any case, the FiftyOne App provides a patches view button that allows you to take any <code>Detections</code> field in your dataset and visualize each object as an individual patch in the image grid.</p>"},{"location":"tutorials/evaluate_detections/#evaluate-detections","title":"Evaluate detections\u00b6","text":"<p>Now that we have samples with ground truth and predicted objects, let's use FiftyOne to evaluate the quality of the detections.</p> <p>FiftyOne provides a powerful evaluation API that contains a collection of methods for performing evaluation of model predictions. Since we're working with object detections here, we'll use detection evaluation.</p>"},{"location":"tutorials/evaluate_detections/#running-evaluation","title":"Running evaluation\u00b6","text":"<p>We can run evaluation on our samples via evaluate_detections(). Note that this method is available on both the <code>Dataset</code> and <code>DatasetView</code> classes, which means that we can run evaluation on our <code>high_conf_view</code> to assess the quality of only the high confidence predictions in our dataset.</p> <p>By default, this method will use the COCO evaluation protocol, plus some extra goodies that we will use later.</p>"},{"location":"tutorials/evaluate_detections/#aggregate-results","title":"Aggregate results\u00b6","text":"<p>The <code>results</code> object returned by the evaluation routine provides a number of convenient methods for analyzing our predictions.</p> <p>For example, let's print a classification report for the top-10 most common classes in the dataset:</p>"},{"location":"tutorials/evaluate_detections/#sample-level-analysis","title":"Sample-level analysis\u00b6","text":"<p>The evaluation routine also populated some new fields on our dataset that contain helpful information that we can use to evaluate our predictions at the sample-level.</p> <p>In particular, each sample now contains new fields:</p> <ul> <li><code>eval_tp</code>: the number of true positive (TP) predictions in the sample</li> <li><code>eval_fp</code>: the number of false positive (FP) predictions in the sample</li> <li><code>eval_fn</code>: the number of false negative (FN) predictions in the sample</li> </ul>"},{"location":"tutorials/evaluate_detections/#evaluation-views","title":"Evaluation views\u00b6","text":"<p>So, now that we have a sense for the aggregate performance of our model, let's dive into sample-level analysis by creating an evaluation view.</p> <p>Any evaluation that you stored on your dataset can be used to generate an evaluation view that is a patches view creating a sample for every true positive, false positive, and false negative in your dataset. Through this view, you can quickly filter and sort evaluated detections by their type (TP/FP/FN), evaluated IoU, and if they are matched to a crowd object.</p> <p>These evaluation views can be created through Python or directly in the App as shown below.</p>"},{"location":"tutorials/evaluate_detections/#view-the-best-performing-samples","title":"View the best-performing samples\u00b6","text":"<p>To dig in further, let's create a view that sorts by <code>eval_tp</code> so we can see the best-performing cases of our model (i.e., the samples with the most correct predictions):</p>"},{"location":"tutorials/evaluate_detections/#view-the-worst-performing-samples","title":"View the worst-performing samples\u00b6","text":"<p>Similarly, we can sort by the <code>eval_fp</code> field to see the worst-performing cases of our model (i.e., the samples with the most false positive predictions):</p>"},{"location":"tutorials/evaluate_detections/#filtering-by-bounding-box-area","title":"Filtering by bounding box area\u00b6","text":"<p>Dataset views are extremely powerful. For example, let's look at how our model performed on small objects by creating a view that contains only predictions whose bounding box area is less than <code>32^2</code> pixels:</p>"},{"location":"tutorials/evaluate_detections/#viewing-detections-in-a-crowd","title":"Viewing detections in a crowd\u00b6","text":"<p>If you're familiar with the COCO data format, you'll know that the ground truth annotations have an <code>iscrowd = 0/1</code> attribute that indicates whether a box contains multiple instances of the same object.</p>"},{"location":"tutorials/evaluate_detections/#more-complex-insights","title":"More complex insights\u00b6","text":"<p>Let's combine our previous operations to form more complex queries that provide deeper insight into the quality of our detections.</p> <p>For example, let's sort our view of crowded images from the previous section in decreasing order of false positive counts, so that we can see samples that have many (allegedly) spurious predictions in images that are known to contain crowds of objects:</p>"},{"location":"tutorials/evaluate_detections/#tagging-and-next-steps","title":"Tagging and next steps\u00b6","text":"<p>In practice, the next step is to take action on the issues that we identified above. A natural first step is to tag the issues so they can be retrieved and dealt with later. FiftyOne provides support for tagging samples and labels, both programmatically and via the App.</p> <p>In your App instance, try tagging the predictions with missing ground truth detections. You can do this by clicking on the boxes of the predictions of interest and using the tagging element in the top-right corner to assign a <code>possibly-missing</code> tag.</p> <p>Alternatively, we can programmatically tag a batch of labels by creating a view that contains the objects of interest and then applying tag_labels():</p>"},{"location":"tutorials/evaluate_detections/#summary","title":"Summary\u00b6","text":"<p>In this tutorial, we covered loading a dataset into FiftyOne and analyzing the performance of an out-of-the-box object detection model on the dataset.</p> <p>So, what's the takeaway?</p> <p>Aggregate evaluation results for an object detector are important, but they alone don't tell the whole story of a model's performance. It's critical to study the failure modes of your model so you can take the right actions to improve them.</p> <p>In this tutorial, we covered two types of analysis:</p> <ul> <li>Analyzing the performance of your detector across different strata, like high confidence, small objects in crowded scenes</li> <li>Inspecting the hardest samples in your dataset to diagnose the underlying issue, whether it be your detector or the ground truth annotations</li> </ul>"},{"location":"tutorials/labelbox_annotation/","title":"Annotating Datasets with Labelbox","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone labelbox\n</pre> !pip install fiftyone labelbox <p>You'll also need to set up a Labelbox account. FiftyOne supports both standard Labelbox cloud accounts and Labelbox enterprise solutions.</p> <p>The easiest way to get started is to use the default Labelbox server, which simply requires creating an account and then providing your API key as shown below.</p> In\u00a0[\u00a0]: Copied! <pre>!export FIFTYONE_LABELBOX_API_KEY=...\n</pre> !export FIFTYONE_LABELBOX_API_KEY=... <p>Alternatively, for a more permanent solution, you can store your credentials in your FiftyOne annotation config located at <code>~/.fiftyone/annotation_config.json</code>:</p> In\u00a0[\u00a0]: Copied! <pre>{\n    \"backends\": {\n        \"labelbox\": {\n            \"api_key\": ...,\n        }\n    }\n}\n</pre> {     \"backends\": {         \"labelbox\": {             \"api_key\": ...,         }     } } <p>See this page for more advanced Labelbox setup options.</p> In\u00a0[2]: Copied! <pre>import fiftyone as fo\n</pre> import fiftyone as fo In\u00a0[\u00a0]: Copied! <pre>dataset = fo.Dataset.from_dir(dataset_dir=\"/path/to/dir\", dataset_type=fo.types.ImageDirectory)\n</pre> dataset = fo.Dataset.from_dir(dataset_dir=\"/path/to/dir\", dataset_type=fo.types.ImageDirectory) <p>Another method is to use publically available datasets that may be relevant. For example, the Open Images dataset contains millions of images available for public use and can be accessed directly through the FiftyOne Dataset Zoo.</p> In\u00a0[3]: Copied! <pre>import fiftyone.zoo as foz\n</pre> import fiftyone.zoo as foz In\u00a0[15]: Copied! <pre>dataset = foz.load_zoo_dataset(\n    \"open-images-v6\",\n    split=\"validation\",\n    max_samples=100,\n    label_types=[],\n    dataset_name=\"labelbox_dataset\",\n)\n</pre> dataset = foz.load_zoo_dataset(     \"open-images-v6\",     split=\"validation\",     max_samples=100,     label_types=[],     dataset_name=\"labelbox_dataset\", ) <pre>Downloading split 'validation' to '/home/eric/fiftyone/open-images-v6/validation' if necessary\nNecessary images already downloaded\nExisting download of split 'validation' is sufficient\nLoading 'open-images-v6' split 'validation'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [72.9ms elapsed, 0s remaining, 1.4K samples/s]   \nDataset 'labelbox_dataset' created\n</pre> <p>Either way, once your data is in FiftyOne, we can visualize it in the FiftyOne App.</p> In\u00a0[18]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) Activate In\u00a0[23]: Copied! <pre>session.freeze() # Screenshot the App in this notebook\n</pre> session.freeze() # Screenshot the App in this notebook <p>FiftyOne provides a variety of methods that can help you understand the quality of the dataset and pick the best samples to annotate. For example, the <code>compute_similarity()</code> the method can be used to find both the most similar, and the most unique samples, ensuring that your dataset will contain an even distribution of data.</p> In\u00a0[19]: Copied! <pre>import fiftyone.brain as fob\n</pre> import fiftyone.brain as fob In\u00a0[20]: Copied! <pre>results = fob.compute_similarity(dataset, brain_key=\"img_sim\")\n</pre> results = fob.compute_similarity(dataset, brain_key=\"img_sim\") <pre>Computing embeddings...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [1.0m elapsed, 0s remaining, 1.8 samples/s]      \n</pre> In\u00a0[21]: Copied! <pre>results.find_unique(10)\n</pre> results.find_unique(10) <pre>Generating index...\nIndex complete\nComputing unique samples...\nthreshold: 1.000000, kept: 100, target: 10\nthreshold: 2.000000, kept: 100, target: 10\nthreshold: 4.000000, kept: 100, target: 10\nthreshold: 8.000000, kept: 70, target: 10\nthreshold: 16.000000, kept: 5, target: 10\nthreshold: 12.000000, kept: 14, target: 10\nthreshold: 14.000000, kept: 7, target: 10\nthreshold: 13.000000, kept: 10, target: 10\nUniqueness computation complete\n</pre> <p>Now to select only the slice of our dataset that contains the 10 most unique samples.</p> In\u00a0[22]: Copied! <pre>unique_view = dataset.select(results.unique_ids)\n</pre> unique_view = dataset.select(results.unique_ids) In\u00a0[24]: Copied! <pre>session.view = unique_view\n</pre> session.view = unique_view Activate In\u00a0[25]: Copied! <pre>session.freeze()\n</pre> session.freeze() In\u00a0[28]: Copied! <pre>anno_key = \"annotation_run_1\"\n\nresults = unique_view.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_field=\"detections\",\n    classes=[\"vehicle\", \"animal\", \"plant\"],\n    label_type=\"detections\",\n    launch_editor=True,\n)\n</pre> anno_key = \"annotation_run_1\"  results = unique_view.annotate(     anno_key,     backend=\"labelbox\",     label_field=\"detections\",     classes=[\"vehicle\", \"animal\", \"plant\"],     label_type=\"detections\",     launch_editor=True, ) <pre>Initializing Labelbox client at 'https://api.labelbox.com/graphql'\nUploading samples to Labelbox...\nUpload complete\nInitializing Labelbox client at 'https://api.labelbox.com/graphql'\nLaunching editor at 'https://editor.labelbox.com/?project=ckx83uywu09j610a9ertke8ny'...\n</pre> Out[28]: <pre>&lt;fiftyone.utils.labelbox.LabelboxAnnotationResults at 0x7fa3c01b4eb8&gt;</pre> <p></p> <p>The annotations can then be loaded back into FiftyOne in just one more line.</p> In\u00a0[30]: Copied! <pre>unique_view.load_annotations(anno_key)\n</pre> unique_view.load_annotations(anno_key) <pre>Initializing Labelbox client at 'https://api.labelbox.com/graphql'\nDownloading labels from Labelbox...\n  133.4Kb [7.2ms elapsed, ? remaining, 91.0Mb/s] \nDownload complete\nLoading labels for field 'detections'...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [26.2ms elapsed, 0s remaining, 305.1 samples/s] \n</pre> In\u00a0[31]: Copied! <pre>session.view = unique_view\n</pre> session.view = unique_view Activate In\u00a0[32]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>This API provides advanced customization options for your annotation tasks. For example, we can construct a sophisticated schema to define the annotations we want and even directly assign the annotators:</p> In\u00a0[33]: Copied! <pre>anno_key = \"labelbox_assign_users\"\n\nmembers = [\n    (\"fiftyone_labelbox_user1@gmail.com\", \"LABELER\"),\n    (\"fiftyone_labelbox_user2@gmail.com\", \"REVIEWER\"),\n    (\"fiftyone_labelbox_user3@gmail.com\", \"TEAM_MANAGER\"),\n]\n\n# Set up the Labelbox editor to reannotate existing \"detections\" labels and a new \"keypoints\" field\nlabel_schema = {\n    \"detections_new\": {\n        \"type\": \"detections\",\n        \"classes\": dataset.distinct(\"detections.detections.label\"),\n    },\n    \"keypoints\": {\n        \"type\": \"keypoints\",\n        \"classes\": [\"Person\"],\n    }\n}\n\nresults = unique_view.annotate(\n    anno_key,\n    backend=\"labelbox\",\n    label_schema=label_schema,\n    members=members,\n    launch_editor=True,\n)\n</pre> anno_key = \"labelbox_assign_users\"  members = [     (\"fiftyone_labelbox_user1@gmail.com\", \"LABELER\"),     (\"fiftyone_labelbox_user2@gmail.com\", \"REVIEWER\"),     (\"fiftyone_labelbox_user3@gmail.com\", \"TEAM_MANAGER\"), ]  # Set up the Labelbox editor to reannotate existing \"detections\" labels and a new \"keypoints\" field label_schema = {     \"detections_new\": {         \"type\": \"detections\",         \"classes\": dataset.distinct(\"detections.detections.label\"),     },     \"keypoints\": {         \"type\": \"keypoints\",         \"classes\": [\"Person\"],     } }  results = unique_view.annotate(     anno_key,     backend=\"labelbox\",     label_schema=label_schema,     members=members,     launch_editor=True, ) <pre>Experimental features have been enabled\nInitializing Labelbox client at 'https://api.labelbox.com/graphql'\nUploading samples to Labelbox...\nUploading existing labels in field 'detections' to Labelbox is not yet supported\nYour organization has reached its limit of 5 members. Cannot invite new member fiftyone_labelbox_user3@gmail.com to project 'FiftyOne_labelbox_dataset'\nUpload complete\nExperimental features have been enabled\nInitializing Labelbox client at 'https://api.labelbox.com/graphql'\nLaunching editor at 'https://editor.labelbox.com/?project=ckx845cbd0atq10a948ze988n'...\n</pre> Out[33]: <pre>&lt;fiftyone.utils.labelbox.LabelboxAnnotationResults at 0x7fa45ffcdc88&gt;</pre> <p>After you're finished annotating in Labelbox, you can easily download the results:</p> In\u00a0[35]: Copied! <pre># Download results and clean the run from FiftyOne and Labelbox\nunique_view.load_annotations(anno_key, cleanup=True)\n</pre> # Download results and clean the run from FiftyOne and Labelbox unique_view.load_annotations(anno_key, cleanup=True) <pre>Experimental features have been enabled\nInitializing Labelbox client at 'https://api.labelbox.com/graphql'\nDownloading labels from Labelbox...\n    24.0b [8.4ms elapsed, ? remaining, 58.0Kb/s]   \nDownload complete\nLoading labels for field 'detections'...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 0/0 [12.1ms elapsed, ? remaining, ? samples/s] \nLoading labels for field 'keypoints'...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 0/0 [1.8ms elapsed, ? remaining, ? samples/s] \nExperimental features have been enabled\nInitializing Labelbox client at 'https://api.labelbox.com/graphql'\nDeleting project 'ckx845cbd0atq10a948ze988n'...\n</pre> In\u00a0[36]: Copied! <pre># Load an existing dataset with predictions\ndataset = foz.load_zoo_dataset(\"quickstart\")\n</pre> # Load an existing dataset with predictions dataset = foz.load_zoo_dataset(\"quickstart\") <pre>Dataset already downloaded\nLoading existing dataset 'quickstart'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n</pre> In\u00a0[37]: Copied! <pre># Evaluate model predictions\ndataset.evaluate_detections(\n    \"predictions\",\n    gt_field=\"ground_truth\",\n    eval_key=\"eval\",\n)\n</pre> # Evaluate model predictions dataset.evaluate_detections(     \"predictions\",     gt_field=\"ground_truth\",     eval_key=\"eval\", ) <pre>Evaluating detections...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [6.8s elapsed, 0s remaining, 29.6 samples/s]      \n</pre> Out[37]: <pre>&lt;fiftyone.utils.eval.detection.DetectionResults at 0x7fa45fcdf0b8&gt;</pre> <p>We can use the powerful querying capabilities of the FiftyOne API to create a view filtering these model results by false positives with high confidence which generally indicates an error in the ground truth annotation.</p> In\u00a0[39]: Copied! <pre>from fiftyone import ViewField as F\n\nfp_view = dataset.filter_labels(\n    \"predictions\",\n    (F(\"confidence\") &gt; 0.8) &amp; (F(\"eval\") == \"fp\"),\n)\n\nsession = fo.launch_app(view=fp_view)\n</pre> from fiftyone import ViewField as F  fp_view = dataset.filter_labels(     \"predictions\",     (F(\"confidence\") &gt; 0.8) &amp; (F(\"eval\") == \"fp\"), )  session = fo.launch_app(view=fp_view) Activate In\u00a0[40]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>This sample appears to be missing a ground truth annotation of skis. Let's tag it in FiftyOne, and send it to Labelbox for reannotation.</p> In\u00a0[41]: Copied! <pre>session = fo.launch_app(view=fp_view)\n</pre> session = fo.launch_app(view=fp_view) Activate In\u00a0[42]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>The workflow for reannotating an existing label field is to annotate a new field, then merge the new field into the existing field.</p> In\u00a0[43]: Copied! <pre>view = dataset.match_tags(\"reannotate\")\n\nlabel_schema = {\n    \"ground_truth_edits\": {\n        \"type\": \"detections\",\n        \"classes\": dataset.distinct(\"ground_truth.detections.label\"),\n    }\n}\n\nanno_key = \"fix_labels\"\nresults = view.annotate(\n    anno_key,\n    label_schema=label_schema,\n    backend=\"labelbox\",\n)\n</pre> view = dataset.match_tags(\"reannotate\")  label_schema = {     \"ground_truth_edits\": {         \"type\": \"detections\",         \"classes\": dataset.distinct(\"ground_truth.detections.label\"),     } }  anno_key = \"fix_labels\" results = view.annotate(     anno_key,     label_schema=label_schema,     backend=\"labelbox\", ) <pre>Initializing Labelbox client at 'https://api.labelbox.com/graphql'\nUploading samples to Labelbox...\nUploading existing labels in field 'ground_truth' to Labelbox is not yet supported\nUpload complete\n</pre> Out[43]: <pre>&lt;fiftyone.utils.labelbox.LabelboxAnnotationResults at 0x7fa45fab4c88&gt;</pre> In\u00a0[44]: Copied! <pre>view.load_annotations(anno_key)\n</pre> view.load_annotations(anno_key) <pre>Initializing Labelbox client at 'https://api.labelbox.com/graphql'\nDownloading labels from Labelbox...\n   14.6Kb [3.2ms elapsed, ? remaining, 37.7Mb/s]   \nDownload complete\nLoading labels for field 'ground_truth'...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [18.3ms elapsed, 0s remaining, 54.8 samples/s] \n</pre> In\u00a0[\u00a0]: Copied! <pre>view.merge_labels(\"ground_truth_edits\", \"ground_truth\")\n</pre> view.merge_labels(\"ground_truth_edits\", \"ground_truth\") In\u00a0[45]: Copied! <pre>session.view = view\n</pre> session.view = view Activate In\u00a0[46]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>Iterating over this process of training a model, evaluating its failure modes, and improving the dataset is the most surefire way to produce high-quality datasets and subsequently high-performing models.</p> <p>You can perform additional Labelbox-specific operations to monitor the progress of an annotation project initiated through this integration with FiftyOne.</p> <p>For example, you can view the status of an existing project:</p> In\u00a0[47]: Copied! <pre>results = dataset.load_annotation_results(anno_key)\nresults.print_status()\n</pre> results = dataset.load_annotation_results(anno_key) results.print_status() <pre>Initializing Labelbox client at 'https://api.labelbox.com/graphql'\n\nProject: FiftyOne_quickstart\nID: ckx84egkf0kx60za90ukgc02a\nCreated at: 2021-12-15 22:43:41+00:00\nUpdated at: 2021-12-15 22:43:43+00:00\nNumber of labeled samples: 1\nMembers:\n\n\tUser: Eric Hofesmann\n\tName: eric.hofesmann\n\tRole: Admin\n\tEmail: eric.hofesmann@voxel51.com\n\tID: ckl137jfiss1c07320dacd81l\n\n\nReviews:\n\tPositive: 0\n\tNegative: 0\n\tZero: 0\n</pre> <p>You can also delete projects associated with an annotation run directly through the FiftyOne API.</p> In\u00a0[\u00a0]: Copied! <pre>results = dataset.load_annotation_results(anno_key)\napi = results.connect_to_api()\n\nprint(results.project_id)\n# \"bktes8fl60p4s0yba11npdjwm\"\n\napi.delete_project(results.project_id, delete_datasets=True)\n\n# OR\n\napi.delete_projects([results.project_id], delete_datasets=True)\n\n# List all projects or datasets associated with your Labelbox account\nproject_ids = api.list_projects()\ndataset_ids = api.list_datasets()\n\n# Delete all projects and datsets from your Labelbox account\napi.delete_projects(project_ids_to_delete)\napi.delete_datasets(dataset_ids_to_delete)\n</pre> results = dataset.load_annotation_results(anno_key) api = results.connect_to_api()  print(results.project_id) # \"bktes8fl60p4s0yba11npdjwm\"  api.delete_project(results.project_id, delete_datasets=True)  # OR  api.delete_projects([results.project_id], delete_datasets=True)  # List all projects or datasets associated with your Labelbox account project_ids = api.list_projects() dataset_ids = api.list_datasets()  # Delete all projects and datsets from your Labelbox account api.delete_projects(project_ids_to_delete) api.delete_datasets(dataset_ids_to_delete)"},{"location":"tutorials/labelbox_annotation/#annotating-datasets-with-labelbox","title":"Annotating Datasets with\u00a0Labelbox\u00b6","text":"<p>All successful computer vision projects start with the same thing: LOTS OF LABELED DATA!</p> <p>In this walkthrough, we'll cover how to use the integration between FiftyOne and the popular annotation tool Labelbox to build a high-quality labeled dataset.</p> <p>Specifically, this walkthrough covers:</p> <ul> <li>Using the FiftyOne Brain to select unique samples for annotation</li> <li>Annotating unlabeled samples with Labelbox</li> <li>Improving existing annotations using FiftyOne tagging and Labelbox</li> <li>Additional utilities for managing Labelbox annotation projects</li> </ul> <p>So, what's the takeaway?</p> <p>FiftyOne Datasets are the best way to explore, understand, and improve your datasets and models. This walkthrough covers how to use our Labelbox integration to streamline the annotation creation and improvement process.</p>"},{"location":"tutorials/labelbox_annotation/#setup","title":"Setup\u00b6","text":"<p>To get started, you need to install FiftyOne and the Labelbox Python client:</p>"},{"location":"tutorials/labelbox_annotation/#raw-data","title":"Raw Data\u00b6","text":"<p>To start, you need to gather raw image or video data relevant to your task. The internet has a lot of places to look for free data. Assuming you have your raw data downloaded locally, you can easily load it into FiftyOne.</p>"},{"location":"tutorials/labelbox_annotation/#annotation","title":"Annotation\u00b6","text":"<p>The integration between FiftyOne and Labelbox allows you to begin annotating your image or video data by calling a single method!</p>"},{"location":"tutorials/labelbox_annotation/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you have a labeled dataset, you can go ahead and start training a model. FiftyOne lets you export your data to disk in a variety of formats (ex: COCO, YOLO, etc) expected by most training pipelines. It also provides workflows for using popular model training libraries like PyTorch, PyTorch Lightning Flash, and Tensorflow.</p> <p>Once the model is trained, the model predictions can be loaded back into FiftyOne. These predictions can then be evaluated against the ground truth annotations to find where the model is performing well, and where it is performing poorly. This provides insight into the type of samples that need to be added to the training set, as well as any annotation errors that may exist.</p>"},{"location":"tutorials/labelbox_annotation/#additional-utilities","title":"Additional Utilities\u00b6","text":""},{"location":"tutorials/labelbox_annotation/#summary","title":"Summary\u00b6","text":"<p>No matter what computer vision projects you are working on, you will need a dataset. FiftyOne makes it easy to curate and dig into your dataset to understand all aspects of it, including what needs to be annotated or reannotated.</p> <p>In addition, using our Labelbox integration can streamline the annotation process and help you build higher quality datasets and models, faster.</p>"},{"location":"tutorials/monocular_depth_estimation/","title":"Monocular Depth Estimation with FiftyOne","text":"<p>In this walkthrough, you'll learn how to run monocular depth estimation models on your data using FiftyOne, Replicate, and Hugging Face libraries!</p> <p>It covers the following:</p> <ul> <li>What is monocular depth estimation?</li> <li>Downloading the SUN RGB-D dataset from source and loading it into FiftyOne</li> <li>Running monocular depth estimation models on your data</li> <li>Evaluating prediction performance</li> <li>Visualizing the results in FiftyOne</li> </ul> <p>Monocular depth estimation is the task of predicting the depth of a scene from a single image. Often, depth information is necessary for downstream tasks, such as 3D reconstruction or scene understanding. However, depth sensors are expensive and not always available.</p> <p>This is a challenging task because depth is inherently ambiguous from a single image. The same scene can be projected onto the image plane in many different ways, and it is impossible to know which one is correct without additional information.</p> <p>If you have multiple cameras, you can use stereo depth estimation techniques. But in some real world scenarios, you may be constrained to a single camera. When this is the case, you must rely on other cues, such as object size, occlusion, and perspective.</p> <p>First, we import all the necessary libraries, installing <code>fiftyone</code> if necessary:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone In\u00a0[\u00a0]: Copied! <pre>from glob import glob\nimport numpy as np\nfrom PIL import Image\nimport torch\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.brain as fob\nfrom fiftyone import ViewField as F\n</pre> from glob import glob import numpy as np from PIL import Image import torch  import fiftyone as fo import fiftyone.zoo as foz import fiftyone.brain as fob from fiftyone import ViewField as F <p>Download the SUN RGB-D dataset from here and unzip it, or use the following command to download it directly:</p> In\u00a0[\u00a0]: Copied! <pre>!curl -o sunrgbd.zip https://rgbd.cs.princeton.edu/data/SUNRGBD.zip\n</pre> !curl -o sunrgbd.zip https://rgbd.cs.princeton.edu/data/SUNRGBD.zip <p>and then unzip it:</p> In\u00a0[\u00a0]: Copied! <pre>!unzip sunrgbd.zip\n</pre> !unzip sunrgbd.zip <p>The SUN RGB-D dataset contains 10,335 RGB-D images, each of which has a corresponding RGB image, depth image, and camera intrinsics. It contains images from the NYU depth v2, Berkeley B3DO, and SUN3D datasets. SUN RGB-D is one of the most popular datasets for monocular depth estimation and semantic segmentation tasks!</p> <p>If you want to use the dataset for other tasks, you can fully convert the annotations and load them into your <code>fiftyone.Dataset</code>. However, for this tutorial, we will only be using the depth images, so we will only use the RGB images and the depth images (stored in the <code>depth_bfx</code> sub-directories).</p> <p>Because we are just interested in getting the point across, we'll restrict ourselves to the first 20 samples.</p> In\u00a0[3]: Copied! <pre>dataset = fo.Dataset(name=\"SUNRGBD-20\", persistent=True)\n</pre> dataset = fo.Dataset(name=\"SUNRGBD-20\", persistent=True) <p>Load in images and ground truth data</p> In\u00a0[4]: Copied! <pre>## restrict to 20 scenes\nscene_dirs = glob(\"SUNRGBD/k*/*/*\")[:20]\n</pre> ## restrict to 20 scenes scene_dirs = glob(\"SUNRGBD/k*/*/*\")[:20] <p>We will be representing depth maps with FiftyOne's Heatmap labels. For a thorough guide to working with heatmaps in FiftyOne, check out these FiftyOne Heatmaps Tips and Tricks!</p> <p>We are going to store everything in terms of normalized, relative distances, where 255 represents the maximum distance in the scene and 0 represents the minimum distance in the scene. This is a common way to represent depth maps, although it is far from the only way to do so. If we were interested in absolute distances, we could store sample-wise parameters for the minimum and maximum distances in the scene, and use these to reconstruct the absolute distances from the relative distances.</p> In\u00a0[5]: Copied! <pre>samples = []\nfor scene_dir in scene_dirs:\n    ## Get image file path from scene directory\n    image_path = glob(f\"{scene_dir}/image/*\")[0]\n\n    ## Get depth map file path from scene directory\n    depth_path = glob(f\"{scene_dir}/depth_bfx/*\")[0]\n\n    depth_map = np.array(Image.open(depth_path))\n    depth_map = (depth_map * 255 / np.max(depth_map)).astype(\"uint8\")\n    sample = fo.Sample(\n        filepath=image_path,\n        gt_depth=fo.Heatmap(map=depth_map),\n    )\n    \n    samples.append(sample)\n\ndataset.add_samples(samples);\n</pre> samples = [] for scene_dir in scene_dirs:     ## Get image file path from scene directory     image_path = glob(f\"{scene_dir}/image/*\")[0]      ## Get depth map file path from scene directory     depth_path = glob(f\"{scene_dir}/depth_bfx/*\")[0]      depth_map = np.array(Image.open(depth_path))     depth_map = (depth_map * 255 / np.max(depth_map)).astype(\"uint8\")     sample = fo.Sample(         filepath=image_path,         gt_depth=fo.Heatmap(map=depth_map),     )          samples.append(sample)  dataset.add_samples(samples); <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [192.2ms elapsed, 0s remaining, 104.1 samples/s]     \n</pre> <p>We can then visualize our images and depth maps in the FiftyOne App:</p> In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(dataset, auto=False)\n## then open tab to localhost:5151 in browser\n</pre> session = fo.launch_app(dataset, auto=False) ## then open tab to localhost:5151 in browser <p></p> <p>When working with depth maps, the color scheme and opacity of the heatmap are important. We can customize these as illustrated here.</p> <p></p> <p>Inspecting these RGB images and depth maps, we can see that there are some inaccuracies in the ground truth depth maps. For example, in this image, the dark rift through the center of the image is actually the farthest part of the scene, but the ground truth depth map shows it as the closest part of the scene:</p> <p></p> <p>Now that we have our dataset loaded in, we can run monocular depth estimation models on it! For a long time, the state-of-the-art models for monocular depth estimation such as DORN and DenseDepth were built with convolutional neural networks. Recently, however, both transformer-based models (DPT, GLPN) and diffusion-based models (Marigold) have achieved remarkable results!</p> <p>The first model we'll run is a Transformer-based model called DPT. The checkpoint below uses MiDaS, which returns the inverse depth map, so we have to invert it back to get a comparable depth map.</p> <p>If necessary, install <code>transformers</code>:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install transformers\n</pre> !pip install transformers In\u00a0[8]: Copied! <pre>from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n\nimage_processor = AutoImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\")\ndpt_model = AutoModelForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\")\n\n## you can also us a different model:\n# image_processor = AutoImageProcessor.from_pretrained(\"Intel/dpt-large\")\n# dpt_model = AutoModelForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n\ndef apply_dpt_model(sample, model, label_field):\n    image = Image.open(sample.filepath)\n    inputs = image_processor(images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        predicted_depth = outputs.predicted_depth\n\n    prediction = torch.nn.functional.interpolate(\n        predicted_depth.unsqueeze(1),\n        size=image.size[::-1],\n        mode=\"bicubic\",\n        align_corners=False,\n    )\n\n    output = prediction.squeeze().cpu().numpy()\n    ## flip b/c MiDaS returns inverse depth\n    formatted = (255 - output * 255 / np.max(output)).astype(\"uint8\")\n\n    sample[label_field] = fo.Heatmap(map=formatted)\n    sample.save()\n\nfor sample in dataset.iter_samples(autosave=True, progress=True):\n    apply_dpt_model(sample, dpt_model, \"dpt\")\n</pre> from transformers import AutoImageProcessor, AutoModelForDepthEstimation  image_processor = AutoImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\") dpt_model = AutoModelForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\")  ## you can also us a different model: # image_processor = AutoImageProcessor.from_pretrained(\"Intel/dpt-large\") # dpt_model = AutoModelForDepthEstimation.from_pretrained(\"Intel/dpt-large\")  def apply_dpt_model(sample, model, label_field):     image = Image.open(sample.filepath)     inputs = image_processor(images=image, return_tensors=\"pt\")      with torch.no_grad():         outputs = model(**inputs)         predicted_depth = outputs.predicted_depth      prediction = torch.nn.functional.interpolate(         predicted_depth.unsqueeze(1),         size=image.size[::-1],         mode=\"bicubic\",         align_corners=False,     )      output = prediction.squeeze().cpu().numpy()     ## flip b/c MiDaS returns inverse depth     formatted = (255 - output * 255 / np.max(output)).astype(\"uint8\")      sample[label_field] = fo.Heatmap(map=formatted)     sample.save()  for sample in dataset.iter_samples(autosave=True, progress=True):     apply_dpt_model(sample, dpt_model, \"dpt\") <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [15.1s elapsed, 0s remaining, 1.5 samples/s]      \n</pre> In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) <p></p> <p>In our <code>apply_dpt_model()</code> function, between the model's forward pass and the heatmap generation, notice that we make a call to <code>torch.nn.functional.interpolate()</code>. This is because the model's forward pass is run on a downsampled version of the image, and we want to return a heatmap that is the same size as the original image.</p> <p>Why do we need to do this? If we just want to look at the heatmaps, this would not matter. But if we want to compare the ground truth depth maps to the model's predictions on a per-pixel basis, we need to make sure that they are the same size.</p> <p>In this example, we manually applied the <code>transformers</code> model to our data to generate heatmaps. In practice, we have made it even easier to apply transformer-based models (for monocular depth estimation as well as other tasks) to your data via FiftyOne's Hugging Face Transformers Integration!</p> <p>You can load the transformer models via Hugging Face's <code>transformers</code> library, and then just apply them to FiftyOne datasets via the <code>apply_model()</code> method:</p> In\u00a0[\u00a0]: Copied! <pre># DPT\nfrom transformers import DPTForDepthEstimation\nmodel = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n\n# GLPN\nfrom transformers import GLPNForDepthEstimation\nmodel = GLPNForDepthEstimation.from_pretrained(\"vinvino02/glpn-kitti\")\n\n# Depth Anything\nfrom transformers import AutoModelForDepthEstimation\nmodel = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n\ndataset.apply_model(model, label_field=\"depth_predictions\")\n\nsession = fo.launch_app(dataset)\n</pre> # DPT from transformers import DPTForDepthEstimation model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")  # GLPN from transformers import GLPNForDepthEstimation model = GLPNForDepthEstimation.from_pretrained(\"vinvino02/glpn-kitti\")  # Depth Anything from transformers import AutoModelForDepthEstimation model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")  dataset.apply_model(model, label_field=\"depth_predictions\")  session = fo.launch_app(dataset) <p>Alternatively, you can load any Hugging Face Transformers model directly from the FiftyOne Model Zoo via the name <code>depth-estimation-transformer-torch</code>, and specifying the model's location on the Hugging Face Hub (<code>repo_id</code>) via the <code>name_or_path</code> parameter. To load and apply this DPT MiDaS hybrid model, for instance, you would use the following:</p> In\u00a0[\u00a0]: Copied! <pre>import fiftyone.zoo as foz\n\nmodel = foz.load_zoo_model(\n    \"depth-estimation-transformer-torch\",\n    name_or_path=\"Intel/dpt-hybrid-midas\",\n)\n\ndataset.apply_model(model, label_field=\"dpt_hybrid_midas\")\nsession = fo.launch_app(dataset)\n</pre> import fiftyone.zoo as foz  model = foz.load_zoo_model(     \"depth-estimation-transformer-torch\",     name_or_path=\"Intel/dpt-hybrid-midas\", )  dataset.apply_model(model, label_field=\"dpt_hybrid_midas\") session = fo.launch_app(dataset) <p>Install the <code>replicate</code> Python client if necessary:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install replicate\n</pre> !pip install replicate <p>And set your API Token:</p> <p>Then run the following command:</p> In\u00a0[\u00a0]: Copied! <pre>!export REPLICATE_API_TOKEN=r8_&lt;your_token_here&gt;\n</pre> !export REPLICATE_API_TOKEN=r8_ <p>\ud83d\udca1 It might take a minute for the model to load into memory on the server (cold-start problem), but once it does the prediction should only take a few seconds.</p> In\u00a0[\u00a0]: Copied! <pre>import replicate\nimport requests\n\nrgb_fp = dataset.first().filepath\n\noutput = replicate.run(\n    \"cjwbw/midas:a6ba5798f04f80d3b314de0f0a62277f21ab3503c60c84d4817de83c5edfdae0\",\n    input={\n        \"model_type\": \"dpt_beit_large_512\",\n        \"image\":open(rgb_fp, \"rb\")\n    }\n)\nprint(output)\n</pre> import replicate import requests  rgb_fp = dataset.first().filepath  output = replicate.run(     \"cjwbw/midas:a6ba5798f04f80d3b314de0f0a62277f21ab3503c60c84d4817de83c5edfdae0\",     input={         \"model_type\": \"dpt_beit_large_512\",         \"image\":open(rgb_fp, \"rb\")     } ) print(output) <p>While diffusion is a very powerful approach to monocular depth estimation, it is also very computationally expensive and can take a while. I personally recommend going for option 2, where predictions with Replicate take about 15 seconds per image.</p> <p>Clone the Marigold GH repo:</p> In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/prs-eth/Marigold.git\n</pre> !git clone https://github.com/prs-eth/Marigold.git In\u00a0[\u00a0]: Copied! <pre>from Marigold.marigold import MarigoldPipeline\npipe = MarigoldPipeline.from_pretrained(\"Bingxin/Marigold\")\n</pre> from Marigold.marigold import MarigoldPipeline pipe = MarigoldPipeline.from_pretrained(\"Bingxin/Marigold\") <p>Then prediction looks like:</p> In\u00a0[\u00a0]: Copied! <pre>rgb_image = Image.open(dataset.first().filepath)\noutput = pipe(rgb_image)\ndepth_image = output['depth_colored']\n</pre> rgb_image = Image.open(dataset.first().filepath) output = pipe(rgb_image) depth_image = output['depth_colored'] <p>\ud83d\udca1 It might take a minute for the model to load into memory on the server (cold-start problem), but once it does the prediction should only take a few seconds.</p> In\u00a0[29]: Copied! <pre>import replicate\nimport requests\nimport io\n\ndef marigold_model(rgb_image):\n    output = replicate.run(\n        \"adirik/marigold:1a363593bc4882684fc58042d19db5e13a810e44e02f8d4c32afd1eb30464818\",\n        input={\n            \"image\":rgb_image\n        }\n    )\n    ## get the black and white depth map\n    response = requests.get(output[1]).content\n    return response\n\ndef apply_marigold_model(sample, model, label_field):\n    rgb_image = open(sample.filepath, \"rb\")\n    response = model(rgb_image)\n    depth_image = np.array(Image.open(io.BytesIO(response)))[:, :, 0] ## all channels are the same\n    formatted = (255 - depth_image).astype(\"uint8\")\n    sample[label_field] = fo.Heatmap(map=formatted)\n    sample.save()\n\nfor sample in dataset.iter_samples(autosave=True, progress=True):\n    apply_marigold_model(sample, marigold_model, \"marigold\")\n</pre> import replicate import requests import io  def marigold_model(rgb_image):     output = replicate.run(         \"adirik/marigold:1a363593bc4882684fc58042d19db5e13a810e44e02f8d4c32afd1eb30464818\",         input={             \"image\":rgb_image         }     )     ## get the black and white depth map     response = requests.get(output[1]).content     return response  def apply_marigold_model(sample, model, label_field):     rgb_image = open(sample.filepath, \"rb\")     response = model(rgb_image)     depth_image = np.array(Image.open(io.BytesIO(response)))[:, :, 0] ## all channels are the same     formatted = (255 - depth_image).astype(\"uint8\")     sample[label_field] = fo.Heatmap(map=formatted)     sample.save()  for sample in dataset.iter_samples(autosave=True, progress=True):     apply_marigold_model(sample, marigold_model, \"marigold\") <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [5.3m elapsed, 0s remaining, 0.1 samples/s]    \n</pre> In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) <p></p> <p>Now that we have predictions from multiple models, let's evaluate them! We will leverage sklearn to apply three simple metrics commonly used for monocular depth estimation: root mean squared error (RMSE), peak signal to noise ratio (PSNR), and structural similarity index (SSIM).</p> <p>\ud83d\udca1 Higher PSNR and SSIM scores indicate better predictions, while lower RMSE scores indicate better predictions.</p> In\u00a0[38]: Copied! <pre>from skimage.metrics import peak_signal_noise_ratio, mean_squared_error, structural_similarity\n\ndef rmse(gt, pred):\n    \"\"\"Compute root mean squared error between ground truth and prediction\"\"\"\n    return np.sqrt(mean_squared_error(gt, pred))\n</pre> from skimage.metrics import peak_signal_noise_ratio, mean_squared_error, structural_similarity  def rmse(gt, pred):     \"\"\"Compute root mean squared error between ground truth and prediction\"\"\"     return np.sqrt(mean_squared_error(gt, pred)) In\u00a0[48]: Copied! <pre>def evaluate_depth(dataset, prediction_field, gt_field):\n    for sample in dataset.iter_samples(autosave=True, progress=True):\n        gt_map = sample[gt_field].map\n        pred = sample[prediction_field]\n        pred_map = pred.map\n        pred[\"rmse\"] = rmse(gt_map, pred_map)\n        pred[\"psnr\"] = peak_signal_noise_ratio(gt_map, pred_map)\n        pred[\"ssim\"] = structural_similarity(gt_map, pred_map)\n        sample[prediction_field] = pred\n    \n    ## add dynamic fields to dataset so we can view them in the App\n    dataset.add_dynamic_sample_fields()\n</pre> def evaluate_depth(dataset, prediction_field, gt_field):     for sample in dataset.iter_samples(autosave=True, progress=True):         gt_map = sample[gt_field].map         pred = sample[prediction_field]         pred_map = pred.map         pred[\"rmse\"] = rmse(gt_map, pred_map)         pred[\"psnr\"] = peak_signal_noise_ratio(gt_map, pred_map)         pred[\"ssim\"] = structural_similarity(gt_map, pred_map)         sample[prediction_field] = pred          ## add dynamic fields to dataset so we can view them in the App     dataset.add_dynamic_sample_fields() In\u00a0[\u00a0]: Copied! <pre>evaluate_depth(dataset, \"dpt\", \"gt_depth\")\nevaluate_depth(dataset, \"marigold\", \"gt_depth\")\n</pre> evaluate_depth(dataset, \"dpt\", \"gt_depth\") evaluate_depth(dataset, \"marigold\", \"gt_depth\") <p>We can then compute average metrics across the entire dataset very easily:</p> In\u00a0[66]: Copied! <pre>print(\"Mean Error Metrics\")\nfor model in [\"dpt\", \"marigold\"]:\n    print(\"-\"*50)\n    for metric in [\"rmse\", \"psnr\", \"ssim\"]:\n        mean_metric_value = dataset.mean(f\"{model}.{metric}\")\n        print(f\"Mean {metric} for {model}: {mean_metric_value}\")\n</pre> print(\"Mean Error Metrics\") for model in [\"dpt\", \"marigold\"]:     print(\"-\"*50)     for metric in [\"rmse\", \"psnr\", \"ssim\"]:         mean_metric_value = dataset.mean(f\"{model}.{metric}\")         print(f\"Mean {metric} for {model}: {mean_metric_value}\") <pre>Mean Error Metrics\n--------------------------------------------------\nMean rmse for dpt: 49.8915828817003\nMean psnr for dpt: 14.805904629602551\nMean ssim for dpt: 0.8398022368184576\n--------------------------------------------------\nMean rmse for marigold: 104.0061165272178\nMean psnr for marigold: 7.93015537185192\nMean ssim for marigold: 0.42766803372861134\n</pre> <p>All of the metrics seem to agree that DPT outperforms Marigold. However, it is important to note that these metrics are not perfect. For example, RMSE is very sensitive to outliers, and SSIM is not very sensitive to small errors. For a more thorough evaluation, we can filter by these metrics in the app in order to visualize what the model is doing well and what it is doing poorly \u2014 or where the metrics are failing to capture the model's performance.</p> <p>Toggling masks on and off is a great way to visualize the differences between the ground truth and the model's predictions:</p> <p></p> <p>Now that we've explored some model predictions, let's quickly recap some of the key challenges with monocular depth estimation:</p> <p>In this walkthrough, we learned how to run monocular depth estimation models on your data using FiftyOne, Replicate, and Hugging Face libraries! We also learned how to evaluate the predictions using common metrics, and how to visualize the results in FiftyOne. In real-world applications, it is important to look at the depth maps themselves, and not just the metrics! It is also important to understand that model performance is limited by the quality, quantity, and diversity of data they are trained on.</p>"},{"location":"tutorials/monocular_depth_estimation/#monocular-depth-estimation-with-fiftyone","title":"Monocular Depth Estimation with FiftyOne\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#what-is-monocular-depth-estimation","title":"What is Monocular Depth Estimation?\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#applications","title":"Applications\u00b6","text":"<p>Monocular depth estimation has many applications in computer vision. For example, it can be used for:</p> <ul> <li>3D reconstruction: Given a single image, estimate the depth of the scene and reconstruct the 3D geometry of the scene.</li> <li>Scene understanding: Given a single image, estimate the depth of the scene and use it to understand the scene better.</li> <li>Autonomous driving: Given a single image, estimate the depth of the scene and use it to navigate the vehicle.</li> <li>Augmented reality: Given a single image, estimate the depth of the scene and use it to place virtual objects in the scene.</li> </ul> <p>Beyond these industry applications, the ability to extract high-quality depth information from a single image has found fascinating use cases in content creation and editing, for instance:</p> <ul> <li>Image editing: Given a single image, estimate the depth of the scene and use it to apply depth-aware effects to the image.</li> <li>Image generation: Given a single image, estimate the depth of the scene and use it to generate a 3D model of the scene.</li> <li>Depth-map guided text-to-image generation: Given a single image, estimate the depth of the scene and use it to generate a new image that both adheres to your input text prompt and has the same depth map. (See ControlNet!)</li> </ul>"},{"location":"tutorials/monocular_depth_estimation/#create-dataset","title":"Create Dataset\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#ground-truth","title":"Ground Truth?\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#run-monocular-depth-estimation-models","title":"Run Monocular Depth Estimation Models\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#dpt-transformer-models","title":"DPT (Transformer Models)\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#option-1-run-locally-with-hugging-face-transformers","title":"Option 1: Run locally with Hugging Face Transformers\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#interpolating-depth-maps","title":"Interpolating Depth Maps\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#hugging-face-transformers-integration","title":"Hugging Face Transformers Integration\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#option-2-run-with-replicate","title":"Option 2: Run with Replicate\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#marigold-diffusion-models","title":"Marigold (Diffusion Models)\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#option-1-download-and-run-locally-with-hugging-face-diffusers","title":"Option 1: Download and run locally with Hugging Face Diffusers\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#option-2-run-via-replicate","title":"Option 2: Run via Replicate\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#evaluate-predictions","title":"Evaluate Predictions\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#key-challenges-with-monocular-depth-estimation","title":"Key Challenges with Monocular Depth Estimation\u00b6","text":""},{"location":"tutorials/monocular_depth_estimation/#data-quality-and-quantity","title":"Data quality and quantity\u00b6","text":"<p>Ground truth data is hard to come by, and is often noisy. For example, the SUN RGB-D dataset contains 10,335 RGB-D images, which is a lot, but it is still a relatively small dataset compared to other datasets such as ImageNet, which contains 1.2 million images. And in many cases, the ground truth data is noisy. For example, the ground truth depth maps in the SUN RGB-D dataset are generated by projecting the 3D point clouds onto the 2D image plane, and then computing the Euclidean distance between the projected points and the camera. This process is inherently noisy, and the resulting depth maps are often noisy as well.</p>"},{"location":"tutorials/monocular_depth_estimation/#poor-generalization","title":"Poor generalization\u00b6","text":"<p>Models often struggle to generalize to new environments. Outdoors, for example, is a very different environment than indoors, and models trained on indoor data often fail to generalize to outdoor data.</p>"},{"location":"tutorials/monocular_depth_estimation/#precarious-metrics","title":"Precarious metrics\u00b6","text":"<p>Metrics are not always a good indicator of model performance. For example, a model might have a low RMSE, but still produce very noisy depth maps. This is why it is important to look at the depth maps themselves, and not just the metrics!</p>"},{"location":"tutorials/monocular_depth_estimation/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/open_images/","title":"Downloading and Evaluating Open Images","text":"<p>So, what's the takeaway?</p> <p>Starting a new ML project takes data and time, and the datasets in the FiftyOne Dataset Zoo can help jump start the development process.</p> <p>Open Images in particular is one of the largest publicly available datasets for object detections, classification, segmentation, and more. Additionally, with Open Images evaluation available natively in FiftyOne, you can quickly evaluate your models and compute mAP and PR curves.</p> <p>While metrics like mAP are often used to compare models, the best way to improve your model's performance isn't to look at aggregate metrics but instead to get hands-on with your evaluation and visualize how your model performs on individual samples. All of this is made easy with FiftyOne!</p> <p>If you haven't already, install FiftyOne:</p> In\u00a0[1]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>In this tutorial, we\u2019ll use some TensorFlow models and PyTorch to generate predictions and embeddings, and we\u2019ll use the UMAP method to reduce the dimensionality of embeddings, so we need to install the corresponding packages:</p> In\u00a0[2]: Copied! <pre>!pip install tensorflow torch torchvision umap-learn\n</pre> !pip install tensorflow torch torchvision umap-learn <p>This tutorial also includes some of FiftyOne's interactive plotting capabilities.</p> <p>The recommended way to work with FiftyOne\u2019s interactive plots is in Jupyter notebooks or JupyterLab. In these environments, you can leverage the full power of plots by attaching them to the FiftyOne App and bidirectionally interacting with the plots and the App to identify interesting subsets of your data.</p> <p>To use interactive plots in Jupyter notebooks, ensure that you have the <code>ipywidgets</code> package installed:</p> In\u00a0[3]: Copied! <pre>!pip install 'ipywidgets&gt;=8,&lt;9'\n</pre> !pip install 'ipywidgets&gt;=8,&lt;9' <p>If you\u2019re working in JupyterLab, refer to these instructions to get setup.</p> <p>Support for interactive plots in non-notebook contexts and Google Colab is coming soon! In the meantime, you can still use FiftyOne's plotting features in those environments, but you must manually call <code>plot.show()</code> to update the state of a plot to match the state of a connected session, and any callbacks that would normally be triggered in response to interacting with a plot will not be triggered.</p> In\u00a0[4]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n</pre> import fiftyone as fo import fiftyone.zoo as foz In\u00a0[5]: Copied! <pre>dataset = foz.load_zoo_dataset(\n    \"open-images-v7\",\n    split=\"validation\",\n    max_samples=100,\n    seed=51,\n    shuffle=True,\n)\n</pre> dataset = foz.load_zoo_dataset(     \"open-images-v7\",     split=\"validation\",     max_samples=100,     seed=51,     shuffle=True, ) <p>Now let's launch the FiftyOne App so we can explore the dataset we just downloaded.</p> In\u00a0[6]: Copied! <pre>session = fo.launch_app(dataset.view())\n</pre> session = fo.launch_app(dataset.view()) <pre>Connected to FiftyOne on port 5151 at localhost.\nIf you are not connecting to a remote session, you may need to start a new session and specify a port\n</pre> Activate <p>Loading Open Images with FiftyOne also automatically stores relevant labels and metadata like classes, attributes, and a class hierarchy that is used for evaluation in the dataset's <code>info</code> dictionary:</p> In\u00a0[7]: Copied! <pre>print(dataset.info.keys())\n</pre> print(dataset.info.keys()) <pre>dict_keys(['hierarchy', 'attributes_map', 'attributes', 'segmentation_classes', 'point_classes', 'classes_map'])\n</pre> <p>When loading Open Images from the dataset zoo, there are a variety of available parameters that you can pass to <code>load_zoo_dataset()</code> to specify a subset of the images and/or label types to download:</p> <ul> <li><code>label_types</code> - a list of label types to load. The supported values are (<code>\"detections\", \"classifications\", \"points\", \"segmentations\", \"relationships\"</code>) for Open Images V7. Open Images v6 is the same except that it does not contain point labels. By default, all available labels types will be loaded. Specifying <code>[]</code> will load only the images</li> <li><code>classes</code> - a list of classes of interest. If specified, only samples with at least one object, segmentation, or image-level label in the specified classes will be downloaded</li> <li><code>attrs</code> - a list of attributes of interest. If specified, only download samples if they contain at least one attribute in <code>attrs</code> or one class in <code>classes</code> (only applicable when <code>label_types</code> contains <code>\"relationships\"</code>)</li> <li><code>load_hierarchy</code> - whether to load the class hierarchy into <code>dataset.info[\"hierarchy\"]</code></li> <li><code>image_ids</code> - an array of specific image IDs to download</li> <li><code>image_ids_file</code> - a path to a <code>.txt</code>, <code>.csv</code>, or <code>.json</code> file containing image IDs to download</li> </ul> <p>In addition, like all other zoo datasets, you can specify:</p> <ul> <li><code>max_samples</code> - the maximum number of samples to load</li> <li><code>shuffle</code> - whether to randomly chose which samples to load if <code>max_samples</code> is given</li> <li><code>seed</code> - a random seed to use when shuffling</li> </ul> <p>Let's use some of these parameters to download a 100 sample subset of Open Images containing segmentations and image-level labels for the classes \"Burrito\", \"Cheese\", and \"Popcorn\".</p> In\u00a0[8]: Copied! <pre>dataset = foz.load_zoo_dataset(\n    \"open-images-v7\", \n    split=\"validation\", \n    label_types=[\"segmentations\", \"classifications\"], \n    classes = [\"Burrito\", \"Cheese\", \"Popcorn\"],\n    max_samples=100,\n    seed=51,\n    shuffle=True,\n    dataset_name=\"open-images-food\",\n)\n</pre> dataset = foz.load_zoo_dataset(     \"open-images-v7\",      split=\"validation\",      label_types=[\"segmentations\", \"classifications\"],      classes = [\"Burrito\", \"Cheese\", \"Popcorn\"],     max_samples=100,     seed=51,     shuffle=True,     dataset_name=\"open-images-food\", ) <pre>Downloading split 'validation' to 'datasets/open-images-v7/validation' if necessary\nOnly found 83 (&lt;100) samples matching your requirements\nNecessary images already downloaded\nExisting download of split 'validation' is sufficient\nLoading existing dataset 'open-images-food'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n</pre> In\u00a0[9]: Copied! <pre>session.view = dataset.view()\n</pre> session.view = dataset.view() Activate In\u00a0[10]: Copied! <pre>session.freeze() # screenshots App for sharing\n</pre> session.freeze() # screenshots App for sharing <p>We can do the same for visual relationships. For example, we can download only samples that contain a relationship with the \"Wooden\" attribute.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = foz.load_zoo_dataset(\n    \"open-images-v7\",\n    split=\"validation\", \n    label_types=[\"relationships\"], \n    attrs=[\"Wooden\"],\n    max_samples=100,\n    seed=51,\n    shuffle=True,\n    dataset_name=\"open-images-relationships\", \n)\n</pre> dataset = foz.load_zoo_dataset(     \"open-images-v7\",     split=\"validation\",      label_types=[\"relationships\"],      attrs=[\"Wooden\"],     max_samples=100,     seed=51,     shuffle=True,     dataset_name=\"open-images-relationships\",  ) <p>You can visualize relationships in the App by clicking on a sample to open the App's expanded view. From there, you can hover over objects to see their attributes in a tooltip.</p> <p>Alternatively, you can use the settings menu in the lower-right corner of the media player to set <code>show_attributes</code> to True to make attributes appear as persistent boxes (as shown below). This can also be achieved programmatically by configuring the App:</p> In\u00a0[12]: Copied! <pre># Launch a new App instance with a customized config\napp_config = fo.AppConfig()\napp_config.show_attributes = True\n\nsession = fo.launch_app(dataset, config=app_config)\n</pre> # Launch a new App instance with a customized config app_config = fo.AppConfig() app_config.show_attributes = True  session = fo.launch_app(dataset, config=app_config) Activate <p>With Open Images V7, Google added point labels to the dataset, which are represented as <code>Keypoint</code> labels in FiftyOne. This means that we can select a dataset with points with ground truth point labels (potentially positive, negative, or mixed) for the classes <code>Tortoise</code> and <code>Sea turtle</code>:</p> In\u00a0[\u00a0]: Copied! <pre>dataset = foz.load_zoo_dataset(\n    \"open-images-v7\",\n    split=\"validation\", \n    label_types=[\"points\"], \n    classes = [\"Tortoise\", \"Sea turtle\"],\n    seed=51,\n    shuffle=True,\n    dataset_name=\"open-images-point\", \n)\n</pre> dataset = foz.load_zoo_dataset(     \"open-images-v7\",     split=\"validation\",      label_types=[\"points\"],      classes = [\"Tortoise\", \"Sea turtle\"],     seed=51,     shuffle=True,     dataset_name=\"open-images-point\",  ) In\u00a0[14]: Copied! <pre>session.view = dataset.view()\n</pre> session.view = dataset.view() Activate In\u00a0[15]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>As we can see by inspecting the samples in the grid, some of the point labels have more votes than others, and there are different <code>source</code> values, denoting the various methods used to generated these point labels. For details, see the Open Images V7 paper. If we just want point labels that we are relatively sure are positive, we can filter the <code>Keypoints</code> for these using <code>filter_labels()</code>:</p> In\u00a0[16]: Copied! <pre>from fiftyone import ViewField as F\npositive_dataset = dataset.filter_labels(\"points\", F(\"estimated_yes_no\") == \"yes\")\n</pre> from fiftyone import ViewField as F positive_dataset = dataset.filter_labels(\"points\", F(\"estimated_yes_no\") == \"yes\") In\u00a0[17]: Copied! <pre>session.view = positive_dataset.view()\n</pre> session.view = positive_dataset.view() Activate <p>As a basis for the rest of this walkthrough, let's download a subset of Open Images containing dog and cat objects on which we can evaluate a model.</p> <p>To ensure that we have exactly the same number of labels for each class, let's download two subsets, one for dogs and one for cats, and merge them together.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = foz.load_zoo_dataset(\n    \"open-images-v7\", \n    split=\"validation\", \n    label_types=[\"detections\", \"classifications\"], \n    classes=[\"Cat\"],\n    max_samples=250,\n    seed=51,\n    shuffle=True,\n    dataset_name=\"open-images-cat-dog\",\n)\n\ndog_subset = foz.load_zoo_dataset(\n    \"open-images-v7\", \n    split=\"validation\", \n    label_types=[\"detections\", \"classifications\"], \n    classes=[\"Dog\"],\n    max_samples=250,\n    seed=51,\n    shuffle=True,\n    dataset_name=\"dog-subset\",\n)\n</pre> dataset = foz.load_zoo_dataset(     \"open-images-v7\",      split=\"validation\",      label_types=[\"detections\", \"classifications\"],      classes=[\"Cat\"],     max_samples=250,     seed=51,     shuffle=True,     dataset_name=\"open-images-cat-dog\", )  dog_subset = foz.load_zoo_dataset(     \"open-images-v7\",      split=\"validation\",      label_types=[\"detections\", \"classifications\"],      classes=[\"Dog\"],     max_samples=250,     seed=51,     shuffle=True,     dataset_name=\"dog-subset\", ) <p>Now let's merge the samples together into one dataset:</p> In\u00a0[19]: Copied! <pre># Merge the samples together into the same dataset\ndataset.merge_samples(dog_subset)\n</pre> # Merge the samples together into the same dataset dataset.merge_samples(dog_subset) In\u00a0[20]: Copied! <pre>session.dataset = dataset\n</pre> session.dataset = dataset Activate <p>The FiftyOne App provides a patches view that can be used to view every object in the dataset as an individual image. Just click the patches icon and select the appropriate detections field:</p> In\u00a0[21]: Copied! <pre>session.show()\n</pre> session.show() Activate In\u00a0[22]: Copied! <pre>session.freeze()  # screenshot for sharing\n</pre> session.freeze()  # screenshot for sharing <p>The FiftyOne Model Zoo does not (yet!) contain models trained on Open Images, so instead we'll use a model trained on COCO and evaluate only classes that overlap between COCO and Open Images.</p> <p>Note that, if you want to instead evaluate your own model predictions, adding custom model predictions to a FiftyOne dataset is very easy.</p> <p>The model we are using requires TensorFlow Models, which we can easily install using ETA, a package bundled with FiftyOne:</p> In\u00a0[23]: Copied! <pre>!eta install models\n</pre> !eta install models <p>Now let's load the model and run inference on our dataset using FiftyOne:</p> In\u00a0[24]: Copied! <pre>model = foz.load_zoo_model(\"ssd-mobilenet-v1-coco-tf\")\n</pre> model = foz.load_zoo_model(\"ssd-mobilenet-v1-coco-tf\") In\u00a0[25]: Copied! <pre>dataset.apply_model(model, label_field=\"predictions\", confidence_thresh=0.5)\n</pre> dataset.apply_model(model, label_field=\"predictions\", confidence_thresh=0.5) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [48.9m elapsed, 0s remaining, 0.2 samples/s]    \n</pre> <p>The dataset contains ground truth objects and now model predictions in its <code>predictions</code> field. However, we are only interested in the classes \"Cat\" and \"Dog\" for this example, so we will create a view containing only the labels of interest.</p> <p>Since we specified these classes when downloading the dataset, all images are guaranteed to be related to the classes \"Cat\" and \"Dog\", we just need to filter the individual labels.</p> <p>In addition, there is a capitalization difference between the class names of Open Images (\"Cat\" and \"Dog\") and COCO (\"cat\" and \"dog\"), so we'll use FiftyOne to normalize the labels:</p> In\u00a0[26]: Copied! <pre>from fiftyone import ViewField as F\n\noi_classes = [\"Dog\", \"Cat\"]\ncoco_classes = [\"dog\", \"cat\"]\n\neval_view = (\n    dataset\n    .filter_labels(\"detections\", F(\"label\").is_in(oi_classes), only_matches=False)\n    .filter_labels(\"predictions\", F(\"label\").is_in(coco_classes), only_matches=False)\n    .map_labels(\"predictions\", {\"dog\": \"Dog\", \"cat\": \"Cat\"})\n)\n</pre> from fiftyone import ViewField as F  oi_classes = [\"Dog\", \"Cat\"] coco_classes = [\"dog\", \"cat\"]  eval_view = (     dataset     .filter_labels(\"detections\", F(\"label\").is_in(oi_classes), only_matches=False)     .filter_labels(\"predictions\", F(\"label\").is_in(coco_classes), only_matches=False)     .map_labels(\"predictions\", {\"dog\": \"Dog\", \"cat\": \"Cat\"}) ) <p>To see a human-readable description of the view, just call print:</p> In\u00a0[27]: Copied! <pre>print(eval_view)\n</pre> print(eval_view) <pre>Dataset:     open-images-cat-dog\nMedia type:  image\nNum samples: 500\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    positive_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n    negative_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n    detections:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\nView stages:\n    1. FilterLabels(field='detections', filter={'$in': ['$$this.label', [...]]}, only_matches=False, trajectories=False)\n    2. FilterLabels(field='predictions', filter={'$in': ['$$this.label', [...]]}, only_matches=False, trajectories=False)\n    3. MapLabels(field='predictions', map={'cat': 'Cat', 'dog': 'Dog'})\n</pre> <p>We're now ready to evaluate the contents of <code>eval_view</code> in FiftyOne with one line of code.</p> <p>Before we do this, note that Open Images evaluation provides a few additions on top of the evaluation protocol for Pascal VOC 2010:</p> <ul> <li>You can specify sample-level positive and negative labels; any object whose class is not in either list is ignored for that sample</li> <li>A class hierarchy can be used to expand ground truth or predicted classes</li> <li>Ground truth objects can use the <code>IsGroupOf</code> attribute to indicate multiple instances of a class existing within the bounding box</li> </ul> <p>All of these are required when evaluating a model on Open Images to compute the official mAP used to compare models in challenges and in research papers. If you are developing a custom dataset, you can choose to incorporate any number of these features into your dataset schema and selectively activate them when evaluating in FiftyOne.</p> <p>The method call below computes the official Open Images mAP for our model predictions, leveraging the required image-level labels and expanded hierarchies that were automatically populated when we loaded the dataset from the FiftyOne Dataset Zoo:</p> In\u00a0[\u00a0]: Copied! <pre>results = eval_view.evaluate_detections(\n    \"predictions\",\n    gt_field=\"detections\",\n    method=\"open-images\",\n    pos_label_field=\"positive_labels\",\n    neg_label_field=\"negative_labels\",\n    hierarchy=dataset.info[\"hierarchy\"],\n    expand_pred_hierarchy=True,\n)\n</pre> results = eval_view.evaluate_detections(     \"predictions\",     gt_field=\"detections\",     method=\"open-images\",     pos_label_field=\"positive_labels\",     neg_label_field=\"negative_labels\",     hierarchy=dataset.info[\"hierarchy\"],     expand_pred_hierarchy=True, ) <p>The returned <code>results</code> object is an OpenImagesDetectionResults instance that provides methods like mAP(), plot_confusion_matrix() and plot_pr_curves() that you can use to view common evaluation metrics.</p> In\u00a0[29]: Copied! <pre>results.mAP()\n</pre> results.mAP() Out[29]: <pre>0.7817133327903734</pre> In\u00a0[30]: Copied! <pre>results.plot_confusion_matrix()\n</pre> results.plot_confusion_matrix() <pre></pre> <pre></pre> Out[30]: <pre></pre> In\u00a0[31]: Copied! <pre>results.plot_pr_curves()\n</pre> results.plot_pr_curves() <pre></pre> <pre></pre> Out[31]: <pre></pre> <p>Using image-level labels in evaluation is useful to determine how well the model is able to detect specifically the objects that exist in the image.</p> <p>However, in this walkthrough, we are interested in evaluating false positives where the model was confused about the class of an object. This is something that we would not get by only evaluating classes specified by image-level labels, since the model may predict a cat in an image where \"Cat\" was not an image-level label.</p> <p>To perform this inter-class evaluation, we will set the parameter <code>classwise=False</code> and remove the image-level labels from the evaluation routine. Additionally, since our predictions are from a model trained without a class hierarchy, we will not expand the ground truth detections:</p> In\u00a0[32]: Copied! <pre>results = eval_view.evaluate_detections(\n    \"predictions\",\n    gt_field=\"detections\",\n    method=\"open-images\",\n    eval_key=\"eval\",\n    classwise=False,\n    expand_gt_hierarchy=False,\n)\n</pre> results = eval_view.evaluate_detections(     \"predictions\",     gt_field=\"detections\",     method=\"open-images\",     eval_key=\"eval\",     classwise=False,     expand_gt_hierarchy=False, ) <pre>Evaluating detections...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [5.3s elapsed, 0s remaining, 103.9 samples/s]      \n</pre> In\u00a0[33]: Copied! <pre>results.mAP()\n</pre> results.mAP() Out[33]: <pre>0.7714961284473072</pre> <p>The slight drop in mAP is expected when matching predicted objects with ground truth of different classes, but this is desirable when trying to closely evaluate and understand your model. By default, when <code>classwise=True</code>, all false positives indicate that a predicted object was left unmatched. On the other hand, with <code>classwise=False</code>, some false positives now indicate that a prediction matched a ground truth object with a different class. This implies that the model was confident about the object being the incorrect class and that is information that we want to know.</p> In\u00a0[34]: Copied! <pre>plot = results.plot_pr_curves()\nplot.show()\n</pre> plot = results.plot_pr_curves() plot.show() <pre></pre> <pre></pre> In\u00a0[35]: Copied! <pre>plot = results.plot_confusion_matrix(classes=[\"Dog\", \"Cat\"])\nplot.show(height=512)\n</pre> plot = results.plot_confusion_matrix(classes=[\"Dog\", \"Cat\"]) plot.show(height=512) <pre>FigureWidget({\n    'data': [{'mode': 'markers',\n              'opacity': 0.1,\n              'type': 'scatter',\u2026</pre> <p>Note that, since we decided to evaluate with <code>classwise=False</code>, the off-diagonal elements of the confusion matrix are populated with instances where the model prediction was matched with a ground truth of a different class.</p> In\u00a0[36]: Copied! <pre>session.view=eval_view\n</pre> session.view=eval_view Activate In\u00a0[37]: Copied! <pre>session.freeze()\n</pre> session.freeze() In\u00a0[38]: Copied! <pre>session.show()\n</pre> session.show() Activate In\u00a0[39]: Copied! <pre># If you are in a Jupyter notebook, attach plot to session\nsession.plots.attach(plot)\n</pre> # If you are in a Jupyter notebook, attach plot to session session.plots.attach(plot) In\u00a0[40]: Copied! <pre>plot.connect()\nplot.show()\n</pre> plot.connect() plot.show() <pre></pre> <pre></pre> <p>Thanks to the interactive plotting in FiftyOne, we can attach this plot to our <code>session</code> object so that when you click a cell of the confusion matrix, the session will automatically update to show the relevant samples.</p> <p>For example, if we click the top middle cell, the session will be updated to only show the examples where a dog was predicted as a cat.</p> <p>Note: Interactive plotting is currently only available in Jupyter notebooks, but it will soon be available in all environments!</p> In\u00a0[41]: Copied! <pre>session.freeze()  # screenshot App and attached plots\n</pre> session.freeze()  # screenshot App and attached plots <p>When running evaluate_detections(), specifying an <code>eval_key</code> stores true positive, false positive, and false negative labels on the relevant ground truth and predicted objects. It also stores the ID of the matched ground truth object and the IoU of the match under <code>&lt;eval_key&gt;_id</code> and <code>&lt;eval_key&gt;_iou</code>.</p> <p>The <code>eval_key</code> in this example was set to <code>\"eval\"</code>:</p> In\u00a0[42]: Copied! <pre>print(eval_view.first().predictions.detections[0])\n</pre> print(eval_view.first().predictions.detections[0]) <pre>&lt;Detection: {\n    'id': '63ec56fed3964b0824a4dae9',\n    'attributes': {},\n    'tags': [],\n    'label': 'Cat',\n    'bounding_box': [\n        0.4742983281612396,\n        0.009991496801376343,\n        0.5113846361637115,\n        0.9810408651828766,\n    ],\n    'mask': None,\n    'confidence': 0.8927010297775269,\n    'index': None,\n    'eval_iou': 0.8808952775,\n    'eval_id': '63ec539ad3964b0824a4c724',\n    'eval': 'tp',\n}&gt;\n</pre> <p>You can rerun evaluate_detections() multiple times with different <code>eval_key</code> values to store multiple sets of evaluation runs on a dataset.</p> <p>Previous evaluation runs can easily be loaded, viewed, and deleted at any time, including in future Python sessions.</p> In\u00a0[43]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) Activate <p>We could also construct the same view programmatically:</p> In\u00a0[44]: Copied! <pre>from fiftyone import ViewField as F\n\ndog_cat_view = (\n    dataset\n    .filter_labels(\n        \"predictions\", \n        (F(\"label\").is_in([\"dog\", \"cat\"])) &amp; (F(\"confidence\") &gt; 0.7),\n    )\n    .sort_by(\"eval_tp\", reverse=True)\n)\n</pre> from fiftyone import ViewField as F  dog_cat_view = (     dataset     .filter_labels(         \"predictions\",          (F(\"label\").is_in([\"dog\", \"cat\"])) &amp; (F(\"confidence\") &gt; 0.7),     )     .sort_by(\"eval_tp\", reverse=True) ) In\u00a0[45]: Copied! <pre>print(dog_cat_view)\n</pre> print(dog_cat_view) <pre>Dataset:     open-images-cat-dog\nMedia type:  image\nNum samples: 412\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    positive_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n    negative_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n    detections:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\nView stages:\n    1. FilterLabels(field='predictions', filter={'$and': [{...}, {...}]}, only_matches=True, trajectories=False)\n    2. SortBy(field_or_expr='eval_tp', reverse=True)\n</pre> In\u00a0[46]: Copied! <pre>session.view = dog_cat_view\n</pre> session.view = dog_cat_view Activate <p>These views are easy to create but can be incredibly useful to explore and query your dataset and model predictions.</p> <p>For example, we can find all high confidence predictions of \"Dog\" that ended up being false positives.</p> In\u00a0[47]: Copied! <pre>fp_dog_view = dataset.filter_labels(\n    \"predictions\", \n    (F(\"eval\") == \"fp\") &amp; (F(\"confidence\") &gt; 0.9),\n)\n</pre> fp_dog_view = dataset.filter_labels(     \"predictions\",      (F(\"eval\") == \"fp\") &amp; (F(\"confidence\") &gt; 0.9), ) In\u00a0[48]: Copied! <pre>session.view = fp_dog_view\n</pre> session.view = fp_dog_view Activate In\u00a0[49]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>Looking through some of these examples, we can see that when the model confuses dogs for cats roughly the same amount as cats for dogs. Additionally, the model occasionally has issues localizing bounding boxes resulting in unmatched detections due to an IoU lower than 0.5.</p> <p>In the example above, there are two ground truth \"cat\" boxes and one detected \"dog\" box containing both cats. This implies that we should look more closely at our training data to verify that there are no cats mistakenly annotated as dogs and that the boxes are localized properly.</p> <p>The same workflow can be performed through the App using evaluation views. After evaluating detections and storing the results in an <code>eval_key</code>, you can click the following button in the App to open the evaluation view allowing you to explore individual TP/FP/FN patches.</p> In\u00a0[50]: Copied! <pre>session.view = eval_view\n</pre> session.view = eval_view Activate <p>Now let's perform the same evaluation for \"Cat\". The eval view contains the <code>type</code> scalar field which we can use to select only false positives. Then under <code>predictions</code>, we can select only \"Cat\" predictions and slide the confidence up to 0.9.</p> In\u00a0[51]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>The cell below uses compute_visualization() to generate a 2D representation of the objects in the <code>predictions</code> field of our <code>eval_view</code> view.</p> <p>Internally, the method generates deep embeddings for each object patch and then uses UMAP to generate the 2D representation.</p> In\u00a0[52]: Copied! <pre>import fiftyone.brain as fob\n\nresults = fob.compute_visualization(\n    eval_view,\n    patches_field=\"predictions\",\n    brain_key=\"eval_patches\", # provide a brain key to save results to the dataset\n    num_dims=2,\n    method=\"umap\",\n    verbose=True,\n    seed=51,\n)\n</pre> import fiftyone.brain as fob  results = fob.compute_visualization(     eval_view,     patches_field=\"predictions\",     brain_key=\"eval_patches\", # provide a brain key to save results to the dataset     num_dims=2,     method=\"umap\",     verbose=True,     seed=51, ) <pre>Computing patch embeddings...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [1.4m elapsed, 0s remaining, 6.4 samples/s]      \nGenerating visualization...\nUMAP(random_state=51, verbose=True)\nTue Feb 14 22:52:41 2023 Construct fuzzy simplicial set\nTue Feb 14 22:52:41 2023 Finding Nearest Neighbors\nTue Feb 14 22:52:41 2023 Finished Nearest Neighbor Search\nTue Feb 14 22:52:41 2023 Construct embedding\n</pre> <pre>Tue Feb 14 22:52:42 2023 Finished embedding\n</pre> <p>First let's launch a new App instance for this exploration:</p> In\u00a0[53]: Copied! <pre>session.view=eval_view\n</pre> session.view=eval_view Activate <p>Now let's visualize the object embeddings with each point colored by label and scaled by the size of the bounding box:</p> In\u00a0[54]: Copied! <pre># Computes the area of each predicted object\nbbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\nareas = eval_view.values(\"predictions.detections[]\", bbox_area)\n\nplot = results.visualize(labels=\"predictions.detections.label\", sizes=areas)\nplot.show()\n</pre> # Computes the area of each predicted object bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3] areas = eval_view.values(\"predictions.detections[]\", bbox_area)  plot = results.visualize(labels=\"predictions.detections.label\", sizes=areas) plot.show() <pre></pre> <pre></pre> <pre>Executing took 0.165 seconds\n</pre> <p>Note: These plots are currently only interactive in Jupyter notebooks. A future release will provide interactive plots in all environments.</p> In\u00a0[55]: Copied! <pre># If you are in a Jupyter notebook, attach plot to session\nsession.plots.attach(plot)\n</pre> # If you are in a Jupyter notebook, attach plot to session session.plots.attach(plot) <p>If you're working in a Jupyter notebook, click the lasso tool on the plot to select a region of points that you want to visualize in the App.</p> <p>You can clearly see the cat detections delinated from the dog detections.</p> <p>Now try hiding the \"Dog\" points by clicking on the corresponding legend entry in the upper right so that you see only the \"Cat\" points. You can then lasso the cluster of \"Cat\" points that reside in the \"Dog\" cluster. These points are the false positives that the model predicted as \"Cat\" but were in fact dogs!</p> <p>This kind of visiualization can be invaluable for a multitude of reasons, particularly for a dataset like Open Images that contains machine-generated labels. Visualizing and interactively exploring embeddings lets you quickly spot check which labels may need to be reviewed by human annotators.</p> In\u00a0[56]: Copied! <pre>session.freeze()\n</pre> session.freeze() In\u00a0[57]: Copied! <pre>import fiftyone.brain as fob\n\n# Indexes the images in the dataset by visual similarity\nfob.compute_similarity(dataset, brain_key=\"similarity\")\n</pre> import fiftyone.brain as fob  # Indexes the images in the dataset by visual similarity fob.compute_similarity(dataset, brain_key=\"similarity\") <pre>Computing embeddings...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [1.8m elapsed, 0s remaining, 4.3 samples/s]      \n</pre> Out[57]: <pre>&lt;fiftyone.brain.similarity.SimilarityResults at 0x72755b4870&gt;</pre> <p>Under the hood, deep embeddings are again being used to generate the index. By default, a general purpose model packaged with FiftyOne is used, but you can also provide your own embeddings via the optional <code>embeddings</code> argument.</p> <p>Once similarity has been computed, we can sort the samples in the dataset based on their similarity to selected sample(s) of interest. This can be done either (a) programmatically via the sort_by_similarity() view stage, or (b) in the App by clicking the sort by similarity button as shown below.</p> In\u00a0[58]: Copied! <pre>session.view = dataset.view()\n</pre> session.view = dataset.view() Activate In\u00a0[59]: Copied! <pre>session.freeze()\n</pre> session.freeze() In\u00a0[60]: Copied! <pre># Download some images that contain cattle from Open Images\ndataset = foz.load_zoo_dataset(\n    \"open-images-v7\", \n    split=\"test\", \n    label_types=[\"detections\"], \n    classes=[\"Cattle\"],\n    max_samples=200,\n    seed=51,\n    shuffle=True,\n    dataset_name=\"open-images-cattle\",\n)\n</pre> # Download some images that contain cattle from Open Images dataset = foz.load_zoo_dataset(     \"open-images-v7\",      split=\"test\",      label_types=[\"detections\"],      classes=[\"Cattle\"],     max_samples=200,     seed=51,     shuffle=True,     dataset_name=\"open-images-cattle\", ) <pre>Downloading split 'test' to 'datasets/open-images-v7/test' if necessary\nDownloading 'https://storage.googleapis.com/openimages/2018_04/test/test-images-with-rotation.csv' to 'datasets/open-images-v7/test/metadata/image_ids.csv'\nDownloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to 'datasets/open-images-v7/test/metadata/classes.csv'\nDownloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to '/var/folders/8f/wbp6tz9j19z4nff5zt3d1_k80000gn/T/tmpxgkd59dc/metadata/hierarchy.json'\nDownloading 'https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv' to 'datasets/open-images-v7/test/labels/detections.csv'\nDownloading 200 images\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [6.9s elapsed, 0s remaining, 28.7 files/s]       \nDataset info written to 'datasets/open-images-v7/info.json'\nLoading 'open-images-v7' split 'test'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [1.0s elapsed, 0s remaining, 199.2 samples/s]         \nDataset 'open-images-cattle' created\n</pre> In\u00a0[61]: Copied! <pre>from fiftyone import ViewField as F\n\n# Create a view that only contains cattle detections\ncattle_view = dataset.filter_labels(\"ground_truth\", F(\"label\") == \"Cattle\")\n</pre> from fiftyone import ViewField as F  # Create a view that only contains cattle detections cattle_view = dataset.filter_labels(\"ground_truth\", F(\"label\") == \"Cattle\") In\u00a0[62]: Copied! <pre>session.view=cattle_view\n</pre> session.view=cattle_view Activate In\u00a0[63]: Copied! <pre>session.freeze()\n</pre> session.freeze() In\u00a0[64]: Copied! <pre>import fiftyone.brain as fob\n\n# Generate a 2D representation of the cattle objects\nresults = fob.compute_visualization(\n    cattle_view,\n    patches_field=\"ground_truth\",\n    num_dims=2,\n    method=\"umap\",\n    verbose=True,\n    seed=51,\n)\n</pre> import fiftyone.brain as fob  # Generate a 2D representation of the cattle objects results = fob.compute_visualization(     cattle_view,     patches_field=\"ground_truth\",     num_dims=2,     method=\"umap\",     verbose=True,     seed=51, ) <pre>Computing patch embeddings...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [50.2s elapsed, 0s remaining, 4.4 samples/s]      \nGenerating visualization...\nUMAP(random_state=51, verbose=True)\nTue Feb 14 23:01:46 2023 Construct fuzzy simplicial set\nTue Feb 14 23:01:46 2023 Finding Nearest Neighbors\nTue Feb 14 23:01:47 2023 Finished Nearest Neighbor Search\nTue Feb 14 23:01:47 2023 Construct embedding\n</pre> <pre>Tue Feb 14 23:01:47 2023 Finished embedding\n</pre> In\u00a0[65]: Copied! <pre>import fiftyone.brain as fob\n\n# Generate a 2D representation of the cattle objects\ncattle_patches_view=cattle_view.to_patches(field=\"ground_truth\")\npatches_results = fob.compute_visualization(\n    cattle_patches_view,\n    patches_field=\"ground_truth\",\n    brain_key=\"cattle_patches_gt\",\n    num_dims=2,\n    method=\"umap\",\n    verbose=True,\n    seed=51,\n)\n</pre> import fiftyone.brain as fob  # Generate a 2D representation of the cattle objects cattle_patches_view=cattle_view.to_patches(field=\"ground_truth\") patches_results = fob.compute_visualization(     cattle_patches_view,     patches_field=\"ground_truth\",     brain_key=\"cattle_patches_gt\",     num_dims=2,     method=\"umap\",     verbose=True,     seed=51, ) <pre>Computing patch embeddings...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 413/413 [52.7s elapsed, 0s remaining, 9.2 samples/s]      \nGenerating visualization...\nUMAP(random_state=51, verbose=True)\nWed Feb 15 00:33:37 2023 Construct fuzzy simplicial set\nWed Feb 15 00:33:37 2023 Finding Nearest Neighbors\nWed Feb 15 00:33:37 2023 Finished Nearest Neighbor Search\nWed Feb 15 00:33:37 2023 Construct embedding\n</pre> <pre>Wed Feb 15 00:33:38 2023 Finished embedding\n</pre> In\u00a0[66]: Copied! <pre>patches_view=dataset.to_patches(field=\"ground_truth\")\ngt_patches_results = fob.compute_visualization(\n    patches_view,\n    patches_field=\"ground_truth\",\n    brain_key=\"patches_gt\",\n    num_dims=2,\n    method=\"umap\",\n    verbose=True,\n    seed=51,\n)\n</pre> patches_view=dataset.to_patches(field=\"ground_truth\") gt_patches_results = fob.compute_visualization(     patches_view,     patches_field=\"ground_truth\",     brain_key=\"patches_gt\",     num_dims=2,     method=\"umap\",     verbose=True,     seed=51, ) <pre>Computing patch embeddings...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1696/1696 [3.3m elapsed, 0s remaining, 7.1 samples/s]      \nGenerating visualization...\nUMAP(random_state=51, verbose=True)\nWed Feb 15 01:00:42 2023 Construct fuzzy simplicial set\nWed Feb 15 01:00:43 2023 Finding Nearest Neighbors\nWed Feb 15 01:00:43 2023 Finished Nearest Neighbor Search\nWed Feb 15 01:00:43 2023 Construct embedding\n</pre> <pre>Wed Feb 15 01:00:45 2023 Finished embedding\n</pre> <p>The snippet below visualizes the cattle instances that we downloaded in a 2D space, with each point scaled by the size of the bounding box:</p> In\u00a0[67]: Copied! <pre># Computes the area of each cattle detection in the view\n# Bounding box coordinates are in the format: [top-left-x, top-left-y, width, height]\nbbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\nareas = cattle_patches_view.values(\"ground_truth\", bbox_area) #cattle_view.values(\"ground_truth.detections[]\", bbox_area)\n</pre> # Computes the area of each cattle detection in the view # Bounding box coordinates are in the format: [top-left-x, top-left-y, width, height] bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3] areas = cattle_patches_view.values(\"ground_truth\", bbox_area) #cattle_view.values(\"ground_truth.detections[]\", bbox_area)  In\u00a0[68]: Copied! <pre>plot = patches_results.visualize(labels=\"ground_truth.label\", sizes=areas)\n</pre> plot = patches_results.visualize(labels=\"ground_truth.label\", sizes=areas) <pre>FigureWidget({\n    'data': [{'customdata': array(['63ec8290d3964b0824a4f481', '63ec8290d3964b0824a4f485',\n    \u2026</pre> <pre>Executing took 0.136 seconds\n</pre> <p>Note: These plots are currently only interactive in Jupyter notebooks. A future release will provide interactive plots in all environments.</p> In\u00a0[69]: Copied! <pre>session = fo.launch_app(view=cattle_view)\n</pre> session = fo.launch_app(view=cattle_view) Activate In\u00a0[70]: Copied! <pre># If you are in a Jupyter notebook, attach plot to session\nsession.plots.attach(plot)\nplot.connect()\n</pre> # If you are in a Jupyter notebook, attach plot to session session.plots.attach(plot) plot.connect() <p>If you're working in a Jupyter notebook, click the lasso tool on the plot and then select a cluster of points so you can visualize them in the App. Once you have identified a set of samples that you want to tag, select them and then click the tag icon in the App and assign an appropriate tag.</p> <p>From here, you could, for example, export the tagged subset and send to human annotators for verification and relabeling.</p> In\u00a0[71]: Copied! <pre>session.freeze()\n</pre> session.freeze()"},{"location":"tutorials/open_images/#downloading-and-evaluating-open-images","title":"Downloading and Evaluating Open Images\u00b6","text":"<p>Downloading Google's Open Images dataset is now easier than ever with the FiftyOne Dataset Zoo! You can load all three splits of Open Images V7, including image-level labels, detections, segmentations, visual relationships, and point labels.</p> <p>FiftyOne also natively supports Open Images-style evaluation, so you can easily evaluate your object detection models and explore the results directly in the library.</p> <p>This walkthrough covers:</p> <ul> <li>Downloading Open Images from the FiftyOne Dataset Zoo</li> <li>Computing predictions using a model from the FiftyOne Model Zoo</li> <li>Performing Open Images-style evaluation in FiftyOne to evaluate a model and compute its mAP</li> <li>Exploring the dataset and evaluation results</li> <li>Visualizing embeddings through interactive plots</li> </ul>"},{"location":"tutorials/open_images/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/open_images/#loading-open-images","title":"Loading Open Images\u00b6","text":"<p>In this section, we'll load various subsets of Open Images from the FiftyOne Dataset Zoo and visualize them using FiftyOne.</p> <p>Let's start by downloading a small sample of 100 randomly chosen images + annotations:</p>"},{"location":"tutorials/open_images/#open-images-style-evaluation","title":"Open Images-style evaluation\u00b6","text":"<p>FiftyOne natively supports Open Images detection evaluation, so you can easily evaluate your object detection models and explore the results directly in FiftyOne.</p> <p>This section produces object detection predictions from a model in the FiftyOne Model Zoo and evaluates them with FiftyOne.</p> <p>Evaluating in FiftyOne is much more flexible than other evaluation APIs, which produce only aggregate performance metrics. For example, FiftyOne evaluation also marks individual ground truth and predicted detections as either true positive, false positive, and false negative, allowing you to explore your model results and easily find failure modes of your model or even annotation mistakes.</p>"},{"location":"tutorials/open_images/#generate-predictions","title":"Generate predictions\u00b6","text":""},{"location":"tutorials/open_images/#running-evaluation","title":"Running evaluation\u00b6","text":""},{"location":"tutorials/open_images/#open-images-challenge-evaluation","title":"Open Images Challenge evaluation\u00b6","text":"<p>FiftyOne's implementation of Open Images-style evaluation matches the reference implementation from the TF Object Detection API, so you can use FiftyOne to compute the official mAP used for the Open Images Challenge.</p> <p>In addition, by using FiftyOne, you'll also gain access to helpful sample- and label-level results like true positives, false positives, and false negatives that can be used to evaluate and analyze your model performance across various slices of your dataset.</p>"},{"location":"tutorials/open_images/#custom-dataset-evaluation","title":"Custom dataset evaluation\u00b6","text":""},{"location":"tutorials/open_images/#analyzing-the-results","title":"Analyzing the results\u00b6","text":"<p>FiftyOne evaluation results also allow you to plot PR curves and interactivley explore confusion matrices:</p>"},{"location":"tutorials/open_images/#advanced-dataset-exploration","title":"Advanced dataset exploration\u00b6","text":"<p>FiftyOne allows you to easily explore any fields that you have on your dataset both through code by creating a view and through the App.</p> <p>For example, let's find all instances of <code>dog</code> or <code>cat</code> predictions with confidence &gt;= 0.7 and sort the matching samples by number of true positives in the sample.</p> <p>We can construct this view via the App, we just need to click \"add stage\", select <code>SortBy</code>, and enter the field <code>eval_tp</code> with <code>reverse=True</code>. Then click the down arrow next to the <code>predictions</code> field we want to filter, type in the labels we want to include (cat and dog), and adjust the confidence slider threshold to 0.7.</p>"},{"location":"tutorials/open_images/#visualize-embeddings","title":"Visualize embeddings\u00b6","text":"<p>FiftyOne is designed to make it easy to explore the labels and attributes that you add to your datasets yourself. However, it can also provide much deeper insights.</p> <p>For example, FiftyOne provides methods for sample uniqueness, label mistakes, and sample hardness. It also provides support for automatically generating and visualizing embeddings, which we'll use next.</p>"},{"location":"tutorials/open_images/#sort-by-similarity","title":"Sort by similarity\u00b6","text":"<p>FiftyOne also supports sorting samples and objects by visual similarity.</p> <p>To use this feature, we first use compute_similarity() to index our dataset (the images, in this case):</p>"},{"location":"tutorials/open_images/#tagging","title":"Tagging\u00b6","text":"<p>Interactive plots and embeddings can power valuable workflows like semi-supervised annotation, removing duplicates, detecting annotation mistakes, and much more.</p> <p>For example, Open Images contains a class for \"Cattle\". However, this class contains animals like cows, sheep, and goats. We can use FiftyOne to visualize clusters of embeddings for \"Cattle\" and use the App's tagging feature to assign fine-grained labels to each type of cattle, which will conveniently form clusters when visualized.</p>"},{"location":"tutorials/open_images/#summary","title":"Summary\u00b6","text":"<p>In this tutorial, we saw how to download, explore, and evaluate using Open Images. In particular, we covered:</p> <ul> <li>Downloading the Open Images dataset from the FiftyOne Dataset Zoo</li> <li>Computing predictions with a model from the FiftyOne Model Zoo</li> <li>Using FiftyOne's native support for Open Images evaluation to evaluate a model and compute its mAP</li> <li>Exploring the dataset and evaluation results</li> <li>Visualizing embeddings through interactive plots</li> </ul> <p>So, what's the takeaway?</p> <p>Open Images is a massive and thoroughly labeled dataset that would make a useful addition to your data lake and model training workflows. And, the easiest way to download and explore Open Images is using FiftyOne!</p> <p>With huge and diverse datasets like Open Images, hands-on evaluation of your model results can be difficult. FiftyOne makes it easy to understand your dataset, find failure modes in your model, and reveal hidden patterns in your data using techniques like embedding visualization.</p>"},{"location":"tutorials/pandas_comparison/","title":"pandas-style queries in FiftyOne","text":"<p>pandas is a Python library for data analysis. The central object in pandas is a <code>DataFrame</code>, which is a two-dimensional labeled data structure that handles tabular data. pandas is optimized for storing, manipulating, and analyzing tabular data, making it useful for a wide variety of data science, data engineering, and machine learning tasks.</p> <p>FiftyOne, is an open-source Python library for building high-quality datasets and computer vision models. The central object in FiftyOne is the <code>Dataset</code>, which allows for efficient handling of datasets consisting of images, videos, geospatial, or 3D data, as well as the corresponding metadata and labels associated with the media (which are often more complex than what can be represented in a two-dimensional data structure).</p> <p>While they apply to different types of data, the pandas <code>DataFrame</code> and FiftyOne <code>Dataset</code> classes share many similar functionalities. In this overview, we'll present a side-by-side comparison of common operations in the two libraries.</p> <p>If you're already a pandas power user, then you'll be a FiftyOne power user too after running through this tutorial!</p> <p>The first thing to do is to install FiftyOne:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>Then we will import pandas and FiftyOne:</p> In\u00a0[2]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F  # For handling expressions in matching and filtering\n</pre> import fiftyone as fo import fiftyone.zoo as foz from fiftyone import ViewField as F  # For handling expressions in matching and filtering In\u00a0[3]: Copied! <pre>import numpy as np\nimport pandas as pd\n</pre> import numpy as np import pandas as pd <p>In this tutorial, we will download example data for illustrative purposes. Before doing so, we demonstrate how to create empty <code>pd.DataFrame</code> and <code>fo.Dataset</code> objects</p> In\u00a0[4]: Copied! <pre>empty_df = pd.DataFrame()\n</pre> empty_df = pd.DataFrame() <p>we can get basic information about the <code>DataFrame</code> using the info property:</p> In\u00a0[5]: Copied! <pre>empty_df.info\n</pre> empty_df.info Out[5]: <pre>&lt;bound method DataFrame.info of Empty DataFrame\nColumns: []\nIndex: []&gt;</pre> <p>We can also give the <code>DataFrame</code> object a name:</p> In\u00a0[6]: Copied! <pre>empty_df.name = 'empty_df'\n</pre> empty_df.name = 'empty_df' <p>We can similarly create a <code>Dataset</code> object by calling the FiftyOne core fo.Dataset() method without any arguments:</p> In\u00a0[7]: Copied! <pre>empty_dataset = fo.Dataset()\n</pre> empty_dataset = fo.Dataset() <p>We can get basic info about the <code>Dataset</code> object using <code>print</code>:</p> In\u00a0[8]: Copied! <pre>print(empty_dataset)\n</pre> print(empty_dataset) <pre>Name:        2022.11.18.18.14.41\nMedia type:  None\nNum samples: 0\nPersistent:  False\nTags:        []\nSample fields:\n    id:       fiftyone.core.fields.ObjectIdField\n    filepath: fiftyone.core.fields.StringField\n    tags:     fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n</pre> <p>We can see a few things:</p> <ol> <li>Calling the <code>fo.DataFrame()</code> method without an input name resulted in a name being autogenerated based on the time of creation.</li> <li>Whereas the empty Pandas <code>DataFrame</code> has a (trivial) <code>Index</code>, the initialized FiftyOne <code>Dataset</code> has empty <code>Tags</code> (accessible via <code>dataset.tags</code>), and each entry - called a <code>Sample</code>, has predefined fields, including <code>id</code> and <code>filepath</code>. These are necessary for properly accessing and addressing the samples, as the <code>Dataset</code> stores pointers to the media files, not the media objects themselves.</li> </ol> <p>If we wanted to name an existing <code>Dataset</code>, we could do so in analogous fashion to pandas:</p> In\u00a0[9]: Copied! <pre>empty_dataset.name = \"empty-dataset\"\n</pre> empty_dataset.name = \"empty-dataset\" In\u00a0[10]: Copied! <pre>print(empty_dataset)\n</pre> print(empty_dataset) <pre>Name:        empty-dataset\nMedia type:  None\nNum samples: 0\nPersistent:  False\nTags:        []\nSample fields:\n    id:       fiftyone.core.fields.ObjectIdField\n    filepath: fiftyone.core.fields.StringField\n    tags:     fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n</pre> <p>Alternatively, if we want to initialize the dataset with a name, we can pass a name in:</p> In\u00a0[11]: Copied! <pre>empty_dataset = fo.Dataset('empty-ds')\n</pre> empty_dataset = fo.Dataset('empty-ds') <p>For the rest of this tutorial, we will use the following example data:</p> In\u00a0[12]: Copied! <pre>df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n</pre> df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv') In\u00a0[13]: Copied! <pre>df.info()\n</pre> df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n</pre> In\u00a0[14]: Copied! <pre>df.columns\n</pre> df.columns Out[14]: <pre>Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n       'species'],\n      dtype='object')</pre> In\u00a0[\u00a0]: Copied! <pre>ds = foz.load_zoo_dataset(\"quickstart\")\n</pre> ds = foz.load_zoo_dataset(\"quickstart\") In\u00a0[16]: Copied! <pre>print(ds)\n</pre> print(ds) <pre>Name:        quickstart\nMedia type:  image\nNum samples: 200\nPersistent:  True\nTags:        []\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\n</pre> <p>To start to get a feel for the data, we might want to inspect a few entries. For instance, we might want to look at the first few entries, or the last few entries. In both pandas and FiftyOne, these can be accomplished with the head() and tail() methods, which have identical syntax.</p> In\u00a0[17]: Copied! <pre>df.head(5)\n</pre> df.head(5) Out[17]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa In\u00a0[18]: Copied! <pre>first_few_samples = ds.head()\n</pre> first_few_samples = ds.head() <p>Running <code>DataFrame.head(n)</code> for instance returns the first $n$ rows of the original <code>DataFrame</code>. Running <code>Dataset.head(5)</code> for instance returns the first five samples of the original <code>Dataset</code>.</p> <p>In a pandas <code>DataFrame</code>, two-dimensional tabular data is represented in rows and columns.</p> <p>Analogously, a FiftyOne <code>Dataset</code> consists of samples and fields. More explicitly:</p> Pandas DataFrame FiftyOne Dataset Row Sample Column Field <p>In pandas, we expect that a fixed set of columns, each representing a different feature, suffices to represent the data. Some rows might not have values for each column, but each row has the same schema. This is ideal for dealing with a wide variety of data, from housing prices to time series predictions.</p> <p>FiftyOne is built for dealing with the unstructured data often encountered in computer vision applications. As such, a FiftyOne <code>Dataset</code> does not assume such a uniform schema. In this example, <code>ds</code> let's consider the field <code>predictions</code>. This field consists of a list of <code>Detection</code> objects, each of which has its own label, bounding box, and confidence score. These represent a model's predictions for detected objects in the image corresponding to the sample. Not all images are guaranteed to contain the same number of predicted objects, so it is preferable for samples to be more flexible than the rows in a <code>DataFrame</code>!</p> <p>To get the last $n$ entries (rows or samples), we can use the <code>tail(n)</code> method</p> In\u00a0[19]: Copied! <pre>df.tail(5)\n</pre> df.tail(5) Out[19]: sepal_length sepal_width petal_length petal_width species 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica In\u00a0[20]: Copied! <pre>last_few_samples = ds.tail()\n</pre> last_few_samples = ds.tail() <p>If we only want the first sample in a <code>Dataset</code>, we can use the first() method, which is equivalent to <code>ds.head()[0]</code></p> In\u00a0[21]: Copied! <pre>first_sample = ds.first()\n</pre> first_sample = ds.first() <p>Similarly, if we only want the last sample, we can use the last() method, which is equivalent to <code>ds.tail()[0]</code></p> In\u00a0[22]: Copied! <pre>last_sample = ds.last()\n</pre> last_sample = ds.last() <p>In pandas, if we want to get the element at index $j$ in a <code>DataFrame</code>, we can employ the <code>loc[j]</code> or <code>iloc[j]</code> functionality, depending on our usage. For instance,</p> In\u00a0[23]: Copied! <pre>j = 10\n</pre> j = 10 In\u00a0[24]: Copied! <pre>df.loc[j]\n</pre> df.loc[j] Out[24]: <pre>sepal_length       5.4\nsepal_width        3.7\npetal_length       1.5\npetal_width        0.2\nspecies         setosa\nName: 10, dtype: object</pre> <p>In FiftyOne, we can achieve the same functionality of picking out the $j^{th}$ sample by running:</p> In\u00a0[25]: Copied! <pre>sample = ds.skip(j).first()\n</pre> sample = ds.skip(j).first() <p>However, in many cases, one is more interested in extracting samples based on their sample id or filepath. In these cases, the syntactical sugar mirrors pandas: both <code>sample = ds[id]</code> and <code>sample = ds[filepath]</code> achieve the desired result.</p> In\u00a0[26]: Copied! <pre>filepath = sample.filepath\nprint(ds[filepath].id == sample.id)\n</pre> filepath = sample.filepath print(ds[filepath].id == sample.id) <pre>True\n</pre> <p>We can get the number of samples in a <code>fo.Dataset</code> just the same as we would get the number of rows in a <code>pd.DataFrame</code> object - by passing it to Python's <code>len()</code> function.</p> In\u00a0[27]: Copied! <pre>len(df)\n</pre> len(df) Out[27]: <pre>150</pre> In\u00a0[28]: Copied! <pre>len(ds)\n</pre> len(ds) Out[28]: <pre>200</pre> <p>There are $150$ flowers in the Iris dataset, and $200$ images in our FiftyOne Quickstart dataset</p> <p>In pandas, where all rows in a <code>DataFrame</code> share the same columns, we can get the names of the columns with the <code>DataFrame.columns</code> property.</p> In\u00a0[29]: Copied! <pre>df.columns\n</pre> df.columns Out[29]: <pre>Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n       'species'],\n      dtype='object')</pre> <p>In FiftyOne, the core field schema is shared among samples, but the structure within these first-level fields can vary. We can get the field schema by calling the get_field_schema() method.</p> In\u00a0[30]: Copied! <pre>ds.get_field_schema()\n</pre> ds.get_field_schema() Out[30]: <pre>OrderedDict([('id', &lt;fiftyone.core.fields.ObjectIdField at 0x2a0a65a90&gt;),\n             ('filepath', &lt;fiftyone.core.fields.StringField at 0x2a0a5b2b0&gt;),\n             ('tags', &lt;fiftyone.core.fields.ListField at 0x2a0a8c460&gt;),\n             ('metadata',\n              &lt;fiftyone.core.fields.EmbeddedDocumentField at 0x2a0a8c100&gt;),\n             ('ground_truth',\n              &lt;fiftyone.core.fields.EmbeddedDocumentField at 0x2a0a651f0&gt;),\n             ('uniqueness', &lt;fiftyone.core.fields.FloatField at 0x2a0a8cd90&gt;),\n             ('predictions',\n              &lt;fiftyone.core.fields.EmbeddedDocumentField at 0x2a0a8c1f0&gt;),\n             ('eval_tp', &lt;fiftyone.core.fields.IntField at 0x2a0a8cf40&gt;),\n             ('eval_fp', &lt;fiftyone.core.fields.IntField at 0x2a0a8cf70&gt;),\n             ('eval_fn', &lt;fiftyone.core.fields.IntField at 0x2a0a78550&gt;),\n             ('abstractness',\n              &lt;fiftyone.core.fields.FloatField at 0x2a0a78580&gt;),\n             ('new_const_field',\n              &lt;fiftyone.core.fields.IntField at 0x2a0a785b0&gt;),\n             ('computed_field',\n              &lt;fiftyone.core.fields.IntField at 0x2a0a785e0&gt;)])</pre> <p>In video tasks, <code>get_field_schema</code> is replaced by get_frame_field_schema().</p> <p>Some of the field types, such as FloatField (float) and StringField (string) correspond in straightforward fashion to data types in pandas, or in Python more generally. As we will see below, the EmbeddedDocumentField, which does not have a perfect analog in pandas, is part of what gives the FiftyOne <code>Dataset</code> its powerful flexibility for tackling computer vision tasks.</p> <p>If we just want the field names for all samples in the dataset, you can do the following:</p> In\u00a0[31]: Copied! <pre>field_names = list(ds.get_field_schema().keys())\nprint(field_names)\n</pre> field_names = list(ds.get_field_schema().keys()) print(field_names) <pre>['id', 'filepath', 'tags', 'metadata', 'ground_truth', 'uniqueness', 'predictions', 'eval_tp', 'eval_fp', 'eval_fn', 'abstractness', 'new_const_field', 'computed_field']\n</pre> <p>In pandas, the entries in each column or <code>pd.Series</code> object must themselves be objects of the type of one of the numpy data types. Thus, when all of the values in a column are extracted, the resulting list will have depth one:</p> In\u00a0[33]: Copied! <pre>col = \"sepal_length\"\nsepal_lengths = df[col].tolist()\nprint(sepal_lengths[:10])\n</pre> col = \"sepal_length\" sepal_lengths = df[col].tolist() print(sepal_lengths[:10]) <pre>[5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9]\n</pre> <p>FiftyOne supports this functionality as well. For instance, each image in our dataset has a uniqueness score, which is a measure of how unique a given image is in the context of the complete dataset. We can extract these values for each image using the values() method as follows:</p> In\u00a0[34]: Copied! <pre>uniqueness = ds.values(\"uniqueness\")\nprint(uniqueness[:10])\n</pre> uniqueness = ds.values(\"uniqueness\") print(uniqueness[:10]) <pre>[0.8175834390151201, 0.6844698885072961, 0.725267119762334, 0.7164587220038886, 0.6874799405473135, 0.6773349111042449, 0.6948791555330056, 0.6157872732023304, 0.6692531238595459, 0.7257486965960712]\n</pre> <p>Some of the relevant information for computer vision tasks, however, is less structured. In our example dataset, this is the case for both the <code>ground_truth</code> and <code>predictions</code> fields, each of which contains a number of object detections in the embedded <code>detections</code> field. The <code>values</code> method also gives us access to these embedded fields.</p> <p>Let's see this in action by using the <code>values</code> method to pull out the confidence score for each predicted detection:</p> In\u00a0[35]: Copied! <pre>pred_confs = ds.values(\"predictions.detections.confidence\")\n</pre> pred_confs = ds.values(\"predictions.detections.confidence\") In\u00a0[36]: Copied! <pre>print(type(pred_confs))\nprint(len(pred_confs))\nprint(type(pred_confs[0]))\n</pre> print(type(pred_confs)) print(len(pred_confs)) print(type(pred_confs[0])) <pre>&lt;class 'list'&gt;\n200\n&lt;class 'list'&gt;\n</pre> <p>As with <code>values(\"uniqueness\")</code>, we get a list with one result per image. However, now we have a sublist for each image, rather than just a single value. We can peak inside one of these sublists at the confidence scores for each detection:</p> In\u00a0[37]: Copied! <pre>print(pred_confs[0])\n</pre> print(pred_confs[0]) <pre>[0.9750854969024658, 0.759726881980896, 0.6569182276725769, 0.2359301745891571, 0.221974179148674, 0.1965726613998413, 0.18904592096805573, 0.11480894684791565, 0.11089690029621124, 0.0971052274107933, 0.08403241634368896, 0.07699568569660187, 0.058097004890441895, 0.0519101656973362]\n</pre> <p>Let's get the lengths of these sublists and print the first few. In the section on <code>fo.Expression</code>, we will see a more natural (and efficient) way of performing this operation.</p> In\u00a0[38]: Copied! <pre>pred_conf_lens = [len(p) for p in pred_confs]\nprint(pred_conf_lens[:10])\n</pre> pred_conf_lens = [len(p) for p in pred_confs] print(pred_conf_lens[:10]) <pre>[14, 20, 10, 51, 27, 13, 2, 9, 7, 13]\n</pre> <p>We can see that the number of confidence scores - and correspondingly the number of predictions - for each image is not fixed. This scenario is fairly typical in object detection tasks, where images can have varying numbers of objects!</p> <p>Suppose we want to make a copy of the original data and modify the copy without the changes propagating back to the original.</p> <p>In pandas, we can do this with the <code>copy</code> method:</p> In\u00a0[39]: Copied! <pre>copy_df = df.copy()\ncopy_df['species'] = 'none'\ndf.head()\n</pre> copy_df = df.copy() copy_df['species'] = 'none' df.head() Out[39]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa <p>In FiftyOne, we can do this with the clone() method:</p> In\u00a0[40]: Copied! <pre>copy_ds = ds.clone()\ncopy_ds.name = 'copy_ds'\nprint(ds.name)\n</pre> copy_ds = ds.clone() copy_ds.name = 'copy_ds' print(ds.name) <pre>quickstart\n</pre> <p>In pandas if we want to get a slice of a <code>DataFrame</code>, we can do so with the notation <code>df[start:end]</code>.</p> In\u00a0[41]: Copied! <pre>start = 10\nend = 14\n</pre> start = 10 end = 14 In\u00a0[42]: Copied! <pre>df[start:end]\n</pre> df[start:end] Out[42]: sepal_length sepal_width petal_length petal_width species 10 5.4 3.7 1.5 0.2 setosa 11 4.8 3.4 1.6 0.2 setosa 12 4.8 3.0 1.4 0.1 setosa 13 4.3 3.0 1.1 0.1 setosa <p>In FiftyOne, a <code>Dataset</code> can be sliced using the same notation:</p> In\u00a0[43]: Copied! <pre>ds[start:end]\n</pre> ds[start:end] Out[43]: <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 4\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\nView stages:\n    1. Skip(skip=10)\n    2. Limit(limit=4)</pre> <p>However, as we can see from the output of the preceding command, this is merely syntactical sugar for the expression:</p> In\u00a0[44]: Copied! <pre>ds.skip(start).limit(end - start)\n</pre> ds.skip(start).limit(end - start) Out[44]: <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 4\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\nView stages:\n    1. Skip(skip=10)\n    2. Limit(limit=4)</pre> <p>which utilizes the skip() and limit() methods.</p> <p>When working with datasets, it is often the case that one might want to select a random set of samples. One typically wants either (a) a fixed number of random samples, or (b) to sample some fraction of the data randomly. We will show how to do both:</p> In\u00a0[45]: Copied! <pre>k = 20\n</pre> k = 20 <p>In pandas, you can use the <code>sample()</code> method, passing in either a number, as in <code>sample(n = k)</code>, or a fraction, as we show below</p> In\u00a0[46]: Copied! <pre>rand_samples_df = df.sample(n=k)\n</pre> rand_samples_df = df.sample(n=k) In\u00a0[47]: Copied! <pre>rand_samples_df.head()\n</pre> rand_samples_df.head() Out[47]: sepal_length sepal_width petal_length petal_width species 101 5.8 2.7 5.1 1.9 virginica 129 7.2 3.0 5.8 1.6 virginica 1 4.9 3.0 1.4 0.2 setosa 79 5.7 2.6 3.5 1.0 versicolor 100 6.3 3.3 6.0 2.5 virginica <p>In FiftyOne, we can use the take() method, to which we can pass in a random seed, or let it seed the random number generator with the time.</p> In\u00a0[48]: Copied! <pre>rand_samples_ds = ds.take(k, seed=123)\n</pre> rand_samples_ds = ds.take(k, seed=123) In\u00a0[49]: Copied! <pre>rand_samples_ds\n</pre> rand_samples_ds Out[49]: <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 20\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\nView stages:\n    1. Take(size=20, seed=123)</pre> <p>With the random utils in FiftyOne, you can also sample flexibly with user-input weighting schemes, but that is beyond the present scope.</p> In\u00a0[50]: Copied! <pre>p = 0.05\n</pre> p = 0.05 In\u00a0[51]: Copied! <pre>df.sample(frac=p).head()\n</pre> df.sample(frac=p).head() Out[51]: sepal_length sepal_width petal_length petal_width species 140 6.7 3.1 5.6 2.4 virginica 14 5.8 4.0 1.2 0.2 setosa 40 5.0 3.5 1.3 0.3 setosa 58 6.6 2.9 4.6 1.3 versicolor 90 5.5 2.6 4.4 1.2 versicolor In\u00a0[52]: Copied! <pre># We need to convert from fraction p to an integer k\nk = int(len(ds) * p)\nds.take(k, seed=123)\n</pre> # We need to convert from fraction p to an integer k k = int(len(ds) * p) ds.take(k, seed=123) Out[52]: <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 10\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\nView stages:\n    1. Take(size=10, seed=123)</pre> <p>In a similar vein to randomly selecting samples, one might want to create a new view in which the entire dataset is shuffled.</p> <p>In pandas, we can accomplish this by randomly sampling all the rows ($\\mathrm{frac}=1$) without replacement:</p> In\u00a0[53]: Copied! <pre>shuffled_df_view = df.sample(frac=1)\n</pre> shuffled_df_view = df.sample(frac=1) <p>In FiftyOne, we can just call the shuffle() method:</p> In\u00a0[54]: Copied! <pre>shuffled_ds_view = ds.shuffle(seed=123)\n</pre> shuffled_ds_view = ds.shuffle(seed=123) <p>It is also quite natural to want to filter out the data based on some condition. For the Iris data, for instance, let's get all of the flowers that have a sepal length greater than seven:</p> In\u00a0[55]: Copied! <pre>sepal_length_thresh = 7\nlarge_sepal_len_view = df[df.sepal_length &gt; sepal_length_thresh]\n</pre> sepal_length_thresh = 7 large_sepal_len_view = df[df.sepal_length &gt; sepal_length_thresh] In\u00a0[56]: Copied! <pre>print(len(large_sepal_len_view))\nprint(large_sepal_len_view.head())\n</pre> print(len(large_sepal_len_view)) print(large_sepal_len_view.head()) <pre>12\n     sepal_length  sepal_width  petal_length  petal_width    species\n102           7.1          3.0           5.9          2.1  virginica\n105           7.6          3.0           6.6          2.1  virginica\n107           7.3          2.9           6.3          1.8  virginica\n109           7.2          3.6           6.1          2.5  virginica\n117           7.7          3.8           6.7          2.2  virginica\n</pre> <p>In FiftyOne, we can perform an analogous filtering operation on the quickstart images, using the match() method and the ViewField to select all images that have a \"uniqueness\" score above some threshold:</p> In\u00a0[57]: Copied! <pre>unique_thresh = 0.75\nunique_view = ds.match(F(\"uniqueness\") &gt; unique_thresh)\nprint(unique_view)\nprint(\"values: \", unique_view.values(\"uniqueness\"))\n</pre> unique_thresh = 0.75 unique_view = ds.match(F(\"uniqueness\") &gt; unique_thresh) print(unique_view) print(\"values: \", unique_view.values(\"uniqueness\")) <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 8\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\nView stages:\n    1. Match(filter={'$expr': {'$gt': [...]}})\nvalues:  [0.8175834390151201, 1.0, 0.922046961894074, 0.799848556973409, 0.7806850524560267, 0.7950646615140298, 0.7505336395700778, 0.7530639609974709]\n</pre> <p>However, in FiftyOne, given the potentially nested structure of the data in a <code>Dataset</code>, we can perform far more complex filtering operations using the same machinery, combined with the filter() method. Crucially, these matching and filtering operations apply equally well to embedded fields.</p> <p>As an example, let's say we want to filter for all images in our dataset that had at least one object prediction with very high confidence. In this case, the confidence score is an embedded field within the predicted detections for each image. Thus, we can create a filter on confidence scores, and then apply this filter to the embedded <code>detections</code> field within <code>predictions</code>:</p> In\u00a0[58]: Copied! <pre>high_conf_filter = F(\"confidence\") &gt; 0.995\n\nhigh_conf_view = ds.match(\n    F(\"predictions.detections\").filter(high_conf_filter).length() &gt; 0\n)\n</pre> high_conf_filter = F(\"confidence\") &gt; 0.995  high_conf_view = ds.match(     F(\"predictions.detections\").filter(high_conf_filter).length() &gt; 0 ) In\u00a0[59]: Copied! <pre>high_conf_view\n</pre> high_conf_view Out[59]: <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 116\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\nView stages:\n    1. Match(filter={'$expr': {'$gt': [...]}})</pre> <p>For video tasks, the method match_frames() allows one to perform filtering on the frames of a video collection.</p> <p>We explore this filtering and matching machinery a little more in the section on expressions, but a comprehensive discussion will be the subject of an upcoming tutorial.</p> <p>We might also want to sort by certain properties. Let's see how that is done in pandas and FiftyOne.</p> <p>In pandas, we use the <code>sort_values</code> method.</p> <p>Suppose that we want to sort by petal length. We can do this as follows:</p> In\u00a0[60]: Copied! <pre>petal_length_view = df.sort_values(by=\"petal_length\", ascending=False)\n</pre> petal_length_view = df.sort_values(by=\"petal_length\", ascending=False) In\u00a0[61]: Copied! <pre>petal_length_view.head()\n</pre> petal_length_view.head() Out[61]: sepal_length sepal_width petal_length petal_width species 118 7.7 2.6 6.9 2.3 virginica 122 7.7 2.8 6.7 2.0 virginica 117 7.7 3.8 6.7 2.2 virginica 105 7.6 3.0 6.6 2.1 virginica 131 7.9 3.8 6.4 2.0 virginica <p>In FiftyOne, we use the sort_by() method. Let's sort the samples by the number of \"ground truth\" objects in the sample images:</p> In\u00a0[62]: Copied! <pre>field = \"ground_truth.detections\"\nview = ds.sort_by(F(field).length(), reverse=True)\n</pre> field = \"ground_truth.detections\" view = ds.sort_by(F(field).length(), reverse=True) In\u00a0[63]: Copied! <pre>print(len(view.first().ground_truth.detections))  # 39\nprint(len(view.last().ground_truth.detections))  # 0\n</pre> print(len(view.first().ground_truth.detections))  # 39 print(len(view.last().ground_truth.detections))  # 0 <pre>39\n0\n</pre> <p>Now we can see that the most crowded image has $39$ objects, while the least crowded image is actually empty!</p> <p>If we are resource-constrained, we can delete old <code>DataFrame</code> or <code>Dataset</code> objects so that they no longer occupy memory.</p> <p>In pandas we do this using the <code>del</code> command and the garbage collector utility. To delete the <code>petal_length_view</code> view, we can do the following:</p> In\u00a0[64]: Copied! <pre>import gc\ndel petal_length_view\ngc.collect()\n</pre> import gc del petal_length_view gc.collect() Out[64]: <pre>16</pre> <p>In FiftyOne, we can use the built-in delete() method:</p> In\u00a0[65]: Copied! <pre>copy_ds.delete()\n</pre> copy_ds.delete() <p>It is also worth mentioning that in FiftyOne, the <code>Dataset</code> is best thought of as an in-memory object. This means that a <code>Dataset</code> is deleted after closing Python (this is true in both Python interpreters and notebooks). If you want to use the dataset in the future, you can avoid this end-of-session deletion by setting the <code>persistent</code> property to <code>True</code>:</p> In\u00a0[66]: Copied! <pre>ds.persistent = True\n</pre> ds.persistent = True <p>Given a set of values for a column or field, it is often desired to compute aggregate quantities over all of these values. pandas <code>DataFrame</code> objects and FiftyOne <code>Dataset</code> objects both come with this functionality built in.</p> <p>The general syntax is that in pandas, aggregations are methods of <code>pd.Series</code> objects, which represent the columns in a <code>DataFrame</code>. In FiftyOne, the aggregations are methods of the <code>Dataset</code> or <code>DatasetView</code> object, which take as input the field to be aggregated over.</p> <p>In both pandas and FiftyOne, the count() method returns the total number of occurrences.</p> <p>In pandas, this counts the number of values in the column, which is by construction equal to the number of rows in the <code>DataFrame</code>:</p> In\u00a0[67]: Copied! <pre>print(df['species'].count())\nprint(len(df))\n</pre> print(df['species'].count()) print(len(df)) <pre>150\n150\n</pre> <p>In FiftyOne, the <code>count</code> method returns the total number of occurrences of a certain field, which is not necessarily the same as the number of samples.</p> In\u00a0[68]: Copied! <pre>num_predictions = ds.count(\"predictions.detections.label\")\nprint(len(ds))\nprint(num_predictions)\n</pre> num_predictions = ds.count(\"predictions.detections.label\") print(len(ds)) print(num_predictions) <pre>200\n5620\n</pre> <p>Both pandas and FiftyOne have the sum() method</p> In\u00a0[69]: Copied! <pre>sum_sepal_lengths = df.sepal_length.sum()\nprint(sum_sepal_lengths)\n</pre> sum_sepal_lengths = df.sepal_length.sum() print(sum_sepal_lengths) <pre>876.5\n</pre> In\u00a0[70]: Copied! <pre>sum_pred_confs = ds.sum(\"predictions.detections.confidence\")\nprint(sum_pred_confs)\n</pre> sum_pred_confs = ds.sum(\"predictions.detections.confidence\") print(sum_pred_confs) <pre>1966.6705134399235\n</pre> <p>In pandas, the <code>unique</code> method returns a list of all unique values in the input <code>pd.Series</code>.</p> In\u00a0[71]: Copied! <pre>df.species.unique()\n</pre> df.species.unique() Out[71]: <pre>array(['setosa', 'versicolor', 'virginica'], dtype=object)</pre> <p>In FiftyOne, the distinct() method reproduces this functionality.</p> In\u00a0[72]: Copied! <pre>rand_samples_ds.distinct(\"predictions.detections.label\")\n</pre> rand_samples_ds.distinct(\"predictions.detections.label\") Out[72]: <pre>['banana',\n 'bed',\n 'bench',\n 'bicycle',\n 'bird',\n 'boat',\n 'book',\n 'bowl',\n 'broccoli',\n 'bus',\n 'cake',\n 'car',\n 'carrot',\n 'cat',\n 'cell phone',\n 'chair',\n 'clock',\n 'couch',\n 'cow',\n 'cup',\n 'dining table',\n 'dog',\n 'elephant',\n 'fire hydrant',\n 'fork',\n 'frisbee',\n 'giraffe',\n 'handbag',\n 'horse',\n 'keyboard',\n 'kite',\n 'knife',\n 'laptop',\n 'person',\n 'pizza',\n 'sandwich',\n 'scissors',\n 'sheep',\n 'skateboard',\n 'skis',\n 'snowboard',\n 'spoon',\n 'sports ball',\n 'stop sign',\n 'surfboard',\n 'tie',\n 'traffic light',\n 'train',\n 'truck',\n 'tv',\n 'umbrella']</pre> <p>In pandas, you compute the minimum and maximum value of a <code>pd.Series</code> separately:</p> In\u00a0[73]: Copied! <pre>min_sepal_len = df.sepal_length.min()\nmax_sepal_len = df.sepal_length.max()\nprint(\"min_sepal_len: {}, max_sepal_len: {}\".format(min_sepal_len, max_sepal_len))\n</pre> min_sepal_len = df.sepal_length.min() max_sepal_len = df.sepal_length.max() print(\"min_sepal_len: {}, max_sepal_len: {}\".format(min_sepal_len, max_sepal_len)) <pre>min_sepal_len: 4.3, max_sepal_len: 7.9\n</pre> <p>When working with a FiftyOne Dataset or DataView, the min and max are returned together in a tuple when the bounds() method is called on a field:</p> In\u00a0[74]: Copied! <pre>(min_pred_conf, max_pred_conf) = ds.bounds(\"predictions.detections.confidence\")\nprint(\"min_pred_conf: {}, max_pred_conf: {}\".format(min_pred_conf, max_pred_conf))\n</pre> (min_pred_conf, max_pred_conf) = ds.bounds(\"predictions.detections.confidence\") print(\"min_pred_conf: {}, max_pred_conf: {}\".format(min_pred_conf, max_pred_conf)) <pre>min_pred_conf: 0.05003104358911514, max_pred_conf: 0.9999035596847534\n</pre> <p>Both pandas <code>DataFrame</code> objects and FiftyOne <code>Dataset</code> objects employ the method mean()</p> In\u00a0[75]: Copied! <pre>mean_sepal_len = df.sepal_length.mean()\nprint(mean_sepal_len)\n</pre> mean_sepal_len = df.sepal_length.mean() print(mean_sepal_len) <pre>5.843333333333334\n</pre> In\u00a0[76]: Copied! <pre>mean_pred_conf = ds.mean(\"predictions.detections.confidence\")\nprint(mean_pred_conf)\n</pre> mean_pred_conf = ds.mean(\"predictions.detections.confidence\") print(mean_pred_conf) <pre>0.34994137249820706\n</pre> <p>Both pandas <code>DataFrame</code> objects and FiftyOne <code>Dataset</code> objects employ the method std():</p> In\u00a0[77]: Copied! <pre>std_sepal_len = df.sepal_length.std()\nprint(std_sepal_len)\n</pre> std_sepal_len = df.sepal_length.std() print(std_sepal_len) <pre>0.828066127977863\n</pre> In\u00a0[78]: Copied! <pre>std_pred_conf = ds.std(\"predictions.detections.confidence\")\nprint(std_pred_conf)\n</pre> std_pred_conf = ds.std(\"predictions.detections.confidence\") print(std_pred_conf) <pre>0.3184061813934825\n</pre> <p>If you don't want just the mean, but instead want the value for a given column or field at arbitrary percentiles in the dataset, you can use the quantiles() method, which takes in a list of percentiles.</p> In\u00a0[79]: Copied! <pre>percentiles = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n</pre> percentiles = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0] In\u00a0[80]: Copied! <pre>sepal_len_quanties = df.sepal_length.quantile(percentiles)\nprint(sepal_len_quanties)\n</pre> sepal_len_quanties = df.sepal_length.quantile(percentiles) print(sepal_len_quanties) <pre>0.0    4.30\n0.2    5.00\n0.4    5.60\n0.6    6.10\n0.8    6.52\n1.0    7.90\nName: sepal_length, dtype: float64\n</pre> In\u00a0[81]: Copied! <pre>pred_conf_quantiles = ds.quantiles(\"predictions.detections.confidence\", percentiles)\nprint(pred_conf_quantiles)\n</pre> pred_conf_quantiles = ds.quantiles(\"predictions.detections.confidence\", percentiles) print(pred_conf_quantiles) <pre>[0.05003104358911514, 0.08101843297481537, 0.14457139372825623, 0.2922309935092926, 0.6890143156051636, 0.9999035596847534]\n</pre> <p>Some aggregations which are native to pandas, such as computing the median, are not native to FiftyOne. In these cases, the canonical way to compute the aggregation is by first extracting the values from the <code>Dataset</code> field, and then using native numpy or scipy functionality.</p> <p>Here we illustrate this procedure for computing the median. If you use the <code>values</code> method on the <code>predictions.detections.confidence</code> field with default arguments, we get a jagged array.</p> In\u00a0[82]: Copied! <pre>pred_confs_jagged = ds.values(\"predictions.detections.confidence\")\nprint([len(pc) for pc in pred_confs_jagged][:10])\nprint(sum([len(pc) for pc in pred_confs_jagged]))\n</pre> pred_confs_jagged = ds.values(\"predictions.detections.confidence\") print([len(pc) for pc in pred_confs_jagged][:10]) print(sum([len(pc) for pc in pred_confs_jagged])) <pre>[14, 20, 10, 51, 27, 13, 2, 9, 7, 13]\n5620\n</pre> <p>However, we can simplify our lives by flattening the result passing in the argument <code>unwind = True</code>:</p> In\u00a0[83]: Copied! <pre>pred_confs_flat = ds.values(\"predictions.detections.confidence\", unwind = True)\nprint(len(pred_confs_flat))\n</pre> pred_confs_flat = ds.values(\"predictions.detections.confidence\", unwind = True) print(len(pred_confs_flat)) <pre>5620\n</pre> <p>And from this we can easily compute the median:</p> In\u00a0[84]: Copied! <pre>pred_confs_median = np.median(pred_confs_flat)\nprint(pred_confs_median)\n</pre> pred_confs_median = np.median(pred_confs_flat) print(pred_confs_median) <pre>0.20251326262950897\n</pre> <p>There are many scenarios in which one might want to add another column/field to a dataset. From a practical standpoint, these come in three distinct flavors.</p> <ol> <li>Add a new column/field with a default (constant) value for each row/sample.</li> <li>Add new column/field defined with external or already computed data.</li> <li>Create new column/field programmatically from other columns/fields.</li> </ol> <p>In this section we show how to efficiently handle each of these cases in pandas and FiftyOne.</p> <p>In pandas, the easiest way to create a new column <code>const_col</code> with constant  value <code>const_val</code> is:</p> In\u00a0[85]: Copied! <pre>df['const_col'] = 'const_val'\ndf.head()\n</pre> df['const_col'] = 'const_val' df.head() Out[85]: sepal_length sepal_width petal_length petal_width species const_col 0 5.1 3.5 1.4 0.2 setosa const_val 1 4.9 3.0 1.4 0.2 setosa const_val 2 4.7 3.2 1.3 0.2 setosa const_val 3 4.6 3.1 1.5 0.2 setosa const_val 4 5.0 3.6 1.4 0.2 setosa const_val <p>which implicitly broadcasts the single value <code>const_val</code> to all rows in the <code>DataFrame</code>.</p> <p>In FiftyOne, the canonical process for efficiently creating and populating a new field involves three steps. (1) a new field is added to the <code>Dataset</code> using the add_sample_field() method with <code>add_sample_field(field_name, ftype)</code>. (2) The field is populated, using either set_field() or set_values(), as we will illustrate below. (3) the <code>Dataset</code> or <code>DatasetView</code> is saved using save(), saving the changes.</p> <p>There is one key distinction in usage between <code>set_field</code> and <code>set_values</code>. Whereas <code>set_values</code> sets the values on the <code>Dataset</code> directly, using <code>set_field</code> creates a new <code>DatasetView</code>, and this <code>DatasetView</code> is what must be saved!</p> <p>Before illustrating these more efficient approaches, it is also worth mentioning that you can also loop through the samples in a <code>Dataset</code> or <code>DatasetView</code> and add or set fields one at a time.</p> In\u00a0[86]: Copied! <pre>for sample in ds.iter_samples(autosave=True):\n    sample[\"new_const_field\"] = 51\n    sample[\"computed_field\"] = len(sample.ground_truth.detections)\n</pre> for sample in ds.iter_samples(autosave=True):     sample[\"new_const_field\"] = 51     sample[\"computed_field\"] = len(sample.ground_truth.detections) <p>However, this is not an efficient approach. It is recommended to use <code>set_field</code> or <code>set_values</code> instead.</p> <p>In the simplest scenario - analogous to the Pandas example above, we can pass a single value into <code>set_field</code> along with the name of the field:</p> In\u00a0[87]: Copied! <pre>ds.add_sample_field(\"const_field\", fo.StringField)\nview = ds.set_field(\"const_field\", \"const_val\")\nview.save()\n\nprint(ds.first().field_names)\nprint(ds.values(\"const_field\")[:10])\n</pre> ds.add_sample_field(\"const_field\", fo.StringField) view = ds.set_field(\"const_field\", \"const_val\") view.save()  print(ds.first().field_names) print(ds.values(\"const_field\")[:10]) <pre>('id', 'filepath', 'tags', 'metadata', 'ground_truth', 'uniqueness', 'predictions', 'eval_tp', 'eval_fp', 'eval_fn', 'abstractness', 'new_const_field', 'computed_field', 'const_field')\n['const_val', 'const_val', 'const_val', 'const_val', 'const_val', 'const_val', 'const_val', 'const_val', 'const_val', 'const_val']\n</pre> <p>As we will see shortly, however, <code>set_field</code> is far more flexible and powerful than this, as a result of FiftyOne's robust matching and filtering capabilities.</p> <p>Starting with pandas, suppose that our data team comes to us and tells us that now they also have the stem length for each flower, and they want us to incorporate that data into our models.</p> <p>For instance, let's say the stem lengths are:</p> In\u00a0[88]: Copied! <pre>stem_lengths = np.random.uniform(5, 10, len(df))\n</pre> stem_lengths = np.random.uniform(5, 10, len(df)) <p>We can add this into our dataset using a similar syntax as above. The only difference is that this time, the assignment is taking in an array (here a numpy array) instead of a single value.</p> In\u00a0[89]: Copied! <pre>df['stem_length'] = stem_lengths\n</pre> df['stem_length'] = stem_lengths In\u00a0[90]: Copied! <pre>df.head()\n</pre> df.head() Out[90]: sepal_length sepal_width petal_length petal_width species const_col stem_length 0 5.1 3.5 1.4 0.2 setosa const_val 9.519895 1 4.9 3.0 1.4 0.2 setosa const_val 9.230470 2 4.7 3.2 1.3 0.2 setosa const_val 8.312255 3 4.6 3.1 1.5 0.2 setosa const_val 6.762648 4 5.0 3.6 1.4 0.2 setosa const_val 8.624046 <p>In FiftyOne, we can do something similar by passing an array of values into <code>set_values</code>.</p> <p>As an example, let's say we have an <code>abstractness</code> score between zero and one for each image.</p> In\u00a0[91]: Copied! <pre>abstractness = np.random.uniform(0, 1, len(ds))\n</pre> abstractness = np.random.uniform(0, 1, len(ds)) In\u00a0[92]: Copied! <pre>ds.set_values(\"abstractness\", abstractness)\nprint(ds.first().field_names)\nprint(ds.values(\"abstractness\")[:10])\n</pre> ds.set_values(\"abstractness\", abstractness) print(ds.first().field_names) print(ds.values(\"abstractness\")[:10]) <pre>('id', 'filepath', 'tags', 'metadata', 'ground_truth', 'uniqueness', 'predictions', 'eval_tp', 'eval_fp', 'eval_fn', 'abstractness', 'new_const_field', 'computed_field', 'const_field')\n[0.18992196548662132, 0.4195423356383746, 0.9782249923275138, 0.3555547463728417, 0.9019379850096877, 0.3647814428112852, 0.3030278060870243, 0.241988161650587, 0.7872455674533378, 0.44774858997738953]\n</pre> <p>Note that when using <code>set_values</code> we are modifying the <code>Dataset</code> directly. Thus, as opposed to <code>set_field</code>, we do not need to preface the method call with <code>add_sample_field</code>, and we do not need to explicitly save the <code>Dataset</code> with <code>save</code> afterwards.</p> <p>Finally, often either in the process of feature engineering or data analysis, you want to generate new columns or fields from existing ones.</p> <p>In pandas, the canonical way of doing this is with the <code>apply</code> method. Suppose we want to create a new feature called \"sepal volume\" derived by taking the product of sepal length and sepal width. With <code>apply</code> we can map row-wise onto the columns:</p> In\u00a0[93]: Copied! <pre>df[\"sepal_volume\"] = df.apply(lambda x: x[\"sepal_length\"]*x[\"sepal_width\"], axis=1)\n</pre> df[\"sepal_volume\"] = df.apply(lambda x: x[\"sepal_length\"]*x[\"sepal_width\"], axis=1) In\u00a0[94]: Copied! <pre>df.head()\n</pre> df.head() Out[94]: sepal_length sepal_width petal_length petal_width species const_col stem_length sepal_volume 0 5.1 3.5 1.4 0.2 setosa const_val 9.519895 17.85 1 4.9 3.0 1.4 0.2 setosa const_val 9.230470 14.70 2 4.7 3.2 1.3 0.2 setosa const_val 8.312255 15.04 3 4.6 3.1 1.5 0.2 setosa const_val 6.762648 14.26 4 5.0 3.6 1.4 0.2 setosa const_val 8.624046 18.00 <p>In FiftyOne, we can perform operations like this by combining <code>set_field</code> with the <code>Viewfield</code>, here loaded as <code>F</code>.</p> <p>To compute the number of predicted object detections for each sample in the <code>Dataset</code> we can write:</p> In\u00a0[95]: Copied! <pre>view = ds.set_field(\n    \"predictions.num_predictions\",\n    F(\"$predictions.detections\").length(),\n)\nview.save()\nprint(ds.first().predictions.field_names)\nprint(ds.values(\"predictions.num_predictions\")[:10])\n</pre> view = ds.set_field(     \"predictions.num_predictions\",     F(\"$predictions.detections\").length(), ) view.save() print(ds.first().predictions.field_names) print(ds.values(\"predictions.num_predictions\")[:10]) <pre>('detections', 'num_predictions')\n[14, 20, 10, 51, 27, 13, 2, 9, 7, 13]\n</pre> <p>The above also highlights that all of the aforementioned operations also work on embedded fields. Note however that as we are not changing the base field_schema, we do not need to call <code>add_sample_field</code>!</p> <p>Sometimes you want to look at a dataset without a certain column/field. More precisely, there are two related things one might want to do.</p> <ol> <li>Create a new view of the dataset without specific column/field, or</li> <li>Delete specific column/field from the original dataset.</li> </ol> <p>Here, we show how to do both of these in Pandas and FiftyOne.</p> <p>In pandas, you can create a view without specific columns using the <code>drop</code> method:</p> In\u00a0[96]: Copied! <pre>df.head()\n</pre> df.head() Out[96]: sepal_length sepal_width petal_length petal_width species const_col stem_length sepal_volume 0 5.1 3.5 1.4 0.2 setosa const_val 9.519895 17.85 1 4.9 3.0 1.4 0.2 setosa const_val 9.230470 14.70 2 4.7 3.2 1.3 0.2 setosa const_val 8.312255 15.04 3 4.6 3.1 1.5 0.2 setosa const_val 6.762648 14.26 4 5.0 3.6 1.4 0.2 setosa const_val 8.624046 18.00 In\u00a0[97]: Copied! <pre>no_const_view = df.drop([\"const_col\"], axis=1)\n# equvalent to df.drop(columns=[\"const\"])\n\nno_const_view.head()\n</pre> no_const_view = df.drop([\"const_col\"], axis=1) # equvalent to df.drop(columns=[\"const\"])  no_const_view.head() Out[97]: sepal_length sepal_width petal_length petal_width species stem_length sepal_volume 0 5.1 3.5 1.4 0.2 setosa 9.519895 17.85 1 4.9 3.0 1.4 0.2 setosa 9.230470 14.70 2 4.7 3.2 1.3 0.2 setosa 8.312255 15.04 3 4.6 3.1 1.5 0.2 setosa 6.762648 14.26 4 5.0 3.6 1.4 0.2 setosa 8.624046 18.00 <p>If one wants to delete the column from the original <code>DataFrame</code>, one does so by assigning the variable for the original <code>DataFrame</code> to the dropped view:</p> In\u00a0[98]: Copied! <pre>df = df.drop([\"const_col\"], axis=1)\ndf.head()\n</pre> df = df.drop([\"const_col\"], axis=1) df.head() Out[98]: sepal_length sepal_width petal_length petal_width species stem_length sepal_volume 0 5.1 3.5 1.4 0.2 setosa 9.519895 17.85 1 4.9 3.0 1.4 0.2 setosa 9.230470 14.70 2 4.7 3.2 1.3 0.2 setosa 8.312255 15.04 3 4.6 3.1 1.5 0.2 setosa 6.762648 14.26 4 5.0 3.6 1.4 0.2 setosa 8.624046 18.00 <p>In FiftyOne, you can create a <code>ViewStage</code> without a particular field using the exclude_fields() method:</p> In\u00a0[99]: Copied! <pre>no_predictions_view = ds.exclude_fields(\"predictions\")\nprint(no_predictions_view.first().field_names)\n</pre> no_predictions_view = ds.exclude_fields(\"predictions\") print(no_predictions_view.first().field_names) <pre>('id', 'filepath', 'tags', 'metadata', 'ground_truth', 'uniqueness', 'eval_tp', 'eval_fp', 'eval_fn', 'abstractness', 'new_const_field', 'computed_field', 'const_field')\n</pre> <p>Alternatively, you can delete a field from the <code>Dataset</code> using delete_sample_field().</p> In\u00a0[100]: Copied! <pre>ds.delete_sample_field(\"const_field\")\nprint(ds.first().field_names)\n</pre> ds.delete_sample_field(\"const_field\") print(ds.first().field_names) <pre>('id', 'filepath', 'tags', 'metadata', 'ground_truth', 'uniqueness', 'predictions', 'eval_tp', 'eval_fp', 'eval_fn', 'abstractness', 'new_const_field', 'computed_field')\n</pre> <p>Both the <code>exclude_field</code> and <code>delete_sample_field</code> methods also work with embedded fields:</p> In\u00a0[101]: Copied! <pre>ds.delete_sample_field(\"predictions.num_predictions\")\nprint(ds.first().predictions.field_names)\n</pre> ds.delete_sample_field(\"predictions.num_predictions\") print(ds.first().predictions.field_names) <pre>('detections',)\n</pre> <p>To delete multiple fields at once, you can use the related delete_sample_fields() method.</p> <p>Alternatively, if you only want to create a view with a small subset of columns/fields, it might be easier to specify those directly. As with removing columns, this can be done in a way that creates a new view while preserving the original, or in a way that deletes the columns/fields from the original dataset. We show both approaches below.</p> <p>In pandas, to create a new view with only the \"sepal_length\" and \"sepal_width\" columns, one could write:</p> In\u00a0[102]: Copied! <pre>sepal_df = df[[\"sepal_length\", \"sepal_width\"]]\nsepal_df.head()\n</pre> sepal_df = df[[\"sepal_length\", \"sepal_width\"]] sepal_df.head() Out[102]: sepal_length sepal_width 0 5.1 3.5 1 4.9 3.0 2 4.7 3.2 3 4.6 3.1 4 5.0 3.6 <p>In contrast, the following propagates the changes back to the original <code>DataFrame</code>:</p> In\u00a0[103]: Copied! <pre>sepal_df = sepal_df[[\"sepal_length\"]]\nsepal_df.head()\n</pre> sepal_df = sepal_df[[\"sepal_length\"]] sepal_df.head() Out[103]: sepal_length 0 5.1 1 4.9 2 4.7 3 4.6 4 5.0 <p>In FiftyOne, if we want to create a separate view with only specified fields kept, we should first clone the original dataset and then apply the select_fields() method. when we apply the keep_fields() method following application of <code>select_fields</code>, the changes propagate from the <code>DatasetView</code> back to the underlying <code>Dataset</code>.</p> <p>Let's create two clones of our base <code>Dataset</code> to showcase this distinction.</p> In\u00a0[104]: Copied! <pre>ds_clone1 = ds.clone()\nds_clone2 = ds.clone()\n</pre> ds_clone1 = ds.clone() ds_clone2 = ds.clone() <p>For both of these clones, let's create views which select only the <code>ground_truth</code> field:</p> In\u00a0[105]: Copied! <pre>clone1_view = ds_clone1.select_fields(\"ground_truth\")\nclone2_view = ds_clone2.select_fields(\"ground_truth\")\nprint(clone1_view.first().field_names)\nprint(clone2_view.first().field_names)\n</pre> clone1_view = ds_clone1.select_fields(\"ground_truth\") clone2_view = ds_clone2.select_fields(\"ground_truth\") print(clone1_view.first().field_names) print(clone2_view.first().field_names) <pre>('id', 'filepath', 'tags', 'metadata', 'ground_truth')\n('id', 'filepath', 'tags', 'metadata', 'ground_truth')\n</pre> <p>The <code>id</code>, <code>filepath</code>, <code>tags</code>, and <code>metadata</code> are by default preserved, even when not passed in to <code>select_fields</code>. Aside from these and <code>ground_truth</code>, all other fields have been omitted from view. Now let's only apply <code>keep_fields</code> on the first clone, and see what changes propagate back.</p> In\u00a0[106]: Copied! <pre>clone1_view.keep_fields()\n</pre> clone1_view.keep_fields() In\u00a0[107]: Copied! <pre>print(ds_clone1.first().field_names)\nprint(ds_clone2.first().field_names)\n</pre> print(ds_clone1.first().field_names) print(ds_clone2.first().field_names) <pre>('id', 'filepath', 'tags', 'metadata', 'ground_truth')\n('id', 'filepath', 'tags', 'metadata', 'ground_truth', 'uniqueness', 'predictions', 'eval_tp', 'eval_fp', 'eval_fn', 'abstractness', 'new_const_field', 'computed_field')\n</pre> <p>As we can see, the changes only propagated back to the dataset (in this case <code>ds_clone1</code>) when we applied <code>keep_fields</code>.</p> <p>Finally, we note that when dealing with video datasets, the methods <code>exclude_fields</code> and <code>select_fields</code> have analogous methods for frames - exclude_frames() and select_frames().</p> <p>Suppose we have two datasets we want to combine or concatenate.</p> <p>In both pandas and FiftyOne, we can concatenate them using the <code>concat</code> method.</p> <p>In pandas, we can combine two <code>DataFrame</code> objects:</p> In\u00a0[108]: Copied! <pre>df1 = df[df.species == 'setosa']\ndf2 = df[df.species == 'virginica']\nconcat_df = pd.concat([df1, df2])\nprint(len(concat_df))\n</pre> df1 = df[df.species == 'setosa'] df2 = df[df.species == 'virginica'] concat_df = pd.concat([df1, df2]) print(len(concat_df)) <pre>100\n</pre> <p>In FiftyOne, we can use the concat() method to combine views from the same dataset:</p> In\u00a0[109]: Copied! <pre>view1 = ds.match(F(\"uniqueness\") &lt; 0.2)\nview2 = ds.match(F(\"uniqueness\") &gt; 0.7)\n</pre> view1 = ds.match(F(\"uniqueness\") &lt; 0.2) view2 = ds.match(F(\"uniqueness\") &gt; 0.7) In\u00a0[110]: Copied! <pre>print(len(view1))\nprint(len(view2))\n</pre> print(len(view1)) print(len(view2)) <pre>19\n17\n</pre> In\u00a0[111]: Copied! <pre>concat_view = view1.concat(view2)\nprint(len(view1) + len(view2))\nprint(len(concat_view))\n</pre> concat_view = view1.concat(view2) print(len(view1) + len(view2)) print(len(concat_view)) <pre>36\n36\n</pre> <p>The slightly more complicated operation of concatenating <code>Dataset</code> objects <code>ds1</code> and <code>ds2</code> (as opposed to <code>DatasetView</code> objects) can be achieved using merge_samples(), i.e., <code>ds1.merge_samples(ds2)</code>.</p> <p>Often times, we just want to enhance a dataset by adding in one sample at a time.</p> <p>In pandas, the fastest way to do this is to use the same <code>concat</code> method as above. If the row data is in a dictionary format, we convert it to its own <code>DataFrame</code> first.</p> In\u00a0[112]: Copied! <pre>len(df1)\n</pre> len(df1) Out[112]: <pre>50</pre> In\u00a0[113]: Copied! <pre>single_row = df2.iloc[0]\ndf1_plus = pd.concat([df1, pd.DataFrame([single_row])], axis=1)\nprint(len(df1_plus))\n</pre> single_row = df2.iloc[0] df1_plus = pd.concat([df1, pd.DataFrame([single_row])], axis=1) print(len(df1_plus)) <pre>51\n</pre> <p>In FiftyOne, we can use the add_sample() method. Notice that this is an in-place operation, and no assignment is needed. Also note that this does not work for views - a sample can only be added to a <code>Dataset</code>, not to a <code>Dataview</code>. As such, we first clone the view to turn it into its own <code>Dataset</code>.</p> In\u00a0[114]: Copied! <pre>single_sample = view2.first()\nview1_plus = view1.clone()\nprint(len(view1_plus))\nview1_plus.add_sample(single_sample)\nprint(len(view1_plus))\n</pre> single_sample = view2.first() view1_plus = view1.clone() print(len(view1_plus)) view1_plus.add_sample(single_sample) print(len(view1_plus)) <pre>19\n20\n</pre> <p>We can also add a collection of samples to a dataset using the add_samples() method, which takes as input a list of <code>fo.Sample</code> objects.</p> In\u00a0[115]: Copied! <pre>print(len(view1_plus))\nview1_plus.add_samples(view2.skip(1).head(3))\nprint(len(view1_plus))\n</pre> print(len(view1_plus)) view1_plus.add_samples(view2.skip(1).head(3)) print(len(view1_plus)) <pre>20\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [35.6ms elapsed, 0s remaining, 84.2 samples/s]     \n23\n</pre> <p>The same in-place vs out-of-place considerations for pandas, and <code>Dataset</code> vs <code>DatasetView</code> considerations for FiftyOne apply to rows/samples as applied to columns/fields.</p> <p>In pandas, rows are removed by index using the <code>drop</code> method.</p> In\u00a0[116]: Copied! <pre>### Randomly select a set of rows to remove\nimport random\nrows_to_remove = random.sample(range(len(df)), 10)\n</pre> ### Randomly select a set of rows to remove import random rows_to_remove = random.sample(range(len(df)), 10) <p>To create a new view:</p> In\u00a0[117]: Copied! <pre>sub_df = df.drop(rows_to_remove)\nprint(len(sub_df))\nprint(len(df))\n</pre> sub_df = df.drop(rows_to_remove) print(len(sub_df)) print(len(df)) <pre>140\n150\n</pre> <p>To remove the rows from the original <code>DataFrame</code>:</p> In\u00a0[118]: Copied! <pre>copy_df = df.copy()\ncopy_df = copy_df.drop(rows_to_remove)\nprint(len(copy_df))\n</pre> copy_df = df.copy() copy_df = copy_df.drop(rows_to_remove) print(len(copy_df)) <pre>140\n</pre> <p>In FiftyOne, exclude() creates a view without the specified samples:</p> In\u00a0[119]: Copied! <pre>samples_to_remove = ds.take(10)\n</pre> samples_to_remove = ds.take(10) In\u00a0[120]: Copied! <pre>sub_view = ds.exclude(samples_to_remove)\nprint(len(ds))\nprint(len(sub_view))\nprint(type(sub_view))\n</pre> sub_view = ds.exclude(samples_to_remove) print(len(ds)) print(len(sub_view)) print(type(sub_view)) <pre>200\n190\n&lt;class 'fiftyone.core.view.DatasetView'&gt;\n</pre> <p>On the other hand, delete_samples() is an in-place operation which deletes the samples from the underlying <code>Dataset</code>:</p> In\u00a0[121]: Copied! <pre>sub_ds = ds.clone()\nsub_ds.delete_samples(samples_to_remove)\nprint(len(sub_ds))\n</pre> sub_ds = ds.clone() sub_ds.delete_samples(samples_to_remove) print(len(sub_ds)) <pre>190\n</pre> <p>As with columns/fields, one might want to pick out specific rows/samples. In the section on filtering and expressions, we'll cover more advanced operations. Here we show how to select the data corresponding to a given list of rows/samples.</p> In\u00a0[122]: Copied! <pre>rows_to_keep = list(random.sample(range(len(df)), 80))\n</pre> rows_to_keep = list(random.sample(range(len(df)), 80)) In\u00a0[123]: Copied! <pre>sub_df = df.iloc[rows_to_keep]\nprint(len(sub_df))\n</pre> sub_df = df.iloc[rows_to_keep] print(len(sub_df)) <pre>80\n</pre> In\u00a0[124]: Copied! <pre>sample_ids = ds.values(\"id\")\nids_to_keep = [sample_ids[ind] for ind in rows_to_keep]\nprint(len(ids_to_keep))\nprint(len(ds.select(ids_to_keep)))\n</pre> sample_ids = ds.values(\"id\") ids_to_keep = [sample_ids[ind] for ind in rows_to_keep] print(len(ids_to_keep)) print(len(ds.select(ids_to_keep))) <pre>80\n80\n</pre> <p>In pandas, you can rename columns by passing a dictionary or mapping into the <code>rename()</code> method with the <code>columns</code> argument. This is not an in-place operation:</p> In\u00a0[125]: Copied! <pre>renamed_df = df.rename(columns = {\"sepal_length\": \"sl\", \"sepal_width\": \"sw\"})\nrenamed_df.head()\n</pre> renamed_df = df.rename(columns = {\"sepal_length\": \"sl\", \"sepal_width\": \"sw\"}) renamed_df.head() Out[125]: sl sw petal_length petal_width species stem_length sepal_volume 0 5.1 3.5 1.4 0.2 setosa 9.519895 17.85 1 4.9 3.0 1.4 0.2 setosa 9.230470 14.70 2 4.7 3.2 1.3 0.2 setosa 8.312255 15.04 3 4.6 3.1 1.5 0.2 setosa 6.762648 14.26 4 5.0 3.6 1.4 0.2 setosa 8.624046 18.00 <p>In FiftyOne, you can rename fields using an analogous (but in-place) name mapping, passed in to the rename_sample_fields() method.</p> In\u00a0[126]: Copied! <pre>renamed_ds = ds.clone()\nrenamed_ds.rename_sample_fields({\"ground_truth\": \"gt\", \"predictions\":\"pred\"})\nprint(renamed_ds.first().field_names)\n</pre> renamed_ds = ds.clone() renamed_ds.rename_sample_fields({\"ground_truth\": \"gt\", \"predictions\":\"pred\"}) print(renamed_ds.first().field_names) <pre>('id', 'filepath', 'tags', 'metadata', 'gt', 'uniqueness', 'pred', 'eval_tp', 'eval_fp', 'eval_fn', 'abstractness', 'new_const_field', 'computed_field')\n</pre> <p>Alternatively, if you just want to rename a single field, you can also do so with the rename_sample_field() method as <code>rename_sample_field(old_field_name, new_field_name)</code>:</p> In\u00a0[127]: Copied! <pre>renamed_ds.rename_sample_field(\"gt\", \"gt_new\")\nprint(renamed_ds.first().field_names)\n</pre> renamed_ds.rename_sample_field(\"gt\", \"gt_new\") print(renamed_ds.first().field_names) <pre>('id', 'filepath', 'tags', 'metadata', 'gt_new', 'uniqueness', 'pred', 'eval_tp', 'eval_fp', 'eval_fn', 'abstractness', 'new_const_field', 'computed_field')\n</pre> <p>Both of these methods extend naturally to embedded fields:</p> In\u00a0[128]: Copied! <pre>renamed_ds.first().pred.detections[0].eval_iou\n</pre> renamed_ds.first().pred.detections[0].eval_iou Out[128]: <pre>0.8575063187115628</pre> In\u00a0[129]: Copied! <pre>renamed_ds.rename_sample_field(\"pred.detections.eval_iou\", \"pred.detections.iou\")\nprint(renamed_ds.first().pred.detections[0].field_names)\n</pre> renamed_ds.rename_sample_field(\"pred.detections.eval_iou\", \"pred.detections.iou\") print(renamed_ds.first().pred.detections[0].field_names) <pre>('id', 'attributes', 'tags', 'label', 'bounding_box', 'mask', 'confidence', 'index', 'eval', 'eval_id', 'iou')\n</pre> <p>As introduced above, the <code>filter</code>, and <code>match</code> methods, along with the <code>ViewField</code>, can be remarkably useful in selecting subsets of datasets that satisfy user-defined conditions. In this section, we demonstrate how to combine these components to perform Pandas-style queries.</p> <p>A common theme throughout this section is that while in pandas, expressions (over a given set of rows) can only be applied to the values in the columns, in FiftyOne, expressions can be applied to fields, including embedded fields, or directly to labels or tags! As such, FiftyOne provides match_labels() and match_tags() methods.</p> <p>In both pandas and FiftyOne, the element comparison operators <code>==</code>, <code>&gt;</code>, <code>&lt;</code>, <code>!=</code>, <code>&gt;=</code>, and <code>&lt;=</code> all conform to the same syntax. The following examples show this functionality.</p> In\u00a0[130]: Copied! <pre>setosa_df = df[df.species == \"setosa\"]\nprint(len(setosa_df))\n</pre> setosa_df = df[df.species == \"setosa\"] print(len(setosa_df)) <pre>50\n</pre> In\u00a0[131]: Copied! <pre>ds.match(F(\"filepath\") == '/root/fiftyone/quickstart/data/000880.jpg')\n</pre> ds.match(F(\"filepath\") == '/root/fiftyone/quickstart/data/000880.jpg') Out[131]: <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 0\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\nView stages:\n    1. Match(filter={'$expr': {'$eq': [...]}})</pre> In\u00a0[132]: Copied! <pre>short_sepal_cond = df.sepal_length &lt;= 5\nshort_sepal_df = df[short_sepal_cond]\nshort_sepal_df.head()\n</pre> short_sepal_cond = df.sepal_length &lt;= 5 short_sepal_df = df[short_sepal_cond] short_sepal_df.head() Out[132]: sepal_length sepal_width petal_length petal_width species stem_length sepal_volume 1 4.9 3.0 1.4 0.2 setosa 9.230470 14.70 2 4.7 3.2 1.3 0.2 setosa 8.312255 15.04 3 4.6 3.1 1.5 0.2 setosa 6.762648 14.26 4 5.0 3.6 1.4 0.2 setosa 8.624046 18.00 6 4.6 3.4 1.4 0.3 setosa 5.066091 15.64 In\u00a0[133]: Copied! <pre>non_unique_filter = F(\"uniqueness\") &lt;= 0.2\nnon_unique_view = ds.match(non_unique_filter)\nnon_unique_view\n</pre> non_unique_filter = F(\"uniqueness\") &lt;= 0.2 non_unique_view = ds.match(non_unique_filter) non_unique_view Out[133]: <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 19\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\nView stages:\n    1. Match(filter={'$expr': {'$lte': [...]}})</pre> <p>If we have an expression and we want to find all rows/samples that do not satisfy this expression, we can use the complement operator <code>~</code>. Let's use this to get the complementary rows/samples to those picked out by the expression above:</p> In\u00a0[134]: Copied! <pre>non_short_sepal_df = df[~short_sepal_cond]\nnon_short_sepal_df.head()\n</pre> non_short_sepal_df = df[~short_sepal_cond] non_short_sepal_df.head() Out[134]: sepal_length sepal_width petal_length petal_width species stem_length sepal_volume 0 5.1 3.5 1.4 0.2 setosa 9.519895 17.85 5 5.4 3.9 1.7 0.4 setosa 9.171235 21.06 10 5.4 3.7 1.5 0.2 setosa 8.236024 19.98 14 5.8 4.0 1.2 0.2 setosa 5.914960 23.20 15 5.7 4.4 1.5 0.4 setosa 6.215238 25.08 In\u00a0[135]: Copied! <pre>unique_view = ds.match(~non_unique_filter)\nunique_view\n</pre> unique_view = ds.match(~non_unique_filter) unique_view Out[135]: <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 181\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\nView stages:\n    1. Match(filter={'$expr': {'$not': {...}}})</pre> <p>In pandas and FiftyOne, the logical <code>AND</code> of two conditions can be evaluated with the <code>&amp;</code> operator:</p> In\u00a0[136]: Copied! <pre>pd_cond1 = (df.sepal_volume &lt; 20)\npd_cond2 = (df.species == \"setosa\")\nprint(\"{} rows satisfy condition1\".format(len(df[pd_cond1])))\nprint(\"{} rows satisfy condition2\".format(len(df[pd_cond2])))\nprint(\"{} rows satisfy condition1 AND condition2\".format(len(df[pd_cond1 &amp; pd_cond2])))\n</pre> pd_cond1 = (df.sepal_volume &lt; 20) pd_cond2 = (df.species == \"setosa\") print(\"{} rows satisfy condition1\".format(len(df[pd_cond1]))) print(\"{} rows satisfy condition2\".format(len(df[pd_cond2]))) print(\"{} rows satisfy condition1 AND condition2\".format(len(df[pd_cond1 &amp; pd_cond2]))) <pre>109 rows satisfy condition1\n50 rows satisfy condition2\n43 rows satisfy condition1 AND condition2\n</pre> In\u00a0[137]: Copied! <pre>fo_cond1 = F(\"uniqueness\") &gt; 0.4\nfo_cond2 = F(\"uniqueness\") &lt; 0.55\nprint(\"{} samples satisfy condition1\".format(len(ds.match(fo_cond1))))\nprint(\"{} samples satisfy condition2\".format(len(ds.match(fo_cond2))))\nprint(\"{} samples satisfy condition1 AND condition2\".format(len(ds.match(fo_cond1 &amp; fo_cond2))))\n</pre> fo_cond1 = F(\"uniqueness\") &gt; 0.4 fo_cond2 = F(\"uniqueness\") &lt; 0.55 print(\"{} samples satisfy condition1\".format(len(ds.match(fo_cond1)))) print(\"{} samples satisfy condition2\".format(len(ds.match(fo_cond2)))) print(\"{} samples satisfy condition1 AND condition2\".format(len(ds.match(fo_cond1 &amp; fo_cond2)))) <pre>100 samples satisfy condition1\n109 samples satisfy condition2\n9 samples satisfy condition1 AND condition2\n</pre> <p>Additionally, if we want to evaluate the logical <code>AND</code> of a list of conditions, in FiftyOne we can do so using all():</p> In\u00a0[138]: Copied! <pre>fo_cond3 = F(\"predictions.detections\").length() &gt;= 10\nprint(ds.match(F.all([fo_cond1, fo_cond2, fo_cond3])))\n</pre> fo_cond3 = F(\"predictions.detections\").length() &gt;= 10 print(ds.match(F.all([fo_cond1, fo_cond2, fo_cond3]))) <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 5\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\nView stages:\n    1. Match(filter={'$expr': {'$and': [...]}})\n</pre> <p>In pandas and FiftyOne, the logical <code>OR</code> of two conditions can be evaluated with the <code>|</code> operator:</p> In\u00a0[139]: Copied! <pre>print(\"{} rows satisfy condition1\".format(len(df[pd_cond1])))\nprint(\"{} rows satisfy condition2\".format(len(df[pd_cond2])))\nprint(\"{} rows satisfy condition1 OR condition2\".format(len(df[pd_cond1 | pd_cond2])))\n</pre> print(\"{} rows satisfy condition1\".format(len(df[pd_cond1]))) print(\"{} rows satisfy condition2\".format(len(df[pd_cond2]))) print(\"{} rows satisfy condition1 OR condition2\".format(len(df[pd_cond1 | pd_cond2]))) <pre>109 rows satisfy condition1\n50 rows satisfy condition2\n116 rows satisfy condition1 OR condition2\n</pre> In\u00a0[140]: Copied! <pre>print(\"{} samples satisfy condition1\".format(len(ds.match(fo_cond1))))\nprint(\"{} samples satisfy condition3\".format(len(ds.match(fo_cond3))))\nprint(\"{} samples satisfy condition1 OR condition3\".format(len(ds.match(fo_cond1 | fo_cond3))))\n</pre> print(\"{} samples satisfy condition1\".format(len(ds.match(fo_cond1)))) print(\"{} samples satisfy condition3\".format(len(ds.match(fo_cond3)))) print(\"{} samples satisfy condition1 OR condition3\".format(len(ds.match(fo_cond1 | fo_cond3)))) <pre>100 samples satisfy condition1\n134 samples satisfy condition3\n166 samples satisfy condition1 OR condition3\n</pre> <p>Mirroring our usage of <code>all</code>, in FiftyOne we can use any() to evaluate the logical <code>OR</code> of a list of conditions:</p> In\u00a0[141]: Copied! <pre>print(ds.match(F.any([fo_cond1, fo_cond3])))\n</pre> print(ds.match(F.any([fo_cond1, fo_cond3]))) <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 166\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\nView stages:\n    1. Match(filter={'$expr': {'$or': [...]}})\n</pre> <p>We note that these <code>all</code> and <code>any</code> methods in FiftyOne are distinctly different from the methods with the same names in pandas.</p> <p>In pandas, we can check whether the entries in a column are in a given list of values using the <code>isin</code> method:</p> In\u00a0[142]: Copied! <pre>df.species.isin(['setosa', 'versicolor'])\n</pre> df.species.isin(['setosa', 'versicolor']) Out[142]: <pre>0       True\n1       True\n2       True\n3       True\n4       True\n       ...  \n145    False\n146    False\n147    False\n148    False\n149    False\nName: species, Length: 150, dtype: bool</pre> <p>In FiftyOne, the analogous method is is_in(). We can filter our dataset for only detected animals, for instance, with the following:</p> In\u00a0[143]: Copied! <pre>ANIMALS = [\n    \"bear\", \"bird\", \"cat\", \"cow\", \"dog\", \"elephant\", \"giraffe\",\n    \"horse\", \"sheep\", \"zebra\"\n]\n\nanimal_view = ds.filter_labels(\"predictions\", F(\"label\").is_in(ANIMALS))\nprint(animal_view)\n</pre> ANIMALS = [     \"bear\", \"bird\", \"cat\", \"cow\", \"dog\", \"elephant\", \"giraffe\",     \"horse\", \"sheep\", \"zebra\" ]  animal_view = ds.filter_labels(\"predictions\", F(\"label\").is_in(ANIMALS)) print(animal_view) <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 87\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\nView stages:\n    1. FilterLabels(field='predictions', filter={'$in': ['$$this.label', [...]]}, only_matches=True, trajectories=False)\n</pre> <p>Additionally, when the FiftyOne fields contain lists, we might want to check if these lists are subsets of other lists. We can do this with the is_subset() method:</p> In\u00a0[144]: Copied! <pre>empty_dataset.add_samples(\n    [\n        fo.Sample(\n            filepath=\"image1.jpg\",\n            tags=[\"a\", \"b\", \"a\", \"b\"]\n        )\n    ]\n)\n\nprint(empty_dataset.values(F(\"tags\").is_subset([\"a\", \"b\", \"c\"])))\n</pre> empty_dataset.add_samples(     [         fo.Sample(             filepath=\"image1.jpg\",             tags=[\"a\", \"b\", \"a\", \"b\"]         )     ] )  print(empty_dataset.values(F(\"tags\").is_subset([\"a\", \"b\", \"c\"]))) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [6.3ms elapsed, 0s remaining, 177.5 samples/s] \n[True]\n</pre> <p>We can also flip this operation on its head and ask whether the column/field entries contain something else. In pandas, the entries in a <code>DataFrame</code> cannot be lists, so the only sensible type of containment is string containment, i.e., checking whether the strings in a column contain a substring:</p> In\u00a0[145]: Copied! <pre>df.species.str.contains(\"set\").sum()\n</pre> df.species.str.contains(\"set\").sum() Out[145]: <pre>50</pre> <p>This has a parallel in FiftyOne: contains_str():</p> In\u00a0[146]: Copied! <pre>ze_view = ds.filter_labels(\"predictions\", F(\"label\").contains_str(\"ze\"))\nprint(ze_view)\n</pre> ze_view = ds.filter_labels(\"predictions\", F(\"label\").contains_str(\"ze\")) print(ze_view) <pre>Dataset:     quickstart\nMedia type:  image\nNum samples: 5\nSample fields:\n    id:              fiftyone.core.fields.ObjectIdField\n    filepath:        fiftyone.core.fields.StringField\n    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:      fiftyone.core.fields.FloatField\n    predictions:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    eval_tp:         fiftyone.core.fields.IntField\n    eval_fp:         fiftyone.core.fields.IntField\n    eval_fn:         fiftyone.core.fields.IntField\n    abstractness:    fiftyone.core.fields.FloatField\n    new_const_field: fiftyone.core.fields.IntField\n    computed_field:  fiftyone.core.fields.IntField\nView stages:\n    1. FilterLabels(field='predictions', filter={'$regexMatch': {'input': '$$this.label', 'options': None, 'regex': 'ze'}}, only_matches=True, trajectories=False)\n</pre> <p>On a related note, FiftyOne has other useful string operations, including starts_with() and ends_with().</p> <p>What's more, in FiftyOne, where fields themselves can be lists, we can check containment in those lists using the contains() method.</p> <p>If we want to create a view which contains either cats or dogs, we can do so with:</p> In\u00a0[147]: Copied! <pre># Only contains samples with \"cat\" or \"dog\" predictions\ncats_or_dogs_view = ds.match(\n    F(\"predictions.detections.label\").contains([\"cat\", \"dog\"])\n)\nprint(cats_or_dogs_view.count())\n</pre> # Only contains samples with \"cat\" or \"dog\" predictions cats_or_dogs_view = ds.match(     F(\"predictions.detections.label\").contains([\"cat\", \"dog\"]) ) print(cats_or_dogs_view.count()) <pre>39\n</pre> <p>If instead we want a view of all samples that contain both cats and dogs, we can pass in the <code>all=True</code> argument:</p> In\u00a0[148]: Copied! <pre># Only contains samples with \"cat\" and \"dog\" predictions\ncats_and_dogs_view = ds.match(\n    F(\"predictions.detections.label\").contains([\"cat\", \"dog\"], all=True)\n)\nprint(cats_and_dogs_view.count())\n</pre> # Only contains samples with \"cat\" and \"dog\" predictions cats_and_dogs_view = ds.match(     F(\"predictions.detections.label\").contains([\"cat\", \"dog\"], all=True) ) print(cats_and_dogs_view.count()) <pre>10\n</pre> <p>In recent versions of pandas, one can check if the data type of a <code>DataFrame</code> column is numeric or is a string by importing the corresponding functions:</p> In\u00a0[149]: Copied! <pre>from pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nprint(is_numeric_dtype(df.sepal_length))\nprint(is_string_dtype(df.sepal_length))\n</pre> from pandas.api.types import is_string_dtype from pandas.api.types import is_numeric_dtype print(is_numeric_dtype(df.sepal_length)) print(is_string_dtype(df.sepal_length)) <pre>True\nFalse\n</pre> <p>In FiftyOne, these are taken care of by the is_number() and is_strin() methods:</p> In\u00a0[150]: Copied! <pre>print(ds.match(F(\"uniqueness\").is_number()).count())\nprint(ds.match(F(\"uniqueness\").is_string()).count())\n</pre> print(ds.match(F(\"uniqueness\").is_number()).count()) print(ds.match(F(\"uniqueness\").is_string()).count()) <pre>200\n0\n</pre> <p>In pandas, one checks whether data is null using the <code>isna</code> method:</p> In\u00a0[151]: Copied! <pre>df.isna().any()\n</pre> df.isna().any() Out[151]: <pre>sepal_length    False\nsepal_width     False\npetal_length    False\npetal_width     False\nspecies         False\nstem_length     False\nsepal_volume    False\ndtype: bool</pre> <p>In FiftyOne, the is_null() method does this:</p> In\u00a0[152]: Copied! <pre>null_view = ds.set_field(\n    \"uniqueness\",\n    (F(\"uniqueness\") &gt;= 0.25).if_else(F(\"uniqueness\"), None)\n)\n\n# Create view that only contains samples with uniqueness = None\nnot_unique_view = null_view.match(F(\"uniqueness\").is_null())\n\nprint(len(not_unique_view))\n</pre> null_view = ds.set_field(     \"uniqueness\",     (F(\"uniqueness\") &gt;= 0.25).if_else(F(\"uniqueness\"), None) )  # Create view that only contains samples with uniqueness = None not_unique_view = null_view.match(F(\"uniqueness\").is_null())  print(len(not_unique_view)) <pre>92\n</pre> <p>Because a FiftyOne <code>Dataset</code> can consist of samples of inhomogenous field schema, FiftyOne also provides the related methods, exists(), and its converse, is_missing(), which checks sample-wise if a field has a value.</p> <p>In FiftyOne, fields can also contain arrays. We can check for this with the is_array() method:</p> In\u00a0[153]: Copied! <pre>ds.match(F(\"tags\").is_array()).count()\n</pre> ds.match(F(\"tags\").is_array()).count() Out[153]: <pre>200</pre> <p>FiftyOne and pandas are both open source Python libraries that make dealing with your data easy. While they serve different purposes - pandas is built for tabular data, while FiftyOne helps users tackle the unstructured data prevalent in computer vision tasks - their syntax and functionality are closely aligned. Both pandas and FiftyOne are important components to many data science and machine learning workflows!</p>"},{"location":"tutorials/pandas_comparison/#pandas-style-queries-in-fiftyone","title":"pandas-style queries in FiftyOne\u00b6","text":""},{"location":"tutorials/pandas_comparison/#overview","title":"Overview\u00b6","text":""},{"location":"tutorials/pandas_comparison/#getting-started","title":"Getting started\u00b6","text":""},{"location":"tutorials/pandas_comparison/#create-empty","title":"Create empty\u00b6","text":""},{"location":"tutorials/pandas_comparison/#create-empty-pddataframe","title":"Create empty <code>pd.DataFrame</code>\u00b6","text":""},{"location":"tutorials/pandas_comparison/#create-empty-fodataset","title":"Create empty <code>fo.Dataset</code>\u00b6","text":""},{"location":"tutorials/pandas_comparison/#example-data","title":"Example data\u00b6","text":""},{"location":"tutorials/pandas_comparison/#iris-dataset","title":"Iris Dataset\u00b6","text":""},{"location":"tutorials/pandas_comparison/#fiftyone-quickstart-data","title":"FiftyOne Quickstart Data\u00b6","text":""},{"location":"tutorials/pandas_comparison/#basics","title":"Basics\u00b6","text":""},{"location":"tutorials/pandas_comparison/#head-and-tail","title":"Head and tail\u00b6","text":""},{"location":"tutorials/pandas_comparison/#head","title":"Head\u00b6","text":""},{"location":"tutorials/pandas_comparison/#tail","title":"Tail\u00b6","text":""},{"location":"tutorials/pandas_comparison/#first-and-last","title":"First and last\u00b6","text":""},{"location":"tutorials/pandas_comparison/#get-single-element","title":"Get single element\u00b6","text":""},{"location":"tutorials/pandas_comparison/#number-of-rowssamples","title":"Number of rows/samples\u00b6","text":""},{"location":"tutorials/pandas_comparison/#getting-columnsfield-schema","title":"Getting columns/field schema\u00b6","text":""},{"location":"tutorials/pandas_comparison/#all-values-in-a-columnfield","title":"All values in a column/field\u00b6","text":""},{"location":"tutorials/pandas_comparison/#view-stages","title":"View stages\u00b6","text":""},{"location":"tutorials/pandas_comparison/#making-a-copy","title":"Making a copy\u00b6","text":""},{"location":"tutorials/pandas_comparison/#slicing","title":"Slicing\u00b6","text":""},{"location":"tutorials/pandas_comparison/#get-random-samples","title":"Get random samples\u00b6","text":""},{"location":"tutorials/pandas_comparison/#select-k-random-samples","title":"Select $k$ random samples\u00b6","text":""},{"location":"tutorials/pandas_comparison/#randomly-select-fraction-p1-of-samples","title":"Randomly select fraction $p&lt;1$ of samples\u00b6","text":""},{"location":"tutorials/pandas_comparison/#shuffle-data","title":"Shuffle data\u00b6","text":""},{"location":"tutorials/pandas_comparison/#filtering","title":"Filtering\u00b6","text":""},{"location":"tutorials/pandas_comparison/#sorting","title":"Sorting\u00b6","text":""},{"location":"tutorials/pandas_comparison/#deleting","title":"Deleting\u00b6","text":""},{"location":"tutorials/pandas_comparison/#aggregations","title":"Aggregations\u00b6","text":""},{"location":"tutorials/pandas_comparison/#count","title":"Count\u00b6","text":""},{"location":"tutorials/pandas_comparison/#sum","title":"Sum\u00b6","text":""},{"location":"tutorials/pandas_comparison/#unique","title":"Unique\u00b6","text":""},{"location":"tutorials/pandas_comparison/#bounds","title":"Bounds\u00b6","text":""},{"location":"tutorials/pandas_comparison/#mean","title":"Mean\u00b6","text":""},{"location":"tutorials/pandas_comparison/#standard-deviation","title":"Standard deviation\u00b6","text":""},{"location":"tutorials/pandas_comparison/#quantiles","title":"Quantiles\u00b6","text":""},{"location":"tutorials/pandas_comparison/#median-and-other-aggregations","title":"Median and other aggregations\u00b6","text":""},{"location":"tutorials/pandas_comparison/#structural-change-operations","title":"Structural change operations\u00b6","text":""},{"location":"tutorials/pandas_comparison/#add-new-columnfield","title":"Add new column/field\u00b6","text":""},{"location":"tutorials/pandas_comparison/#add-new-columnfield-with-default-value","title":"Add new column/field with default value\u00b6","text":""},{"location":"tutorials/pandas_comparison/#add-new-columnfield-from-external-data","title":"Add new column/field from external data\u00b6","text":""},{"location":"tutorials/pandas_comparison/#add-a-new-columnframe-from-existing-columnsfields","title":"Add a new column/frame from existing columns/fields\u00b6","text":""},{"location":"tutorials/pandas_comparison/#remove-a-columnfield","title":"Remove a column/field\u00b6","text":""},{"location":"tutorials/pandas_comparison/#keep-only-specified-columnsfields","title":"Keep only specified columns/fields\u00b6","text":""},{"location":"tutorials/pandas_comparison/#concatenation","title":"Concatenation\u00b6","text":""},{"location":"tutorials/pandas_comparison/#adding-a-single-rowsample","title":"Adding a single row/sample\u00b6","text":""},{"location":"tutorials/pandas_comparison/#remove-rowssamples","title":"Remove rows/samples\u00b6","text":""},{"location":"tutorials/pandas_comparison/#keep-only-specified-rowssamples","title":"Keep only specified rows/samples\u00b6","text":""},{"location":"tutorials/pandas_comparison/#rename-columnfield","title":"Rename column/field\u00b6","text":""},{"location":"tutorials/pandas_comparison/#expressions","title":"Expressions\u00b6","text":""},{"location":"tutorials/pandas_comparison/#element-comparison-expressions","title":"Element comparison expressions\u00b6","text":""},{"location":"tutorials/pandas_comparison/#exact-equality","title":"Exact equality\u00b6","text":""},{"location":"tutorials/pandas_comparison/#less-than-or-equal-to","title":"Less than or equal to\u00b6","text":""},{"location":"tutorials/pandas_comparison/#logical-expressions","title":"Logical expressions\u00b6","text":""},{"location":"tutorials/pandas_comparison/#logical-complement","title":"Logical complement\u00b6","text":""},{"location":"tutorials/pandas_comparison/#logical-and","title":"Logical AND\u00b6","text":""},{"location":"tutorials/pandas_comparison/#logical-or","title":"Logical OR\u00b6","text":""},{"location":"tutorials/pandas_comparison/#subset-superset","title":"Subset-superset\u00b6","text":""},{"location":"tutorials/pandas_comparison/#is-in","title":"Is in\u00b6","text":""},{"location":"tutorials/pandas_comparison/#contains","title":"Contains\u00b6","text":""},{"location":"tutorials/pandas_comparison/#checking-data-types","title":"Checking data types\u00b6","text":""},{"location":"tutorials/pandas_comparison/#numeric-and-string-types","title":"Numeric and string types\u00b6","text":""},{"location":"tutorials/pandas_comparison/#null","title":"Null\u00b6","text":""},{"location":"tutorials/pandas_comparison/#array","title":"Array\u00b6","text":""},{"location":"tutorials/pandas_comparison/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"tutorials/pointe/","title":"Build a 3D self-driving dataset from scratch with OpenAI's Point-E and FiftyOne","text":"<p>In this walkthrough, we will show you how to build your own $3D$ point cloud dataset using OpenAI's Point-E for $3D$ point cloud synthesis, and FiftyOne for dataset curation and visualization.</p> <p>Specifically, this walkthrough covers:</p> <ul> <li>Generating $3D$ point clouds from text with Point-E</li> <li>Loading point cloud data into FiftyOne</li> <li>Curating synthetically generated data assets</li> <li>Constructing a high-quality point-cloud dataset for self-driving applications</li> </ul> <p>So, what's the takeaway?</p> <p>FiftyOne can help you to understand, curate, and process $3D$ point cloud data and build high quality $3D$ datasets</p> <p></p> <p>To get started, you need to install FiftyOne and Point-E:</p> <p>To install FiftyOne, you can use the Python package installer <code>pip</code>:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>To install Point-E, you will need to clone the Point-E github repo:</p> In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/openai/point-e.git\n</pre> !git clone https://github.com/openai/point-e.git <p>And then <code>cd</code> into the <code>point-e</code> directory and install the package locally:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -e .\n</pre> !pip install -e . <p>You will also need to have Open3D and PyTorch installed:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install open3d torch\n</pre> !pip install open3d torch <p>Next, we'll import all of the relevant modules that we will be using in this walkthrough:</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport open3d as o3d\nimport random\nimport torch\nfrom tqdm.auto import tqdm\nimport uuid\n</pre> import numpy as np import open3d as o3d import random import torch from tqdm.auto import tqdm import uuid In\u00a0[4]: Copied! <pre>import fiftyone as fo\nimport fiftyone.brain as fob\nimport fiftyone.zoo as foz\nimport fiftyone.utils.utils3d as fou3d\nfrom fiftyone import ViewField as F\n</pre> import fiftyone as fo import fiftyone.brain as fob import fiftyone.zoo as foz import fiftyone.utils.utils3d as fou3d from fiftyone import ViewField as F In\u00a0[18]: Copied! <pre>from point_e.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config\nfrom point_e.diffusion.sampler import PointCloudSampler\nfrom point_e.models.download import load_checkpoint\nfrom point_e.models.configs import MODEL_CONFIGS, model_from_config\nfrom point_e.util.plotting import plot_point_cloud\n</pre> from point_e.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config from point_e.diffusion.sampler import PointCloudSampler from point_e.models.download import load_checkpoint from point_e.models.configs import MODEL_CONFIGS, model_from_config from point_e.util.plotting import plot_point_cloud <p>We will also set our device:</p> In\u00a0[5]: Copied! <pre>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n</pre> device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') <p>Following the OpenAI's text2pointcloud example notebook, we will show how to generate a $3D$ point cloud with an input text prompt.</p> <p>For this walkthrough, we will use OpenAI's <code>base40M-textvec</code> model, which is a model with $40M$ parameters which takes a text prompt as input and generates an embedding vector.</p> In\u00a0[14]: Copied! <pre>base_name = 'base40M-textvec'\nbase_model = model_from_config(MODEL_CONFIGS[base_name], device)\nbase_model.eval();\nbase_model.load_state_dict(load_checkpoint(base_name, device));\nbase_diffusion = diffusion_from_config(DIFFUSION_CONFIGS[base_name])\n</pre> base_name = 'base40M-textvec' base_model = model_from_config(MODEL_CONFIGS[base_name], device) base_model.eval(); base_model.load_state_dict(load_checkpoint(base_name, device)); base_diffusion = diffusion_from_config(DIFFUSION_CONFIGS[base_name]) <p>Applied on its own, this model will generate a point cloud with $1024$ points. We will also use an upsampler to generate from this a point cloud with $4096$ points:</p> In\u00a0[15]: Copied! <pre>upsampler_model = model_from_config(MODEL_CONFIGS['upsample'], device)\nupsampler_model.eval()\nupsampler_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['upsample'])\nupsampler_model.load_state_dict(load_checkpoint('upsample', device))\n</pre> upsampler_model = model_from_config(MODEL_CONFIGS['upsample'], device) upsampler_model.eval() upsampler_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['upsample']) upsampler_model.load_state_dict(load_checkpoint('upsample', device)) Out[15]: <pre>&lt;All keys matched successfully&gt;</pre> <p>The base diffusion model and upsampling diffusion model are joined together in a <code>PointCloudSampler</code> object, which will take in a text prompt, and output a point cloud with $4096$ points:</p> In\u00a0[16]: Copied! <pre>sampler = PointCloudSampler(\n    device=device,\n    models=[base_model, upsampler_model],\n    diffusions=[base_diffusion, upsampler_diffusion],\n    num_points=[1024, 4096 - 1024],\n    aux_channels=['R', 'G', 'B'],\n    guidance_scale=[3.0, 0.0],\n    model_kwargs_key_filter=('texts', ''), # Do not condition the upsampler at all\n)\n</pre> sampler = PointCloudSampler(     device=device,     models=[base_model, upsampler_model],     diffusions=[base_diffusion, upsampler_diffusion],     num_points=[1024, 4096 - 1024],     aux_channels=['R', 'G', 'B'],     guidance_scale=[3.0, 0.0],     model_kwargs_key_filter=('texts', ''), # Do not condition the upsampler at all ) <p>Let's see this point cloud diffusion model in action, with the text prompt 'red and silver headphones':</p> In\u00a0[\u00a0]: Copied! <pre># Set a prompt to condition on.\nprompt = 'red and silver headphones'\n\n# Produce a sample from the model.\nsamples = None\nfor x in tqdm(sampler.sample_batch_progressive(batch_size=1, model_kwargs=dict(texts=[prompt]))):\n    samples = x\n</pre> # Set a prompt to condition on. prompt = 'red and silver headphones'  # Produce a sample from the model. samples = None for x in tqdm(sampler.sample_batch_progressive(batch_size=1, model_kwargs=dict(texts=[prompt]))):     samples = x <p>We can visualize this with Point-E's native visualizer:</p> In\u00a0[\u00a0]: Copied! <pre>pc = sampler.output_to_point_clouds(samples)[0]\nfig = plot_point_cloud(pc, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))\n</pre> pc = sampler.output_to_point_clouds(samples)[0] fig = plot_point_cloud(pc, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75))) <p></p> <p>Point-E's diffusion model is probabilistic, so if we were to run the model again, we would get a different result.</p> <p>It is nice that Point-E provides its own native visualizer, but these two-dimensional projections are inherently limited. We can far more thoroughly and interactively visualize point clouds in FiftyOne's $3D$ visualizer. Let's see how to load a Point-E point cloud into FiftyOne:</p> <p>In order to load the point cloud into FiftyOne, we will convert the point cloud from Point-E's native format into a more standard Open3D format, and create a sample in FiftyOne. First, let's see what the data structure for Point-E point clouds look like:</p> In\u00a0[\u00a0]: Copied! <pre>print(pc)\n</pre> print(pc) <pre><code>    PointCloud(coords=array([[ 0.00992463,  0.18218482, -0.40539104],\n        [ 0.0122245 ,  0.21034709, -0.32078362],\n        [ 0.10288931,  0.4029989 , -0.40072548],\n        ...,\n        [-0.06964707, -0.33723998, -0.48611435],\n        [ 0.00664746,  0.3134488 , -0.4915944 ],\n        [ 0.1077411 ,  0.3176389 , -0.423187  ]], dtype=float32), channels={'R': array([0.03921569, 0.04313726, 0.9450981 , ..., 0.9490197 , 0.9490197 ,\n        0.9490197 ], dtype=float32), 'G': array([0.04313726, 0.04705883, 0.05490196, ..., 0.04705883, 0.05490196,\n        0.03529412], dtype=float32), 'B': array([0.04313726, 0.04705883, 0.0509804 , ..., 0.04705883, 0.0509804 ,\n        0.03137255], dtype=float32)})</code></pre> <p>Position coordinates are represented by a $(4096, 3)$ array in the <code>coords</code> attribute:</p> In\u00a0[27]: Copied! <pre>print(pc.coords.shape)\n</pre> print(pc.coords.shape) <pre>(4096, 3)\n</pre> <p>And point colors are stored in a dict object within <code>channels</code>:</p> In\u00a0[31]: Copied! <pre>print(pc.channels.keys())\nprint(len(pc.channels['R']))\n</pre> print(pc.channels.keys()) print(len(pc.channels['R'])) <pre>dict_keys(['R', 'G', 'B'])\n4096\n</pre> <p>We can write a simple function that will take in a text prompt, generate the Point-E point cloud, and convert this into a standard Open3D point cloud (<code>open3d.geometry.PointCloud</code>) object:</p> In\u00a0[32]: Copied! <pre>def generate_pcd_from_text(prompt):\n    samples = None\n    for x in sampler.sample_batch_progressive(batch_size=1, model_kwargs=dict(texts=[prompt])):\n        samples = x\n    pointe_pcd = sampler.output_to_point_clouds(samples)[0]\n\n    channels = pointe_pcd.channels\n    r, g, b = channels[\"R\"], channels[\"G\"], channels[\"B\"]\n    colors = np.vstack((r, g, b)).T\n    points = pointe_pcd.coords\n\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(points)\n    pcd.colors = o3d.utility.Vector3dVector(colors)\n    return pcd\n</pre> def generate_pcd_from_text(prompt):     samples = None     for x in sampler.sample_batch_progressive(batch_size=1, model_kwargs=dict(texts=[prompt])):         samples = x     pointe_pcd = sampler.output_to_point_clouds(samples)[0]      channels = pointe_pcd.channels     r, g, b = channels[\"R\"], channels[\"G\"], channels[\"B\"]     colors = np.vstack((r, g, b)).T     points = pointe_pcd.coords      pcd = o3d.geometry.PointCloud()     pcd.points = o3d.utility.Vector3dVector(points)     pcd.colors = o3d.utility.Vector3dVector(colors)     return pcd <p>To load this Open3D point cloud into FiftyOne, we can use Open3D's <code>open3d.io</code> module to write the point cloud to a <code>.pcd</code> file, and then create a FiftyOne Sample object associated with this file:</p> In\u00a0[\u00a0]: Copied! <pre>headphone_pcd = generate_pcd_from_text('red and silver headphones')\nheadphone_file = \"headphone.pcd\"\no3d.io.write_point_cloud(headphone_file, headphone_pcd)\n</pre> headphone_pcd = generate_pcd_from_text('red and silver headphones') headphone_file = \"headphone.pcd\" o3d.io.write_point_cloud(headphone_file, headphone_pcd) In\u00a0[\u00a0]: Copied! <pre>headphone_dataset = fo.Dataset(name = \"headphone_dataset\")\nheadphone_dataset.add_sample(\n    fo.Sample(filepath=headphone_file)\n)\nsession = fo.launch_app(headphone_dataset)\n</pre> headphone_dataset = fo.Dataset(name = \"headphone_dataset\") headphone_dataset.add_sample(     fo.Sample(filepath=headphone_file) ) session = fo.launch_app(headphone_dataset) <p></p> <p>Now that we have a workflow for generating $3D$ point cloud samples in FiftyOne with Point-E, we can generate an entire dataset of synthetic $3D$ point clouds.</p> <p>In this walkthrough, we will generate a variety of vehicles. In particular, we will generate point clouds for vehicles of type <code>car</code>, <code>bike</code>, <code>bus</code>, and <code>motorcycle</code>. We will specify what vehicle we want Point-E to generate in our text prompt. Due to the probabilistic nature of diffusion models like Point-E, merely running a simple prompt like \"a bicycle\" multiple times will generate distinct point cloud models.</p> <p>To add even more variety, we will instruct Point-E to paint each of the vehicles two randomly chosen colors with a prompt of the form: <code>\"a $(COLOR1) $(VEHICLE_TYPE) with $(COLOR2) wheels\"</code>. This is just an illustrative example.</p> In\u00a0[1]: Copied! <pre>VEHICLE_TYPES = [\"car\", \"bus\", \"bike\", \"motorcycle\"]\nVEHICLE_COLORS = [\"red\", \"blue\", \"green\", \"yellow\", \"white\"]\n</pre> VEHICLE_TYPES = [\"car\", \"bus\", \"bike\", \"motorcycle\"] VEHICLE_COLORS = [\"red\", \"blue\", \"green\", \"yellow\", \"white\"] <p>In this example, we will generate random filenames for each of the point cloud models, but you can specify filenames however you'd like:</p> In\u00a0[2]: Copied! <pre>def generate_filename():\n    rand_str = str(uuid.uuid1()).split('-')[0]\n    return \"pointe_vehicles/\" + rand_str + \".pcd\"\n</pre> def generate_filename():     rand_str = str(uuid.uuid1()).split('-')[0]     return \"pointe_vehicles/\" + rand_str + \".pcd\" In\u00a0[3]: Copied! <pre>def generate_pointe_vehicle_dataset(\n    dataset_name = \"point-e-vehicles\",\n    num_samples = 100\n):\n    samples = []\n    for i in tqdm(range(num_samples)):\n        vehicle_type = random.choice(VEHICLE_TYPES)\n        cols = random.choices(VEHICLE_COLORS, k=2)\n        prompt = f\"a {cols[0]} {vehicle_type} with {cols[1]} wheels\"\n        pcd = generate_pcd_from_text(prompt)\n        ofile = generate_filename()\n        o3d.io.write_point_cloud(ofile, pcd)\n        \n        sample = fo.Sample(\n            filepath = ofile,\n            vehicle_type = fo.Classification(label = vehicle_type)\n        )\n        samples.append(sample)\n        \n    dataset = fo.Dataset(dataset_name)\n    dataset.add_samples(samples)\n    return dataset\n</pre> def generate_pointe_vehicle_dataset(     dataset_name = \"point-e-vehicles\",     num_samples = 100 ):     samples = []     for i in tqdm(range(num_samples)):         vehicle_type = random.choice(VEHICLE_TYPES)         cols = random.choices(VEHICLE_COLORS, k=2)         prompt = f\"a {cols[0]} {vehicle_type} with {cols[1]} wheels\"         pcd = generate_pcd_from_text(prompt)         ofile = generate_filename()         o3d.io.write_point_cloud(ofile, pcd)                  sample = fo.Sample(             filepath = ofile,             vehicle_type = fo.Classification(label = vehicle_type)         )         samples.append(sample)              dataset = fo.Dataset(dataset_name)     dataset.add_samples(samples)     return dataset In\u00a0[\u00a0]: Copied! <pre>vehicle_dataset = generate_pointe_vehicle_dataset(\n    dataset_name = \"point-e-vehicles\",\n    num_samples = 100\n)\n</pre> vehicle_dataset = generate_pointe_vehicle_dataset(     dataset_name = \"point-e-vehicles\",     num_samples = 100 ) <p>We will then make the dataset persistent so that it saves to database and we can load it at a later time.</p> In\u00a0[\u00a0]: Copied! <pre>vehicle_dataset.persistent = True\n</pre> vehicle_dataset.persistent = True <p>Before viewing this in the FiftyOne App, we can use FiftyOne's $3D$ utils to generate a two dimensional image for each point cloud, which will allow us to preview our samples in the sample grid. To do this, we will project the point clouds onto a $2D$ plane with the <code>fou3d.compute_orthographic_projection_images()</code> method. We will pass in a vector for the <code>projection_normal</code> argument to specify the plane about which to perform the orthographic projection.</p> In\u00a0[\u00a0]: Copied! <pre>size = (-1, 608) \n## height of images should be 608 pixels\n## - with aspect ratio preserved\n\nfou3d.compute_orthographic_projection_images(\n    vehicle_dataset,\n    size,\n    \"/vehicle_side_view_images\",\n    shading_mode=\"height\",\n    projection_normal = (0, -1, 0)\n)\n</pre> size = (-1, 608)  ## height of images should be 608 pixels ## - with aspect ratio preserved  fou3d.compute_orthographic_projection_images(     vehicle_dataset,     size,     \"/vehicle_side_view_images\",     shading_mode=\"height\",     projection_normal = (0, -1, 0) ) <p>Now we are ready to look at our $3D$ point cloud models of vehicles:</p> In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(vehicle_dataset)\n</pre> session = fo.launch_app(vehicle_dataset) <p></p> <p>Taking a look at these point cloud models (or their orthographic projections), we can see that the vehicles are facing a variety of directions. For the sake of consistency (this will come in handy in the next section), let's use FiftyOne's in-app tagging capabilities to tag each of the point cloud samples with an orientation $\\in[$ <code>left</code>, <code>right</code>, <code>front</code>, <code>back</code>$]$. Once we tag a set of samples, we can omit samples with this tag from view in the app, so the we are only looking at untagged samples.</p> <p></p> <p>Additionally, as we are curating a high quality dataset of vehicle point cloud assets, we can identify models that do not fit our needs. This may include point cloud models that are not good enough representations of vehicles, models that are out of our desired distribution, or models we feel are too similar to other models in our dataset. We will tag these samples as \"bad\".</p> <p></p> <p>Once we have tagged all of our samples, we can convert the orientation tags into a new <code>orientation</code> field on our samples:</p> In\u00a0[\u00a0]: Copied! <pre>orientations = [\"left\", \"right\", \"front\", \"back\"]\n\nvehicle_dataset.add_sample_field(\"orientation\", fo.StringField)\n\nfor orientation in orientations:\n    view = vehicle_dataset.match_tags(orientation)\n    view.set_values(\"orientation\", [orientation]*len(view))\n</pre> orientations = [\"left\", \"right\", \"front\", \"back\"]  vehicle_dataset.add_sample_field(\"orientation\", fo.StringField)  for orientation in orientations:     view = vehicle_dataset.match_tags(orientation)     view.set_values(\"orientation\", [orientation]*len(view)) <p>Additionally, we can pick out our desired subset of vehicle assets by matching for samples without the <code>bad</code> tag:</p> In\u00a0[\u00a0]: Copied! <pre>usable_vehicles_dataset = vehicle_dataset.match(~F(\"tags\").contains(\"bad\"))\n</pre> usable_vehicles_dataset = vehicle_dataset.match(~F(\"tags\").contains(\"bad\")) <p>Now that we have a dataset of usable $3D$ point cloud models for vehicles, we can use these to construct a $3D$ point cloud dataset of road scenes, for use in self-driving applications. In this section, we'll show you how to get started building your own simple road scenes dataset.</p> <p>In this walkthough, we will limit our scope to basic roads with one lane of traffic going in each direction, separated by a dashed yellow line. We will use the same road as the foundation for each scene. Let's construct a simple point cloud model for the road:</p> In\u00a0[\u00a0]: Copied! <pre>ROAD_LENGTH = 40\nROAD_WIDTH = 6\nFRONT = -ROAD_LENGTH/2.\nBACK = ROAD_LENGTH/2.\nLANE_WIDTH = ROAD_WIDTH/2.\nLINE_POINTS = 1000\n</pre> ROAD_LENGTH = 40 ROAD_WIDTH = 6 FRONT = -ROAD_LENGTH/2. BACK = ROAD_LENGTH/2. LANE_WIDTH = ROAD_WIDTH/2. LINE_POINTS = 1000 <p>We will build the point cloud model for the road out of two white lines, for the left and right edges of the scene, and a dashed yellow line as a divider. We will save the point cloud model in the file <code>road.pcd</code>:</p> In\u00a0[\u00a0]: Copied! <pre>def generate_road_pcd():\n    ## LEFT LINE\n    road_line_left_points = np.vstack(\n        (np.zeros(LINE_POINTS) - ROAD_WIDTH/2., \n        np.linspace(FRONT, BACK, LINE_POINTS),\n        np.zeros(LINE_POINTS))\n    ).T\n    \n    ## RIGHT LINE\n    road_line_right_points = np.copy(-road_line_left_points)\n    \n    ## CENTER LINE\n    road_line_center_points_y = np.linspace(\n        FRONT, \n        BACK, \n        LINE_POINTS\n    )\n    road_line_center_points_y = road_line_center_points_y[\n        np.where(np.mod(road_line_center_points_y, 1) &gt; 0.3)[0]\n    ]\n    num_center_points = len(road_line_center_points_y)\n    road_line_center_points = np.vstack(\n        (np.zeros(num_center_points), \n        road_line_center_points_y,\n        np.zeros(num_center_points))\n    ).T\n    \n    ## CONCATENATE\n    road_pcd_points = np.concatenate(\n        (road_line_center_points,\n         road_line_left_points,\n        road_line_right_points,\n        )\n    )\n    \n    ## COLOR\n    ## white\n    road_pcd_colors = 255 * np.ones(road_pcd_points.shape)\n    ## yellow\n    road_pcd_colors[:num_center_points, 2] = 0\n    \n    road_pcd = o3d.geometry.PointCloud()\n    road_pcd.points = o3d.utility.Vector3dVector(road_pcd_points)\n    road_pcd.colors = o3d.utility.Vector3dVector(road_pcd_colors)\n    o3d.io.write_point_cloud(\"road.pcd\", road_pcd)\n</pre> def generate_road_pcd():     ## LEFT LINE     road_line_left_points = np.vstack(         (np.zeros(LINE_POINTS) - ROAD_WIDTH/2.,          np.linspace(FRONT, BACK, LINE_POINTS),         np.zeros(LINE_POINTS))     ).T          ## RIGHT LINE     road_line_right_points = np.copy(-road_line_left_points)          ## CENTER LINE     road_line_center_points_y = np.linspace(         FRONT,          BACK,          LINE_POINTS     )     road_line_center_points_y = road_line_center_points_y[         np.where(np.mod(road_line_center_points_y, 1) &gt; 0.3)[0]     ]     num_center_points = len(road_line_center_points_y)     road_line_center_points = np.vstack(         (np.zeros(num_center_points),          road_line_center_points_y,         np.zeros(num_center_points))     ).T          ## CONCATENATE     road_pcd_points = np.concatenate(         (road_line_center_points,          road_line_left_points,         road_line_right_points,         )     )          ## COLOR     ## white     road_pcd_colors = 255 * np.ones(road_pcd_points.shape)     ## yellow     road_pcd_colors[:num_center_points, 2] = 0          road_pcd = o3d.geometry.PointCloud()     road_pcd.points = o3d.utility.Vector3dVector(road_pcd_points)     road_pcd.colors = o3d.utility.Vector3dVector(road_pcd_colors)     o3d.io.write_point_cloud(\"road.pcd\", road_pcd) In\u00a0[\u00a0]: Copied! <pre>generate_road_pcd()\n</pre> generate_road_pcd() <p>Next, we piece together a workflow for placing individual vehicles on the road in a scene. This workflow needs to take into account a few factors:</p> <ul> <li>Point-E point clouds are not always centered at the origin</li> <li>The Point-E point clouds were all generated to fill the same volume, so a <code>bus</code> point cloud model will be the same size as a <code>bicycle</code> point cloud model</li> <li>The vehicle models were generated with variable orientations, which we have categorized, but need to be taken into account in our road scenes</li> <li>Vehicles must face in the direction that traffic is flowing, which depends on the side of the road on which we place the vehicle</li> </ul> <p>With these considerations in mind, we compose the following steps:</p> <ol> <li>Center the vehicle at the origin</li> <li>Scale the vehicle according to its vehicle type</li> <li>Pick a side of the road for the vehicle</li> <li>Orient the vehicle, given its initial orientation and selected side of the road</li> <li>Position the vehicle on the road</li> </ol> <p>In the cells below, we implement minimal functions for these steps, leveraging Open3D's comprehensive point cloud functionality:</p> In\u00a0[17]: Copied! <pre>ORIGIN = (0.0, 0.0, 0.0)\n\ndef center_vehicle(vehicle_pcd):\n    vehicle_pcd.translate(ORIGIN, relative=False)\n</pre> ORIGIN = (0.0, 0.0, 0.0)  def center_vehicle(vehicle_pcd):     vehicle_pcd.translate(ORIGIN, relative=False) In\u00a0[\u00a0]: Copied! <pre>VEHICLE_SCALE_MAP = {\n    \"car\": 4.0,\n    \"bus\": 6.0,\n    \"motorcycle\": 2.5,\n    \"bike\": 1.5\n}\n\ndef scale_vehicle(vehicle_pcd, vehicle_type):\n    vehicle_scale = VEHICLE_SCALE_MAP[vehicle_type]\n    vehicle_pcd.scale(vehicle_scale, ORIGIN)\n</pre> VEHICLE_SCALE_MAP = {     \"car\": 4.0,     \"bus\": 6.0,     \"motorcycle\": 2.5,     \"bike\": 1.5 }  def scale_vehicle(vehicle_pcd, vehicle_type):     vehicle_scale = VEHICLE_SCALE_MAP[vehicle_type]     vehicle_pcd.scale(vehicle_scale, ORIGIN) In\u00a0[\u00a0]: Copied! <pre>def choose_side_of_road():\n    return random.choice([\"left\", \"right\"])\n</pre> def choose_side_of_road():     return random.choice([\"left\", \"right\"]) In\u00a0[\u00a0]: Copied! <pre>ORIENTATION_TO_ROTATION = {\n    \"back\": 0.,\n    \"right\": np.pi/2.,\n    \"front\": np.pi,\n    \"left\": 3 * np.pi/2.,\n}\n\ndef orient_vehicle(vehicle_pcd, side_of_road, initial_orientation):\n    rot_xyz = [0., 0., 0.]\n    rot_xyz[2] += ORIENTATION_TO_ROTATION[initial_orientation]\n    \n    if side_of_road == \"left\":\n        rot_xyz[2] += np.pi\n    R = vehicle_pcd.get_rotation_matrix_from_xyz(rot_xyz)\n    vehicle_pcd.rotate(R)\n</pre> ORIENTATION_TO_ROTATION = {     \"back\": 0.,     \"right\": np.pi/2.,     \"front\": np.pi,     \"left\": 3 * np.pi/2., }  def orient_vehicle(vehicle_pcd, side_of_road, initial_orientation):     rot_xyz = [0., 0., 0.]     rot_xyz[2] += ORIENTATION_TO_ROTATION[initial_orientation]          if side_of_road == \"left\":         rot_xyz[2] += np.pi     R = vehicle_pcd.get_rotation_matrix_from_xyz(rot_xyz)     vehicle_pcd.rotate(R) In\u00a0[\u00a0]: Copied! <pre>## randomly position the vehicle in its lane\ndef position_vehicle(vehicle_pcd, side_of_road):\n    ## raise vehicle so it is ON the road\n    minz = np.amin(np.array(vehicle_pcd.points), axis = 0)[-1]\n    \n    xpos = np.random.normal(loc = LANE_WIDTH/2., scale = 0.4)\n    if side_of_road == \"left\":\n        xpos *= -1\n        \n    ypos = np.random.uniform(low = FRONT, high=BACK)\n    \n    translation = [xpos, ypos, -minz]\n    vehicle_pcd.translate(translation)\n</pre> ## randomly position the vehicle in its lane def position_vehicle(vehicle_pcd, side_of_road):     ## raise vehicle so it is ON the road     minz = np.amin(np.array(vehicle_pcd.points), axis = 0)[-1]          xpos = np.random.normal(loc = LANE_WIDTH/2., scale = 0.4)     if side_of_road == \"left\":         xpos *= -1              ypos = np.random.uniform(low = FRONT, high=BACK)          translation = [xpos, ypos, -minz]     vehicle_pcd.translate(translation) <p>We can then wrap all of this up in a function which takes in a sample from the usable subset of the FiftyOne point cloud vehicle asset dataset, and returns a tuple containing the transformed point cloud for the vehicle, and a label for its vehicle type:</p> In\u00a0[\u00a0]: Copied! <pre>def generate_scene_vehicle(sample):\n    vehicle_type = sample.vehicle_type.label\n    \n    initial_orientation = sample.orientation\n    side_of_road = choose_side_of_road()\n    \n    vehicle_pcd = o3d.io.read_point_cloud(sample.filepath)\n    center_vehicle(vehicle_pcd)\n    scale_vehicle(vehicle_pcd, vehicle_type)\n    orient_vehicle(vehicle_pcd, side_of_road, initial_orientation)\n    position_vehicle(vehicle_pcd, side_of_road)\n    return (vehicle_pcd, vehicle_type)\n</pre> def generate_scene_vehicle(sample):     vehicle_type = sample.vehicle_type.label          initial_orientation = sample.orientation     side_of_road = choose_side_of_road()          vehicle_pcd = o3d.io.read_point_cloud(sample.filepath)     center_vehicle(vehicle_pcd)     scale_vehicle(vehicle_pcd, vehicle_type)     orient_vehicle(vehicle_pcd, side_of_road, initial_orientation)     position_vehicle(vehicle_pcd, side_of_road)     return (vehicle_pcd, vehicle_type) <p>We can generate a \"scene\" with this vehicle placed on the road by adding this point cloud to the point cloud for the road:</p> In\u00a0[\u00a0]: Copied! <pre>sample = usable_vehicles_dataset.take(1).first()\nvehicle_pcd, vehicle_type = generate_scene_vehicle(sample)\nroad_pcd = o3d.io.read_point_cloud(\"road.pcd\")\nvehicle_on_road_pcd = road_pcd + vehicle_pcd\n</pre> sample = usable_vehicles_dataset.take(1).first() vehicle_pcd, vehicle_type = generate_scene_vehicle(sample) road_pcd = o3d.io.read_point_cloud(\"road.pcd\") vehicle_on_road_pcd = road_pcd + vehicle_pcd <p></p> <p>Now that we have a workflow for placing a single vehicle on the road, we can construct \"scenes\" including multiple vehicles on the road.</p> <p>To do this, we need to ensure that none of the vehicles in our scene overlap with each other. We can implement this logic by using Open3D's <code>compute_point_cloud_distance()</code> method to compute the distance between the point cloud models for vehicles that are already in a given scene, and the point cloud model for a prospective vehicle that will potentially be placed in the scene. If the minimum distance between any of the existing scene vehicle point clouds and the candidate vehicle point cloud is below some threshold, then we retry randomly placing a vehicle in the scene.</p> <p>We will wrap this logic in a new <code>check_compatibility()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre>def check_compatibility(\n    vehicle,\n    scene_vehicles,\n    thresh = 0.2\n):\n    for sv in scene_vehicles:\n        dists = vehicle[0].compute_point_cloud_distance(sv[0])\n        if np.amin(np.array(dists)) &lt; thresh:\n            return False\n    return True\n</pre> def check_compatibility(     vehicle,     scene_vehicles,     thresh = 0.2 ):     for sv in scene_vehicles:         dists = vehicle[0].compute_point_cloud_distance(sv[0])         if np.amin(np.array(dists)) &lt; thresh:             return False     return True <p>The last ingredient is a simple function to randomly select a single sample from our usable vehicle assets dataset:</p> In\u00a0[\u00a0]: Copied! <pre>def choose_vehicle_sample():\n    return usable_vehicles_dataset.take(1).first()\n</pre> def choose_vehicle_sample():     return usable_vehicles_dataset.take(1).first() <p>Finally, we are ready to generate road scenes! The following function generates a FiftyOne point cloud sample for a road scene with <code>num_vehicles</code> vehicles, and stores the point cloud for the scene in <code>scene_filepath</code>:</p> In\u00a0[\u00a0]: Copied! <pre>def generate_scene_sample(num_vehicles, scene_filepath):\n    ZERO_ROT = [0., 0., 0.]\n    \n    sample = choose_vehicle_sample()\n    scene_vehicles = [generate_scene_vehicle(sample)]\n    \n    k = 1\n    while k &lt; num_vehicles:\n        sample = choose_vehicle_sample()\n        candidate_vehicle = generate_scene_vehicle(sample)\n        if check_compatibility(\n            candidate_vehicle,\n            scene_vehicles\n        ):\n            scene_vehicles.append(candidate_vehicle)\n            k += 1\n    \n    detections = []\n    scene_pcd = o3d.io.read_point_cloud(\"road.pcd\")\n    for vehicle in scene_vehicles:\n        vehicle_pcd, vehicle_type = vehicle\n        scene_pcd = scene_pcd + vehicle_pcd\n        obb = vehicle_pcd.get_oriented_bounding_box()\n        dim, loc = obb.extent, obb.center\n        dim = [dim[2], dim[0], dim[1]]\n        detection = fo.Detection(\n            label = vehicle_type,\n            location = list(loc),\n            dimensions = list(dim),\n            rotation = list(ZERO_ROT)\n        )\n        detections.append(detection)\n    \n    o3d.io.write_point_cloud(scene_filepath, scene_pcd)\n    sample = fo.Sample(\n        filepath = scene_filepath,\n        ground_truth = fo.Detections(detections=detections)\n    )\n    \n    return sample\n</pre> def generate_scene_sample(num_vehicles, scene_filepath):     ZERO_ROT = [0., 0., 0.]          sample = choose_vehicle_sample()     scene_vehicles = [generate_scene_vehicle(sample)]          k = 1     while k &lt; num_vehicles:         sample = choose_vehicle_sample()         candidate_vehicle = generate_scene_vehicle(sample)         if check_compatibility(             candidate_vehicle,             scene_vehicles         ):             scene_vehicles.append(candidate_vehicle)             k += 1          detections = []     scene_pcd = o3d.io.read_point_cloud(\"road.pcd\")     for vehicle in scene_vehicles:         vehicle_pcd, vehicle_type = vehicle         scene_pcd = scene_pcd + vehicle_pcd         obb = vehicle_pcd.get_oriented_bounding_box()         dim, loc = obb.extent, obb.center         dim = [dim[2], dim[0], dim[1]]         detection = fo.Detection(             label = vehicle_type,             location = list(loc),             dimensions = list(dim),             rotation = list(ZERO_ROT)         )         detections.append(detection)          o3d.io.write_point_cloud(scene_filepath, scene_pcd)     sample = fo.Sample(         filepath = scene_filepath,         ground_truth = fo.Detections(detections=detections)     )          return sample <p>This <code>generate_scene_sample()</code> function not only generates a complete point cloud scene - it also generates labeled $3D$ object detection bounding boxes for each vehicle using Open3D's <code>get_oriented_bounding_box()</code> method. You can then use these as ground truth labels to train your own $3D$ road scenes object detection model.</p> <p>All that is left to do is populate a new FiftyOne dataset with these generated road scenes.</p> <p>To generate this dataset, we will randomly select a number of vehicles for each scene, from within a set range:</p> In\u00a0[\u00a0]: Copied! <pre>MIN_SCENE_VEHICLES = 1\nMAX_SCENE_VEHICLES = 7\n\ndef choose_num_scene_vehicles():\n    return random.randint(\n        MIN_SCENE_VEHICLES, \n        MAX_SCENE_VEHICLES\n    )\n</pre> MIN_SCENE_VEHICLES = 1 MAX_SCENE_VEHICLES = 7  def choose_num_scene_vehicles():     return random.randint(         MIN_SCENE_VEHICLES,          MAX_SCENE_VEHICLES     ) <p>And we will randomly generate filepaths for the scenes:</p> In\u00a0[\u00a0]: Copied! <pre>def generate_scene_filepath():\n    rand_str = str(uuid.uuid1()).split('-')[0]\n    return \"pointe_road_scenes/\" + rand_str + \".pcd\"\n</pre> def generate_scene_filepath():     rand_str = str(uuid.uuid1()).split('-')[0]     return \"pointe_road_scenes/\" + rand_str + \".pcd\" <p>Putting it all together:</p> In\u00a0[\u00a0]: Copied! <pre>def generate_road_scenes_dataset(num_scenes):\n    samples = []\n    for i in range(num_scenes):\n        num_scene_vehicles = choose_num_scene_vehicles()\n        scene_filepath = generate_scene_filepath()\n        \n        sample = generate_scene_sample(\n            num_scene_vehicles,\n            scene_filepath\n        )\n        samples.append(sample)\n    \n    dataset = fo.Dataset(name = \"point-e-road-scenes\")\n    dataset.add_samples(samples)\n    dataset.persistent = True\n    return dataset\n</pre> def generate_road_scenes_dataset(num_scenes):     samples = []     for i in range(num_scenes):         num_scene_vehicles = choose_num_scene_vehicles()         scene_filepath = generate_scene_filepath()                  sample = generate_scene_sample(             num_scene_vehicles,             scene_filepath         )         samples.append(sample)          dataset = fo.Dataset(name = \"point-e-road-scenes\")     dataset.add_samples(samples)     dataset.persistent = True     return dataset In\u00a0[\u00a0]: Copied! <pre>num_scenes = 100\nroad_scene_dataset = generate_road_scenes_dataset(num_scenes)\n</pre> num_scenes = 100 road_scene_dataset = generate_road_scenes_dataset(num_scenes) <p>If you'd like, you can also generate bird's eye view projection images for these scenes, so you can preview scenes in the sample grid:</p> In\u00a0[\u00a0]: Copied! <pre>size = (-1, 608)\n\nfou3d.compute_orthographic_projection_images(\n    road_scene_dataset,\n    size,\n    \"/road_scene_bev_images\",\n    shading_mode=\"rgb\",\n)\n</pre> size = (-1, 608)  fou3d.compute_orthographic_projection_images(     road_scene_dataset,     size,     \"/road_scene_bev_images\",     shading_mode=\"rgb\", ) In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(road_scene_dataset)\n</pre> session = fo.launch_app(road_scene_dataset) <p></p> <p>FiftyOne is a valuable tool that can help you to build high quality computer vision datasets. This is true whether you are working with images, videos, point clouds, or geo data. And this is true whether you are adapting existing datasets, or constructing your own datasets from scratch!</p>"},{"location":"tutorials/pointe/#build-a-3d-self-driving-dataset-from-scratch-with-openais-point-e-and-fiftyone","title":"Build a 3D self-driving dataset from scratch with OpenAI's Point-E and FiftyOne\u00b6","text":""},{"location":"tutorials/pointe/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/pointe/#generating-a-point-cloud-from-text","title":"Generating a point cloud from text\u00b6","text":""},{"location":"tutorials/pointe/#loading-a-point-e-point-cloud-into-fiftyone","title":"Loading a Point-E point cloud into FiftyOne\u00b6","text":""},{"location":"tutorials/pointe/#curating-synthetic-3d-point-cloud-assets","title":"Curating synthetic $3D$ point cloud assets\u00b6","text":""},{"location":"tutorials/pointe/#constructing-a-self-driving-dataset","title":"Constructing a self-driving dataset\u00b6","text":""},{"location":"tutorials/pointe/#constructing-a-road-point-cloud","title":"Constructing a road point cloud\u00b6","text":""},{"location":"tutorials/pointe/#placing-a-vehicle-on-the-road","title":"Placing a vehicle on the road\u00b6","text":""},{"location":"tutorials/pointe/#constructing-road-scenes","title":"Constructing road scenes\u00b6","text":""},{"location":"tutorials/pointe/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"tutorials/qdrant/","title":"Nearest Neighbor Embeddings Classification with Qdrant","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>We'll also need the Qdrant Python client:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install qdrant_client\n</pre> !pip install qdrant_client <p>In this example, we will also be making use of torchvision models from the FiftyOne Model Zoo.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install torchvision\n</pre> !pip install torchvision In\u00a0[\u00a0]: Copied! <pre>!docker run -p \"6333:6333\" -p \"6334:6334\" -d qdrant/qdrant\n</pre> !docker run -p \"6333:6333\" -p \"6334:6334\" -d qdrant/qdrant <p>After running the command we\u2019ll have the Qdrant server running, with HTTP API exposed at port 6333 and gRPC interface at 6334.</p> In\u00a0[1]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.brain as fob\n</pre> import fiftyone as fo import fiftyone.zoo as foz import fiftyone.brain as fob In\u00a0[3]: Copied! <pre># Load the data\ndataset = foz.load_zoo_dataset(\"mnist\", max_samples=2500)\ntrain_view = dataset.match_tags(tags=[\"train\"])\n</pre> # Load the data dataset = foz.load_zoo_dataset(\"mnist\", max_samples=2500) train_view = dataset.match_tags(tags=[\"train\"]) <pre>Split 'train' already downloaded\nSplit 'test' already downloaded\nLoading 'mnist' split 'train'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [4.1s elapsed, 0s remaining, 685.3 samples/s]      \nLoading 'mnist' split 'test'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [4.0s elapsed, 0s remaining, 644.8 samples/s]      \nDataset 'mnist-2500' created\n</pre> <p>Let's start by taking a look at the dataset in the FiftyOne App.</p> In\u00a0[34]: Copied! <pre>session = fo.launch_app(train_view)\n</pre> session = fo.launch_app(train_view) Activate In\u00a0[35]: Copied! <pre>session.freeze()  # screenshot the App\n</pre> session.freeze()  # screenshot the App In\u00a0[4]: Copied! <pre># Compute embeddings\nmodel = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\n\ntrain_embeddings = train_view.compute_embeddings(model)\n</pre> # Compute embeddings model = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")  train_embeddings = train_view.compute_embeddings(model) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [3.7m elapsed, 0s remaining, 7.7 samples/s]       \n</pre> <p>As of FiftyOne version 0.20.0, FiftyOne natively integrates with Qdrant! This means that uploading your embeddings is now as simple as computing similarity on your dataset with the qdrant backend.</p> In\u00a0[\u00a0]: Copied! <pre># Compute similarity with Qdrant backend\nfob.compute_similarity(train_view, embeddings=train_embeddings, brain_key=\"qdrant_example\", backend=\"qdrant\")\n</pre> # Compute similarity with Qdrant backend fob.compute_similarity(train_view, embeddings=train_embeddings, brain_key=\"qdrant_example\", backend=\"qdrant\") <p>Alternatively, if you don't want to precompute your embeddings, you can pass in a model parameter to compute_similarity and the embeddings will be computed for you and stored in the given field name upon completion.</p> In\u00a0[\u00a0]: Copied! <pre># Compute similarity with Qdrant backend without precomputed embeddings\n#fob.compute_similarity(train_view, model = model, embeddings=train_embeddings_new, brain_key=\"qdrant_example\", backend=\"qdrant\")\n</pre> # Compute similarity with Qdrant backend without precomputed embeddings #fob.compute_similarity(train_view, model = model, embeddings=train_embeddings_new, brain_key=\"qdrant_example\", backend=\"qdrant\") In\u00a0[9]: Copied! <pre># Assign the labels to test embeddings by selecting the most common label\n# among the neighbours of each sample\ntest_view = dataset.match_tags(tags=[\"test\"])\ntest_embeddings = test_view.compute_embeddings(model)\n</pre> # Assign the labels to test embeddings by selecting the most common label # among the neighbours of each sample test_view = dataset.match_tags(tags=[\"test\"]) test_embeddings = test_view.compute_embeddings(model) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [3.9m elapsed, 0s remaining, 12.1 samples/s]      \n</pre> <p>Time for some magic. Let's simply iterate through the test dataset's samples and their corresponding embeddings, and use sort_by_similarity to find the 15 closest embeddings from the training set. We'll also need to calculate the class counts to determine the most common label. This will be stored as an <code>\"ann_prediction\"</code> on each test sample in FiftyOne.</p> <p>The function below takes an embedding vector as input, uses FiftyOne's sort_by_similarity functionality to find the nearest neighbors to the test embedding, generates a class prediction, and returns a FiftyOne Classification object that we can store in our FiftyOne dataset.</p> In\u00a0[13]: Copied! <pre>def generate_fiftyone_classification(embedding, brain_key, field=\"ground_truth\"):\n    search_results = dataset.sort_by_similarity(embedding, k=15, brain_key=brain_key)\n    \n    # Count the occurrences of each class and select the most common label\n    # with the confidence estimated as the number of occurrences of the most\n    # common label divided by a total number of results.\n    class_counts = search_results.count_values(field + \".label\")\n    predicted_class = max(class_counts, key=class_counts.get)\n    occurences_num = class_counts[predicted_class]\n    confidence = occurences_num / sum(class_counts.values())\n    prediction = fo.Classification(\n        label=predicted_class, confidence=confidence\n    )\n    return prediction\n</pre> def generate_fiftyone_classification(embedding, brain_key, field=\"ground_truth\"):     search_results = dataset.sort_by_similarity(embedding, k=15, brain_key=brain_key)          # Count the occurrences of each class and select the most common label     # with the confidence estimated as the number of occurrences of the most     # common label divided by a total number of results.     class_counts = search_results.count_values(field + \".label\")     predicted_class = max(class_counts, key=class_counts.get)     occurences_num = class_counts[predicted_class]     confidence = occurences_num / sum(class_counts.values())     prediction = fo.Classification(         label=predicted_class, confidence=confidence     )     return prediction In\u00a0[10]: Copied! <pre>from tqdm import tqdm\n\npredictions = []\n\n# Call Qdrant to find the closest data points\nfor embedding in tqdm(test_embeddings):\n    prediction = generate_fiftyone_classification(embedding, brain_key=\"qdrant_example\")\n    predictions.append(prediction)\n\ntest_view.set_values(\"ann_prediction\", predictions)\n</pre> from tqdm import tqdm  predictions = []  # Call Qdrant to find the closest data points for embedding in tqdm(test_embeddings):     prediction = generate_fiftyone_classification(embedding, brain_key=\"qdrant_example\")     predictions.append(prediction)  test_view.set_values(\"ann_prediction\", predictions) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [01:42&lt;00:00, 24.40it/s]\n</pre> <p>By the way, we estimated the confidence by calculating the fraction of samples belonging to the most common label. That gives us an intuition of how sure we were while predicting the label for each case and can be used in FiftyOne to easily spot confusing examples.</p> In\u00a0[6]: Copied! <pre>session = fo.launch_app(test_view)\n</pre> session = fo.launch_app(test_view) Activate In\u00a0[7]: Copied! <pre>session.freeze()  # screenshot the App\n</pre> session.freeze()  # screenshot the App <p>FiftyOne provides a variety of built-in methods for evaluating your model predictions, including regressions, classifications, detections, polygons, instance and semantic segmentations, on both image and video datasets. In two lines of code, we can compute and print an evaluation report of our classifier.</p> In\u00a0[11]: Copied! <pre># Evaluate the ANN predictions, with respect to the values in ground_truth\nresults = test_view.evaluate_classifications(\n    \"ann_prediction\", gt_field=\"ground_truth\", eval_key=\"eval_simple\"\n)\n\n# Display the classification metrics\nresults.print_report()\n</pre> # Evaluate the ANN predictions, with respect to the values in ground_truth results = test_view.evaluate_classifications(     \"ann_prediction\", gt_field=\"ground_truth\", eval_key=\"eval_simple\" )  # Display the classification metrics results.print_report() <pre>              precision    recall  f1-score   support\n\n    0 - zero       0.87      0.98      0.92       219\n     1 - one       0.94      0.98      0.96       287\n     2 - two       0.87      0.72      0.79       276\n   3 - three       0.81      0.87      0.84       254\n    4 - four       0.84      0.92      0.88       275\n    5 - five       0.76      0.77      0.77       221\n     6 - six       0.94      0.91      0.93       225\n   7 - seven       0.83      0.81      0.82       257\n   8 - eight       0.95      0.91      0.93       242\n    9 - nine       0.94      0.87      0.90       244\n\n    accuracy                           0.87      2500\n   macro avg       0.88      0.87      0.87      2500\nweighted avg       0.88      0.87      0.87      2500\n\n</pre> <p>After performing the evaluation in FiftyOne, we can use the <code>results</code> object to generate an interactive confusion matrix allowing us to click on cells and automatically update the App to show the corresponding samples.</p> In\u00a0[37]: Copied! <pre>plot = results.plot_confusion_matrix()\nplot.show()\n</pre> plot = results.plot_confusion_matrix() plot.show() In\u00a0[\u00a0]: Copied! <pre>session.plots.attach(plot)\n</pre> session.plots.attach(plot) <p>Let's dig in a bit further. We can use the sophisticated query language of FiftyOne to easily find all predictions that did not match the ground truth, yet were predicted with high confidence. These will generally be the most confusing samples for the dataset and the ones from which we can gather the most insight.</p> In\u00a0[3]: Copied! <pre>from fiftyone import ViewField as F\n\n# Display results in the FiftyOne App, but include only the wrong predictions that were\n# predicted with high confidence\nfalse_view = (\n    test_view\n    .match(F(\"eval_simple\") == False)\n    .filter_labels(\"ann_prediction\", F(\"confidence\") &gt; 0.7)\n)\n</pre> from fiftyone import ViewField as F  # Display results in the FiftyOne App, but include only the wrong predictions that were # predicted with high confidence false_view = (     test_view     .match(F(\"eval_simple\") == False)     .filter_labels(\"ann_prediction\", F(\"confidence\") &gt; 0.7) ) In\u00a0[4]: Copied! <pre>session = fo.launch_app(false_view)\n</pre> session = fo.launch_app(false_view) Activate In\u00a0[5]: Copied! <pre>session.freeze()  # screenshot the App\n</pre> session.freeze()  # screenshot the App <p>These are the most confusing samples for the model and, as you can see, they are fairly irregular compared to other images in the dataset. A next step we could take to improve the performance of the model could be to use FiftyOne to curate additional samples similar to these. From there, those samples can then be annotated through the integrations between FiftyOne and tools like CVAT and Labelbox. Additionally, we could use some more vectors for training or just perform a fine-tuning of the model with similarity learning, for example using the triplet loss. But right now this example of using FiftyOne and Qdrant for vector similarity classification is working pretty well already.</p> <p>And that's it! As simple as that, we created an ANN classification model using FiftyOne with Qdrant as an embeddings backend, so finding the similarity between vectors can stop being a bottleneck as it would in the case of a traditional k-NN.</p> In\u00a0[8]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n# The path to the source files that you manually downloaded\nsource_dir = \"/path/to/dir-with-bdd100k-files\"\n\ndataset = foz.load_zoo_dataset(\n    \"bdd100k\",\n    split=\"validation\",\n    source_dir=source_dir,\n    max_samples=1000,\n)\n</pre> import fiftyone as fo import fiftyone.zoo as foz  # The path to the source files that you manually downloaded source_dir = \"/path/to/dir-with-bdd100k-files\"  dataset = foz.load_zoo_dataset(     \"bdd100k\",     split=\"validation\",     source_dir=source_dir,     max_samples=1000, ) <pre>Split 'validation' already prepared\nLoading 'bdd100k' split 'validation'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [21.4s elapsed, 0s remaining, 48.2 samples/s]      \nDataset 'bdd100k-validation-1000' created\n</pre> <p>Let's split this dataset so that 30% of the samples are missing the <code>timeofday</code> classification. We will then compute this classification using nearest neighbors.</p> In\u00a0[9]: Copied! <pre>import fiftyone.utils.random as four\n\nfour.random_split(dataset, {\"train\": 0.7, \"test\": 0.3})\n\ntrain_view = dataset.match_tags(\"train\")\ntest_view = dataset.match_tags(\"test\")\n\n# Remove labels from test view for this example\ntest_view.set_field(\"timeofday\", None).save()\n</pre> import fiftyone.utils.random as four  four.random_split(dataset, {\"train\": 0.7, \"test\": 0.3})  train_view = dataset.match_tags(\"train\") test_view = dataset.match_tags(\"test\")  # Remove labels from test view for this example test_view.set_field(\"timeofday\", None).save() <p>Note: This is a larger model than the one used in the previous example. It is recommended this is run on a machine with GPU support or in a Colab notebook.</p> In\u00a0[10]: Copied! <pre># Load a resnet from the model zoo\nmodel = foz.load_zoo_model(\"resnet50-imagenet-torch\")\n\n# Verify that the model exposes embeddings\nprint(model.has_embeddings)\n# True\n\n# Compute embeddings for each image\ntrain_embeddings = train_view.compute_embeddings(model)\n</pre> # Load a resnet from the model zoo model = foz.load_zoo_model(\"resnet50-imagenet-torch\")  # Verify that the model exposes embeddings print(model.has_embeddings) # True  # Compute embeddings for each image train_embeddings = train_view.compute_embeddings(model) <pre>True\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 700/700 [24.8m elapsed, 0s remaining, 0.5 samples/s]    \n</pre> In\u00a0[21]: Copied! <pre>train_embeddings.shape\n</pre> train_embeddings.shape Out[21]: <pre>(700, 2048)</pre> <p>From here we can use the functions defined above to load the embeddings into Qdrant and then generate classifications for the dataset. Specifically, we are looking to generate pre-annotations of the <code>timeofday</code> label.</p> In\u00a0[11]: Copied! <pre># Compute similarity with Qdrant backend\nfob.compute_similarity(train_view, embeddings=train_embeddings, brain_key=\"bdd100k_qdrant_example\", backend=\"qdrant\")\n</pre> # Compute similarity with Qdrant backend fob.compute_similarity(train_view, embeddings=train_embeddings, brain_key=\"bdd100k_qdrant_example\", backend=\"qdrant\") In\u00a0[16]: Copied! <pre>test_embeddings = test_view.compute_embeddings(model)\n</pre> test_embeddings = test_view.compute_embeddings(model) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [10.2m elapsed, 0s remaining, 0.5 samples/s]    \n</pre> In\u00a0[23]: Copied! <pre>from tqdm import tqdm\n\npredictions = []\n# Call Qdrant to find the closest data points\nfor embedding in tqdm(test_embeddings):\n    prediction = generate_fiftyone_classification(embedding, brain_key=\"bdd100k_qdrant_example\", field=\"timeofday\")\n    predictions.append(prediction)\n\ntest_view.set_values(\"timeofday\", predictions)\n</pre> from tqdm import tqdm  predictions = [] # Call Qdrant to find the closest data points for embedding in tqdm(test_embeddings):     prediction = generate_fiftyone_classification(embedding, brain_key=\"bdd100k_qdrant_example\", field=\"timeofday\")     predictions.append(prediction)  test_view.set_values(\"timeofday\", predictions) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:09&lt;00:00, 30.38it/s]\n</pre> <p>Now we can pull up the FiftyOne App to inspect the <code>timeofday</code> labels on the test set to inspect how the classifier performed.</p> In\u00a0[24]: Copied! <pre># Inspect how the annotations look\nsession = fo.launch_app(test_view)\n</pre> # Inspect how the annotations look session = fo.launch_app(test_view) Activate In\u00a0[26]: Copied! <pre>session.show()\n</pre> session.show() Activate In\u00a0[27]: Copied! <pre>session.freeze()  # screenshot the App\n</pre> session.freeze()  # screenshot the App <p>From here, there are multiple ways forward to further refine the preannotations. Two of these paths are:</p> <ul> <li>Bringing in a QA team to use the FiftyOne App to tag mistakes</li> <li>Using the mistakenness feature of the FiftyOne Brain to automatically detect potential annotation mistakes</li> </ul> <p>Either way, these preannotations can then easily be passed to the annotation tools that FiftyOne integrates with like CVAT, Labelbox, etc. for further refinement.</p> In\u00a0[2]: Copied! <pre>dataset = foz.load_zoo_dataset(\"quickstart\")\nprint(dataset)\n</pre> dataset = foz.load_zoo_dataset(\"quickstart\") print(dataset) <pre>Dataset already downloaded\nLoading 'quickstart'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [1.7s elapsed, 0s remaining, 115.5 samples/s]         \nDataset 'quickstart' created\nName:        quickstart\nMedia type:  image\nNum samples: 200\nPersistent:  False\nTags:        []\nSample fields:\n    id:           fiftyone.core.fields.ObjectIdField\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n    uniqueness:   fiftyone.core.fields.FloatField\n    predictions:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n</pre> <p>Note: Each of the following sections can be run independently of each other and in any order.</p> In\u00a0[3]: Copied! <pre>fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",      # zoo model used (if applicable)\n    embeddings=False,                   # don't compute embeddings\n    collection_name=\"your-collection\",  # the existing Qdrant collection\n    brain_key=\"existing_qdrant_index\",\n    backend=\"qdrant\",\n)\n</pre> fob.compute_similarity(     dataset,     model=\"clip-vit-base32-torch\",      # zoo model used (if applicable)     embeddings=False,                   # don't compute embeddings     collection_name=\"your-collection\",  # the existing Qdrant collection     brain_key=\"existing_qdrant_index\",     backend=\"qdrant\", ) Out[3]: <pre>&lt;fiftyone.brain.internal.core.qdrant.QdrantSimilarityIndex at 0x7fb4aa2d4160&gt;</pre> In\u00a0[4]: Copied! <pre>import numpy as np\n\nqdrant_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"add_or_remove_qdrant_embeddings\",\n    backend=\"qdrant\",\n)\n\nprint(\"Starting Index Size: \" + str(qdrant_index.total_index_size))  # 200\n\nview = dataset.take(10)\nids = view.values(\"id\")\n\n# Delete 10 samples from a dataset\ndataset.delete_samples(view)\n\n# Delete the corresponding vectors from the index\nqdrant_index.remove_from_index(sample_ids=ids)\n\nprint(\"Post Delete Index Size: \" + str(qdrant_index.total_index_size))  # 190\n\n# Add 20 samples to a dataset\nsamples = [fo.Sample(filepath=\"tmp%d.jpg\" % i) for i in range(20)]\nsample_ids = dataset.add_samples(samples)\n\n# Add corresponding embeddings to the index\nembeddings = np.random.rand(20, 512)\nqdrant_index.add_to_index(embeddings, sample_ids)\n\nprint(\"Post Addition Index Size: \" + str(qdrant_index.total_index_size))  # 210\n</pre> import numpy as np  qdrant_index = fob.compute_similarity(     dataset,     model=\"clip-vit-base32-torch\",     brain_key=\"add_or_remove_qdrant_embeddings\",     backend=\"qdrant\", )  print(\"Starting Index Size: \" + str(qdrant_index.total_index_size))  # 200  view = dataset.take(10) ids = view.values(\"id\")  # Delete 10 samples from a dataset dataset.delete_samples(view)  # Delete the corresponding vectors from the index qdrant_index.remove_from_index(sample_ids=ids)  print(\"Post Delete Index Size: \" + str(qdrant_index.total_index_size))  # 190  # Add 20 samples to a dataset samples = [fo.Sample(filepath=\"tmp%d.jpg\" % i) for i in range(20)] sample_ids = dataset.add_samples(samples)  # Add corresponding embeddings to the index embeddings = np.random.rand(20, 512) qdrant_index.add_to_index(embeddings, sample_ids)  print(\"Post Addition Index Size: \" + str(qdrant_index.total_index_size))  # 210 <pre>Computing embeddings...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [15.6s elapsed, 0s remaining, 13.6 samples/s]      \nStarting Index Size: 200\nPost Delete Index Size: 190\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [5.5ms elapsed, 0s remaining, 3.6K samples/s]       \nPost Addition Index Size: 210\n</pre> <p>Since we modified this dataset, we're going to reload it so that it works as expected for the remaining examples.</p> In\u00a0[5]: Copied! <pre>dataset = foz.load_zoo_dataset(\"quickstart\", drop_existing_dataset=True)\n</pre> dataset = foz.load_zoo_dataset(\"quickstart\", drop_existing_dataset=True) <pre>Dataset already downloaded\nDeleting existing dataset 'quickstart'\nLoading 'quickstart'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [1.8s elapsed, 0s remaining, 108.5 samples/s]         \nDataset 'quickstart' created\n</pre> In\u00a0[6]: Copied! <pre>qdrant_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"qdrant_index\",\n    backend=\"qdrant\",\n)\n\n# Retrieve embeddings for the entire dataset\nids = dataset.values(\"id\")\nembeddings, sample_ids, _ = qdrant_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (200, 512)\nprint(sample_ids.shape)  # (200,)\n\n# Retrieve embeddings for a view\nids = dataset.take(10).values(\"id\")\nembeddings, sample_ids, _ = qdrant_index.get_embeddings(sample_ids=ids)\nprint(embeddings.shape)  # (10, 512)\nprint(sample_ids.shape)  # (10,)\n</pre> qdrant_index = fob.compute_similarity(     dataset,     model=\"clip-vit-base32-torch\",     brain_key=\"qdrant_index\",     backend=\"qdrant\", )  # Retrieve embeddings for the entire dataset ids = dataset.values(\"id\") embeddings, sample_ids, _ = qdrant_index.get_embeddings(sample_ids=ids) print(embeddings.shape)  # (200, 512) print(sample_ids.shape)  # (200,)  # Retrieve embeddings for a view ids = dataset.take(10).values(\"id\") embeddings, sample_ids, _ = qdrant_index.get_embeddings(sample_ids=ids) print(embeddings.shape)  # (10, 512) print(sample_ids.shape)  # (10,) <pre>Computing embeddings...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [15.6s elapsed, 0s remaining, 12.2 samples/s]      \n(200, 512)\n(200,)\n(10, 512)\n(10,)\n</pre> In\u00a0[7]: Copied! <pre>qdrant_index = fob.compute_similarity(\n    dataset,\n    model=\"clip-vit-base32-torch\",\n    brain_key=\"access_qdrant_client\",\n    backend=\"qdrant\",\n)\n\nqdrant_client = qdrant_index.client\nprint(qdrant_client)\nprint(qdrant_client.get_collections())\n</pre> qdrant_index = fob.compute_similarity(     dataset,     model=\"clip-vit-base32-torch\",     brain_key=\"access_qdrant_client\",     backend=\"qdrant\", )  qdrant_client = qdrant_index.client print(qdrant_client) print(qdrant_client.get_collections()) <pre>Computing embeddings...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [15.5s elapsed, 0s remaining, 13.2 samples/s]      \n&lt;qdrant_client.qdrant_client.QdrantClient object at 0x7fb4e8f2c550&gt;\ncollections=[CollectionDescription(name='fiftyone-quickstart'), CollectionDescription(name='fiftyone-quickstart-0czljr'), CollectionDescription(name='fiftyone-quickstart-9x04wr'), CollectionDescription(name='fiftyone-bdd100k-validation-1000-9dll7r'), CollectionDescription(name='fiftyone_docs'), CollectionDescription(name='fiftyone-quickstart-40zten'), CollectionDescription(name='fiftyone-quickstart-183n07'), CollectionDescription(name='fiftyone-mnist-2500'), CollectionDescription(name='fiftyone-bdd100k-validation-1000'), CollectionDescription(name='fiftyone-mnist-2500-ais83e'), CollectionDescription(name='fiftyone-quickstart-jg78it'), CollectionDescription(name='fiftyone-quickstart-c2w0tp'), CollectionDescription(name='fiftyone-quickstart-bsjo0g'), CollectionDescription(name='fiftyone-mnist-2500-sipg85'), CollectionDescription(name='fiftyone-quickstart-tlu0iq'), CollectionDescription(name='fiftyone-quickstart-9jv2wl')]\n</pre>"},{"location":"tutorials/qdrant/#nearest-neighbor-embeddings-classification-with-qdrant","title":"Nearest Neighbor Embeddings Classification with Qdrant\u00b6","text":"<p>FiftyOne provides powerful workflows centered around embeddings, including pre-annotation, finding annotation mistakes, finding hard samples, and visual similarity searches. However, performing nearest neighbor searches on large datasets requires the right infrastructure.</p> <p>Vector search engines have been developed for the purpose of efficiently storing, searching, and managing embedding vectors. Qdrant is a vector database designed to perform an approximate nearest neighbor search (ANN) on dense neural embeddings, which is a key part of any production-ready system that is expected to scale to large amounts of data. And best of all, it's open-source!</p> <p>In this tutorial, we'll load the MNIST dataset into FiftyOne and then use Qdrant to perform ANN-based classification, where the data points will be classified by selecting the most common ground truth label among the K nearest points from our training dataset. In other words, for each test example, we'll select the K nearest neighbors in embedding space and assign the best label by voting. We'll then evaluate the results of this classification strategy using FiftyOne.</p> <p>So, what's the takeaway?</p> <p>FiftyOne and Qdrant can be used together to easily perform an approximate nearest neighbors search on the embeddings of your datasets and kickstart pre-annotation workflows.</p>"},{"location":"tutorials/qdrant/#setup","title":"Setup\u00b6","text":"<p>If you haven\u2019t already, install FiftyOne:</p>"},{"location":"tutorials/qdrant/#qdrant-installation","title":"Qdrant installation\u00b6","text":"<p>If you want to start using the semantic search with Qdrant, you need to run an instance of it, as this tool works in a client-server manner. The easiest way to do this is to use an official Docker image and start Qdrant with just a single command:</p>"},{"location":"tutorials/qdrant/#loading-the-dataset","title":"Loading the dataset\u00b6","text":"<p>There are several steps we need to take to get things running smoothly. First of all, we need to load the MNIST dataset and extract the train examples from it, as we're going to use them in our search operations. To make everything even faster, we're not going to use all the examples, but just 2500 samples. We can use the FiftyOne Dataset Zoo to load the subset of MNIST we want in just one line of code.</p>"},{"location":"tutorials/qdrant/#generating-embeddings-and-loading-into-qdrant","title":"Generating embeddings and loading into Qdrant\u00b6","text":"<p>The next step is to generate embeddings on the samples in the dataset. This can always be done outside of FiftyOne, with your own custom models. However, FiftyOne also provides various different models in the FiftyOne Model Zoo that can be used right out of the box to generate embeddings.</p> <p>In this example, we use MobileNetv2 trained on ImageNet to compute an embedding for each image.</p>"},{"location":"tutorials/qdrant/#nearest-neighbor-classification","title":"Nearest neighbor classification\u00b6","text":"<p>Now to perform inference on the dataset. We can create the embeddings for our test dataset, but just ignore the ground truth and try to find it out using ANN, then compare if both match. Let's take one step at a time and start with creating the embeddings.</p>"},{"location":"tutorials/qdrant/#evaluation-in-fiftyone","title":"Evaluation in FiftyOne\u00b6","text":"<p>It's high time for some results! Let's start by visualizing how this classifier has performed. We can easily launch the FiftyOne App to view the ground truth, predictions, and images themselves.</p>"},{"location":"tutorials/qdrant/#bdd100k-example","title":"BDD100k example\u00b6","text":"<p>Let's take everything we learned in the previous example and apply it to a more realisitc use-case. In this section, we take a look at how to use nearest neighbor embedding classification for pre-annotation of the BDD100k road-scene dataset to apply a scene-level label determining the time of day.</p> <p>This dataset is also available in the FiftyOne Dataset Zoo. If you want to follow along youself, you will need to register at https://bdd-data.berkeley.edu in order to get links to download the data. See the zoo docs for details on loading the dataset.</p> <p>We\u2019ll be working with a 1,000 image subset of the validation split:</p>"},{"location":"tutorials/qdrant/#other-ways-to-interact-with-qdrant","title":"Other ways to interact with Qdrant\u00b6","text":"<p>In these examples, we have shown you how to compute embeddings and load them into Qdrant using compute_similarity. However, you can do many other things with the Qdrant integration as well!</p> <p>To quickly show you some of this functionality, we'll use th quickstart dataset which contains 200 images.</p>"},{"location":"tutorials/qdrant/#use-an-existing-index","title":"Use an existing index\u00b6","text":"<p>Suppose you already have a Qdrant collection storing the embedding vectors you need for the samples or patches in your dataset. You can connect to them directly by passing in the <code>collection_name</code> parameter to compute_similarity().</p>"},{"location":"tutorials/qdrant/#add-or-remove-embeddings-from-an-index","title":"Add or remove embeddings from an index\u00b6","text":"<p>Now suppose you have made changes to your dataset that involve removing or adding samples. You would need to then modify your embeddings in Qdrant to also reflect these changes. You can use add_to_index() and remove_from_index() to add or remove embeddings from an existing Qdrant index.</p>"},{"location":"tutorials/qdrant/#retrieve-embeddings-from-an-index","title":"Retrieve embeddings from an index\u00b6","text":"<p>Do you have embeddings already calculated on a dataset that you want to retrieve in full? Or perhaps you have a view on your dataset that you want embeddings for? Use the function get_embeddings() to get your embeddings from a Qdrant index by ID.</p>"},{"location":"tutorials/qdrant/#access-the-qdrant-client-directly","title":"Access the Qdrant client directly\u00b6","text":"<p>While FiftyOne provides many ways to interact with Qdrant, sometimes you just need to connect directly to get your task done. Thus, you can directly access the Qdrant client instance by using the <code>client</code> property of a Qdrant index. From there, you can use the Qdrant methods as desired.</p>"},{"location":"tutorials/qdrant/#and-more","title":"And more!\u00b6","text":"<p>To read up on everything you can do with the FiftyOne Qdrant integration, check out the Qdrant integration page in our docs!</p>"},{"location":"tutorials/qdrant/#summary","title":"Summary\u00b6","text":"<p>FiftyOne and Qdrant can be used together to efficiently perform a nearest neighbor search on embeddings and act on the results on your image and video datasets. The beauty of this process lies in its flexibility and repeatability. You can easily load additional ground truth labels for new fields into both FiftyOne and Qdrant and repeat this pre-annotation process using the existing embeddings. This can quickly cut down on annotation costs and result in higher-quality datasets, faster.</p>"},{"location":"tutorials/query_flickr/","title":"Query flickr","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nSimple utility to download images from Flickr based on a text query.\n\nRequires a user-specified API key, which can be obtained for free at\nhttps://www.flickr.com/services/apps/create.\n\nCopyright 2017-2024, Voxel51, Inc.\nvoxel51.com\n\"\"\"\nimport argparse\nfrom itertools import takewhile\nimport os\n</pre> \"\"\" Simple utility to download images from Flickr based on a text query.  Requires a user-specified API key, which can be obtained for free at https://www.flickr.com/services/apps/create.  Copyright 2017-2024, Voxel51, Inc. voxel51.com \"\"\" import argparse from itertools import takewhile import os In\u00a0[\u00a0]: Copied! <pre>import flickrapi\n</pre> import flickrapi In\u00a0[\u00a0]: Copied! <pre>import eta.core.storage as etas\n</pre> import eta.core.storage as etas In\u00a0[\u00a0]: Copied! <pre>def query_flickr(\n    key, secret, query, number=50, path=\"data\", query_in_path=True\n):\n    # Flickr api access key\n    flickr = flickrapi.FlickrAPI(key, secret, cache=True)\n\n    # could also query by tags and tag_mode='all'\n    photos = flickr.walk(\n        text=query, extras=\"url_c\", per_page=50, sort=\"relevance\"\n    )\n\n    urls = []\n    for photo in takewhile(lambda _: len(urls) &lt; number, photos):\n        url = photo.get(\"url_c\")\n        if url is not None:\n            urls.append(url)\n\n    if query_in_path:\n        basedir = os.path.join(path, query)\n    else:\n        basedir = path\n\n    print(\n        \"Downloading %d images matching query '%s' to '%s'\"\n        % (len(urls), query, basedir)\n    )\n    client = etas.HTTPStorageClient()\n    for url in urls:\n        outpath = os.path.join(basedir, client.get_filename(url))\n        client.download(url, outpath)\n        print(\"Downloading image to '%s'\" % outpath)\n</pre> def query_flickr(     key, secret, query, number=50, path=\"data\", query_in_path=True ):     # Flickr api access key     flickr = flickrapi.FlickrAPI(key, secret, cache=True)      # could also query by tags and tag_mode='all'     photos = flickr.walk(         text=query, extras=\"url_c\", per_page=50, sort=\"relevance\"     )      urls = []     for photo in takewhile(lambda _: len(urls) &lt; number, photos):         url = photo.get(\"url_c\")         if url is not None:             urls.append(url)      if query_in_path:         basedir = os.path.join(path, query)     else:         basedir = path      print(         \"Downloading %d images matching query '%s' to '%s'\"         % (len(urls), query, basedir)     )     client = etas.HTTPStorageClient()     for url in urls:         outpath = os.path.join(basedir, client.get_filename(url))         client.download(url, outpath)         print(\"Downloading image to '%s'\" % outpath) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(add_help=True)\n\n    parser.add_argument(\"key\", type=str, help=\"Flickr API key\")\n    parser.add_argument(\"secret\", type=str, help=\"Secret to Flickr API key\")\n    parser.add_argument(\"query\", type=str, help=\"Query string to use\")\n    parser.add_argument(\n        \"-n\",\n        \"--number\",\n        type=int,\n        default=50,\n        help=\"number of images to download (default: 50)\",\n    )\n    parser.add_argument(\n        \"-p\",\n        \"--path\",\n        type=str,\n        default=\"data\",\n        help=\"path to download the images (created if needed)\",\n    )\n    parser.add_argument(\n        \"--query-in-path\", \"-i\", dest=\"query_in_path\", action=\"store_true\"\n    )\n    parser.add_argument(\n        \"--no-query-in-path\", dest=\"query_in_path\", action=\"store_false\"\n    )\n    parser.set_defaults(query_in_path=True)\n\n    args = parser.parse_args()\n\n    query_flickr(\n        key=args.key,\n        secret=args.secret,\n        query=args.query,\n        number=args.number,\n        path=args.path,\n        query_in_path=args.query_in_path,\n    )\n</pre> if __name__ == \"__main__\":     parser = argparse.ArgumentParser(add_help=True)      parser.add_argument(\"key\", type=str, help=\"Flickr API key\")     parser.add_argument(\"secret\", type=str, help=\"Secret to Flickr API key\")     parser.add_argument(\"query\", type=str, help=\"Query string to use\")     parser.add_argument(         \"-n\",         \"--number\",         type=int,         default=50,         help=\"number of images to download (default: 50)\",     )     parser.add_argument(         \"-p\",         \"--path\",         type=str,         default=\"data\",         help=\"path to download the images (created if needed)\",     )     parser.add_argument(         \"--query-in-path\", \"-i\", dest=\"query_in_path\", action=\"store_true\"     )     parser.add_argument(         \"--no-query-in-path\", dest=\"query_in_path\", action=\"store_false\"     )     parser.set_defaults(query_in_path=True)      args = parser.parse_args()      query_flickr(         key=args.key,         secret=args.secret,         query=args.query,         number=args.number,         path=args.path,         query_in_path=args.query_in_path,     )"},{"location":"tutorials/small_object_detection/","title":"Detecting Small Objects with SAHI","text":"<p>Object detection is one of the fundamental tasks in computer vision, but detecting small objects can be particularly challenging.</p> <p>In this walkthrough, you'll learn how to use a technique called SAHI (Slicing Aided Hyper Inference) in conjunction with state-of-the-art object detection models to improve the detection of small objects. We'll apply SAHI with Ultralytics' YOLOv8 model to detect small objects in the VisDrone dataset, and then evaluate these predictions to better understand how slicing impacts detection performance.</p> <p>It covers the following:</p> <ul> <li>Loading the VisDrone dataset from the Hugging Face Hub</li> <li>Applying Ultralytics' YOLOv8 model to the dataset</li> <li>Using SAHI to run inference on slices of the images</li> <li>Evaluating model performance with and without SAHI</li> </ul> <p>For this walkthrough, we'll be using the following libraries:</p> <ul> <li><code>fiftyone</code> for dataset exploration and manipulation</li> <li><code>huggingface_hub</code> for loading the VisDrone dataset</li> <li><code>ultralytics</code> for running object detection with YOLOv8</li> <li><code>sahi</code> for slicing aided hyper inference</li> </ul> <p>If you haven't already, install the latest versions of these libraries:</p> In\u00a0[62]: Copied! <pre>pip install -U fiftyone sahi ultralytics huggingface_hub --quiet\n</pre> pip install -U fiftyone sahi ultralytics huggingface_hub --quiet <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> <p>Let's get started! \ud83d\ude80</p> <p>First, import the necessary modules from FiftyOne:</p> In\u00a0[1]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.utils.huggingface as fouh\nfrom fiftyone import ViewField as F\n</pre> import fiftyone as fo import fiftyone.zoo as foz import fiftyone.utils.huggingface as fouh from fiftyone import ViewField as F <p>Now, let's download some data. We'll be taking advantage of FiftyOne's Hugging Face Hub integration to load a subset of the VisDrone dataset directly from the Hugging Face Hub:</p> In\u00a0[17]: Copied! <pre>dataset = fouh.load_from_hub(\"Voxel51/VisDrone2019-DET\", name=\"sahi-test\", max_samples=100, overwrite=True)\n</pre> dataset = fouh.load_from_hub(\"Voxel51/VisDrone2019-DET\", name=\"sahi-test\", max_samples=100, overwrite=True) <pre>Downloading config file fiftyone.yml from Voxel51/VisDrone2019-DET\nLoading dataset\nImporting samples...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [33.1ms elapsed, 0s remaining, 3.0K samples/s]      \n</pre> <p>Before adding any predictions, let's take a look at the dataset:</p> In\u00a0[22]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) <pre>Session launched. Run `session.show()` to open the App in a cell output.\n</pre> <p></p> <p>Now that we know what our data looks like, let's run our standard inference pipeline with a YOLOv8 (large-variant) model. We can load the model from Ultralytics and then apply this directly to our FiftyOne dataset using <code>apply_model()</code>, thanks to FiftyOne's Ultralytics integration:</p> In\u00a0[26]: Copied! <pre>from ultralytics import YOLO\n\nckpt_path = \"yolov8l.pt\"\nmodel = YOLO(ckpt_path)\n## fiftyone will work directly with the Ultralytics.YOLO model\n\ndataset.apply_model(model, label_field=\"base_model\")\n</pre> from ultralytics import YOLO  ckpt_path = \"yolov8l.pt\" model = YOLO(ckpt_path) ## fiftyone will work directly with the Ultralytics.YOLO model  dataset.apply_model(model, label_field=\"base_model\") <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [25.0s elapsed, 0s remaining, 4.0 samples/s]      \n</pre> <p>Alternatively, if we want FiftyOne to handle the model downloading and location management for us, we can load the same model directly from the FiftyOne Model Zoo:</p> In\u00a0[\u00a0]: Copied! <pre>## comment this out if you want to use the model from the zoo\n# model = foz.load_zoo_model(\"yolov8l-coco-torch\")\n# ckpt_path = model.config.model_path\n# dataset.apply_model(model, label_field=\"base_model\")\n</pre> ## comment this out if you want to use the model from the zoo # model = foz.load_zoo_model(\"yolov8l-coco-torch\") # ckpt_path = model.config.model_path # dataset.apply_model(model, label_field=\"base_model\") <p>Now that we have predictions, we can visualize them in the App:</p> In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) <p></p> <p>Looking at the model's predictions next to the ground truth, we can see a few things.</p> <p>First and foremost, the classes detected by our YOLOv8l model are different from the ground truth classes in the VisDrone dataset. Our YOLO model was trained on the COCO dataset, which has 80 classes, while the VisDrone dataset has 12 classes, including an <code>ignore_regions</code> class. To simplify the comparison, we'll focus on just the few most common classes in the dataset, and will map the VisDrone classes to the COCO classes as follows:</p> In\u00a0[6]: Copied! <pre>mapping = {\"pedestrians\": \"person\", \"people\": \"person\", \"van\": \"car\"}\nmapped_view = dataset.map_labels(\"ground_truth\", mapping)\n</pre> mapping = {\"pedestrians\": \"person\", \"people\": \"person\", \"van\": \"car\"} mapped_view = dataset.map_labels(\"ground_truth\", mapping) <p>And then filter our labels to only include the classes we're interested in:</p> In\u00a0[20]: Copied! <pre>def get_label_fields(sample_collection):\n    \"\"\"Get the (detection) label fields of a Dataset or DatasetView.\"\"\"\n    label_fields = list(\n        sample_collection.get_field_schema(embedded_doc_type=fo.Detections).keys()\n    )\n    return label_fields\n\ndef filter_all_labels(sample_collection):\n    label_fields = get_label_fields(sample_collection)\n\n    filtered_view = sample_collection\n\n    for lf in label_fields:\n        filtered_view = filtered_view.filter_labels(\n            lf, F(\"label\").is_in([\"person\", \"car\", \"truck\"]), only_matches=False\n        )\n    return filtered_view\n</pre> def get_label_fields(sample_collection):     \"\"\"Get the (detection) label fields of a Dataset or DatasetView.\"\"\"     label_fields = list(         sample_collection.get_field_schema(embedded_doc_type=fo.Detections).keys()     )     return label_fields  def filter_all_labels(sample_collection):     label_fields = get_label_fields(sample_collection)      filtered_view = sample_collection      for lf in label_fields:         filtered_view = filtered_view.filter_labels(             lf, F(\"label\").is_in([\"person\", \"car\", \"truck\"]), only_matches=False         )     return filtered_view In\u00a0[51]: Copied! <pre>filtered_view = filter_all_labels(mapped_view)\n</pre> filtered_view = filter_all_labels(mapped_view) In\u00a0[52]: Copied! <pre>session.view = filtered_view.view()\n</pre> session.view = filtered_view.view() <p></p> <p>Now that the classes are aligned and we've reduced the crowding in our images, we can see that while the model does a pretty good job of detecting objects, it struggles with the small objects, especially people in the distance. This can happen with large images, as most detection models are trained on fixed-size images. As an example, YOLOv8 is trained on images with maximum side length $640$. When we feed it an image of size $1920$ x $1080$, the model will downsample the image to $640$ x $360$ before making predictions. This downsampling can cause small objects to be missed, as the model may not have enough information to detect them.</p> <p>Theoretically, one could train a model on larger images to improve detection of small objects, but this would require more memory and computational power. Another option is to introduce a sliding window approach, where we split the image into smaller patches, run the model on each patch, and then combine the results. This is the idea behind Slicing Aided Hyper Inference (SAHI), which we'll use to improve the detection of small objects in the VisDrone dataset!</p> Illustration of Slicing Aided Hyper Inference. Image courtesy of SAHI Github Repo. <p>The SAHI technique is implemented in the <code>sahi</code> Python package that we installed earlier. SAHI is a framework which is compatible with many object detection models, including YOLOv8. We can choose the detection model we want to use and create an instance of any of the classes that subclass <code>sahi.models.DetectionModel</code>, including YOLOv8, YOLOv5, and even Hugging Face Transformers models. We will create our model object using SAHI's <code>AutoDetectionModel</code> class, specifying the model type and the path to the checkpoint file:\\</p> In\u00a0[7]: Copied! <pre>from sahi import AutoDetectionModel\nfrom sahi.predict import get_prediction, get_sliced_prediction\n</pre> from sahi import AutoDetectionModel from sahi.predict import get_prediction, get_sliced_prediction In\u00a0[11]: Copied! <pre>detection_model = AutoDetectionModel.from_pretrained(\n    model_type='yolov8',\n    model_path=ckpt_path,\n    confidence_threshold=0.25, ## same as the default value for our base model\n    image_size=640,\n    device=\"cpu\", # or 'cuda'\n)\n</pre> detection_model = AutoDetectionModel.from_pretrained(     model_type='yolov8',     model_path=ckpt_path,     confidence_threshold=0.25, ## same as the default value for our base model     image_size=640,     device=\"cpu\", # or 'cuda' ) <p>Before we generate sliced predictions, let's inspect the model's predictions on a trial image, using SAHI's <code>get_prediction()</code> function:</p> In\u00a0[60]: Copied! <pre>result = get_prediction(dataset.first().filepath, detection_model, verbose=0)\nprint(result)\n</pre> result = get_prediction(dataset.first().filepath, detection_model, verbose=0) print(result) <pre>&lt;sahi.prediction.PredictionResult object at 0x2b0e9c250&gt;\n</pre> <p>Fortunately, SAHI results objects have a <code>to_fiftyone_detections()</code> method, which converts the results to FiftyOne detections:</p> In\u00a0[61]: Copied! <pre>print(result.to_fiftyone_detections())\n</pre> print(result.to_fiftyone_detections()) <pre>[&lt;Detection: {\n    'id': '661858c20ae3edf77139db7a',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.6646394729614258,\n        0.7850866247106482,\n        0.06464214324951172,\n        0.09088355170355902,\n    ],\n    'mask': None,\n    'confidence': 0.8933132290840149,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db7b',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.6196376800537109,\n        0.7399617513020833,\n        0.06670347849527995,\n        0.09494832356770834,\n    ],\n    'mask': None,\n    'confidence': 0.8731599450111389,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db7c',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.5853352228800456,\n        0.7193766276041667,\n        0.06686935424804688,\n        0.07682359483506944,\n    ],\n    'mask': None,\n    'confidence': 0.8595829606056213,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db7d',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.5635160446166992,\n        0.686444091796875,\n        0.06365642547607422,\n        0.06523607042100694,\n    ],\n    'mask': None,\n    'confidence': 0.854781448841095,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db7e',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.7365047454833984,\n        0.8709894816080729,\n        0.07815799713134766,\n        0.06583930121527778,\n    ],\n    'mask': None,\n    'confidence': 0.8482972383499146,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db7f',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.4387975692749023,\n        0.7973368326822917,\n        0.07478656768798828,\n        0.08685709635416666,\n    ],\n    'mask': None,\n    'confidence': 0.8482537865638733,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db80',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.41441831588745115,\n        0.7553463971173322,\n        0.07797966003417969,\n        0.09232432047526042,\n    ],\n    'mask': None,\n    'confidence': 0.8444766402244568,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db81',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.4094355583190918,\n        0.7256359524197049,\n        0.07238206863403321,\n        0.07048272026909722,\n    ],\n    'mask': None,\n    'confidence': 0.798665463924408,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db82',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.5339123407999674,\n        0.6121687712492766,\n        0.07190316518147787,\n        0.07292734781901042,\n    ],\n    'mask': None,\n    'confidence': 0.7936845421791077,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db83',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.03444666067759196,\n        0.5164913601345487,\n        0.03219949007034302,\n        0.06044175889756945,\n    ],\n    'mask': None,\n    'confidence': 0.740483820438385,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db84',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.3923538525899251,\n        0.6745626378942419,\n        0.06798810958862304,\n        0.07528584798177083,\n    ],\n    'mask': None,\n    'confidence': 0.6714914441108704,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db85',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.5216399192810058,\n        0.5886645846896701,\n        0.06560036341349283,\n        0.059334818522135416,\n    ],\n    'mask': None,\n    'confidence': 0.6649367809295654,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db86',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.5096873283386231,\n        0.5273054334852431,\n        0.0551149050394694,\n        0.07670672381365741,\n    ],\n    'mask': None,\n    'confidence': 0.6273276805877686,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db87',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.37222995758056643,\n        0.5739804303204572,\n        0.06103067398071289,\n        0.06263699001736112,\n    ],\n    'mask': None,\n    'confidence': 0.5973840355873108,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db88',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.05044105052947998,\n        0.44830712212456597,\n        0.02773451805114746,\n        0.054146491156684025,\n    ],\n    'mask': None,\n    'confidence': 0.5668562054634094,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db89',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        0.38649218877156577,\n        0.6419422290943287,\n        0.0629791259765625,\n        0.05787251790364583,\n    ],\n    'mask': None,\n    'confidence': 0.525834858417511,\n    'index': None,\n}&gt;, &lt;Detection: {\n    'id': '661858c20ae3edf77139db8a',\n    'attributes': {},\n    'tags': [],\n    'label': 'car',\n    'bounding_box': [\n        3.7088990211486816e-05,\n        0.5483550460250289,\n        0.027724572022755942,\n        0.06541680230034722,\n    ],\n    'mask': None,\n    'confidence': 0.5090425610542297,\n    'index': None,\n}&gt;]\n</pre> <p>This makes our lives easy, so we can focus on the data and not the nitty gritty details of format conversions.</p> <p>SAHI's <code>get_sliced_prediction()</code> function works in the same way as <code>get_prediction()</code>, with a few additional hyperparameters that let us configure how the image is sliced. In particular, we can specify the slice height and width, and the overlap between slices. Here's an example:</p> In\u00a0[\u00a0]: Copied! <pre>sliced_result = get_sliced_prediction(\n    dataset.skip(40).first().filepath,\n    detection_model,\n    slice_height = 320,\n    slice_width = 320,\n    overlap_height_ratio = 0.2,\n    overlap_width_ratio = 0.2,\n)\n</pre> sliced_result = get_sliced_prediction(     dataset.skip(40).first().filepath,     detection_model,     slice_height = 320,     slice_width = 320,     overlap_height_ratio = 0.2,     overlap_width_ratio = 0.2, ) <p>As a sanity check, we can compare the number of detections in the sliced predictions to the number of detections in the original predictions:</p> In\u00a0[91]: Copied! <pre>num_sliced_dets = len(sliced_result.to_fiftyone_detections())\nnum_orig_dets = len(result.to_fiftyone_detections())\n\nprint(f\"Detections predicted without slicing: {num_orig_dets}\")\nprint(f\"Detections predicted with slicing: {num_sliced_dets}\")\n</pre> num_sliced_dets = len(sliced_result.to_fiftyone_detections()) num_orig_dets = len(result.to_fiftyone_detections())  print(f\"Detections predicted without slicing: {num_orig_dets}\") print(f\"Detections predicted with slicing: {num_sliced_dets}\") <pre>Detections predicted without slicing: 17\nDetections predicted with slicing: 73\n</pre> <p>We can see that the number of predictions increased substantially! We have yet to determine if the additional predictions are valid, or if we just have more false positives. We'll do this using FiftyOne's Evaluation API shortly. We also want to find a good set of hyperparameters for our slicing. To do all of these things, we're going to need to apply SAHI to the entire dataset. Let's do that now!</p> <p>To simplify the process, we'll define a function that adds predictions to a sample in a specified label field, and then we will iterate over the dataset, applying the function to each sample. This function will pass the sample's filepath and slicing hyperparameters to <code>get_sliced_prediction()</code>, and then add the predictions to the sample in the specified label field:</p> In\u00a0[70]: Copied! <pre>def predict_with_slicing(sample, label_field, **kwargs):\n    result = get_sliced_prediction(\n        sample.filepath, detection_model, verbose=0, **kwargs\n    )\n    sample[label_field] = fo.Detections(detections=result.to_fiftyone_detections())\n</pre> def predict_with_slicing(sample, label_field, **kwargs):     result = get_sliced_prediction(         sample.filepath, detection_model, verbose=0, **kwargs     )     sample[label_field] = fo.Detections(detections=result.to_fiftyone_detections()) <p>We'll keep the slice overlap fixed at $0.2$, and see how the slice height and width affect the quality of the predictions:</p> In\u00a0[92]: Copied! <pre>kwargs = {\"overlap_height_ratio\": 0.2, \"overlap_width_ratio\": 0.2}\n\nfor sample in dataset.iter_samples(progress=True, autosave=True):\n    predict_with_slicing(sample, label_field=\"small_slices\", slice_height=320, slice_width=320, **kwargs)\n    predict_with_slicing(sample, label_field=\"large_slices\", slice_height=480, slice_width=480, **kwargs)\n</pre> kwargs = {\"overlap_height_ratio\": 0.2, \"overlap_width_ratio\": 0.2}  for sample in dataset.iter_samples(progress=True, autosave=True):     predict_with_slicing(sample, label_field=\"small_slices\", slice_height=320, slice_width=320, **kwargs)     predict_with_slicing(sample, label_field=\"large_slices\", slice_height=480, slice_width=480, **kwargs) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [13.6m elapsed, 0s remaining, 0.1 samples/s]    \n</pre> <pre>04/12/2024 10:06:59 - INFO - eta.core.utils -    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [13.6m elapsed, 0s remaining, 0.1 samples/s]    \n</pre> <p>Note how these inference times are much longer than the original inference time. This is because we're running the model on multiple slices per image, which increases the number of forward passes the model has to make. This is a trade-off we're making to improve the detection of small objects.</p> <p>Now let's once again filter our labels to only include the classes we're interested in, and visualize the results in the FiftyOne App:</p> In\u00a0[14]: Copied! <pre>filtered_view = filter_all_labels(mapped_view)\n</pre> filtered_view = filter_all_labels(mapped_view) In\u00a0[98]: Copied! <pre>session = fo.launch_app(filtered_view, auto=False)\n</pre> session = fo.launch_app(filtered_view, auto=False) <pre>Session launched. Run `session.show()` to open the App in a cell output.\n</pre> <pre>04/12/2024 10:18:33 - INFO - fiftyone.core.session.session -   Session launched. Run `session.show()` to open the App in a cell output.\n</pre> <p></p> <p>The results certainly look promising! From a few visual examples, slicing seems to improve the coverage of ground truth detections, and smaller slices in particular seem to lead to more of the <code>person</code> detections being captured. But how can we know for sure? Let's run an evaluation routine to mark the detections as true positives, false positives, or false negatives, so that we can compare the sliced predictions to the ground truth. We'll use our filtered view's <code>evaluate_detections()</code> method to do this:</p> <p>Sticking with our filtered view of the dataset, let's run an evaluation routine comparing our predictions from each of the prediction label fields to the ground truth labels. We will use the <code>evaluate_detections()</code> method, which will mark each detection as a true positive, false positive, or false negative. Here we use the default IoU threshold of $0.5$, but you can adjust this as needed:</p> In\u00a0[\u00a0]: Copied! <pre>base_results = filtered_view.evaluate_detections(\"base_model\", gt_field=\"ground_truth\", eval_key=\"eval_base_model\")\nlarge_slice_results = filtered_view.evaluate_detections(\"large_slices\", gt_field=\"ground_truth\", eval_key=\"eval_large_slices\")\nsmall_slice_results = filtered_view.evaluate_detections(\"small_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_slices\")\n</pre> base_results = filtered_view.evaluate_detections(\"base_model\", gt_field=\"ground_truth\", eval_key=\"eval_base_model\") large_slice_results = filtered_view.evaluate_detections(\"large_slices\", gt_field=\"ground_truth\", eval_key=\"eval_large_slices\") small_slice_results = filtered_view.evaluate_detections(\"small_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_slices\") <p>Let's print a report for each:</p> In\u00a0[107]: Copied! <pre>print(\"Base model results:\")\nbase_results.print_report()\n\nprint(\"-\" * 50)\nprint(\"Large slice results:\")\nlarge_slice_results.print_report()\n\nprint(\"-\" * 50)\nprint(\"Small slice results:\")\nsmall_slice_results.print_report()\n</pre> print(\"Base model results:\") base_results.print_report()  print(\"-\" * 50) print(\"Large slice results:\") large_slice_results.print_report()  print(\"-\" * 50) print(\"Small slice results:\") small_slice_results.print_report() <pre>Base model results:\n              precision    recall  f1-score   support\n\n         car       0.81      0.55      0.66       692\n      person       0.94      0.16      0.28      7475\n       truck       0.66      0.34      0.45       265\n\n   micro avg       0.89      0.20      0.33      8432\n   macro avg       0.80      0.35      0.46      8432\nweighted avg       0.92      0.20      0.31      8432\n\n--------------------------------------------------\nLarge slice results:\n              precision    recall  f1-score   support\n\n         car       0.67      0.71      0.69       692\n      person       0.89      0.34      0.49      7475\n       truck       0.55      0.45      0.49       265\n\n   micro avg       0.83      0.37      0.51      8432\n   macro avg       0.70      0.50      0.56      8432\nweighted avg       0.86      0.37      0.51      8432\n\n--------------------------------------------------\nSmall slice results:\n              precision    recall  f1-score   support\n\n         car       0.66      0.75      0.70       692\n      person       0.84      0.42      0.56      7475\n       truck       0.49      0.46      0.47       265\n\n   micro avg       0.80      0.45      0.57      8432\n   macro avg       0.67      0.54      0.58      8432\nweighted avg       0.82      0.45      0.57      8432\n\n</pre> <p>We can see that as we introduce more slices, the number of false positives increases, while the number of false negatives decreases. This is expected, as the model is able to detect more objects with more slices, but also makes more mistakes! You could apply more agressive confidence thresholding to combat this increase in false positives, but even without doing this the $F_1$-score has significantly improved.</p> <p>Let's dive a little bit deeper into these results. We noted earlier that the model struggles with small objects, so let's see how these three approaches fare on objects smaller than $32 \\times 32$ pixels. We can perform this filtering using FiftyOne's ViewField:</p> In\u00a0[110]: Copied! <pre>## Filtering for only small boxes\n\nbox_width, box_height = F(\"bounding_box\")[2], F(\"bounding_box\")[3]\nrel_bbox_area = box_width * box_height\n\nim_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\")\nabs_area = rel_bbox_area * im_width * im_height\n\nsmall_boxes_view = filtered_view\nfor lf in get_label_fields(filtered_view):\n    small_boxes_view = small_boxes_view.filter_labels(lf, abs_area &lt; 32**2, only_matches=False)\n</pre> ## Filtering for only small boxes  box_width, box_height = F(\"bounding_box\")[2], F(\"bounding_box\")[3] rel_bbox_area = box_width * box_height  im_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\") abs_area = rel_bbox_area * im_width * im_height  small_boxes_view = filtered_view for lf in get_label_fields(filtered_view):     small_boxes_view = small_boxes_view.filter_labels(lf, abs_area &lt; 32**2, only_matches=False) In\u00a0[111]: Copied! <pre>session.view = small_boxes_view.view()\n</pre> session.view = small_boxes_view.view() <p></p> In\u00a0[112]: Copied! <pre>small_boxes_base_results = small_boxes_view.evaluate_detections(\"base_model\", gt_field=\"ground_truth\", eval_key=\"eval_small_boxes_base_model\")\nsmall_boxes_large_slice_results = small_boxes_view.evaluate_detections(\"large_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_boxes_large_slices\")\nsmall_boxes_small_slice_results = small_boxes_view.evaluate_detections(\"small_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_boxes_small_slices\")\n</pre> small_boxes_base_results = small_boxes_view.evaluate_detections(\"base_model\", gt_field=\"ground_truth\", eval_key=\"eval_small_boxes_base_model\") small_boxes_large_slice_results = small_boxes_view.evaluate_detections(\"large_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_boxes_large_slices\") small_boxes_small_slice_results = small_boxes_view.evaluate_detections(\"small_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_boxes_small_slices\") <pre>Evaluating detections...\n</pre> <pre>04/12/2024 10:54:36 - INFO - fiftyone.utils.eval.detection -   Evaluating detections...\n</pre> <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [1.1m elapsed, 0s remaining, 7.8 samples/s]       \n</pre> <pre>04/12/2024 10:55:44 - INFO - eta.core.utils -    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [1.1m elapsed, 0s remaining, 7.8 samples/s]       \n</pre> <pre>Evaluating detections...\n</pre> <pre>04/12/2024 10:55:44 - INFO - fiftyone.utils.eval.detection -   Evaluating detections...\n</pre> <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [1.2m elapsed, 0s remaining, 6.2 samples/s]       \n</pre> <pre>04/12/2024 10:56:59 - INFO - eta.core.utils -    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [1.2m elapsed, 0s remaining, 6.2 samples/s]       \n</pre> <pre>Evaluating detections...\n</pre> <pre>04/12/2024 10:56:59 - INFO - fiftyone.utils.eval.detection -   Evaluating detections...\n</pre> <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [1.4m elapsed, 0s remaining, 5.7 samples/s]       \n</pre> <pre>04/12/2024 10:58:23 - INFO - eta.core.utils -    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [1.4m elapsed, 0s remaining, 5.7 samples/s]       \n</pre> In\u00a0[113]: Copied! <pre>print(\"Small Box \u2014 Base model results:\")\nsmall_boxes_base_results.print_report()\n\nprint(\"-\" * 50)\nprint(\"Small Box \u2014 Large slice results:\")\nsmall_boxes_large_slice_results.print_report()\n\nprint(\"-\" * 50)\nprint(\"Small Box \u2014 Small slice results:\")\nsmall_boxes_small_slice_results.print_report()\n</pre> print(\"Small Box \u2014 Base model results:\") small_boxes_base_results.print_report()  print(\"-\" * 50) print(\"Small Box \u2014 Large slice results:\") small_boxes_large_slice_results.print_report()  print(\"-\" * 50) print(\"Small Box \u2014 Small slice results:\") small_boxes_small_slice_results.print_report() <pre>Small Box \u2014 Base model results:\n              precision    recall  f1-score   support\n\n         car       0.71      0.25      0.37       147\n      person       0.83      0.08      0.15      5710\n       truck       0.00      0.00      0.00        28\n\n   micro avg       0.82      0.08      0.15      5885\n   macro avg       0.51      0.11      0.17      5885\nweighted avg       0.82      0.08      0.15      5885\n\n--------------------------------------------------\nSmall Box \u2014 Large slice results:\n              precision    recall  f1-score   support\n\n         car       0.46      0.48      0.47       147\n      person       0.82      0.23      0.35      5710\n       truck       0.20      0.07      0.11        28\n\n   micro avg       0.78      0.23      0.36      5885\n   macro avg       0.49      0.26      0.31      5885\nweighted avg       0.80      0.23      0.36      5885\n\n--------------------------------------------------\nSmall Box \u2014 Small slice results:\n              precision    recall  f1-score   support\n\n         car       0.42      0.53      0.47       147\n      person       0.79      0.31      0.45      5710\n       truck       0.21      0.18      0.19        28\n\n   micro avg       0.75      0.32      0.45      5885\n   macro avg       0.47      0.34      0.37      5885\nweighted avg       0.77      0.32      0.45      5885\n\n</pre> <p>This makes the value of SAHI crystal clear! The recall when using SAHI is much higher for small objects without significant dropoff in precision, leading to improved F1-score. This is especially pronounced for <code>person</code> detections, where the $F_1$ score is tripled!</p> <p>Now that we know SAHI is effective at detecting small objects, let's look at the places where our predictions are most confident but do not align with the ground truth labels. We can do this by creating an evaluation patches view, filtering for predictions tagged as false positives and sorting by confidence:</p> In\u00a0[18]: Copied! <pre>high_conf_fp_view = filtered_view.to_evaluation_patches(eval_key=\"eval_small_slices\").match(F(\"type\")==\"fp\").sort_by(\"small_slices.detection.confidence\")\n</pre> high_conf_fp_view = filtered_view.to_evaluation_patches(eval_key=\"eval_small_slices\").match(F(\"type\")==\"fp\").sort_by(\"small_slices.detection.confidence\") In\u00a0[19]: Copied! <pre>session.view = high_conf_fp_view.view()\n</pre> session.view = high_conf_fp_view.view() <p></p> <p>Looking at these results, we can see that in many cases, our predictions look quite reasonable, and it seems that the ground truth labels are missing! This is certainly not the case for every prediction, but there are definitely enough to motivate a relabeling of the ground truth data. This is where human-in-the-loop (HITL) workflows can be very useful, as they allow human annotators to review and correct the labels. After this process, we can reevaluate our trained models (with or without SAHI) on the updated dataset, and then train new models on the updated data.</p> <p>In this walkthrough, we've covered how to add SAHI predictions to your data, and then rigorously evaluate the impacts of slicing on prediction quality. We've seen how slicing-aided hyper-inference (SAHI) can improve the recall and $F_1$-score for detection, especially for small objects, without needing to train a model on larger images.</p> <p>To maximize the effectiveness of SAHI, you may want to experiment with the following:</p> <ul> <li>Slicing hyperparameters, such as slice height and width, and overlap.</li> <li>Base object detection models, as SAHI is compatible with many models, including YOLOv5, and Hugging Face Transformers models.</li> <li>Confidence thresholding (potentially on a class-by-class basis), to reduce the number of false positives.</li> <li>Post-processing techniques, such as non-maximum suppression (NMS), to reduce the number of overlapping detections.</li> <li>Human-in-the-loop (HITL) workflows, to correct ground truth labels.</li> </ul> <p>You will also want to determine which evaluation metrics make the most sense for your use case!</p> <p>If you found this tutorial helpful, you may also be interested in the following resources:</p> <ul> <li>Tutorial on Evaluating Object Detections</li> <li>Tutorial on Finding Object Detection Mistakes</li> <li>Tutorial on Fine-tuning YOLOv8 on Custom Data</li> <li>FiftyOne Plugin for Comparing Models on Specific Detections</li> </ul>"},{"location":"tutorials/small_object_detection/#detecting-small-objects-with-sahi","title":"Detecting Small Objects with SAHI\u00b6","text":""},{"location":"tutorials/small_object_detection/#setup-and-installation","title":"Setup and Installation\u00b6","text":""},{"location":"tutorials/small_object_detection/#standard-inference-with-yolov8","title":"Standard Inference with YOLOv8\u00b6","text":""},{"location":"tutorials/small_object_detection/#detecting-small-objects-with-sahi","title":"Detecting Small Objects with SAHI\u00b6","text":""},{"location":"tutorials/small_object_detection/#evaluating-sahi-predictions","title":"Evaluating SAHI Predictions\u00b6","text":""},{"location":"tutorials/small_object_detection/#using-fiftyones-evaluation-api","title":"Using FiftyOne's Evaluation API\u00b6","text":""},{"location":"tutorials/small_object_detection/#evaluating-performance-on-small-objects","title":"Evaluating Performance on Small Objects\u00b6","text":""},{"location":"tutorials/small_object_detection/#identifying-edge-cases","title":"Identifying Edge Cases\u00b6","text":""},{"location":"tutorials/small_object_detection/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/small_object_detection/#additional-resources","title":"Additional Resources\u00b6","text":""},{"location":"tutorials/uniqueness/","title":"Exploring Image Uniqueness with FiftyOne","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\n</pre> !pip install fiftyone <p>This tutorial requires either Torchvision Datasets or TensorFlow Datasets to download the CIFAR-10 dataset used below.</p> <p>You can, for example, install PyTorch as follows:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install torch torchvision\n</pre> !pip install torch torchvision In\u00a0[1]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Load the CIFAR-10 test split\n# Downloads the dataset from the web if necessary\ndataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\n</pre> import fiftyone as fo import fiftyone.zoo as foz  # Load the CIFAR-10 test split # Downloads the dataset from the web if necessary dataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\") <pre>Split 'test' already downloaded\nLoading 'cifar10' split 'test'\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [9.6s elapsed, 0s remaining, 1.0K samples/s]        \nDataset 'cifar10-test' created\n</pre> <p>The dataset contains the ground truth labels in a <code>ground_truth</code> field:</p> In\u00a0[2]: Copied! <pre>print(dataset)\n</pre> print(dataset) <pre>Name:           cifar10-test\nMedia type:     image\nNum samples:    10000\nPersistent:     False\nTags:           ['test']\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n</pre> In\u00a0[3]: Copied! <pre>print(dataset.first())\n</pre> print(dataset.first()) <pre>&lt;Sample: {\n    'id': '6066448c7d373b861836bba8',\n    'media_type': 'image',\n    'filepath': '/home/ben/fiftyone/cifar10/test/data/000001.jpg',\n    'tags': BaseList(['test']),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '6066448c7d373b861836bba7',\n        'tags': BaseList([]),\n        'label': 'cat',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n}&gt;\n</pre> <p>Let's launch the FiftyOne App and use the GUI to explore the dataset visually before we go any further:</p> In\u00a0[4]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) Activate In\u00a0[5]: Copied! <pre>import fiftyone.brain as fob\n\nfob.compute_uniqueness(dataset)\n</pre> import fiftyone.brain as fob  fob.compute_uniqueness(dataset) <pre>Generating embeddings...\n   0% ||------------|    16/10000 [95.0ms elapsed, 59.3s remaining, 168.5 samples/s]  100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [1.2m elapsed, 0s remaining, 166.0 samples/s]       \nComputing uniqueness...\nUniqueness computation complete\n</pre> <p>The above method populates a <code>uniqueness</code> field on each sample that contains the sample's uniqueness score. Let's confirm this by printing some information about the dataset:</p> In\u00a0[6]: Copied! <pre># Now the samples have a \"uniqueness\" field on them\nprint(dataset)\n</pre> # Now the samples have a \"uniqueness\" field on them print(dataset) <pre>Name:           cifar10-test\nMedia type:     image\nNum samples:    10000\nPersistent:     False\nTags:           ['test']\nSample fields:\n    filepath:     fiftyone.core.fields.StringField\n    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n    uniqueness:   fiftyone.core.fields.FloatField\n</pre> In\u00a0[7]: Copied! <pre>print(dataset.first())\n</pre> print(dataset.first()) <pre>&lt;Sample: {\n    'id': '6066448c7d373b861836bba8',\n    'media_type': 'image',\n    'filepath': '/home/ben/fiftyone/cifar10/test/data/000001.jpg',\n    'tags': BaseList(['test']),\n    'metadata': None,\n    'ground_truth': &lt;Classification: {\n        'id': '6066448c7d373b861836bba7',\n        'tags': BaseList([]),\n        'label': 'cat',\n        'confidence': None,\n        'logits': None,\n    }&gt;,\n    'uniqueness': 0.4978482190810026,\n}&gt;\n</pre> In\u00a0[8]: Copied! <pre># Sort in increasing order of uniqueness (least unique first)\ndups_view = dataset.sort_by(\"uniqueness\")\n\n# Open view in the App\nsession.view = dups_view\n</pre> # Sort in increasing order of uniqueness (least unique first) dups_view = dataset.sort_by(\"uniqueness\")  # Open view in the App session.view = dups_view Activate <p>You will easily see some near-duplicates in the App. It surprised us that there are duplicates in CIFAR-10, too!</p> <p>Of course, in this scenario, near duplicates are identified from visual inspection. So, how do we get the information out of FiftyOne and back into your working environment. Easy! The <code>session</code> variable provides a bidirectional bridge between the App and your Python environment. In this case, we will use the <code>session.selected</code> bridge. So, in the App, select some of the duplicates and near-duplicates. Then, execute the following code in the Python shell.</p> In\u00a0[9]: Copied! <pre># Get currently selected images from App\ndup_ids = session.selected\n\n# Mark as duplicates\ndups_view = dataset.select(dup_ids)\ndups_view.tag_samples(\"dups\")\n\n# Visualize duplicates-only in App\nsession.view = dups_view\n</pre> # Get currently selected images from App dup_ids = session.selected  # Mark as duplicates dups_view = dataset.select(dup_ids) dups_view.tag_samples(\"dups\")  # Visualize duplicates-only in App session.view = dups_view Activate <p>And the App will only show these samples now. We can, of course access the filepaths and other information about these samples programmatically so you can act on the findings. But, let's do that at the end of Part 2 below!</p> In\u00a0[\u00a0]: Copied! <pre>!pip install flickrapi\n</pre> !pip install flickrapi <p>You will also need to enable ETA's storage support to run this script, if you haven't yet:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install voxel51-eta[storage]\n</pre> !pip install voxel51-eta[storage] <p>Next, let's download three sets of images to process together. I suggest using three distinct object-nouns like \"badger\", \"wolverine\", and \"kitten\". For the actual downloading, we will use the provided query_flickr.py script:</p> In\u00a0[\u00a0]: Copied! <pre>from query_flickr import query_flickr\n\n# Your credentials here\nKEY = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\nSECRET = \"XXXXXXXXXXXXXXXX\"\n\nquery_flickr(KEY, SECRET, \"badger\")\nquery_flickr(KEY, SECRET, \"wolverine\")\nquery_flickr(KEY, SECRET, \"kitten\")\n</pre> from query_flickr import query_flickr  # Your credentials here KEY = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" SECRET = \"XXXXXXXXXXXXXXXX\"  query_flickr(KEY, SECRET, \"badger\") query_flickr(KEY, SECRET, \"wolverine\") query_flickr(KEY, SECRET, \"kitten\") <p>The rest of this walkthrough assumes you've downloaded some images to your local <code>.data/</code> directory.</p> In\u00a0[12]: Copied! <pre>import fiftyone as fo\n\ndataset = fo.Dataset.from_images_dir(\"data\", recursive=True, name=\"flickr-images\")\n\nprint(dataset)\nprint(dataset.first())\n</pre> import fiftyone as fo  dataset = fo.Dataset.from_images_dir(\"data\", recursive=True, name=\"flickr-images\")  print(dataset) print(dataset.first()) <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 167/167 [160.7ms elapsed, 0s remaining, 1.0K samples/s]      \nName:           flickr-images\nMedia type:     image\nNum samples:    167\nPersistent:     False\nTags:           []\nSample fields:\n    filepath: fiftyone.core.fields.StringField\n    tags:     fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n&lt;Sample: {\n    'id': '606647127d373b86183757ea',\n    'media_type': 'image',\n    'filepath': '/home/ben/code/fiftyone/docs/source/tutorials/data/badger/14271824861_122dfd2788_c.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n}&gt;\n</pre> <p>The above command uses a factory method on the <code>Dataset</code> class to traverse a directory of images (including subdirectories) and generate a dataset instance in FiftyOne containing those images.</p> <p>Note that the images are not loaded from disk, so this operation is fast. The first argument is the path to the directory of images on disk, and the third is a name for the dataset.</p> <p>With the dataset loaded into FiftyOne, we can easily launch the App and visualize it:</p> In\u00a0[13]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) Activate <p>Refer to the User Guide for more useful things you can do with the dataset and App.</p> In\u00a0[14]: Copied! <pre>import fiftyone.brain as fob\n\nfob.compute_uniqueness(dataset)\n\n# Now the samples have a \"uniqueness\" field on them\nprint(dataset)\n</pre> import fiftyone.brain as fob  fob.compute_uniqueness(dataset)  # Now the samples have a \"uniqueness\" field on them print(dataset) <pre>Generating embeddings...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 167/167 [1.8s elapsed, 0s remaining, 94.6 samples/s]         \nComputing uniqueness...\nUniqueness computation complete\nName:           flickr-images\nMedia type:     image\nNum samples:    167\nPersistent:     False\nTags:           []\nSample fields:\n    filepath:   fiftyone.core.fields.StringField\n    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n    uniqueness: fiftyone.core.fields.FloatField\n</pre> In\u00a0[15]: Copied! <pre>print(dataset.first())\n</pre> print(dataset.first()) <pre>&lt;Sample: {\n    'id': '606647127d373b86183757ea',\n    'media_type': 'image',\n    'filepath': '/home/ben/code/fiftyone/docs/source/tutorials/data/badger/14271824861_122dfd2788_c.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'uniqueness': 0.3202661340134384,\n}&gt;\n</pre> In\u00a0[16]: Copied! <pre># Sort by uniqueness (most unique first)\nrank_view = dataset.sort_by(\"uniqueness\", reverse=True)\n\n# Visualize in the App\nsession.view = rank_view\n</pre> # Sort by uniqueness (most unique first) rank_view = dataset.sort_by(\"uniqueness\", reverse=True)  # Visualize in the App session.view = rank_view Activate <p>Now, just visualizing the samples is interesting, but we want more. We want to get the most unique samples from our dataset so that we can use them in our work. Let's do just that. In the same Python session, execute the following code.</p> In\u00a0[17]: Copied! <pre># Verify that the most unique sample has the maximal uniqueness of 1.0\nprint(rank_view.first())\n\n# Extract paths to 10 most unique samples\nten_best = [x.filepath for x in rank_view.limit(10)]\n\nfor filepath in ten_best:\n    print(filepath.split('/')[-1])\n\n# Then you can do what you want with these.\n# Output to csv or json, send images to your annotation team, seek additional\n# similar data, etc.\n</pre> # Verify that the most unique sample has the maximal uniqueness of 1.0 print(rank_view.first())  # Extract paths to 10 most unique samples ten_best = [x.filepath for x in rank_view.limit(10)]  for filepath in ten_best:     print(filepath.split('/')[-1])  # Then you can do what you want with these. # Output to csv or json, send images to your annotation team, seek additional # similar data, etc. <pre>&lt;SampleView: {\n    'id': '606647127d373b8618375862',\n    'media_type': 'image',\n    'filepath': '/home/ben/code/fiftyone/docs/source/tutorials/data/wolverine/2428280852_6c77fe2877_c.jpg',\n    'tags': BaseList([]),\n    'metadata': None,\n    'uniqueness': 1.0,\n}&gt;\n2428280852_6c77fe2877_c.jpg\n49733688496_b6fc5cde41_c.jpg\n2843545851_6e1dc16dfc_c.jpg\n7466201514_0a3c7d615a_c.jpg\n6176873587_d0744926cb_c.jpg\n33891021626_4cfe3bf1d2_c.jpg\n8303699893_a7c14c04d3_c.jpg\n388994554_34d60d1b18_c.jpg\n5880167199_906172bc50_c.jpg\n8538740443_a587bfe75c_c.jpg\n</pre> <p>Alternatively, you can simply tag the most unique samples and persist the dataset so you can return to it later in FiftyOne.</p> In\u00a0[18]: Copied! <pre>rank_view.limit(10).tag_samples(\"unique\")\n\ndataset.persistent = True\n</pre> rank_view.limit(10).tag_samples(\"unique\")  dataset.persistent = True In\u00a0[19]: Copied! <pre>session.freeze() # screenshot the active App for sharing\n</pre> session.freeze() # screenshot the active App for sharing"},{"location":"tutorials/uniqueness/#exploring-image-uniqueness-with-fiftyone","title":"Exploring Image Uniqueness with FiftyOne\u00b6","text":"<p>During model training, the best results will be seen when training on unique data samples. For example, finding and removing similar samples in your dataset can avoid accidental concept imbalance that can bias the learning of your model. Or, if duplicate or near-duplicate data is present in both training and validation/test splits, evaluation results may not be reliable. Just to name a few.</p> <p>In this tutorial, we explore how FiftyOne's image uniqueness tool can be used to analyze and extract insights from raw (unlabeled) datasets.</p> <p>We'll cover the following concepts:</p> <ul> <li>Loading a dataset from the FiftyOne Dataset Zoo</li> <li>Applying FiftyOne's uniqueness method to your dataset</li> <li>Launching the FiftyOne App and visualizing/exploring your data</li> <li>Identifying duplicate and near-duplicate images in your dataset</li> <li>Identifying the most unique/representative images in your dataset</li> </ul> <p>So, what's the takeaway?</p> <p>This tutorial shows how FiftyOne can automatically find and remove near-duplicate images in your datasets and recommend the most unique samples in your data, enabling you to start your model training off right with a high-quality bootstrapped training set.</p>"},{"location":"tutorials/uniqueness/#setup","title":"Setup\u00b6","text":"<p>If you haven't already, install FiftyOne:</p>"},{"location":"tutorials/uniqueness/#part-1-finding-duplicate-and-near-duplicate-images","title":"Part 1: Finding duplicate and near-duplicate images\u00b6","text":"<p>A common problem in dataset creation is duplicated data. Although this could be found using file hashing---as in the image_deduplication walkthrough---it is less possible when small manipulations have occurred in the data. Even more critical for workflows involving model training is the need to get as much power out of each data samples as possible; near-duplicates, which are samples that are exceptionally similar to one another, are intrinsically less valuable for the training scenario. Let's see if we can find such duplicates and near-duplicates in a common dataset: CIFAR-10.</p>"},{"location":"tutorials/uniqueness/#load-the-dataset","title":"Load the dataset\u00b6","text":"<p>Open a Python shell to begin. We will use the CIFAR-10 dataset, which is available in the FiftyOne Dataset Zoo:</p>"},{"location":"tutorials/uniqueness/#compute-uniqueness","title":"Compute uniqueness\u00b6","text":"<p>Now we can process the entire dataset for uniqueness. This is a fairly expensive operation, but should finish in a few minutes at most. We are processing through all samples in the dataset, then building a representation that relates the samples to each other. Finally, we analyze this representation to output uniqueness scores for each sample.</p>"},{"location":"tutorials/uniqueness/#visualize-to-find-duplicate-and-near-duplicate-images","title":"Visualize to find duplicate and near-duplicate images\u00b6","text":"<p>Now, let's visually inspect the least unique images in the dataset to see if our dataset has any issues:</p>"},{"location":"tutorials/uniqueness/#part-2-bootstrapping-a-dataset-of-unique-samples","title":"Part 2: Bootstrapping a dataset of unique samples\u00b6","text":"<p>When building a dataset, it is important to create a diverse dataset with unique and representative samples. Here, we explore FiftyOne's ability to help identify the most unique samples in a raw dataset.</p>"},{"location":"tutorials/uniqueness/#download-some-images","title":"Download some images\u00b6","text":"<p>This walkthrough will process a directory of images and compute their uniqueness. The first thing we need to do is get some images. Let's get some images from Flickr, to keep this interesting!</p> <p>You need a Flickr API key to do this. If you already have a Flickr API key, then skip the next steps.</p> <ol> <li>Go to https://www.flickr.com/services/apps/create/</li> <li>Click on Request API Key. (https://www.flickr.com/services/apps/create/apply/) You will need to login (create account if needed, free).</li> <li>Click on \"Non-Commercial API Key\" (this is just for a test usage) and fill in the information on the next page. You do not need to be very descriptive; your API will automatically appear on the following page.</li> <li>Install the Flickr API:</li> </ol>"},{"location":"tutorials/uniqueness/#load-the-data-into-fiftyone","title":"Load the data into FiftyOne\u00b6","text":"<p>Let's now work through getting this data into FiftyOne and working with it.</p>"},{"location":"tutorials/uniqueness/#compute-uniqueness-and-analyze","title":"Compute uniqueness and analyze\u00b6","text":"<p>Now, let's analyze the data. For example, we may want to understand what are the most unique images among the data as they may inform or harm model training; we may want to discover duplicates or redundant samples.</p> <p>Continuing in the same Python shell, let's compute and visualize uniqueness.</p>"},{"location":"tutorials/yolov8/","title":"Fine-tune YOLOv8 models for custom use cases with the help of FiftyOne","text":"<p>Since its initial release back in 2015, the You Only Look Once (YOLO) family of computer vision models has been one of the most popular in the field. In late 2022, Ultralytics announced YOLOv8, which comes with a new backbone.</p> <p>The basic YOLOv8 detection and segmentation models, however, are general purpose, which means for custom use cases they may not be suitable out of the box. With FiftyOne, we can visualize and evaluate YOLOv8 model predictions, and better understand where the model's predictive power breaks down.</p> <p>In this walkthrough, we will show you how to load YOLOv8 model predictions into FiftyOne, and use insights from model evaluation to fine-tune a YOLOv8 model for your custom use case.</p> <p>Specifically, this walkthrough covers:</p> <ul> <li>Loading YOLOv8 model predictions into FiftyOne</li> <li>Evaluating YOLOv8 model predictions</li> <li>Curating a dataset for fine-tuning</li> <li>Fine-tuning YOLOv8 models</li> <li>Comparing the performance of out-of-the-box and fine-tuned YOLOv8 models.</li> </ul> <p>So, what's the takeaway?</p> <p>FiftyOne can help you to achieve better performance using YOLOv8 models on real-time inference tasks for custom use cases.</p> <p>To get started, you need to install FiftyOne and Ultralytics:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone ultralytics\n</pre> !pip install fiftyone ultralytics In\u00a0[\u00a0]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone import ViewField as F\n</pre> import fiftyone as fo import fiftyone.zoo as foz from fiftyone import ViewField as F In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport os\nfrom tqdm import tqdm\n</pre> import numpy as np import os from tqdm import tqdm <p>We will import the YOLO object from Ultralytics and use this to instantiate pretrained detection and segmentation models in Python. Along with the YOLOv8 architecture, Ultralytics released a set of pretrained models, with different sizes, for classification, detection, and segmentation tasks.</p> <p>For the purposes of illustration, we will use the smallest version, YOLOv8 Nano (YOLOv8n), but the same syntax will work for any of the pretrained models on the Ultralytics YOLOv8 GitHub repo.</p> In\u00a0[\u00a0]: Copied! <pre>from ultralytics import YOLO\n\ndetection_model = YOLO(\"yolov8n.pt\")\nseg_model = YOLO(\"yolov8n-seg.pt\")\n</pre> from ultralytics import YOLO  detection_model = YOLO(\"yolov8n.pt\") seg_model = YOLO(\"yolov8n-seg.pt\") <p>In Python, we can apply a YOLOv8 model to an individual image by passing the file path into the model call. For an image with file path <code>path/to/image.jpg</code>, running <code>detection_model(\"path/to/image.jpg\")</code> will generate a list containing a single <code>ultralytics.yolo.engine.results.Results</code> object.</p> <p>We can see this by applying the detection model to Ultralytics' test image:</p> In\u00a0[\u00a0]: Copied! <pre>results = detection_model(\"https://ultralytics.com/images/bus.jpg\")\n</pre> results = detection_model(\"https://ultralytics.com/images/bus.jpg\") <p>A similar result can be obtained if we apply the segmentation model to an image. These results contain bounding boxes, class confidence scores, and integers representing class labels. For a complete discussion of these results objects, see the Ultralytics YOLOv8 Results API Reference.</p> <p>If we want to run tasks on all images in a directory, then we can do so from the command line with the YOLO Command Line Interface by specifying the task <code>[detect, segment, classify]</code> and mode <code>[train, val, predict, export]</code>, along with other arguments.</p> <p>To run inference on a set of images, we must first put the data in the appropriate format. The best way to do so is to load your images into a FiftyOne <code>Dataset</code>, and then export the dataset in YOLOv5Dataset format, as YOLOv5 and YOLOv8 use the same data formats.</p> <p>\ud83d\udca1 FiftyOne's Ultralytics Integration</p> <p>If you just want to run inference on your FiftyOne dataset with an existing YOLOv8 model, you can do so by passing this <code>ultralytics.YOLO</code> model directly into your FiftyOne dataset's <code>apply_model()</code> method:</p> <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n# Load a dataset\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n# Load a YOLOv8 model\nfrom ultralytics import YOLO\nmodel = YOLO(\"yolov8l.pt\")\n\n# Apply the model to the dataset\ndataset.apply_model(model, label_field=\"yolov8l\")\n\n# Launch the App to visualize the results\nsession = fo.launch_app(dataset)\n</pre> <p>For more details, check out the FiftyOne Ultralytics Integration docs!</p> <p>In this walkthrough, we will look at YOLOv8\u2019s predictions on a subset of the MS COCO dataset. This is the dataset on which these models were trained, which means that they are likely to show close to peak performance on this data. Additionally, working with COCO data makes it easy for us to map model outputs to class labels.</p> <p>Load the images and ground truth object detections in COCO\u2019s validation set from the FiftyOne Dataset Zoo.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = foz.load_zoo_dataset(\n    'coco-2017',\n    split='validation',\n)\n</pre> dataset = foz.load_zoo_dataset(     'coco-2017',     split='validation', ) <p>We then generate a mapping from YOLO class predictions to COCO class labels. COCO has 91 classes, and YOLOv8, just like YOLOv3 and YOLOv5, ignores all of the numeric classes and focuses on the remaining 80.</p> In\u00a0[\u00a0]: Copied! <pre>coco_classes = [c for c in dataset.default_classes if not c.isnumeric()]\n</pre> coco_classes = [c for c in dataset.default_classes if not c.isnumeric()] <p>Export the dataset into a directory <code>coco_val</code> in YOLO format:</p> In\u00a0[3]: Copied! <pre>def export_yolo_data(\n    samples, \n    export_dir, \n    classes, \n    label_field = \"ground_truth\", \n    split = None\n    ):\n\n    if type(split) == list:\n        splits = split\n        for split in splits:\n            export_yolo_data(\n                samples, \n                export_dir, \n                classes, \n                label_field, \n                split\n            )   \n    else:\n        if split is None:\n            split_view = samples\n            split = \"val\"\n        else:\n            split_view = samples.match_tags(split)\n\n        split_view.export(\n            export_dir=export_dir,\n            dataset_type=fo.types.YOLOv5Dataset,\n            label_field=label_field,\n            classes=classes,\n            split=split\n        )\n</pre> def export_yolo_data(     samples,      export_dir,      classes,      label_field = \"ground_truth\",      split = None     ):      if type(split) == list:         splits = split         for split in splits:             export_yolo_data(                 samples,                  export_dir,                  classes,                  label_field,                  split             )        else:         if split is None:             split_view = samples             split = \"val\"         else:             split_view = samples.match_tags(split)          split_view.export(             export_dir=export_dir,             dataset_type=fo.types.YOLOv5Dataset,             label_field=label_field,             classes=classes,             split=split         ) In\u00a0[\u00a0]: Copied! <pre>coco_val_dir = \"coco_val\"\nexport_yolo_data(dataset, coco_val_dir, coco_classes)\n</pre> coco_val_dir = \"coco_val\" export_yolo_data(dataset, coco_val_dir, coco_classes) <p>Then run inference on these images:</p> In\u00a0[\u00a0]: Copied! <pre>!yolo task=detect mode=predict model=yolov8n.pt source=coco_val/images/val save_txt=True save_conf=True\n</pre> !yolo task=detect mode=predict model=yolov8n.pt source=coco_val/images/val save_txt=True save_conf=True <p>Running this inference generates a directory <code>runs/detect/predict/labels</code>, which will contain a separate <code>.txt</code> file for each image in the dataset, and a line for each object detection.</p> <p>Each line is in the form: an integer for the class label, a class confidence score, and four values representing the bounding box.</p> In\u00a0[22]: Copied! <pre>label_file = \"runs/detect/predict/labels/000000000139.txt\"\n\nwith open(label_file) as f: \n    print(f.read())\n</pre> label_file = \"runs/detect/predict/labels/000000000139.txt\"  with open(label_file) as f:      print(f.read()) <pre>56 0.663281 0.619718 0.0640625 0.201878 0.265856\n60 0.55625 0.619718 0.184375 0.225352 0.266771\n74 0.710938 0.307512 0.01875 0.0469484 0.277868\n60 0.860156 0.91784 0.279687 0.159624 0.278297\n72 0.744531 0.539906 0.101562 0.295775 0.356417\n75 0.888281 0.820423 0.0609375 0.241784 0.391675\n58 0.385156 0.457746 0.0640625 0.084507 0.420693\n56 0.609375 0.620892 0.090625 0.21831 0.50562\n56 0.650781 0.619718 0.0859375 0.215962 0.508265\n56 0.629687 0.619718 0.128125 0.220657 0.523211\n0 0.686719 0.535211 0.0828125 0.333333 0.712339\n56 0.505469 0.624413 0.0953125 0.230047 0.854189\n62 0.125 0.502347 0.23125 0.225352 0.927385\n\n</pre> <p>We can read a YOLOv8 detection prediction file with $N$ detections into an $(N, 6)$ numpy array:</p> In\u00a0[23]: Copied! <pre>def read_yolo_detections_file(filepath):\n    detections = []\n    if not os.path.exists(filepath):\n        return np.array([])\n    \n    with open(filepath) as f:\n        lines = [line.rstrip('\\n').split(' ') for line in f]\n    \n    for line in lines:\n        detection = [float(l) for l in line]\n        detections.append(detection)\n    return np.array(detections)\n</pre> def read_yolo_detections_file(filepath):     detections = []     if not os.path.exists(filepath):         return np.array([])          with open(filepath) as f:         lines = [line.rstrip('\\n').split(' ') for line in f]          for line in lines:         detection = [float(l) for l in line]         detections.append(detection)     return np.array(detections) <p>From here, we need to convert these detections into FiftyOne\u2019s Detections format.</p> <p>YOLOv8 represents bounding boxes in a centered format with coordinates <code>[center_x, center_y, width, height]</code>, whereas FiftyOne stores bounding boxes in <code>[top-left-x, top-left-y, width, height]</code> format. We can make this conversion by \"un-centering\" the predicted bounding boxes:</p> In\u00a0[24]: Copied! <pre>def _uncenter_boxes(boxes):\n    '''convert from center coords to corner coords'''\n    boxes[:, 0] -= boxes[:, 2]/2.\n    boxes[:, 1] -= boxes[:, 3]/2.\n</pre> def _uncenter_boxes(boxes):     '''convert from center coords to corner coords'''     boxes[:, 0] -= boxes[:, 2]/2.     boxes[:, 1] -= boxes[:, 3]/2. <p>Additionally, we can convert a list of class predictions (indices) to a list of class labels (strings) by passing in the class list:</p> In\u00a0[25]: Copied! <pre>def _get_class_labels(predicted_classes, class_list):\n    labels = (predicted_classes).astype(int)\n    labels = [class_list[l] for l in labels]\n    return labels\n</pre> def _get_class_labels(predicted_classes, class_list):     labels = (predicted_classes).astype(int)     labels = [class_list[l] for l in labels]     return labels <p>Given the output of a <code>read_yolo_detections_file()</code> call, <code>yolo_detections</code>, we can generate the FiftyOne <code>Detections</code> object that captures this data:</p> In\u00a0[26]: Copied! <pre>def convert_yolo_detections_to_fiftyone(\n    yolo_detections, \n    class_list\n    ):\n\n    detections = []\n    if yolo_detections.size == 0:\n        return fo.Detections(detections=detections)\n    \n    boxes = yolo_detections[:, 1:-1]\n    _uncenter_boxes(boxes)\n    \n    confs = yolo_detections[:, -1]\n    labels = _get_class_labels(yolo_detections[:, 0], class_list) \n \n    for label, conf, box in zip(labels, confs, boxes):\n        detections.append(\n            fo.Detection(\n                label=label,\n                bounding_box=box.tolist(),\n                confidence=conf\n            )\n        )\n\n    return fo.Detections(detections=detections)\n</pre> def convert_yolo_detections_to_fiftyone(     yolo_detections,      class_list     ):      detections = []     if yolo_detections.size == 0:         return fo.Detections(detections=detections)          boxes = yolo_detections[:, 1:-1]     _uncenter_boxes(boxes)          confs = yolo_detections[:, -1]     labels = _get_class_labels(yolo_detections[:, 0], class_list)        for label, conf, box in zip(labels, confs, boxes):         detections.append(             fo.Detection(                 label=label,                 bounding_box=box.tolist(),                 confidence=conf             )         )      return fo.Detections(detections=detections) <p>The final ingredient is a function that takes in the file path of an image, and returns the file path of the corresponding YOLOv8 detection prediction text file.</p> In\u00a0[\u00a0]: Copied! <pre>def get_prediction_filepath(filepath, run_number = 1):\n    run_num_string = \"\"\n    if run_number != 1:\n        run_num_string = str(run_number)\n    filename = filepath.split(\"/\")[-1].split(\".\")[0]\n    return f\"runs/detect/predict{run_num_string}/labels/{filename}.txt\"\n</pre> def get_prediction_filepath(filepath, run_number = 1):     run_num_string = \"\"     if run_number != 1:         run_num_string = str(run_number)     filename = filepath.split(\"/\")[-1].split(\".\")[0]     return f\"runs/detect/predict{run_num_string}/labels/{filename}.txt\" <p>If you run multiple inference calls for the same task, the predictions results are stored in a directory with the next available integer appended to <code>predict</code> in the file path. You can account for this in the above function by passing in the <code>run_number</code> argument.</p> <p>Putting the pieces together, we can write a function that adds these YOLOv8 detections to all of the samples in our dataset efficiently by batching the read and write operations to the underlying MongoDB database.</p> In\u00a0[31]: Copied! <pre>def add_yolo_detections(\n    samples,\n    prediction_field,\n    prediction_filepath,\n    class_list\n    ):\n\n    prediction_filepaths = samples.values(prediction_filepath)\n    yolo_detections = [read_yolo_detections_file(pf) for pf in prediction_filepaths]\n    detections =  [convert_yolo_detections_to_fiftyone(yd, class_list) for yd in yolo_detections]\n    samples.set_values(prediction_field, detections)\n</pre> def add_yolo_detections(     samples,     prediction_field,     prediction_filepath,     class_list     ):      prediction_filepaths = samples.values(prediction_filepath)     yolo_detections = [read_yolo_detections_file(pf) for pf in prediction_filepaths]     detections =  [convert_yolo_detections_to_fiftyone(yd, class_list) for yd in yolo_detections]     samples.set_values(prediction_field, detections) <p>Now we can rapidly add the detections in a few lines of code:</p> In\u00a0[\u00a0]: Copied! <pre>filepaths = dataset.values(\"filepath\")\nprediction_filepaths = [get_prediction_filepath(fp) for fp in filepaths]\ndataset.set_values(\n    \"yolov8n_det_filepath\", \n    prediction_filepaths\n)\n\nadd_yolo_detections(\n    dataset, \n    \"yolov8n\", \n    \"yolov8n_det_filepath\", \n    coco_classes\n)\n</pre> filepaths = dataset.values(\"filepath\") prediction_filepaths = [get_prediction_filepath(fp) for fp in filepaths] dataset.set_values(     \"yolov8n_det_filepath\",      prediction_filepaths )  add_yolo_detections(     dataset,      \"yolov8n\",      \"yolov8n_det_filepath\",      coco_classes ) <p>Now we can visualize these YOLOv8 model predictions on the samples in our dataset in the FiftyOne App:</p> In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) <p></p> In\u00a0[\u00a0]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>It is also worth noting that it is possible to convert YOLOv8 predictions directly from the output of a YOLO model call in Python, without first generating external prediction files and reading them in. Let\u2019s see how this can be done for instance segmentations.</p> <p>Like detections, YOLOv8 stores instance segmentations with centered bounding boxes. In addition, YOLOv8 stores a mask that covers the entire image, with only a rectangular region of that mask containing nonzero values. FiftyOne, on the other hand, stores instance segmentations at <code>Detection</code> labels with a mask that only covers the given bounding box.</p> <p>We can convert from YOLOv8 instance segmentations to FiftyOne instance segmentations with this <code>convert_yolo_segmentations_to_fiftyone()</code> function:</p> In\u00a0[32]: Copied! <pre>def convert_yolo_segmentations_to_fiftyone(\n    yolo_segmentations, \n    class_list\n    ):\n\n    detections = []\n    boxes = yolo_segmentations.boxes.xywhn\n    if not boxes.shape or yolo_segmentations.masks is None:\n        return fo.Detections(detections=detections)\n    \n    _uncenter_boxes(boxes)\n    masks = yolo_segmentations.masks.masks\n    labels = _get_class_labels(yolo_segmentations.boxes.cls, class_list)\n\n    for label, box, mask in zip(labels, boxes, masks):\n        ## convert to absolute indices to index mask\n        w, h = mask.shape\n        tmp =  np.copy(box)\n        tmp[2] += tmp[0]\n        tmp[3] += tmp[1]\n        tmp[0] *= h\n        tmp[2] *= h\n        tmp[1] *= w\n        tmp[3] *= w\n        tmp = [int(b) for b in tmp]\n        y0, x0, y1, x1 = tmp\n        sub_mask = mask[x0:x1, y0:y1]\n       \n        detections.append(\n            fo.Detection(\n                label=label,\n                bounding_box = list(box),\n                mask = sub_mask.astype(bool)\n            )\n        )\n\n    return fo.Detections(detections=detections)\n</pre> def convert_yolo_segmentations_to_fiftyone(     yolo_segmentations,      class_list     ):      detections = []     boxes = yolo_segmentations.boxes.xywhn     if not boxes.shape or yolo_segmentations.masks is None:         return fo.Detections(detections=detections)          _uncenter_boxes(boxes)     masks = yolo_segmentations.masks.masks     labels = _get_class_labels(yolo_segmentations.boxes.cls, class_list)      for label, box, mask in zip(labels, boxes, masks):         ## convert to absolute indices to index mask         w, h = mask.shape         tmp =  np.copy(box)         tmp[2] += tmp[0]         tmp[3] += tmp[1]         tmp[0] *= h         tmp[2] *= h         tmp[1] *= w         tmp[3] *= w         tmp = [int(b) for b in tmp]         y0, x0, y1, x1 = tmp         sub_mask = mask[x0:x1, y0:y1]                 detections.append(             fo.Detection(                 label=label,                 bounding_box = list(box),                 mask = sub_mask.astype(bool)             )         )      return fo.Detections(detections=detections) <p>Looping through all samples in the dataset, we can add the predictions from our <code>seg_model</code>, and then view these predicted masks in the FiftyOne App.</p> In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) <p></p> In\u00a0[\u00a0]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>Now that we have YOLOv8 predictions loaded onto the images in our dataset, we can evaluate the quality of these predictions using FiftyOne\u2019s Evaluation API.</p> <p>To evaluate the object detections in the <code>yolov8_det</code> field relative to the <code>ground_truth</code> detections field, we can run:</p> In\u00a0[\u00a0]: Copied! <pre>detection_results = dataset.evaluate_detections(\n    \"yolov8n\", \n    eval_key=\"eval\",\n    compute_mAP=True,\n    gt_field=\"ground_truth\",\n)\n</pre> detection_results = dataset.evaluate_detections(     \"yolov8n\",      eval_key=\"eval\",     compute_mAP=True,     gt_field=\"ground_truth\", ) <p>We can then get the mean average precision (mAP) of the model\u2019s predictions:</p> In\u00a0[44]: Copied! <pre>mAP = detection_results.mAP()\nprint(f\"mAP = {mAP}\")\n</pre> mAP = detection_results.mAP() print(f\"mAP = {mAP}\") <pre>mAP = 0.3121319189417518\n</pre> <p>We can also look at the model\u2019s performance on the 20 most common object classes in the dataset, where it has seen the most examples so the statistics are most meaningful:</p> In\u00a0[45]: Copied! <pre>counts = dataset.count_values(\"ground_truth.detections.label\")\n\ntop20_classes = sorted(\n    counts, \n    key=counts.get, \n    reverse=True\n)[:20]\n\ndetection_results.print_report(classes=top20_classes)\n</pre> counts = dataset.count_values(\"ground_truth.detections.label\")  top20_classes = sorted(     counts,      key=counts.get,      reverse=True )[:20]  detection_results.print_report(classes=top20_classes) <pre>               precision    recall  f1-score   support\n\n       person       0.85      0.68      0.76     11573\n          car       0.71      0.52      0.60      1971\n        chair       0.62      0.34      0.44      1806\n         book       0.61      0.12      0.20      1182\n       bottle       0.68      0.39      0.50      1051\n          cup       0.61      0.44      0.51       907\n dining table       0.54      0.42      0.47       697\ntraffic light       0.66      0.36      0.46       638\n         bowl       0.63      0.49      0.55       636\n      handbag       0.48      0.12      0.19       540\n         bird       0.79      0.39      0.52       451\n         boat       0.58      0.29      0.39       430\n        truck       0.57      0.35      0.44       415\n        bench       0.58      0.27      0.37       413\n     umbrella       0.65      0.52      0.58       423\n          cow       0.81      0.61      0.70       397\n       banana       0.68      0.34      0.45       397\n       carrot       0.56      0.29      0.38       384\n   motorcycle       0.77      0.58      0.66       379\n     backpack       0.51      0.16      0.24       371\n\n    micro avg       0.76      0.52      0.61     25061\n    macro avg       0.64      0.38      0.47     25061\n weighted avg       0.74      0.52      0.60     25061\n\n</pre> <p>From the output of <code>print_report()</code>, we can see that this model performs decently well, but certainly has its limitations. While its precision is relatively good on average, it is lacking when it comes to recall. This is especially pronounced for certain classes like the <code>book</code> class.</p> <p>Fortunately, we can dig deeper into these results with FiftyOne. Using the FiftyOne App, we can for instance filter by class for both ground truth and predicted detections so that only <code>book</code> detections appear in the samples.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>Scrolling through the samples in the sample grid, we can see that a lot of the time, COCO\u2019s purported ground truth labels for the <code>book</code> class appear to be imperfect. Sometimes, individual books are bounded, other times rows or whole bookshelves are encompassed in a single box, and yet other times books are entirely unlabeled. Unless our desired computer vision application specifically requires good <code>book</code> detection, this should probably not be a point of concern when we are assessing the quality of the model. After all, the quality of a model is limited by the quality of the data it is trained on - this is why data-centric approaches to computer vision are so important!</p> <p>For other classes like the <code>bird</code> class, however, there appear to be challenges. One way to see this is to filter for <code>bird</code> ground truth detections and then convert to an EvaluationPatchesView. Some of these recall errors appear to be related to small features, where the resolution is poor.</p> <p>In other cases though, quick inspection confirms that the object is clearly a bird. This means that there is likely room for improvement.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>For the remainder of this walkthrough, we will pretend that we are working for a bird conservancy group, putting computer vision models in the field to track and protect endangered species. Our goal is to fine-tune a YOLOv8 detection model to detect birds.</p> <p>We will use the COCO validation dataset above as our test set. Since we are only concerned with detecting birds, we can filter out all non-<code>bird</code> ground truth detections using <code>filter_labels()</code>. We will also filter out the non- <code>bird</code> predictions, but will pass the <code>only_matches = False</code> argument into <code>filter_labels()</code> to make sure we keep images that have ground truth <code>bird</code> detections without YOLOv8n <code>bird</code> predictions.</p> In\u00a0[\u00a0]: Copied! <pre>test_dataset = dataset.filter_labels(\n    \"ground_truth\", \n    F(\"label\") == \"bird\"\n).filter_labels(\n    \"yolov8n\", \n    F(\"label\") == \"bird\",\n    only_matches=False\n).clone()\n\ntest_dataset.name = \"birds-test-dataset\"\ntest_dataset.persistent = True\n\n## set classes to just include birds\nclasses = [\"bird\"]\n</pre> test_dataset = dataset.filter_labels(     \"ground_truth\",      F(\"label\") == \"bird\" ).filter_labels(     \"yolov8n\",      F(\"label\") == \"bird\",     only_matches=False ).clone()  test_dataset.name = \"birds-test-dataset\" test_dataset.persistent = True  ## set classes to just include birds classes = [\"bird\"] <p>We then give the dataset a name, make it persistent, and save it to the underlying database. This test set has only 125 images, which we can visualize in the FiftyOne App.</p> In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(dataset)\n</pre> session = fo.launch_app(dataset) <p></p> In\u00a0[\u00a0]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>We can also run <code>evaluate_detections()</code> on this data to evaluate the YOLOv8n model's performance on images with ground truth bird detections. We will store the results under the <code>base</code> evaluation key:</p> In\u00a0[49]: Copied! <pre>base_bird_results = test_dataset.evaluate_detections(\n    \"yolov8n\", \n    eval_key=\"base\",\n    compute_mAP=True,\n)\n</pre> base_bird_results = test_dataset.evaluate_detections(     \"yolov8n\",      eval_key=\"base\",     compute_mAP=True, ) <pre>Evaluating detections...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [886.0ms elapsed, 0s remaining, 141.1 samples/s]      \nPerforming IoU sweep...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [619.1ms elapsed, 0s remaining, 201.9 samples/s]      \n</pre> In\u00a0[54]: Copied! <pre>mAP = base_bird_results.mAP()\nprint(f\"Base mAP = {mAP}\")\n</pre> mAP = base_bird_results.mAP() print(f\"Base mAP = {mAP}\") <pre>Base mAP = 0.24897924786479841\n</pre> In\u00a0[56]: Copied! <pre>base_bird_results.print_report(classes=classes)\n</pre> base_bird_results.print_report(classes=classes) <pre>              precision    recall  f1-score   support\n\n        bird       0.87      0.39      0.54       451\n\n   micro avg       0.87      0.39      0.54       451\n   macro avg       0.87      0.39      0.54       451\nweighted avg       0.87      0.39      0.54       451\n\n</pre> <p>We note that while the recall is the same as in the initial evaluation report over the entire COCO validation split, the precision is higher. This means there are images that have YOLOv8n <code>bird</code> predictions but not ground truth <code>bird</code> detections.</p> <p>The final step in preparing this test set is exporting the data into YOLOv8 format so we can run inference on just these samples with our fine-tuned model when we are done training. We will do so using the <code>export_yolo_data()</code> function we defined earlier.</p> In\u00a0[\u00a0]: Copied! <pre>export_yolo_data(\n    test_dataset, \n    \"birds_test\", \n    classes\n)\n</pre> export_yolo_data(     test_dataset,      \"birds_test\",      classes ) <p>Now we choose the data on which we will fine-tune the base YOLOv8 model. Our goal is to generate a high-quality training dataset whose examples cover all expected scenarios in that subset.</p> <p>In general, this is both an art and a science, and it can involve a variety of techniques, including</p> <ul> <li>pulling in data from other datasets</li> <li>annotating more data that you\u2019ve already collected with ground truth labels,</li> <li>augmenting your data with tools like Albumentations</li> <li>generating synthetic data with diffusion models or GANs.</li> </ul> <p>We\u2019ll take the first approach and incorporate existing high-quality data from Google\u2019s Open Images dataset. For a thorough tutorial on how to work with Open Images data, see Loading Open Images V6 and custom datasets with FiftyOne.</p> <p>The COCO training data on which YOLOv8 was trained contains $3,237$ images with <code>bird</code> detections. Open Images is more expansive, with the train, test, and validation splits together housing $20k+$ images with <code>Bird</code> detections.</p> <p>Let\u2019s create our training dataset. First, we\u2019ll create a dataset, <code>train_dataset</code>, by loading the <code>bird</code> detection labels from the COCO train split using the FiftyOne Dataset Zoo, and cloning this into a new <code>Dataset</code> object:</p> In\u00a0[\u00a0]: Copied! <pre>train_dataset = foz.load_zoo_dataset(\n    'coco-2017',\n    split='train',\n    classes=classes\n).clone()\n\ntrain_dataset.name = \"birds-train-data\"\ntrain_dataset.persistent = True\ntrain_dataset.save()\n</pre> train_dataset = foz.load_zoo_dataset(     'coco-2017',     split='train',     classes=classes ).clone()  train_dataset.name = \"birds-train-data\" train_dataset.persistent = True train_dataset.save() <p>Then, we\u2019ll load Open Images samples with <code>Bird</code> detection labels, passing in <code>only_matching=True</code> to only load the <code>Bird</code> labels. We then map these labels into COCO label format by changing <code>Bird</code> into <code>bird</code>.</p> In\u00a0[\u00a0]: Copied! <pre>oi_samples = foz.load_zoo_dataset(\n    \"open-images-v6\",\n    classes = [\"Bird\"],\n    only_matching=True,\n    label_types=\"detections\"\n).map_labels(\n    \"ground_truth\",\n    {\"Bird\":\"bird\"}\n)\n</pre> oi_samples = foz.load_zoo_dataset(     \"open-images-v6\",     classes = [\"Bird\"],     only_matching=True,     label_types=\"detections\" ).map_labels(     \"ground_truth\",     {\"Bird\":\"bird\"} ) <p>We can add these new samples into our training dataset with <code>merge_samples()</code>:</p> In\u00a0[\u00a0]: Copied! <pre>train_dataset.merge_samples(oi_samples)\n</pre> train_dataset.merge_samples(oi_samples) <p>This dataset contains $24,226$ samples with <code>bird</code> labels, or more than seven times as many birds as the base YOLOv8n model was trained on. In the next section, we'll demonstrate how to fine-tune the model on this data using the YOLO Trainer class.</p> <p>The final step in preparing our data is splitting it into training and validation sets and exporting it into YOLO format. We will use an 80\u201320 train-val split, which we will select randomly using FiftyOne\u2019s random utils.</p> In\u00a0[\u00a0]: Copied! <pre>import fiftyone.utils.random as four\n\n## delete existing tags to start fresh\ntrain_dataset.untag_samples(train_dataset.distinct(\"tags\"))\n\n## split into train and val\nfour.random_split(\n    train_dataset,\n    {\"train\": 0.8, \"val\": 0.2}\n)\n\n## export in YOLO format\nexport_yolo_data(\n    train_dataset, \n    \"birds_train\", \n    classes, \n    split = [\"train\", \"val\"]\n)\n</pre> import fiftyone.utils.random as four  ## delete existing tags to start fresh train_dataset.untag_samples(train_dataset.distinct(\"tags\"))  ## split into train and val four.random_split(     train_dataset,     {\"train\": 0.8, \"val\": 0.2} )  ## export in YOLO format export_yolo_data(     train_dataset,      \"birds_train\",      classes,      split = [\"train\", \"val\"] ) <p>Now all that is left is to do the fine-tuning! We will use YOLO command line syntax, with <code>mode=train</code>. We will specify the initial weights as the starting point for training, the number of epochs, image size, and batch size.</p> In\u00a0[\u00a0]: Copied! <pre>!yolo task=detect mode=train model=yolov8n.pt data=birds_train/dataset.yaml epochs=60 imgsz=640 batch=16\n</pre> !yolo task=detect mode=train model=yolov8n.pt data=birds_train/dataset.yaml epochs=60 imgsz=640 batch=16 <pre><code>  Image sizes 640 train, 640 val\n  Using 8 dataloader workers\n  Logging results to runs/detect/train\n  Starting training for 60 epochs...\n\n  Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n  1/60       6.65G      1.392      1.627      1.345         22        640: 1\n             Class     Images  Instances      Box(P          R      mAP50  m\n               all       4845      12487      0.677      0.524      0.581      0.339\n\n  Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n  2/60       9.58G      1.446      1.407      1.395         30        640: 1\n             Class     Images  Instances      Box(P          R      mAP50  m\n               all       4845      12487      0.669       0.47       0.54      0.316\n\n  Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n  3/60       9.58G       1.54      1.493      1.462         29        640: 1\n             Class     Images  Instances      Box(P          R      mAP50  m\n               all       4845      12487      0.529      0.329      0.349      0.188\n\n                                        ......\n\n  Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n 58/60       9.59G      1.263     0.9489      1.277         47        640: 1\n             Class     Images  Instances      Box(P          R      mAP50  m\n               all       4845      12487      0.751      0.631      0.708      0.446\n\n  Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n 59/60       9.59G      1.264     0.9476      1.277         29        640: 1\n             Class     Images  Instances      Box(P          R      mAP50  m\n               all       4845      12487      0.752      0.631      0.708      0.446\n\n  Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n 60/60       9.59G      1.257     0.9456      1.274         41        640: 1\n             Class     Images  Instances      Box(P          R      mAP50  m\n               all       4845      12487      0.752      0.631      0.709      0.446</code></pre> <p>For this walkthrough, $60$ epochs of training was sufficient to achieve convergence. If you are fine-tuning on a different dataset, you may need to change these parameters.</p> <p>With fine-tuning complete, we can generate predictions on our test data with the \u201cbest\u201d weights found during the training process, which are stored at <code>runs/detect/train/weights/best.pt</code>:</p> In\u00a0[\u00a0]: Copied! <pre>!yolo task=detect mode=predict model=runs/detect/train/weights/best.pt source=birds_test/images/val save_txt=True save_conf=True\n</pre> !yolo task=detect mode=predict model=runs/detect/train/weights/best.pt source=birds_test/images/val save_txt=True save_conf=True <p>Then we can load these predictions onto our data and visualize the predictions in the FiftyOne App:</p> In\u00a0[\u00a0]: Copied! <pre>filepaths = test_dataset.values(\"filepath\")\nprediction_filepaths = [get_prediction_filepath(fp, run_number=2) for fp in filepaths]\n\ntest_dataset.set_values(\n    \"yolov8n_bird_det_filepath\",\n    prediction_filepaths\n)\n\nadd_yolo_detections(\n    birds_test_dataset, \n    \"yolov8n_bird\", \n    \"yolov8n_bird_det_filepath\", \n    classes\n)\n</pre> filepaths = test_dataset.values(\"filepath\") prediction_filepaths = [get_prediction_filepath(fp, run_number=2) for fp in filepaths]  test_dataset.set_values(     \"yolov8n_bird_det_filepath\",     prediction_filepaths )  add_yolo_detections(     birds_test_dataset,      \"yolov8n_bird\",      \"yolov8n_bird_det_filepath\",      classes ) In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(test_dataset)\n</pre> session = fo.launch_app(test_dataset) <p></p> In\u00a0[\u00a0]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>On a holistic level, we can compare the performance of the fine-tuned model to the original, pretrained model by stacking their standard metrics against each other. The easiest way to get these metrics is with FiftyOne\u2019s Evaluation API:</p> In\u00a0[55]: Copied! <pre>finetune_bird_results = test_dataset.evaluate_detections(\n    \"yolov8n_bird\", \n    eval_key=\"finetune\",\n    compute_mAP=True,\n)\n</pre> finetune_bird_results = test_dataset.evaluate_detections(     \"yolov8n_bird\",      eval_key=\"finetune\",     compute_mAP=True, ) <pre>Evaluating detections...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [954.4ms elapsed, 0s remaining, 131.0 samples/s]      \nPerforming IoU sweep...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [751.8ms elapsed, 0s remaining, 166.3 samples/s]      \n</pre> <p>From this, we can immediately see improvement in the mean average precision (mAP):</p> In\u00a0[3]: Copied! <pre>print(\"yolov8n mAP: {}.format(base_bird_results.mAP()))\nprint(\"fine-tuned mAP: {}.format(finetune_bird_results.mAP()))\n</pre> print(\"yolov8n mAP: {}.format(base_bird_results.mAP())) print(\"fine-tuned mAP: {}.format(finetune_bird_results.mAP())) <pre>yolov8n mAP: 0.24897924786479841\nfine-tuned mAP: 0.31339033693212076\n</pre> <p>Printing out a report, we can see that the recall has improved from $0.39$ to $0.56$. This major improvement offsets a minor dip in precision, giving an overall higher F1 score ($0.67$ compared to $0.54$).</p> In\u00a0[56]: Copied! <pre>finetune_bird_results.print_report()\n</pre> finetune_bird_results.print_report() <pre>              precision    recall  f1-score   support\n\n        bird       0.81      0.56      0.67       506\n\n   micro avg       0.81      0.56      0.67       506\n   macro avg       0.81      0.56      0.67       506\nweighted avg       0.81      0.56      0.67       506\n\n</pre> <p>We can also look more closely at individual images to see where the fine-tuned model is having trouble. In particular, we can look at images with the most false negatives, or the most false positives:</p> In\u00a0[\u00a0]: Copied! <pre>fn_view = dataset.sort_by(\"eval_fn\", reverse=True)\nsession.view = fn_view\n</pre> fn_view = dataset.sort_by(\"eval_fn\", reverse=True) session.view = fn_view <p></p> In\u00a0[\u00a0]: Copied! <pre>session.freeze()\n</pre> session.freeze() In\u00a0[\u00a0]: Copied! <pre>fp_view = dataset.sort_by(\"eval_fp\", reverse=True)\nsession.view = fp_view\n</pre> fp_view = dataset.sort_by(\"eval_fp\", reverse=True) session.view = fp_view <p></p> In\u00a0[\u00a0]: Copied! <pre>session.freeze()\n</pre> session.freeze() <p>Looking at both the false positives and false negatives, we can see that the model struggles to correctly handle small features. This poor performance could be in part due to quality of the data, as many of these features are grainy. It could also be due to the training parameters, as both the pre-training and fine-tuning for this model used an image size of $640$ pixels, which might not allow for fine-grained details to be captured.</p> <p>To further improve the model\u2019s performance, we could try a variety of approaches, including:</p> <ul> <li>Using image augmentation to increase the proportion of images with small birds</li> <li>Gathering and annotating more images with small birds</li> <li>Increasing the image size during fine-tuning</li> </ul> <p>While YOLOv8 represents a step forward for real-time object detection and segmentation models, out-of-the-box it\u2019s aimed at general purpose uses. Before deploying the model, it is essential to understand how it performs on your data. Only then can you effectively fine-tune the YOLOv8 architecture to suit your specific needs.</p> <p>You can use FiftyOne to visualize, evaluate, and better understand YOLOv8 model predictions. After all, while YOLO may only look once, a conscientious computer vision engineer or researcher certainly looks twice (or more)!</p>"},{"location":"tutorials/yolov8/#fine-tune-yolov8-models-for-custom-use-cases-with-the-help-of-fiftyone","title":"Fine-tune YOLOv8 models for custom use cases with the help of FiftyOne\u00b6","text":""},{"location":"tutorials/yolov8/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/yolov8/#load-yolov8-predictions-in-fiftyone","title":"Load YOLOv8 predictions in FiftyOne\u00b6","text":""},{"location":"tutorials/yolov8/#generate-predictions","title":"Generate predictions\u00b6","text":""},{"location":"tutorials/yolov8/#load-detections","title":"Load detections\u00b6","text":""},{"location":"tutorials/yolov8/#load-segmentation-masks","title":"Load segmentation masks\u00b6","text":""},{"location":"tutorials/yolov8/#evaluate-yolov8-model-predictions","title":"Evaluate YOLOv8 model predictions\u00b6","text":""},{"location":"tutorials/yolov8/#compute-summary-statistics","title":"Compute summary statistics\u00b6","text":""},{"location":"tutorials/yolov8/#inspect-individual-predictions","title":"Inspect individual predictions\u00b6","text":""},{"location":"tutorials/yolov8/#curate-data-for-fine-tuning","title":"Curate data for fine-tuning\u00b6","text":""},{"location":"tutorials/yolov8/#generate-test-set","title":"Generate test set\u00b6","text":""},{"location":"tutorials/yolov8/#generate-training-set","title":"Generate training set\u00b6","text":""},{"location":"tutorials/yolov8/#fine-tune-a-yolov8-detection-model","title":"Fine-tune a YOLOv8 detection model\u00b6","text":""},{"location":"tutorials/yolov8/#assess-improvement-from-fine-tuning","title":"Assess improvement from fine-tuning\u00b6","text":""},{"location":"tutorials/yolov8/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/zero_shot_classification/","title":"Zero-Shot Image Classification with Multimodal Models and FiftyOne","text":"<p>Traditionally, computer vision models are trained to predict a fixed set of categories. For image classification, for instance, many standard models are trained on the ImageNet dataset, which contains 1,000 categories. All images must be assigned to one of these 1,000 categories, and the model is trained to predict the correct category for each image.</p> <p>For object detection, many popular models like YOLOv5, YOLOv8, and YOLO-NAS are trained on the MS COCO dataset, which contains 80 categories. In other words, the model is trained to detect objects in any of these categories, and ignore all other objects.</p> <p>Thanks to the recent advances in multimodal models, it is now possible to perform zero-shot learning, which allows us to predict categories that were not seen during training. This can be especially useful when:</p> <ul> <li>We want to roughly pre-label images with a new set of categories</li> <li>Obtaining labeled data for all categories is impractical or impossible.</li> <li>The categories are changing over time, and we want to predict new categories without retraining the model.</li> </ul> <p>In this walkthrough, you will learn how to apply and evaluate zero-shot image classification models to your data with FiftyOne, Hugging Face Transformers, and OpenCLIP!</p> <p>It covers the following:</p> <ul> <li>Loading zero-shot image classification models from Hugging Face and OpenCLIP with the FiftyOne Model Zoo</li> <li>Using the models to predict categories for images in your dataset</li> <li>Evaluating the predictions with FiftyOne</li> </ul> <p>We are going to illustrate how to work with many multimodal models:</p> <ul> <li>OpenAI CLIP</li> <li>AltCLIP</li> <li>ALIGN</li> <li>CLIPA</li> <li>SigLIP</li> <li>MetaCLIP</li> <li>EVA-CLIP</li> <li>Data Filtering Network (DFN)</li> </ul> <p>For a breakdown of what each model brings to the table, check out our \ud83d\udd76\ufe0f comprehensive collection of Awesome CLIP Papers.</p> <p>For this walkthrough, we will use the Caltech-256 dataset, which contains 30,607 images across 257 categories. We will use 1000 randomly selected images from the dataset for demonstration purposes. The zero-shot models were not explicitly trained on the Caltech-256 dataset, so we will use this as a test of the models' zero-shot capabilities. Of course, you can use any dataset you like!</p> <p>\ud83d\udca1 Your results may depend on how similar your dataset is to the training data of the zero-shot models.</p> <p>Before we start, let's install the required packages:</p> <pre>pip install -U torch torchvision fiftyone transformers timm open_clip_torch\n</pre> <p>Now let's import the relevant modules and load the dataset:</p> In\u00a0[3]: Copied! <pre>import fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.brain as fob\nfrom fiftyone import ViewField as F\n</pre> import fiftyone as fo import fiftyone.zoo as foz import fiftyone.brain as fob from fiftyone import ViewField as F In\u00a0[6]: Copied! <pre>dataset = foz.load_zoo_dataset(\n    \"caltech256\", \n    max_samples=1000, \n    shuffle=True, \n    persistent=True\n)\ndataset.name = \"CLIP-Comparison\"\n\nsession = fo.launch_app(dataset)\n</pre> dataset = foz.load_zoo_dataset(     \"caltech256\",      max_samples=1000,      shuffle=True,      persistent=True ) dataset.name = \"CLIP-Comparison\"  session = fo.launch_app(dataset) <p></p> <p>Here, we are using the <code>shuffle=True</code> option to randomly select 1000 images from the dataset, and are persisting the dataset to disk so that we can use it in future sessions. We also change the name of the dataset to reflect the experiment we are running.</p> <p>Finally, let's use the dataset's <code>distinct()</code> method to get a list of the distinct categories in the dataset, which we will give to the zero-shot models to predict:</p> In\u00a0[\u00a0]: Copied! <pre>classes = dataset.distinct(\"ground_truth.label\")\n</pre> classes = dataset.distinct(\"ground_truth.label\") <p>In a moment, we'll switch gears to a more explicit demonstration of how to load and apply zero-shot models in FiftyOne. This programmatic approach is useful for more advanced use cases, and illustrates how to use the models in a more flexible manner.</p> <p>For simpler scenarios, check out the FiftyOne Zero-Shot Prediction Plugin, which provides a convenient graphical interface for applying zero-shot models to your dataset. The plugin supports all of the models we are going to use in this walkthrough, and is a great way to quickly experiment with zero-shot models in FiftyOne. In addition to classificaion, the plugin also supports zero-shot object detection, instance segmentation, and semantic segmentation.</p> <p>If you have the FiftyOne Plugin Utils Plugin installed, you can install the Zero-Shot Prediction Plugin from the FiftyOne App:</p> <p></p> <p>If not, you can install the plugin from the command line:</p> <pre>fiftyone plugins download https://github.com/jacobmarks/zero-shot-prediction-plugin\n</pre> <p>Once the plugin is installed, you can run zero-shot models from the FiftyOne App by pressing the backtick key ('`') to open the command palette, selecting <code>zero-shot-predict</code> or <code>zero-shot-classify</code> from the dropdown, and following the prompts:</p> <p></p> <p>In this section, we will show how to explicitly load and apply a variety of zero-shot classification models to your dataset with FiftyOne. Our models will come from three places:</p> <ol> <li>OpenAI's CLIP model, which is natively supported by FiftyOne</li> <li>OpenCLIP, which is a collection of open-source CLIP-style models</li> <li>Hugging Face's Transformers library, which provides a wide variety of zero-shot models</li> </ol> <p>All of these models can be loaded from the FiftyOne Model Zoo via the <code>load_zoo_model()</code> function, although the arguments you pass to the function will depend on the model you are loading!</p> <p>Regardless of the model you are loading, the basic recipe for loading a zero-shot model is as follows:</p> <pre>model = foz.load_zoo_model(\n    \"&lt;zoo-model-name&gt;\",\n    classes=classes,\n    **kwargs\n)\n</pre> <p>The zoo model name is the name under which the model is registered in the FiftyOne Model Zoo.</p> <ul> <li><code>\"clip-vit-base32-torch\"</code> specifies the natively supported CLIP model, CLIP-ViT-B/32</li> <li><code>\"open-clip-torch\"</code> specifies that you want to load a specific model (architecture and pretrained checkpoint) from the OpenCLIP library. You can then specify the architecture with <code>clip_model=\"&lt;clip-architecture&gt;\"</code> and the checkpoint with <code>pretrained=\"&lt;checkpoint-name&gt;\"</code>. We will see examples of this in a moment. For a list of allowed architecture-checkpoint pairs, check out this results table from the OpenCLIP documentation. The <code>name</code> column contains the value for <code>clip_model</code>.</li> <li><code>\"zero-shot-classification-transformer-torch\"</code> specifies that you want to a zero-shot image classification model from the Hugging Face Transformers library. You can then specify the model via the <code>name_or_path</code> argument, which should be the repository name or model identifier of the model you want to load. Again, we will see examples of this in a moment.</li> </ul> <p>\ud83d\udca1 While we won't be exploring this degree of freedom, all of these models accept a <code>text_prompt</code> keyword argument, which allows you to override the prompt template used to embed the class names. Zero-shot classification results can vary based on this text!</p> <p>Once we have our model loaded (and classes set), we can use it like any other image classification model in FiftyOne by calling the dataset's <code>apply_model()</code> method:</p> <pre>dataset.apply_model(\n    model,\n    label_field=\"&lt;where-to-store-predictions&gt;\",\n)\n</pre> <p>For efficiency, we will also set our default batch size to 32, which will speed up the predictions:</p> In\u00a0[12]: Copied! <pre>fo.config.default_batch_size = 32\n</pre> fo.config.default_batch_size = 32 <p>Starting off with the natively supported CLIP model, we can load and apply it to our dataset as follows:</p> In\u00a0[\u00a0]: Copied! <pre>clip = foz.load_zoo_model(\n    \"clip-vit-base32-torch\",\n    classes=classes,\n)\n\ndataset.apply_model(clip, label_field=\"clip\")\n</pre> clip = foz.load_zoo_model(     \"clip-vit-base32-torch\",     classes=classes, )  dataset.apply_model(clip, label_field=\"clip\") <p>If we would like, after adding our predictions in the specified field, we can add some high-level information detailing what the field contains:</p> In\u00a0[\u00a0]: Copied! <pre>field = dataset.get_field(\"clip\")\nfield.description = \"OpenAI CLIP predictions\"\nfield.info = {\"clip_model\": \"CLIP-ViT-B-32\"}\nfield.save()\n</pre> field = dataset.get_field(\"clip\") field.description = \"OpenAI CLIP predictions\" field.info = {\"clip_model\": \"CLIP-ViT-B-32\"} field.save() In\u00a0[\u00a0]: Copied! <pre>session = fo.launch_app(dataset, auto=False)\n</pre> session = fo.launch_app(dataset, auto=False) <p>To see the FiftyOne App, open a tab in your browser and navigate to <code>http://localhost:5151</code>!</p> <p>We will then see this information when we hover over the \"clip\" field in the FiftyOne App. This can be useful if you want to use shorthand field names, or if you want to provide additional context to other users of the dataset.</p> <p>For the rest of the tutorial, we will omit this step for brevity, but you can add this information to any field in your dataset!</p> <p>To make life interesting, we will be running inference with 5 different OpenCLIP models:</p> <ul> <li>CLIPA</li> <li>Data Filtering Network (DFN)</li> <li>EVA-CLIP</li> <li>MetaCLIP</li> <li>SigLIP</li> </ul> <p>To reduce the repetition, we're just going to create a dictionary for the <code>clip_model</code> and <code>pretrained</code> arguments, and then loop through the dictionary to load and apply the models to our dataset:</p> In\u00a0[14]: Copied! <pre>open_clip_args = {\n    \"clipa\": {\n        \"clip_model\": 'hf-hub:UCSC-VLAA/ViT-L-14-CLIPA-datacomp1B',\n        \"pretrained\": '',\n        },\n    \"dfn\": {\n        \"clip_model\": 'ViT-B-16',\n        \"pretrained\": 'dfn2b',\n        },\n    \"eva02_clip\": {\n        \"clip_model\": 'EVA02-B-16',\n        \"pretrained\": 'merged2b_s8b_b131k',\n        },\n    \"metaclip\": {\n        \"clip_model\": 'ViT-B-32-quickgelu',\n        \"pretrained\": 'metaclip_400m',\n        },\n    \"siglip\": {\n        \"clip_model\": 'hf-hub:timm/ViT-B-16-SigLIP',\n        \"pretrained\": '',\n        },\n    }\n</pre> open_clip_args = {     \"clipa\": {         \"clip_model\": 'hf-hub:UCSC-VLAA/ViT-L-14-CLIPA-datacomp1B',         \"pretrained\": '',         },     \"dfn\": {         \"clip_model\": 'ViT-B-16',         \"pretrained\": 'dfn2b',         },     \"eva02_clip\": {         \"clip_model\": 'EVA02-B-16',         \"pretrained\": 'merged2b_s8b_b131k',         },     \"metaclip\": {         \"clip_model\": 'ViT-B-32-quickgelu',         \"pretrained\": 'metaclip_400m',         },     \"siglip\": {         \"clip_model\": 'hf-hub:timm/ViT-B-16-SigLIP',         \"pretrained\": '',         },     } In\u00a0[\u00a0]: Copied! <pre>for name, args in open_clip_args.items():\n    clip_model = args[\"clip_model\"]\n    pretrained = args[\"pretrained\"]\n    model = foz.load_zoo_model(\n        \"open-clip-torch\",\n        clip_model=clip_model,\n        pretrained=pretrained,\n        classes=classes,\n    )\n\n    dataset.apply_model(model, label_field=name)\n</pre> for name, args in open_clip_args.items():     clip_model = args[\"clip_model\"]     pretrained = args[\"pretrained\"]     model = foz.load_zoo_model(         \"open-clip-torch\",         clip_model=clip_model,         pretrained=pretrained,         classes=classes,     )      dataset.apply_model(model, label_field=name) <p>Finally, we will load and apply zero-shot image classification model sfrom the Hugging Face Transformers library. Once again, we will loop through a dictionary of model names and apply the models to our dataset:</p> In\u00a0[15]: Copied! <pre>transformer_model_repo_ids = {\n    \"altclip\": \"BAAI/AltCLIP\",\n    \"align\": \"kakaobrain/align-base\"\n}\n</pre> transformer_model_repo_ids = {     \"altclip\": \"BAAI/AltCLIP\",     \"align\": \"kakaobrain/align-base\" } In\u00a0[\u00a0]: Copied! <pre>for name, repo_id in transformer_model_repo_ids.items():\n    model = foz.load_zoo_model(\n        \"zero-shot-classification-transformer-torch\",\n        name_or_path=repo_id,\n        classes=classes,\n    )\n\n    dataset.apply_model(model, label_field=name)\n</pre> for name, repo_id in transformer_model_repo_ids.items():     model = foz.load_zoo_model(         \"zero-shot-classification-transformer-torch\",         name_or_path=repo_id,         classes=classes,     )      dataset.apply_model(model, label_field=name) <p>Now that we have applied all of our zero-shot models to our dataset, we can evaluate the predictions with FiftyOne! As a first step, let's use FiftyOne's Evaluation API to assign True/False labels to the predictions based on whether they match the ground truth labels.</p> <p>First, we will use the dataset's schema to get a list of all of the fields that contain predictions:</p> In\u00a0[23]: Copied! <pre>classification_fields = sorted(list(\n    dataset.get_field_schema(\n        ftype=fo.EmbeddedDocumentField, embedded_doc_type=fo.Classification\n    ).keys())\n)\n\nprediction_fields = [f for f in classification_fields if f != \"ground_truth\"]\n</pre> classification_fields = sorted(list(     dataset.get_field_schema(         ftype=fo.EmbeddedDocumentField, embedded_doc_type=fo.Classification     ).keys()) )  prediction_fields = [f for f in classification_fields if f != \"ground_truth\"] <p>Then, we will loop through these prediction fields and apply the dataset's <code>evaluate_classifications()</code> method to each one, evaluating against the <code>ground_truth</code> field:</p> In\u00a0[\u00a0]: Copied! <pre>for pf in prediction_fields:\n    eval_key = f\"{pf}_eval\"\n    dataset.evaluate_classifications(\n        pf,\n        gt_field=\"ground_truth\",\n        eval_key=eval_key,\n    )\n</pre> for pf in prediction_fields:     eval_key = f\"{pf}_eval\"     dataset.evaluate_classifications(         pf,         gt_field=\"ground_truth\",         eval_key=eval_key,     ) <p>We can then easily filter the dataset based on which models predicted the ground truth labels correctly, either programmatically in Python, or in the FiftyOne App. For example, here is how we could specify the view into the dataset containing all samples where SigLIP predicted the ground truth label correctly and CLIP did not:</p> In\u00a0[16]: Copied! <pre>dataset = fo.load_dataset(\"CLIP-Comparison\")\n</pre> dataset = fo.load_dataset(\"CLIP-Comparison\") In\u00a0[17]: Copied! <pre>siglip_not_clip_view = dataset.match((F(\"siglip_eval\") == True) &amp; (F(\"clip_eval\") == False))\n</pre> siglip_not_clip_view = dataset.match((F(\"siglip_eval\") == True) &amp; (F(\"clip_eval\") == False)) In\u00a0[20]: Copied! <pre>num_siglip_not_clip = len(siglip_not_clip_view)\nprint(f\"There were {num_siglip_not_clip} samples where the SigLIP model predicted correctly and the CLIP model did not.\")\n</pre> num_siglip_not_clip = len(siglip_not_clip_view) print(f\"There were {num_siglip_not_clip} samples where the SigLIP model predicted correctly and the CLIP model did not.\") <pre>There were 57 samples where the SigLIP model predicted correctly and the CLIP model did not.\n</pre> <p>Here is how we would accomplish the same thing in the FiftyOne App:</p> <p></p> <p>With the predictions evaluated, we can use FiftyOne's aggregation capabilities to get high-level insights into the performance of the zero-shot models.</p> <p>This will allow us to answer questions like:</p> <ul> <li>Which model was \"correct\" most often?</li> <li>What models were most or least confident in their predictions?</li> </ul> <p>For the first question, we can use the <code>count_values()</code> aggregation on the evaluation fields for our predictions, which will give us a count of the number of times each model was correct or incorrect. As an example:</p> In\u00a0[22]: Copied! <pre>dataset.count_values(f\"clip_eval\")\n</pre> dataset.count_values(f\"clip_eval\") Out[22]: <pre>{False: 197, True: 803}</pre> <p>Looping over our prediction fields and turning these raw counts into percentages, we can get a high-level view of the performance of our models:</p> In\u00a0[25]: Copied! <pre>for pf in prediction_fields:\n    eval_results = dataset.count_values(f\"{pf}_eval\")\n    percent_correct = eval_results.get(True, 0) / sum(eval_results.values())\n    print(f\"{pf}:  {percent_correct:.1%} correct\")\n</pre> for pf in prediction_fields:     eval_results = dataset.count_values(f\"{pf}_eval\")     percent_correct = eval_results.get(True, 0) / sum(eval_results.values())     print(f\"{pf}:  {percent_correct:.1%} correct\") <pre>align:  83.7% correct\naltclip:  87.6% correct\nclip:  80.3% correct\nclipa:  88.9% correct\ndfn:  91.0% correct\neva02_clip:  85.6% correct\nmetaclip:  84.3% correct\nsiglip:  64.9% correct\n</pre> <p>At least on this dataset, it looks like the DFN model was the clear winner, with the highest percentage of correct predictions. The other strong performers were CLIPA and AltCLIP.</p> <p>To answer the second question, we can use the <code>mean()</code> aggregation to get the average confidence of each model's predictions. This will give us a sense of how confident each model was in its predictions:</p> In\u00a0[26]: Copied! <pre>for pf in prediction_fields:\n    mean_conf = dataset.mean(F(f\"{pf}.confidence\"))\n    print(f\"Mean confidence for {pf}: {mean_conf:.3f}\")\n</pre> for pf in prediction_fields:     mean_conf = dataset.mean(F(f\"{pf}.confidence\"))     print(f\"Mean confidence for {pf}: {mean_conf:.3f}\") <pre>Mean confidence for align: 0.774\nMean confidence for altclip: 0.883\nMean confidence for clip: 0.770\nMean confidence for clipa: 0.912\nMean confidence for dfn: 0.926\nMean confidence for eva02_clip: 0.843\nMean confidence for metaclip: 0.824\nMean confidence for siglip: 0.673\n</pre> <p>For the most part, mean model confidence seems pretty strongly correlated with model accuracy. The DFN model, which was the most accurate, also had the highest mean confidence!</p> <p>These high-level insights are useful, but as always, they only tell part of the story. To get a more nuanced understanding of the performance of our zero-shot models \u2014 and how the models interface with our data \u2014 we can use FiftyOne's ViewExpressions to construct rich views of our data.</p> <p>One thing we might want to see is where all of the models were correct or incorrect. To probe these questions, we can construct a list with one <code>ViewExpression</code> for each model, and then use the <code>any()</code> and <code>all()</code> methods:</p> In\u00a0[27]: Copied! <pre>exprs = [F(f\"{pf}_eval\") == True for pf in prediction_fields]\n</pre> exprs = [F(f\"{pf}_eval\") == True for pf in prediction_fields] <p>First, let's see how many samples every model got correct:</p> In\u00a0[28]: Copied! <pre>all_right_view = dataset.match(F().all(exprs))\nprint(f\"{len(all_right_view)} samples were right for all models\")\n\nsession = fo.launch_app(all_right_view, auto=False)\n</pre> all_right_view = dataset.match(F().all(exprs)) print(f\"{len(all_right_view)} samples were right for all models\")  session = fo.launch_app(all_right_view, auto=False) <pre>498 samples were right for all models\nSession launched. Run `session.show()` to open the App in a cell output.\n</pre> <p>The fact that about half of the time, all of the models are \"correct\" and in agreement is good validation of both our data quality and the capabilities of our zero-shot models!</p> <p>How about when all of the models are incorrect?</p> In\u00a0[29]: Copied! <pre>all_wrong_view = dataset.match(~F().any(exprs))\nprint(f\"{len(all_wrong_view)} samples were wrong for all models\")\n\nsession = fo.launch_app(all_wrong_view, auto=False)\n</pre> all_wrong_view = dataset.match(~F().any(exprs)) print(f\"{len(all_wrong_view)} samples were wrong for all models\")  session = fo.launch_app(all_wrong_view, auto=False) <pre>45 samples were wrong for all models\nSession launched. Run `session.show()` to open the App in a cell output.\n</pre> <p></p> <p>The samples where all of the models are supposedly incorrect are interesting and merit further investigation. It could be that the ground truth labels are incorrect, or that the images are ambiguous and difficult to classify. It could also be that the zero-shot models are not well-suited to the dataset, or that the models are not well-suited to the task. In any case, these samples are worth a closer look!</p> <p>Looking at some of these samples in the FiftyOne App, we can see that some of the ground truth labels are indeed ambiguous or incorrect. Take the second image, for example. It is labeled as <code>\"treadmill\"</code>, while all but one of the zero-shot models predict <code>\"horse\"</code>. To a human, the image does indeed look like a horse, and the ground truth label is likely incorrect.</p> <p>The seventh image is a prime example of ambiguity. The ground truth label is <code>\"sneaker\"</code>, but almost all of the zero-shot models predict <code>\"tennis-shoes\"</code>. It is difficult to say which label is correct, and it is likely that the ground truth label is not specific enough to be useful.</p> <p>To get a more precise view into the relative quality of our zero-shot models, we would need to handle these edge cases and re-evaluate on the improved dataset.</p> <p>\ud83d\udca1 This is a great example of how the combination of zero-shot models and FiftyOne can be used to iteratively improve the quality of your data and your models!</p> <p>Before we wrap up, let's construct one even more nuanced view of our data: the samples where just one of the models was correct. This will really help us understand the strengths and weaknesses of each model.</p> <p>To construct this view, we will copy the array of expressions, remove one model from the array, and see where that model was correct and the others were not. We will then loop through the models, and find the samples where each any of these conditions is met:</p> In\u00a0[\u00a0]: Copied! <pre>n = len(prediction_fields)\nsub_exprs = []\nfor i in range(n):\n    tmp_exprs = exprs.copy()\n    expr = tmp_exprs.pop(i)\n    sub_exprs.append((expr &amp; ~F().any(tmp_exprs)))\n\none_right_view = dataset.match(F().any(sub_exprs))\n\nsession = fo.launch_app(one_right_view, auto=False)\n</pre> n = len(prediction_fields) sub_exprs = [] for i in range(n):     tmp_exprs = exprs.copy()     expr = tmp_exprs.pop(i)     sub_exprs.append((expr &amp; ~F().any(tmp_exprs)))  one_right_view = dataset.match(F().any(sub_exprs))  session = fo.launch_app(one_right_view, auto=False) <p></p> <p>Looking at these samples in the FiftyOne App, a few things stand out:</p> <ul> <li><p>First, the vast majority of these samples are primarily images of people's faces. A lot of the \"wrong\" predictions are related to people, faces, or facial features, like <code>\"eye-glasses\"</code>, <code>\"iris\"</code>, <code>\"yarmulke\"</code>, and <code>\"human-skeleton\"</code>. This is a good reminder that zero-shot models are not perfect, and that they are not well-suited to all types of images.</p> </li> <li><p>Second, of all 22 samples where only one model was correct, 11 of them were correctly predicted by the DFN model. This is more validation of the DFN model's strong performance on this dataset.</p> </li> </ul> <p>Zero-shot image classification is a powerful tool for predicting categories that were not seen during training. But it is not a panacea, and it is important to understand the strengths and weaknesses of zero-shot models, and how they interface with your data.</p> <p>In this walkthrough, we showed how to not only apply a variety of zero-shot image classification models to your data, but also how to evaluate them and choose the best model for your use case.</p> <p>The same principles can be applied to other types of zero-shot models, like zero-shot object detection, instance segmentation, and semantic segmentation. If you're interested in these use cases, check out the FiftyOne Zero-Shot Prediction Plugin.</p> <p>For zero-shot object detection, here are some resources to get you started:</p> <ul> <li>YOLO-World from Ultralytics</li> <li>Zero-Shot Detection Transformers from Hugging Face</li> <li>Evaluating Object Detections tutorial</li> </ul>"},{"location":"tutorials/zero_shot_classification/#zero-shot-image-classification-with-multimodal-models-and-fiftyone","title":"Zero-Shot Image Classification with Multimodal Models and FiftyOne\u00b6","text":""},{"location":"tutorials/zero_shot_classification/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/zero_shot_classification/#zero-shot-image-classification-with-the-fiftyone-zero-shot-prediction-plugin","title":"Zero-Shot Image Classification with the FiftyOne Zero-Shot Prediction Plugin\u00b6","text":""},{"location":"tutorials/zero_shot_classification/#zero-shot-image-classification-with-the-fiftyone-model-zoo","title":"Zero-Shot Image Classification with the FiftyOne Model Zoo\u00b6","text":""},{"location":"tutorials/zero_shot_classification/#basic-recipe-for-loading-a-zero-shot-model","title":"Basic Recipe for Loading a Zero-Shot Model\u00b6","text":""},{"location":"tutorials/zero_shot_classification/#zero-shot-image-classification-with-openai-clip","title":"Zero-Shot Image Classification with OpenAI CLIP\u00b6","text":""},{"location":"tutorials/zero_shot_classification/#zero-shot-image-classification-with-openclip","title":"Zero-Shot Image Classification with OpenCLIP\u00b6","text":""},{"location":"tutorials/zero_shot_classification/#zero-shot-image-classification-with-hugging-face-transformers","title":"Zero-Shot Image Classification with Hugging Face Transformers\u00b6","text":""},{"location":"tutorials/zero_shot_classification/#evaluating-zero-shot-image-classification-predictions-with-fiftyone","title":"Evaluating Zero-Shot Image Classification Predictions with FiftyOne\u00b6","text":""},{"location":"tutorials/zero_shot_classification/#using-fiftyones-evaluation-api","title":"Using FiftyOne's Evaluation API\u00b6","text":""},{"location":"tutorials/zero_shot_classification/#high-level-insights-using-aggregations","title":"High-Level Insights using Aggregations\u00b6","text":""},{"location":"tutorials/zero_shot_classification/#advanced-insights-using-viewexpressions","title":"Advanced Insights using ViewExpressions\u00b6","text":""},{"location":"tutorials/zero_shot_classification/#summary","title":"Summary\u00b6","text":""}]}