
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.49">
    
    
      
        <title>Clustering Images with Embeddings ¶ - Voxel51 Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6f8fc17f.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#clustering-images-with-embeddings" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Voxel51 Documentation" class="md-header__button md-logo" aria-label="Voxel51 Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Voxel51 Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Clustering Images with Embeddings ¶
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/thesteve0/vdoc-mkdocs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Voxel51 Documentation" class="md-nav__button md-logo" aria-label="Voxel51 Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Voxel51 Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/thesteve0/vdoc-mkdocs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Getting started
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Getting started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/install/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/virtualenv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Virtualenvs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/datasets_samples_fields/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Datasets, Samples, and Fields
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/application_tour/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tour of the Applications
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/troubleshooting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Troubleshooting
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../fiftyone_concepts/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    FiftyOne Concepts
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            FiftyOne Concepts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fiftyone_concepts/basics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Basics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../brain/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Machine Learning
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../recipes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How Do I
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../data_and_models/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Data and Models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6" id="__nav_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Data and Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../data_and_models/dataset_zoo/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Zoo Datasets
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_2" id="__nav_6_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2">
            <span class="md-nav__icon md-icon"></span>
            Zoo Datasets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../data_and_models/dataset_zoo/datasets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Built-In Zoo Datasets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../data_and_models/dataset_zoo/remote/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Remotely-Sourced Zoo Datasets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../data_and_models/dataset_zoo/api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Zoo Data API
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../data_and_models/hugging_face_data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face Datasets
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../integrations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Integrations
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../api/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    References
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_8" id="__nav_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            References
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cli/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CLI
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../community/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Community
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../teams/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fifty One Teams
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../release-notes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Releases
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-is-clustering" class="md-nav__link">
    <span class="md-ellipsis">
      What is Clustering? ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="What is Clustering? ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-building-blocks-of-clustering" class="md-nav__link">
    <span class="md-ellipsis">
      The Building Blocks of Clustering ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-clustering-works" class="md-nav__link">
    <span class="md-ellipsis">
      How Clustering Works ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-features-do-i-cluster-on" class="md-nav__link">
    <span class="md-ellipsis">
      What Features Do I Cluster On? ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#clustering-images-with-fiftyone-and-scikit-learn" class="md-nav__link">
    <span class="md-ellipsis">
      Clustering Images with FiftyOne and Scikit-learn ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Clustering Images with FiftyOne and Scikit-learn ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setup-and-installation" class="md-nav__link">
    <span class="md-ellipsis">
      Setup and Installation ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generating-features" class="md-nav__link">
    <span class="md-ellipsis">
      Generating Features ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#computing-and-visualizing-clusters" class="md-nav__link">
    <span class="md-ellipsis">
      Computing and Visualizing Clusters ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#keeping-track-of-clustering-runs" class="md-nav__link">
    <span class="md-ellipsis">
      Keeping Track of Clustering Runs ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#labeling-clusters-with-gpt-4v" class="md-nav__link">
    <span class="md-ellipsis">
      Labeling Clusters with GPT-4V ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion ¶
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/thesteve0/vdoc-mkdocs/blob/main/docs/tutorials/clustering.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  


<h1 id="clustering-images-with-embeddings">Clustering Images with Embeddings <a href="#Clustering-Images-with-Embeddings" title="Permalink to this headline">¶</a><a class="headerlink" href="#clustering-images-with-embeddings" title="Permanent link">&para;</a></h1>
<p><img alt="Clustering" src="../../_images/clustering_preview.jpg" /></p>
<p><a href="https://en.wikipedia.org/wiki/Cluster_analysis">Clustering</a> is an essential unsupervised learning technique that can help you discover hidden patterns in your data. This walkthrough, you’ll learn how to bring structure your visual data using Scikit-learn and FiftyOne!</p>
<p>It covers the following:</p>
<ul>
<li>
<p>What is clustering?</p>
</li>
<li>
<p>Generating features to cluster images</p>
</li>
<li>
<p>Clustering images using the FiftyOne Clustering Plugin</p>
</li>
<li>
<p>Keeping track of clustering runs in the FiftyOne App</p>
</li>
<li>
<p>Assigning labels to clusters using GPT-4V</p>
</li>
</ul>
<h2 id="what-is-clustering">What is Clustering? <a href="#What-is-Clustering?" title="Permalink to this headline">¶</a><a class="headerlink" href="#what-is-clustering" title="Permanent link">&para;</a></h2>
<h3 id="the-building-blocks-of-clustering">The Building Blocks of Clustering <a href="#The-Building-Blocks-of-Clustering" title="Permalink to this headline">¶</a><a class="headerlink" href="#the-building-blocks-of-clustering" title="Permanent link">&para;</a></h3>
<p>Imagine you have a ton of Lego blocks of all shapes and sizes spread out on the floor. It’s time to put the legos away, and you realize you don’t have a large enough bin to store all of them. Luckily, you find four smaller bins that can each hold roughly the same number of pieces. You <em>could</em> dump a random assortment of Legos in each bin and call it a day. But then, the next time you went to find a specific piece, you’d have quite the time digging around for it.</p>
<p>Instead, you have a better idea: putting similar pieces in the same bin would save you a lot of time and trouble later. But what <em>criterion</em> are you going to use to put Legos in bins? Are you going to assign bins for different colors? Or put all the square pieces in one bin and the circular pieces in another? It really depends on what Legos you have! This, in a nutshell, is clustering.</p>
<p>More formally, <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clustering</a>, or <em>cluster analysis</em>, is a set of techniques for <em>grouping</em> data points. Clustering algorithms take in a bunch of objects, and spit out assignments for each object. Unlike classification, however, clustering does not start with a list of classes to categorize the objects, forcing objects to fall into preset buckets. Rather, clustering attempts to <em>discover</em> the buckets given the data. In other words, clustering is
about <em>uncovering</em> structure in data, not predicting labels in a preexisting structure.</p>
<p>This last point merits repeating: <em>clustering is not about predicting labels</em>. Unlike classification, detection, and segmentation tasks, there are no ground truth labels for clustering tasks. We call algorithms like this <a href="https://cloud.google.com/discover/what-is-unsupervised-learning">unsupervised</a>, contrasting with <a href="https://cloud.google.com/discover/what-is-supervised-learning">supervised</a> and <a href="https://en.wikipedia.org/wiki/Self-supervised_learning">self-supervised</a> learning tasks.</p>
<p>To hammer it home, clustering is <em>training-free</em>. A clustering algorithm will take in features of your data points (the objects) and use those features to split your objects into groups. When successful, those groups highlight unique characteristics, giving you a view into the structure of your data.</p>
<p>💡 This means that clustering is an extremely powerful tool for exploring your data—especially when your data is unlabeled!</p>
<h3 id="how-clustering-works">How Clustering Works <a href="#How-Clustering-Works" title="Permalink to this headline">¶</a><a class="headerlink" href="#how-clustering-works" title="Permanent link">&para;</a></h3>
<p>If you’ve been paying close attention, you may have noticed the distinction subtly drawn between <em>clustering</em> and <em>clustering algorithms</em>. This is because clustering is an umbrella term encompassing various techniques!</p>
<p>Clustering algorithms come in a few flavors, distinguished by the criterion they use to assign cluster membership. A few of the most common flavors of clustering are:</p>
<p><strong>Centroid-based clustering</strong>: for example, techniques like <a href="https://scikit-learn.org/stable/modules/clustering.html#k-means">K-means</a> and <a href="https://scikit-learn.org/stable/modules/clustering.html#mean-shift">Mean Shift</a> clustering. These methods try to find central points by which to define each cluster, called <em>centroids</em>, which seek to maximize some notion of coherence between points <em>within</em> a cluster. This flavor of clustering scales well to large datasets but is sensitive to outliers
and random initialization. Often, multiple runs are performed, and the best one is chosen. You may find that techniques like K-means struggle with high-dimensional data — “the curse of dimensionality” — and can better uncover structure when paired with dimensionality reduction techniques like uniform manifold approximation &amp; projection ( <a href="https://umap-learn.readthedocs.io/en/latest/">UMAP</a>). We’ll explain how to pair the two below.</p>
<p><strong>Density-based clustering</strong>: techniques like <a href="https://scikit-learn.org/stable/modules/clustering.html#dbscan">DBSCAN</a>, <a href="https://scikit-learn.org/stable/modules/clustering.html#hdbscan">HDBSCAN</a>, and <a href="https://scikit-learn.org/stable/modules/clustering.html#optics">OPTICS</a> select clusters based on how sparsely or densely populated the feature space is. Conceptually, these algorithms treat high-density regions as clusters, breaking the clusters off when the points become sufficiently
spread out in feature space. Simple density-based techniques like DBSCAN can have difficulty working with high-dimensional data, where data may not be densely colocated. However, more sophisticated techniques like HDBSCAN can overcome some of these limitations and uncover remarkable structure from high dimensional features.</p>
<p><strong>Hierarchical clustering</strong>: These techniques seek to either:</p>
<ol>
<li>
<p><em>Construct</em> clusters by starting with individual points and iteratively combining clusters into larger composites or</p>
</li>
<li>
<p><em>Deconstruct</em> clusters, starting with all objects in one cluster and iteratively diving clusters into smaller components.</p>
</li>
</ol>
<p>Constructive techniques like <a href="https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering">Agglomerative Clustering</a> become computationally expensive as the dataset grows, but performance can be quite impressive for small-to-medium datasets and low-dimensional features.</p>
<p>📚 For a comprehensive discussion on 10+ of the most commonly used clustering algorithms, check out this intuitive, <a href="https://scikit-learn.org/stable/modules/clustering.html">well-written guide from Scikit-learn</a>!</p>
<h3 id="what-features-do-i-cluster-on">What Features Do I Cluster On? <a href="#What-Features-Do-I-Cluster-On?" title="Permalink to this headline">¶</a><a class="headerlink" href="#what-features-do-i-cluster-on" title="Permanent link">&para;</a></h3>
<p>For the Lego bricks we started this discussion with, the features (length, width, height, curvature, etc.) are independent entities we can view as columns in a data table. After normalizing this data so that no one feature dominates the others, we could pass a row of numerical values as a <em>feature vector</em> into our clustering algorithm for each Lego block. Historically, clustering has had many applications like this, operating on lightly preprocessed numerical values from data tables or time
series.</p>
<p>Unstructured data like images don’t fit quite as nicely into this framework for a few simple reasons:</p>
<ol>
<li>
<p>Images can vary in size (aspect ratio and resolution)</p>
</li>
<li>
<p>Raw pixel values can be very noisy</p>
</li>
<li>
<p>Correlations between pixels can be highly nonlinear</p>
</li>
</ol>
<p>If we were to go through the trouble of reshaping and standardizing all of our image sizes, normalizing pixel values, denoising, and flattening the multidimensional arrays into “feature vectors”, treating these processed pixel arrays as features would put a tremendous amount of stress on the <em>unsupervised</em> clustering algorithm to uncover structure. This can work for simple datasets like <a href="https://docs.voxel51.com/user_guide/dataset_zoo/datasets.html#mnist">MNIST</a>, but it is often not an option
in practice.</p>
<p>Fortunately, we have <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">powerful nonlinear function approximation tools</a> called deep neural networks! Restricting our attention to the image domain, we have models like <a href="https://docs.voxel51.com/user_guide/model_zoo/models.html#clip-vit-base32-torch">CLIP</a> and <a href="https://docs.voxel51.com/user_guide/model_zoo/models.html#dinov2-vitl14-torch">DINOv2</a> whose output is a meaningful representation of the input data, and we have models
trained for specific tasks like image classification, from which we typically take the <a href="https://medium.com/vector-database/how-to-get-the-right-vector-embeddings-83295ced7f35">outputs of the second to last layer</a> of the network. There are also variational autoencoder (VAE) networks, from which it is common to take the representation at the middle layer!</p>
<p>💡Different models have different architectures, and were trained on different datasets and towards different tasks. All of these elements inform the types of features a model learns. Do your homework 📚:)</p>
<h2 id="clustering-images-with-fiftyone-and-scikit-learn">Clustering Images with FiftyOne and Scikit-learn <a href="#Clustering-Images-with-FiftyOne-and-Scikit-learn" title="Permalink to this headline">¶</a><a class="headerlink" href="#clustering-images-with-fiftyone-and-scikit-learn" title="Permanent link">&para;</a></h2>
<h3 id="setup-and-installation">Setup and Installation <a href="#Setup-and-Installation" title="Permalink to this headline">¶</a><a class="headerlink" href="#setup-and-installation" title="Permanent link">&para;</a></h3>
<p>With all that background out of the way, let’s turn theory into practice and learn how to use clustering to structure our unstructured data. We’ll be leveraging two open-source machine learning libraries: <a href="https://scikit-learn.org/stable/index.html">scikit-learn</a>, which comes pre-packaged with <a href="https://scikit-learn.org/stable/modules/clustering.html">implementations of most common clustering algorithms</a>, and fiftyone, which streamlines the management and visualization of unstructured data:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><span class="p">[</span> <span class="p">]:</span>
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">scikit</span><span class="o">-</span><span class="n">learn</span> <span class="n">fiftyone</span>
</span></code></pre></div>
<p>The <a href="https://github.com/jacobmarks/clustering-runs-plugin">FiftyOne Clustering Plugin</a> makes our lives even easier. It provides the connective tissue between scikit-learn’s clustering algorithms and our images and wraps all of this in a simple UI within the <a href="https://docs.voxel51.com/user_guide/app.html">FiftyOne App</a>. We can install the plugin from the CLI:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><span class="p">[</span> <span class="p">]:</span>
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><span class="err">!</span><span class="n">fiftyone</span> <span class="n">plugins</span> <span class="n">download</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">jacobmarks</span><span class="o">/</span><span class="n">clustering</span><span class="o">-</span><span class="n">plugin</span>
</span></code></pre></div>
<p>We will also need two more libraries: <a href="https://github.com/openai/CLIP">OpenAI’s CLIP GitHub repo</a>, enabling us to generate image features with the CLIP model, and the <a href="https://umap-learn.readthedocs.io/en/latest/">umap-learn</a> library, which will let us apply a dimensionality reduction technique called Uniform Manifold Approximation and Projection (UMAP) to those features to visualize them in 2D:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><span class="p">[</span> <span class="p">]:</span>
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">umap</span><span class="o">-</span><span class="n">learn</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">openai</span><span class="o">/</span><span class="n">CLIP</span><span class="o">.</span><span class="n">git</span>
</span></code></pre></div>
<p>Note that neither of these two libraries is strictly necessary — you could generate features with any model from the <a href="https://docs.voxel51.com/user_guide/model_zoo/index.html">FiftyOne Model Zoo</a> that exposes embeddings, and can perform <a href="https://docs.voxel51.com/tutorials/dimension_reduction.html">dimensionality reduction</a> with alternative techniques like PCA or tSNE.</p>
<p>Once you have all of the necessary libraries installed, in a Python process, import the relevant FiftyOne modules, and load a dataset from the <a href="https://docs.voxel51.com/user_guide/dataset_zoo/index.html">FiftyOne Dataset Zoo</a> (or your data if you’d like!). For this walkthrough, we’ll be using the validation split (5,000 samples) from the <a href="https://docs.voxel51.com/user_guide/dataset_zoo/datasets.html#dataset-zoo-coco-2017">MS COCO</a> dataset:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><span class="p">[</span> <span class="p">]:</span>
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><span class="kn">import</span><span class="w"> </span><span class="nn">fiftyone</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">fo</span>
</span><span id="__span-7-2"><span class="kn">import</span><span class="w"> </span><span class="nn">fiftyone.brain</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">fob</span>
</span><span id="__span-7-3"><span class="kn">import</span><span class="w"> </span><span class="nn">fiftyone.zoo</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">foz</span>
</span><span id="__span-7-4"><span class="kn">from</span><span class="w"> </span><span class="nn">fiftyone</span><span class="w"> </span><span class="kn">import</span> <span class="n">ViewField</span> <span class="k">as</span> <span class="n">F</span>
</span><span id="__span-7-5">
</span><span id="__span-7-6"><span class="c1"># load dataset from the zoo, rename, and persist to database</span>
</span><span id="__span-7-7"><span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;coco-2017&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
</span><span id="__span-7-8"><span class="c1"># delete labels to simulate starting with unlabeled data</span>
</span><span id="__span-7-9"><span class="n">dataset</span><span class="o">.</span><span class="n">select_fields</span><span class="p">()</span><span class="o">.</span><span class="n">keep_fields</span><span class="p">()</span>
</span><span id="__span-7-10"><span class="n">dataset</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;clustering-demo&quot;</span>
</span><span id="__span-7-11"><span class="n">dataset</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="__span-7-12">
</span><span id="__span-7-13"><span class="c1"># launch the app to visualize the dataset</span>
</span><span id="__span-7-14"><span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</span></code></pre></div>
<p>If you’re working in a Jupyter Notebook, you can pass auto=False and then open a tab in your browser to wherever <code>session.url</code> is pointing (typically ​​ <a href="​​http://localhost:5151/">http://localhost:5151/</a>) to see the app in its full glory.</p>
<p><img alt="FiftyOne App" src="../../_images/clustering_dataset_in_app.jpg" /></p>
<h3 id="generating-features">Generating Features <a href="#Generating-Features" title="Permalink to this headline">¶</a><a class="headerlink" href="#generating-features" title="Permanent link">&para;</a></h3>
<p>Now that we have our data, we must generate the features we will use to cluster. For this walkthrough, we will look at two different features: the 512-dimensional vectors generated by our CLIP Vision Transformer and the two-dimensional vectors generated by running these high-dimensional vectors through a UMAP dimensionality reduction routine.</p>
<p>To run dimensionality reduction on a FiftyOne sample collection, we will use the FiftyOne Brain’s <code>compute_visualization()</code> function, which supports UMAP, PCA, and tSNE via the method keyword argument. We could generate the CLIP embeddings using our dataset’s <code>compute_embeddings()</code> method and then explicitly pass this into our dimensionality reduction routine. But instead, we can kill two birds with one stone by implicitly telling <code>compute_visualization()</code> to compute embeddings using CLIP
and store these embeddings in a field <code>”clip_embeddings”</code>, then use these to get 2D representations:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><span class="p">[</span> <span class="p">]:</span>
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-9-1"><span class="n">res</span> <span class="o">=</span> <span class="n">fob</span><span class="o">.</span><span class="n">compute_visualization</span><span class="p">(</span>
</span><span id="__span-9-2">    <span class="n">dataset</span><span class="p">,</span>
</span><span id="__span-9-3">    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;clip-vit-base32-torch&quot;</span><span class="p">,</span>
</span><span id="__span-9-4">    <span class="n">embeddings</span><span class="o">=</span><span class="s2">&quot;clip_embeddings&quot;</span><span class="p">,</span>
</span><span id="__span-9-5">    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;umap&quot;</span><span class="p">,</span>
</span><span id="__span-9-6">    <span class="n">brain_key</span><span class="o">=</span><span class="s2">&quot;clip_vis&quot;</span><span class="p">,</span>
</span><span id="__span-9-7">    <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span>
</span><span id="__span-9-8"><span class="p">)</span>
</span><span id="__span-9-9"><span class="n">dataset</span><span class="o">.</span><span class="n">set_values</span><span class="p">(</span><span class="s2">&quot;clip_umap&quot;</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">current_points</span><span class="p">)</span>
</span></code></pre></div>
<p>The <code>brain_key</code> argument allows us to access these results by name, either programmatically or in the FiftyOne App moving forward. The last line takes the array of 2D vectors we generated and stores them in a new field <code>”clip_umap”</code> on our dataset.</p>
<p>Refreshing the app and opening an <a href="https://docs.voxel51.com/user_guide/app.html#embeddings-panel">Embeddings Panel</a>, we should see a 2D representation of our dataset, where each point in the plot corresponds to a single image:</p>
<p><img alt="Embeddings Panel" src="../../_images/clustering_open_embeddings_panel.gif" /></p>
<h3 id="computing-and-visualizing-clusters">Computing and Visualizing Clusters <a href="#Computing-and-Visualizing-Clusters" title="Permalink to this headline">¶</a><a class="headerlink" href="#computing-and-visualizing-clusters" title="Permanent link">&para;</a></h3>
<p>With our feature vectors in hand, we can use the FiftyOne Clustering Plugin to bring structure to our data. In the FiftyOne App, press the backtick key on your keyboard and type <code>compute_clusters</code>. Click on the entry in the dropdown to open the clustering modal.</p>
<p><img alt="Compute Clusters" src="../../_images/clustering_compute_clusters_operator.gif" /></p>
<p>Enter a <code>run_key</code> (similar to the <code>brain_key</code> above) to access the clustering run’s results. As you do so, watch the input form dynamically update. At this point, you have two key decisions to make: what features to cluster on and which clustering algorithm to employ!</p>
<p>Select <code>”kmeans”</code> as your clustering method and <code>”clip_umap”</code> as your feature vectors. Set the number of clusters to 20, using the default values for all other parameters. Hit enter and let the clustering algorithm run. It should only take a few seconds.</p>
<p>Once the computation finishes, notice the new field on your samples containing string representations of integers, which signify which cluster a given sample was assigned to. You can filter on these values directly and view one cluster at a time in the sample grid:</p>
<p><img alt="Filtering Clusters" src="../../_images/clustering_filter_by_cluster_number.gif" /></p>
<p>What is <em>even more</em> interesting is coloring by these cluster labels in our embeddings plot:</p>
<p><img alt="Coloring by Clusters" src="../../_images/clustering_color_by_cluster.gif" /></p>
<p>Visualizing your clusters like this allows you to sanity check the clustering routine and can provide an intuitive view into the structure of your data. In this example, we can see a cluster of teddy bears which is fairly well separated from the rest of our data. This clustering routine also uncovered a boundary between farm animals and more exotic animals like elephants and zebras.</p>
<p>Now, create a new clustering run, increasing the number of clusters to 30 (don’t forget to color the embeddings in this new field). Depending on a bit of randomness (all of the routine’s initializations are random), there’s a strong chance that elephants and zebras will now occupy their own clusters.</p>
<p>Returning to the initial set of clusters, let’s dig into one final area in the embeddings plot. Notice how a few images of people playing soccer got lumped into a cluster of primarily tennis images. This is because we passed 2D dimensionality reduced vectors into our clustering routine rather than the embedding vectors themselves. While 2D projections are helpful for visualization, and techniques like UMAP are fairly good at retaining structure, relative distances are not exactly preserved, and
some information is lost. Suppose we instead pass our CLIP embeddings directly into our clustering computation with the same hyperparameters. In that case, these soccer images are assigned to the same cluster as the rest of the soccer images, along with other field sports like frisbee and baseball:</p>
<p><img alt="UMAP Limitations" src="../../_images/clustering_umap_limitation.gif" /></p>
<p>💡 The key takeaway is that high-dimensional features are not better than low-dimensional ones or vice versa. Every choice comes with a trade-off. This is why you should experiment with different techniques, hyperparameters, and features.</p>
<p>To make this even more apparent, let’s use HDBSCAN as our clustering algorithm, which does not allow us to specify the number of clusters, replacing this with parameters like <code>min_cluster_size</code> and <code>max_cluster_size</code> along with criteria on which to merge clusters. We’ll use our CLIP embeddings as features, and as a rough starting point, we’ll say we only want clusters between 10 and 300 elements. If the cluster is too large, it may not be helpful; if it is too small, it may pick up on noise
rather than signal. The specific values are, of course, dataset-dependent!</p>
<p>When we color by our cluster labels, the results look a bit messy. However, when we look at the images for each cluster individually, we see that we identified some very interesting collections of samples in our dataset.</p>
<p><img alt="HDSCAN Clusters" src="../../_images/clustering_hdbscan.gif" /></p>
<p>Note that for HDBSCAN, label <code>-1</code> is given to all background images. These images are not merged into any of the final clusters.</p>
<h3 id="keeping-track-of-clustering-runs">Keeping Track of Clustering Runs <a href="#Keeping-Track-of-Clustering-Runs" title="Permalink to this headline">¶</a><a class="headerlink" href="#keeping-track-of-clustering-runs" title="Permanent link">&para;</a></h3>
<p>As you test out various combinations of features, clustering techniques, and hyperparameters, you may want to keep track of what “configuration” you used to generate a specific set of clusters. Fortunately, the FiftyOne Clustering Plugin handles all of this for you, using <a href="https://docs.voxel51.com/plugins/developing_plugins.html#storing-custom-runs">custom runs</a>. The plugin exposes an operator <code>get_clustering_run_info</code>, which lets you select a run by run_key and view a nicely formatted
printout of all of the run’s parameters in the app:</p>
<p><img alt="Clustering Run Info" src="../../_images/clustering_get_clustering_info.jpg" /></p>
<p>You can also access this information programmatically by passing the <code>run_key</code> to the dataset’s <code>get_run_info()</code> method!</p>
<h3 id="labeling-clusters-with-gpt-4v">Labeling Clusters with GPT-4V <a href="#Labeling-Clusters-with-GPT-4V" title="Permalink to this headline">¶</a><a class="headerlink" href="#labeling-clusters-with-gpt-4v" title="Permanent link">&para;</a></h3>
<p>Until now, our clusters have only had numbers, which we have used as a glorified housekeeping device. However, if we cluster for some specific characteristic in our dataset, we should be able to identify that and use it to label our samples loosely. Naively, we could go through our clusters individually, select and visualize just the images in a given cluster, and try to tag the cluster ourselves.</p>
<p>Or…we could use a multimodal large language model to do this for us! The FiftyOne Clustering Plugin provides this functionality, leveraging <a href="https://openai.com/research/gpt-4v-system-card">GPT-4V</a>’s multimodal understanding capabilities to give each cluster a conceptual label.</p>
<p>To use this functionality, you must have an OpenAI API key environment variable (creating an account if necessary), which you can set as follows:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-10-1"><span class="p">[</span> <span class="p">]:</span>
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-11-1"><span class="err">!</span><span class="n">export</span> <span class="n">OPENAI_API_KEY</span><span class="o">=</span><span class="n">sk</span><span class="o">-...</span>
</span></code></pre></div>
<p>This functionality is provided via the <code>label_clusters_with_gpt4v</code> operator, which randomly selects five images from each cluster, feeds them into GPT-4V with a task-specific prompt, and processes the results.</p>
<p>Depending on the number of clusters you have (GPT-4V can be slow, and this scales linearly in the number of clusters), you may want to <a href="https://docs.voxel51.com/plugins/using_plugins.html#delegated-operations">delegate execution</a> of the operation by checking the box in the operator’s modal and then launch the job from the command line with:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-12-1"><span class="n">fiftyone</span> <span class="n">delegated</span> <span class="n">launch</span>
</span></code></pre></div>
<p><img alt="Labeling Clusters with GPT-4V" src="../../_images/clustering_gpt4v_labeling.gif" /></p>
<h2 id="conclusion">Conclusion <a href="#Conclusion" title="Permalink to this headline">¶</a><a class="headerlink" href="#conclusion" title="Permanent link">&para;</a></h2>
<p>In this walkthrough, we covered how to combine deep neural networks with popular clustering algorithms to bring structure to unstructured data using scikit-learn and FiftyOne. Along the way, we saw that the feature vectors, the algorithm, and the hyperparameter you choose can greatly impact the final results of clustering computations, both in terms of what the clusters select for and how well they identify structure in your data.</p>
<p>Once you have run these clustering routines on your data, a few key questions arise:</p>
<ol>
<li>
<p>How do I quantitatively compare and contrast these clustering runs?</p>
</li>
<li>
<p>How do I synthesize the insights from multiple clustering runs to better understand my data?</p>
</li>
<li>
<p>How do I leverage these insights to train better models?</p>
</li>
</ol>
<p>Answering these questions will help you reap the rewards of clustering.</p>
<p>If you want to dive deeper into the world of clustering, here are a few avenues that you may want to explore:</p>
<ul>
<li>
<p><strong>Choice of embedding model</strong>: We used CLIP, a semantic foundation model for this walkthrough. See how things change when you use other semantic models from <a href="https://docs.voxel51.com/integrations/huggingface.html#image-embeddings">Hugging Face’s Transformers library</a>, or <a href="https://docs.voxel51.com/integrations/openclip.html">OpenCLIP</a>. Now see how the picture changes when you use a “pixels-and-patches” computer vision model like
<a href="https://docs.voxel51.com/user_guide/model_zoo/models.html#resnet50-imagenet-torch">ResNet50</a>, or a self–supervised model like <a href="https://docs.voxel51.com/user_guide/model_zoo/models.html#dinov2-vitl14-torch">DINOv2</a>.</p>
</li>
<li>
<p><strong>Clustering Hyperparameters</strong>: We barely touched the number of clusters in this walkthrough. Your results may vary as you increase or decrease this number. For some techniques, like k-means clustering, there are heuristics you can use to <a href="https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters/">estimate the optimal number of clusters</a>. Don’t stop there; experiment with other hyperparameters as well!</p>
</li>
<li>
<p><strong>Concept Modeling Techniques</strong>: the built-in concept modeling technique in this walkthrough uses GPT-4V and some light prompting to identify each cluster’s core concept. This is but one way to approach an open-ended problem. Try using <a href="https://github.com/jacobmarks/image-captioning">image captioning</a> and <a href="https://en.wikipedia.org/wiki/Topic_model">topic modeling</a>, or create your own technique!</p>
</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.path", "navigation.indexes", "navigation.top", {"icon": {"repo": "fontawesome/brands/github"}}, "content.action.edit"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.88dd0f4e.min.js"></script>
      
    
  </body>
</html>