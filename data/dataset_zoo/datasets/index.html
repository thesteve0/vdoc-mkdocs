
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../">
      
      
        <link rel="next" href="../remote/">
      
      
      <link rel="icon" href="../../../_static/images/icons/voxel51-166px.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.49">
    
    
      
        <title>Built-In Zoo Datasets - Voxel51 Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.6f8fc17f.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../_static/css/voxel51-docs.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="custom">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#built-in-zoo-datasets" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Voxel51 Documentation" class="md-header__button md-logo" aria-label="Voxel51 Documentation" data-md-component="logo">
      
  <img src="../../../_static/images/voxel51-logo-white-300.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Voxel51 Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Built-In Zoo Datasets
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/thesteve0/vdoc-mkdocs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Voxel51 Documentation" class="md-nav__button md-logo" aria-label="Voxel51 Documentation" data-md-component="logo">
      
  <img src="../../../_static/images/voxel51-logo-white-300.png" alt="logo">

    </a>
    Voxel51 Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/thesteve0/vdoc-mkdocs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../getting_started/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Getting started
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Getting started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/basic/install/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../getting_started/basic/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Basics
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Basics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/basic/install/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/basic/virtualenv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Virtualenvs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/basic/datasets_samples_fields/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Datasets, Samples, and Fields
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/basic/application_tour/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tour of the Application
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/basic/viz_embeddings/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Visualizing Embeddings
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/basic/subset_sample/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Subsetting and Sample Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/basic/zero_shot_label/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Zero Shot Classification for Labeling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/basic/edit_fields/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Creating and Editing Fields and Labels
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/basic/fine_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Fine-Tuning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/basic/model_eval/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Evaluation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/basic/next_steps/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    What's Next
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/basic/troubleshooting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Troubleshooting
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../getting_started/focused_getting_starteds/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Focused Getting Starteds
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            Focused Getting Starteds
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/focused_getting_starteds/medical_imaging/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Medical Imaging
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../fiftyone_concepts/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    FiftyOne Concepts
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            FiftyOne Concepts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/basics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Basics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/running_environments/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Running Environments
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../fiftyone_concepts/dataset_creation/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Loading Data into FiftyOne
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Loading Data into FiftyOne
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/dataset_creation/datasets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Datasets from Disk
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/dataset_creation/samples/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Sample Parsers
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/using_datasets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Datasets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/using_views/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dataset Views
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/groups/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Grouped Datasets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/app/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using the App
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/annotation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Annotating Datasets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/evaluation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Evaluating Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/using_aggregations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Aggregations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/plots/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Interactive Plots
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/export_datasets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exporting Datasets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/draw_labels/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Drawing Labels on Samples
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Configuring FiftyOne
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../fiftyone_concepts/brain/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Machine Learning (FiftyOne Brain)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../how_do_i/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    How Do I
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            How Do I
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../how_do_i/cheat_sheets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cheat Sheets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../how_do_i/recipes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Recipes
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Data
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6" id="__nav_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Data
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Zoo Datasets
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_2" id="__nav_6_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6_2">
            <span class="md-nav__icon md-icon"></span>
            Zoo Datasets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Built-In Zoo Datasets
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Built-In Zoo Datasets
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#activitynet-100" class="md-nav__link">
    <span class="md-ellipsis">
      ActivityNet 100 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#activitynet-200" class="md-nav__link">
    <span class="md-ellipsis">
      ActivityNet 200 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bdd100k" class="md-nav__link">
    <span class="md-ellipsis">
      BDD100K ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#caltech-101" class="md-nav__link">
    <span class="md-ellipsis">
      Caltech-101 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#caltech-256" class="md-nav__link">
    <span class="md-ellipsis">
      Caltech-256 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cifar-10" class="md-nav__link">
    <span class="md-ellipsis">
      CIFAR-10 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cifar-100" class="md-nav__link">
    <span class="md-ellipsis">
      CIFAR-100 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cityscapes" class="md-nav__link">
    <span class="md-ellipsis">
      Cityscapes ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coco-2014" class="md-nav__link">
    <span class="md-ellipsis">
      COCO-2014 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coco-2017" class="md-nav__link">
    <span class="md-ellipsis">
      COCO-2017 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fashion-mnist" class="md-nav__link">
    <span class="md-ellipsis">
      Fashion MNIST ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#families-in-the-wild" class="md-nav__link">
    <span class="md-ellipsis">
      Families in the Wild ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hmbd51" class="md-nav__link">
    <span class="md-ellipsis">
      HMBD51 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imagenet-2012" class="md-nav__link">
    <span class="md-ellipsis">
      ImageNet 2012 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imagenet-sample" class="md-nav__link">
    <span class="md-ellipsis">
      ImageNet Sample ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kinetics-400" class="md-nav__link">
    <span class="md-ellipsis">
      Kinetics 400 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kinetics-600" class="md-nav__link">
    <span class="md-ellipsis">
      Kinetics 600 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kinetics-700" class="md-nav__link">
    <span class="md-ellipsis">
      Kinetics 700 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kinetics-700-2020" class="md-nav__link">
    <span class="md-ellipsis">
      Kinetics 700-2020 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kitti" class="md-nav__link">
    <span class="md-ellipsis">
      KITTI ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kitti-multiview" class="md-nav__link">
    <span class="md-ellipsis">
      KITTI Multiview ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#labeled-faces-in-the-wild" class="md-nav__link">
    <span class="md-ellipsis">
      Labeled Faces in the Wild ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mnist" class="md-nav__link">
    <span class="md-ellipsis">
      MNIST ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#open-images-v6" class="md-nav__link">
    <span class="md-ellipsis">
      Open Images V6 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#open-images-v7" class="md-nav__link">
    <span class="md-ellipsis">
      Open Images V7 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#places" class="md-nav__link">
    <span class="md-ellipsis">
      Places ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quickstart" class="md-nav__link">
    <span class="md-ellipsis">
      Quickstart ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quickstart-geo" class="md-nav__link">
    <span class="md-ellipsis">
      Quickstart Geo ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quickstart-video" class="md-nav__link">
    <span class="md-ellipsis">
      Quickstart Video ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quickstart-groups" class="md-nav__link">
    <span class="md-ellipsis">
      Quickstart Groups ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quickstart-3d" class="md-nav__link">
    <span class="md-ellipsis">
      Quickstart 3D ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sama-coco" class="md-nav__link">
    <span class="md-ellipsis">
      Sama-COCO ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ucf101" class="md-nav__link">
    <span class="md-ellipsis">
      UCF101 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#voc-2007" class="md-nav__link">
    <span class="md-ellipsis">
      VOC-2007 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#voc-2012" class="md-nav__link">
    <span class="md-ellipsis">
      VOC-2012 ¶
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../remote/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Remotely-Sourced Zoo Datasets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Zoo Data API
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hugging_face_data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face Datasets
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../models/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_7" id="__nav_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../models/model_zoo/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Zoo Models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_7_2" id="__nav_7_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_2">
            <span class="md-nav__icon md-icon"></span>
            Zoo Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/model_zoo/models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Built-in Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/model_zoo/remote/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Remote Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/model_zoo/design/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Interface
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/model_zoo/api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hugging_face_data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face Datasets
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../plugins/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Plugins
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_8" id="__nav_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Plugins
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../plugins/using_plugins/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Plugins
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../plugins/developing_plugins/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Developing Plugins
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../integrations/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Integrations
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            Integrations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../references/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    References
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_10" id="__nav_10_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            References
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../cli/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CLI
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../community/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Community
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../teams/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FiftyOne Teams
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../release-notes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Releases
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_14" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../faq/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_14" id="__nav_14_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_14">
            <span class="md-nav__icon md-icon"></span>
            FAQ
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#activitynet-100" class="md-nav__link">
    <span class="md-ellipsis">
      ActivityNet 100 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#activitynet-200" class="md-nav__link">
    <span class="md-ellipsis">
      ActivityNet 200 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bdd100k" class="md-nav__link">
    <span class="md-ellipsis">
      BDD100K ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#caltech-101" class="md-nav__link">
    <span class="md-ellipsis">
      Caltech-101 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#caltech-256" class="md-nav__link">
    <span class="md-ellipsis">
      Caltech-256 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cifar-10" class="md-nav__link">
    <span class="md-ellipsis">
      CIFAR-10 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cifar-100" class="md-nav__link">
    <span class="md-ellipsis">
      CIFAR-100 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cityscapes" class="md-nav__link">
    <span class="md-ellipsis">
      Cityscapes ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coco-2014" class="md-nav__link">
    <span class="md-ellipsis">
      COCO-2014 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coco-2017" class="md-nav__link">
    <span class="md-ellipsis">
      COCO-2017 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fashion-mnist" class="md-nav__link">
    <span class="md-ellipsis">
      Fashion MNIST ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#families-in-the-wild" class="md-nav__link">
    <span class="md-ellipsis">
      Families in the Wild ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hmbd51" class="md-nav__link">
    <span class="md-ellipsis">
      HMBD51 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imagenet-2012" class="md-nav__link">
    <span class="md-ellipsis">
      ImageNet 2012 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imagenet-sample" class="md-nav__link">
    <span class="md-ellipsis">
      ImageNet Sample ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kinetics-400" class="md-nav__link">
    <span class="md-ellipsis">
      Kinetics 400 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kinetics-600" class="md-nav__link">
    <span class="md-ellipsis">
      Kinetics 600 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kinetics-700" class="md-nav__link">
    <span class="md-ellipsis">
      Kinetics 700 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kinetics-700-2020" class="md-nav__link">
    <span class="md-ellipsis">
      Kinetics 700-2020 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kitti" class="md-nav__link">
    <span class="md-ellipsis">
      KITTI ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kitti-multiview" class="md-nav__link">
    <span class="md-ellipsis">
      KITTI Multiview ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#labeled-faces-in-the-wild" class="md-nav__link">
    <span class="md-ellipsis">
      Labeled Faces in the Wild ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mnist" class="md-nav__link">
    <span class="md-ellipsis">
      MNIST ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#open-images-v6" class="md-nav__link">
    <span class="md-ellipsis">
      Open Images V6 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#open-images-v7" class="md-nav__link">
    <span class="md-ellipsis">
      Open Images V7 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#places" class="md-nav__link">
    <span class="md-ellipsis">
      Places ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quickstart" class="md-nav__link">
    <span class="md-ellipsis">
      Quickstart ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quickstart-geo" class="md-nav__link">
    <span class="md-ellipsis">
      Quickstart Geo ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quickstart-video" class="md-nav__link">
    <span class="md-ellipsis">
      Quickstart Video ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quickstart-groups" class="md-nav__link">
    <span class="md-ellipsis">
      Quickstart Groups ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quickstart-3d" class="md-nav__link">
    <span class="md-ellipsis">
      Quickstart 3D ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sama-coco" class="md-nav__link">
    <span class="md-ellipsis">
      Sama-COCO ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ucf101" class="md-nav__link">
    <span class="md-ellipsis">
      UCF101 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#voc-2007" class="md-nav__link">
    <span class="md-ellipsis">
      VOC-2007 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#voc-2012" class="md-nav__link">
    <span class="md-ellipsis">
      VOC-2012 ¶
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/thesteve0/vdoc-mkdocs/blob/main/docs/data/dataset_zoo/datasets.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  


<h1 id="built-in-zoo-datasets">Built-In Zoo Datasets <a href="#built-in-zoo-datasets" title="Permalink to this headline">¶</a><a class="headerlink" href="#built-in-zoo-datasets" title="Permanent link">&para;</a></h1>
<p>This page lists all of the natively available datasets in the FiftyOne Dataset
Zoo.</p>
<p>Check out the <a href="../api/#dataset-zoo-api">API reference</a> for complete instructions
for using the Dataset Zoo.</p>
<p>Note</p>
<p>Some datasets are loaded via the
<a href="https://pytorch.org/vision/stable/datasets.html">TorchVision Datasets</a>
or <a href="https://www.tensorflow.org/datasets">TensorFlow Datasets</a> packages
under the hood.</p>
<p>If you do not have a <a href="../api/#dataset-zoo-ml-backend">suitable package</a>
installed when attempting to download a zoo dataset, you’ll see an error
message that will help you install one.</p>
<table>
<thead>
<tr>
<th>Dataset name</th>
<th>Tags</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#dataset-zoo-activitynet-100">ActivityNet 100</a></td>
<td>video, classification, action-recognition, temporal-detection</td>
</tr>
<tr>
<td><a href="#dataset-zoo-activitynet-200">ActivityNet 200</a></td>
<td>video, classification, action-recognition, temporal-detection</td>
</tr>
<tr>
<td><a href="#dataset-zoo-bdd100k">BDD100K</a></td>
<td>image, multilabel, automotive, manual</td>
</tr>
<tr>
<td><a href="#dataset-zoo-caltech101">Caltech-101</a></td>
<td>image, classification</td>
</tr>
<tr>
<td><a href="#dataset-zoo-caltech256">Caltech-256</a></td>
<td>image, classification</td>
</tr>
<tr>
<td><a href="#dataset-zoo-cifar10">CIFAR-10</a></td>
<td>image, classification</td>
</tr>
<tr>
<td><a href="#dataset-zoo-cifar100">CIFAR-100</a></td>
<td>image, classification</td>
</tr>
<tr>
<td><a href="#dataset-zoo-cityscapes">Cityscapes</a></td>
<td>image, multilabel, automotive, manual</td>
</tr>
<tr>
<td><a href="#dataset-zoo-coco-2014">COCO-2014</a></td>
<td>image, detection, segmentation</td>
</tr>
<tr>
<td><a href="#dataset-zoo-coco-2017">COCO-2017</a></td>
<td>image, detection, segmentation</td>
</tr>
<tr>
<td><a href="#dataset-zoo-fashion-mnist">Fashion MNIST</a></td>
<td>image, classification</td>
</tr>
<tr>
<td><a href="#dataset-zoo-fiw">Families in the Wild</a></td>
<td>image, classification</td>
</tr>
<tr>
<td><a href="#dataset-zoo-hmdb51">HMDB51</a></td>
<td>video, action-recognition</td>
</tr>
<tr>
<td><a href="#dataset-zoo-imagenet-2012">ImageNet 2012</a></td>
<td>image, classification, manual</td>
</tr>
<tr>
<td><a href="#dataset-zoo-imagenet-sample">ImageNet Sample</a></td>
<td>image, classification</td>
</tr>
<tr>
<td><a href="#dataset-zoo-kinetics-400">Kinetics 400</a></td>
<td>video, classification, action-recognition</td>
</tr>
<tr>
<td><a href="#dataset-zoo-kinetics-600">Kinetics 600</a></td>
<td>video, classification, action-recognition</td>
</tr>
<tr>
<td><a href="#dataset-zoo-kinetics-700">Kinetics 700</a></td>
<td>video, classification, action-recognition</td>
</tr>
<tr>
<td><a href="#dataset-zoo-kinetics-700-2020">Kinetics 700-2020</a></td>
<td>video, classification, action-recognition</td>
</tr>
<tr>
<td><a href="#dataset-zoo-kitti">KITTI</a></td>
<td>image, detection</td>
</tr>
<tr>
<td><a href="#dataset-zoo-kitti-multiview">KITTI Multiview</a></td>
<td>image, point-cloud, detection</td>
</tr>
<tr>
<td><a href="#dataset-zoo-lfw">Labeled Faces in the Wild</a></td>
<td>image, classification, facial-recognition</td>
</tr>
<tr>
<td><a href="#dataset-zoo-mnist">MNIST</a></td>
<td>image, classification</td>
</tr>
<tr>
<td><a href="#dataset-zoo-open-images-v6">Open Images V6</a></td>
<td>image, classification, detection, segmentation, relationships</td>
</tr>
<tr>
<td><a href="#dataset-zoo-open-images-v7">Open Images V7</a></td>
<td>image, classification, detection, segmentation, keypoints, relationships</td>
</tr>
<tr>
<td><a href="#dataset-zoo-places">Places</a></td>
<td>image, classification</td>
</tr>
<tr>
<td><a href="#dataset-zoo-quickstart">Quickstart</a></td>
<td>image, quickstart</td>
</tr>
<tr>
<td><a href="#dataset-zoo-quickstart-geo">Quickstart Geo</a></td>
<td>image, location, quickstart</td>
</tr>
<tr>
<td><a href="#dataset-zoo-quickstart-video">Quickstart Video</a></td>
<td>video, quickstart</td>
</tr>
<tr>
<td><a href="#dataset-zoo-quickstart-groups">Quickstart Groups</a></td>
<td>image, point-cloud, quickstart</td>
</tr>
<tr>
<td><a href="#dataset-zoo-quickstart-3d">Quickstart 3D</a></td>
<td>3d, point-cloud, mesh, quickstart</td>
</tr>
<tr>
<td><a href="#dataset-zoo-sama-coco">Sama-COCO</a></td>
<td>image, detection, segmentation</td>
</tr>
<tr>
<td><a href="#dataset-zoo-ucf101">UCF101</a></td>
<td>video, action-recognition</td>
</tr>
<tr>
<td><a href="#dataset-zoo-voc-2007">VOC-2007</a></td>
<td>image, detection</td>
</tr>
<tr>
<td><a href="#dataset-zoo-voc-2012">VOC-2012</a></td>
<td>image, detection</td>
</tr>
</tbody>
</table>
<h2 id="activitynet-100">ActivityNet 100 <a href="#activitynet-100" title="Permalink to this headline">¶</a><a class="headerlink" href="#activitynet-100" title="Permanent link">&para;</a></h2>
<p>ActivityNet is a large-scale video dataset for human activity understanding
supporting the tasks of global video classification, trimmed activity
classification, and temporal activity detection.</p>
<p>This version contains videos and temporal activity detections for the 100 class
version of the dataset.</p>
<p>Note</p>
<p>Check out <a href="../../../integrations/activitynet/#activitynet">this guide</a> for more details on using
FiftyOne to work with ActivityNet.</p>
<p><strong>Notes</strong></p>
<ul>
<li>
<p>ActivityNet 100 and 200 differ in the number of activity classes and
videos per split</p>
</li>
<li>
<p>Partial downloads will download videos (if still available) from YouTube</p>
</li>
<li>
<p>Full splits can be loaded by first downloading the official source files
from the
<a href="https://docs.google.com/forms/d/e/1FAIpQLSeKaFq9ZfcmZ7W0B0PbEhfbTHY41GeEgwsa7WobJgGUhn4DTQ/viewform">ActivityNet maintainers</a></p>
</li>
<li>
<p>The test set does not have annotations</p>
</li>
</ul>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>activitynet-100</code></p>
</li>
<li>
<p>Dataset source: <a href="http://activity-net.org/index.html">http://activity-net.org/index.html</a></p>
</li>
<li>
<p>Dataset size: 223 GB</p>
</li>
<li>
<p>Tags: <code>video, classification, action-recognition, temporal-detection</code></p>
</li>
<li>
<p>Supported splits: <code>train, validation, test</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.ActivityNet100Dataset" title="fiftyone.zoo.datasets.base.ActivityNet100Dataset"><code>ActivityNet100Dataset</code></a></p>
</li>
</ul>
<p><strong>Full split stats</strong></p>
<ul>
<li>
<p>Train split: 4,819 videos (7,151 instances)</p>
</li>
<li>
<p>Test split: 2,480 videos (labels withheld)</p>
</li>
<li>
<p>Validation split: 2,383 videos (3,582 instances)</p>
</li>
</ul>
<p><strong>Partial downloads</strong></p>
<p>FiftyOne provides parameters that can be used to efficiently download specific
subsets of the ActivityNet dataset to suit your needs. When new subsets are
specified, FiftyOne will use existing downloaded data first if possible before
resorting to downloading additional data from YouTube.</p>
<p>The following parameters are available to configure a partial download of
ActivityNet 100 by passing them to
<a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>:</p>
<ul>
<li>
<p><strong>split</strong> ( <em>None</em>) and <strong>splits</strong> ( <em>None</em>): a string or list of strings,
respectively, specifying the splits to load. Supported values are
<code>("train", "test", "validation")</code>. If none are provided, all available
splits are loaded</p>
</li>
<li>
<p><strong>source_dir</strong> ( <em>None</em>): the directory containing the manually downloaded
ActivityNet files used to avoid downloading videos from YouTube</p>
</li>
<li>
<p><strong>classes</strong> ( <em>None</em>): a string or list of strings specifying required
classes to load. If provided, only samples containing at least one instance
of a specified class will be loaded</p>
</li>
<li>
<p><strong>max_duration</strong> ( <em>None</em>): only videos with a duration in seconds that is
less than or equal to the <code>max_duration</code> will be downloaded. By default,
all videos are downloaded</p>
</li>
<li>
<p><strong>copy_files</strong> ( <em>True</em>): whether to move (False) or create copies (True) of
the source files when populating <code>dataset_dir</code>. This is only relevant
when a <code>source_dir</code> is provided</p>
</li>
<li>
<p><strong>num_workers</strong> ( <em>None</em>): the number of processes to use when downloading
individual videos. By default, <code>multiprocessing.cpu_count()</code> is used</p>
</li>
<li>
<p><strong>shuffle</strong> ( <em>False</em>): whether to randomly shuffle the order in which
samples are chosen for partial downloads</p>
</li>
<li>
<p><strong>seed</strong> ( <em>None</em>): a random seed to use when shuffling</p>
</li>
<li>
<p><strong>max_samples</strong> ( <em>None</em>): a maximum number of samples to load per split. If
<code>classes</code> are also specified, only up to the number of samples that
contain at least one specified class will be loaded. By default, all
matching samples are loaded</p>
</li>
</ul>
<p>Note</p>
<p>See
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.ActivityNet100Dataset" title="fiftyone.zoo.datasets.base.ActivityNet100Dataset"><code>ActivityNet100Dataset</code></a> and
<a href="../api/fiftyone.utils.activitynet.md#fiftyone.utils.activitynet.ActivityNetDatasetImporter" title="fiftyone.utils.activitynet.ActivityNetDatasetImporter"><code>ActivityNetDatasetImporter</code></a>
for complete descriptions of the optional keyword arguments that you can
pass to <a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>.</p>
<p><strong>Full split downloads</strong></p>
<p>Many videos have been removed from YouTube since the creation of ActivityNet.
As a result, if you do not specify any partial download parameters defined in
the previous section, you must first download the official source files from
the ActivityNet maintainers in order to load a full split into FiftyOne.</p>
<p>To download the source files, you must fill out
<a href="https://docs.google.com/forms/d/e/1FAIpQLSeKaFq9ZfcmZ7W0B0PbEhfbTHY41GeEgwsa7WobJgGUhn4DTQ/viewform">this form</a>.</p>
<p>Refer to <a href="../../../integrations/activitynet/#activitynet-full-split-downloads">this page</a> to see how to load
full splits by passing the <code>source_dir</code> parameter to
<a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>.</p>
<p><strong>Example usage</strong></p>
<p>Note</p>
<p>In order to work with video datasets, you’ll need to have
<a href="../../../getting_started/basic/troubleshooting/#videos-do-not-load-in-the-app">ffmpeg installed</a>).</p>
<p><img alt="activitynet-100-validation" src="../../../_images/activitynet-100-validation.png" /></p>
<h2 id="activitynet-200">ActivityNet 200 <a href="#activitynet-200" title="Permalink to this headline">¶</a><a class="headerlink" href="#activitynet-200" title="Permanent link">&para;</a></h2>
<p>ActivityNet is a large-scale video dataset for human activity understanding
supporting the tasks of global video classification, trimmed activity
classification, and temporal activity detection.</p>
<p>This version contains videos and temporal activity detections for the 200 class
version of the dataset.</p>
<p>Note</p>
<p>Check out <a href="../../../integrations/activitynet/#activitynet">this guide</a> for more details on using
FiftyOne to work with ActivityNet.</p>
<p><strong>Notes</strong></p>
<ul>
<li>
<p>ActivityNet 200 is a superset of ActivityNet 100</p>
</li>
<li>
<p>ActivityNet 100 and 200 differ in the number of activity classes and videos
per split</p>
</li>
<li>
<p>Partial downloads will download videos (if still available) from YouTube</p>
</li>
<li>
<p>Full splits can be loaded by first downloading the official source files
from the
<a href="https://docs.google.com/forms/d/e/1FAIpQLSeKaFq9ZfcmZ7W0B0PbEhfbTHY41GeEgwsa7WobJgGUhn4DTQ/viewform">ActivityNet maintainers</a></p>
</li>
<li>
<p>The test set does not have annotations</p>
</li>
</ul>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>activitynet-200</code></p>
</li>
<li>
<p>Dataset source: <a href="http://activity-net.org/index.html">http://activity-net.org/index.html</a></p>
</li>
<li>
<p>Dataset size: 500 GB</p>
</li>
<li>
<p>Tags: <code>video, classification, action-recognition, temporal-detection</code></p>
</li>
<li>
<p>Supported splits: <code>train, validation, test</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.ActivityNet200Dataset" title="fiftyone.zoo.datasets.base.ActivityNet200Dataset"><code>ActivityNet200Dataset</code></a></p>
</li>
</ul>
<p><strong>Full split stats</strong></p>
<ul>
<li>
<p>Train split: 10,024 videos (15,410 instances)</p>
</li>
<li>
<p>Test split: 5,044 videos (labels withheld)</p>
</li>
<li>
<p>Validation split: 4,926 videos (7,654 instances)</p>
</li>
</ul>
<p><strong>Partial downloads</strong></p>
<p>FiftyOne provides parameters that can be used to efficiently download specific
subsets of the ActivityNet dataset to suit your needs. When new subsets are
specified, FiftyOne will use existing downloaded data first if possible before
resorting to downloading additional data from YouTube.</p>
<p>The following parameters are available to configure a partial download of
ActivityNet 200 by passing them to
<a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>:</p>
<ul>
<li>
<p><strong>split</strong> ( <em>None</em>) and <strong>splits</strong> ( <em>None</em>): a string or list of strings,
respectively, specifying the splits to load. Supported values are
<code>("train", "test", "validation")</code>. If none are provided, all available
splits are loaded</p>
</li>
<li>
<p><strong>source_dir</strong> ( <em>None</em>): the directory containing the manually downloaded
ActivityNet files used to avoid downloading videos from YouTube</p>
</li>
<li>
<p><strong>classes</strong> ( <em>None</em>): a string or list of strings specifying required
classes to load. If provided, only samples containing at least one instance
of a specified class will be loaded</p>
</li>
<li>
<p><strong>max_duration</strong> ( <em>None</em>): only videos with a duration in seconds that is
less than or equal to the <code>max_duration</code> will be downloaded. By default,
all videos are downloaded</p>
</li>
<li>
<p><strong>copy_files</strong> ( <em>True</em>): whether to move (False) or create copies (True) of
the source files when populating <code>dataset_dir</code>. This is only relevant
when a <code>source_dir</code> is provided</p>
</li>
<li>
<p><strong>num_workers</strong> ( <em>None</em>): the number of processes to use when downloading
individual videos. By default, <code>multiprocessing.cpu_count()</code> is used</p>
</li>
<li>
<p><strong>shuffle</strong> ( <em>False</em>): whether to randomly shuffle the order in which
samples are chosen for partial downloads</p>
</li>
<li>
<p><strong>seed</strong> ( <em>None</em>): a random seed to use when shuffling</p>
</li>
<li>
<p><strong>max_samples</strong> ( <em>None</em>): a maximum number of samples to load per split. If
<code>classes</code> are also specified, only up to the number of samples that
contain at least one specified class will be loaded. By default, all
matching samples are loaded</p>
</li>
</ul>
<p>Note</p>
<p>See
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.ActivityNet200Dataset" title="fiftyone.zoo.datasets.base.ActivityNet200Dataset"><code>ActivityNet200Dataset</code></a> and
<a href="../api/fiftyone.utils.activitynet.md#fiftyone.utils.activitynet.ActivityNetDatasetImporter" title="fiftyone.utils.activitynet.ActivityNetDatasetImporter"><code>ActivityNetDatasetImporter</code></a>
for complete descriptions of the optional keyword arguments that you can
pass to <a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>.</p>
<p><strong>Full split downloads</strong></p>
<p>Many videos have been removed from YouTube since the creation of ActivityNet.
As a result, if you do not specify any partial download parameters defined in
the previous section, you must first download the official source files from
the ActivityNet maintainers in order to load a full split into FiftyOne.</p>
<p>To download the source files, you must fill out
<a href="https://docs.google.com/forms/d/e/1FAIpQLSeKaFq9ZfcmZ7W0B0PbEhfbTHY41GeEgwsa7WobJgGUhn4DTQ/viewform">this form</a>.</p>
<p>Refer to <a href="../../../integrations/activitynet/#activitynet-full-split-downloads">this page</a> to see how to load
full splits by passing the <code>source_dir</code> parameter to
<a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>.</p>
<p><strong>Example usage</strong></p>
<p>Note</p>
<p>In order to work with video datasets, you’ll need to have
<a href="../../../getting_started/basic/troubleshooting/#videos-do-not-load-in-the-app">ffmpeg installed</a>).</p>
<p><img alt="activitynet-200-validation" src="../../../_images/activitynet-200-validation.png" /></p>
<h2 id="bdd100k">BDD100K <a href="#bdd100k" title="Permalink to this headline">¶</a><a class="headerlink" href="#bdd100k" title="Permanent link">&para;</a></h2>
<p>The Berkeley Deep Drive (BDD) dataset is one of the largest and most diverse
video datasets for autonomous vehicles.</p>
<p>The BDD100K dataset contains 100,000 video clips collected from more than
50,000 rides covering New York, San Francisco Bay Area, and other regions.
The dataset contains diverse scene types such as city streets, residential
areas, and highways. Furthermore, the videos were recorded in diverse
weather conditions at different times of the day.</p>
<p>The videos are split into training (70K), validation (10K) and testing
(20K) sets. Each video is 40 seconds long with 720p resolution and a frame
rate of 30fps. The frame at the 10th second of each video is annotated for
image classification, detection, and segmentation tasks.</p>
<p>This version of the dataset contains only the 100K images extracted from
the videos as described above, together with the image classification,
detection, and segmentation labels.</p>
<p>Note</p>
<p>In order to load the BDD100K dataset, you must download the source data
manually. The directory should be organized in the following format:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><span class="n">source_dir</span><span class="o">/</span>
</span><span id="__span-0-2">    <span class="n">labels</span><span class="o">/</span>
</span><span id="__span-0-3">        <span class="n">bdd100k_labels_images_train</span><span class="o">.</span><span class="n">json</span>
</span><span id="__span-0-4">        <span class="n">bdd100k_labels_images_val</span><span class="o">.</span><span class="n">json</span>
</span><span id="__span-0-5">    <span class="n">images</span><span class="o">/</span>
</span><span id="__span-0-6">        <span class="mi">100</span><span class="n">k</span><span class="o">/</span>
</span><span id="__span-0-7">            <span class="n">train</span><span class="o">/</span>
</span><span id="__span-0-8">            <span class="n">test</span><span class="o">/</span>
</span><span id="__span-0-9">            <span class="n">val</span><span class="o">/</span>
</span></code></pre></div>
<p>You can register at <a href="https://bdd-data.berkeley.edu">https://bdd-data.berkeley.edu</a> in order to get links
to download the data.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>bdd100k</code></p>
</li>
<li>
<p>Dataset source: <a href="https://bdd-data.berkeley.edu">https://bdd-data.berkeley.edu</a></p>
</li>
<li>
<p>Dataset size: 7.10 GB</p>
</li>
<li>
<p>Tags: <code>image, multilabel, automotive, manual</code></p>
</li>
<li>
<p>Supported splits: <code>train, validation, test</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.BDD100KDataset" title="fiftyone.zoo.datasets.base.BDD100KDataset"><code>BDD100KDataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p><img alt="bdd100k-validation" src="../../../_images/bdd100k-validation.png" /></p>
<h2 id="caltech-101">Caltech-101 <a href="#caltech-101" title="Permalink to this headline">¶</a><a class="headerlink" href="#caltech-101" title="Permanent link">&para;</a></h2>
<p>The Caltech-101 dataset of images.</p>
<p>The dataset consists of pictures of objects belonging to 101 classes, plus
one background clutter class ( <code>BACKGROUND_Google</code>). Each image is labelled
with a single object.</p>
<p>Each class contains roughly 40 to 800 images, totalling around 9,000
images. Images are of variable sizes, with typical edge lengths of 200-300
pixels. This version contains image-level labels only.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>caltech101</code></p>
</li>
<li>
<p>Dataset source: <a href="https://data.caltech.edu/records/mzrjq-6wc02">https://data.caltech.edu/records/mzrjq-6wc02</a></p>
</li>
<li>
<p>Dataset size: 138.60 MB</p>
</li>
<li>
<p>Tags: <code>image, classification</code></p>
</li>
<li>
<p>Supported splits: <code>N/A</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.Caltech101Dataset" title="fiftyone.zoo.datasets.base.Caltech101Dataset"><code>Caltech101Dataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p><img alt="caltech101" src="../../../_images/caltech101.png" /></p>
<h2 id="caltech-256">Caltech-256 <a href="#caltech-256" title="Permalink to this headline">¶</a><a class="headerlink" href="#caltech-256" title="Permanent link">&para;</a></h2>
<p>The Caltech-256 dataset of images.</p>
<p>The dataset consists of pictures of objects belonging to 256 classes, plus
one background clutter class ( <code>clutter</code>). Each image is labelled with a
single object.</p>
<p>Each class contains between 80 and 827 images, totalling 30,607 images.
Images are of variable sizes, with typical edge lengths of 80-800 pixels.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>caltech256</code></p>
</li>
<li>
<p>Dataset source: <a href="https://data.caltech.edu/records/nyy15-4j048">https://data.caltech.edu/records/nyy15-4j048</a></p>
</li>
<li>
<p>Dataset size: 1.16 GB</p>
</li>
<li>
<p>Tags: <code>image, classification</code></p>
</li>
<li>
<p>Supported splits: <code>N/A</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.Caltech256Dataset" title="fiftyone.zoo.datasets.base.Caltech256Dataset"><code>Caltech256Dataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p><img alt="caltech256" src="../../../_images/caltech256.png" /></p>
<h2 id="cifar-10">CIFAR-10 <a href="#cifar-10" title="Permalink to this headline">¶</a><a class="headerlink" href="#cifar-10" title="Permanent link">&para;</a></h2>
<p>The CIFAR-10 dataset of images.</p>
<p>The dataset consists of 60,000 32 x 32 color images in 10 classes, with 6,000
images per class. There are 50,000 training images and 10,000 test images.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>cifar10</code></p>
</li>
<li>
<p>Dataset source: <a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></p>
</li>
<li>
<p>Dataset size: 132.40 MB</p>
</li>
<li>
<p>Tags: <code>image, classification</code></p>
</li>
<li>
<p>Supported splits: <code>train, test</code></p>
</li>
<li>
<p>ZooDataset classes:</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.tf.html#fiftyone.zoo.datasets.tf.CIFAR10Dataset" title="fiftyone.zoo.datasets.tf.CIFAR10Dataset"><code>CIFAR10Dataset</code></a> (TF backend)</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.torch.html#fiftyone.zoo.datasets.torch.CIFAR10Dataset" title="fiftyone.zoo.datasets.torch.CIFAR10Dataset"><code>CIFAR10Dataset</code></a> (Torch backend)</p>
</li>
</ul>
<p>Note</p>
<p>You must have the
<a href="../api/#dataset-zoo-ml-backend">Torch or TensorFlow backend(s)</a> installed to
load this dataset.</p>
<p><strong>Example usage</strong></p>
<p><img alt="cifar10-test" src="../../../_images/cifar10-test.png" /></p>
<h2 id="cifar-100">CIFAR-100 <a href="#cifar-100" title="Permalink to this headline">¶</a><a class="headerlink" href="#cifar-100" title="Permanent link">&para;</a></h2>
<p>The CIFAR-100 dataset of images.</p>
<p>The dataset consists of 60,000 32 x 32 color images in 100 classes, with
600 images per class. There are 50,000 training images and 10,000 test
images.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>cifar100</code></p>
</li>
<li>
<p>Dataset source: <a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></p>
</li>
<li>
<p>Dataset size: 132.03 MB</p>
</li>
<li>
<p>Tags: <code>image, classification</code></p>
</li>
<li>
<p>Supported splits: <code>train, test</code></p>
</li>
<li>
<p>ZooDataset classes:</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.tf.html#fiftyone.zoo.datasets.tf.CIFAR100Dataset" title="fiftyone.zoo.datasets.tf.CIFAR100Dataset"><code>CIFAR100Dataset</code></a> (TF backend)</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.torch.html#fiftyone.zoo.datasets.torch.CIFAR100Dataset" title="fiftyone.zoo.datasets.torch.CIFAR100Dataset"><code>CIFAR100Dataset</code></a> (Torch backend)</p>
</li>
</ul>
<p>Note</p>
<p>You must have the
<a href="../api/#dataset-zoo-ml-backend">Torch or TensorFlow backend(s)</a> installed to
load this dataset.</p>
<p><strong>Example usage</strong></p>
<p><img alt="cifar100-test" src="../../../_images/cifar100-test.png" /></p>
<h2 id="cityscapes">Cityscapes <a href="#cityscapes" title="Permalink to this headline">¶</a><a class="headerlink" href="#cityscapes" title="Permanent link">&para;</a></h2>
<p>Cityscapes is a large-scale dataset that contains a diverse set of
stereo video sequences recorded in street scenes from 50 different cities,
with high quality pixel-level annotations of 5,000 frames in addition to a
larger set of 20,000 weakly annotated frames.</p>
<p>The dataset is intended for:</p>
<ul>
<li>
<p>Assessing the performance of vision algorithms for major tasks of
semantic urban scene understanding: pixel-level, instance-level, and
panoptic semantic labeling</p>
</li>
<li>
<p>Supporting research that aims to exploit large volumes of (weakly)
annotated data, e.g. for training deep neural networks</p>
</li>
</ul>
<p>Note</p>
<p>In order to load the Cityscapes dataset, you must download the source data
manually. The directory should be organized in the following format:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><span class="n">source_dir</span><span class="o">/</span>
</span><span id="__span-1-2">    <span class="n">leftImg8bit_trainvaltest</span><span class="o">.</span><span class="n">zip</span>
</span><span id="__span-1-3">    <span class="n">gtFine_trainvaltest</span><span class="o">.</span><span class="n">zip</span>             <span class="c1"># optional</span>
</span><span id="__span-1-4">    <span class="n">gtCoarse</span><span class="o">.</span><span class="n">zip</span>                        <span class="c1"># optional</span>
</span><span id="__span-1-5">    <span class="n">gtBbox_cityPersons_trainval</span><span class="o">.</span><span class="n">zip</span>     <span class="c1"># optional</span>
</span></code></pre></div>
<p>You can register at <a href="https://www.cityscapes-dataset.com/register">https://www.cityscapes-dataset.com/register</a> in order
to get links to download the data.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>cityscapes</code></p>
</li>
<li>
<p>Dataset source: <a href="https://www.cityscapes-dataset.com">https://www.cityscapes-dataset.com</a></p>
</li>
<li>
<p>Dataset size: 11.80 GB</p>
</li>
<li>
<p>Tags: <code>image, multilabel, automotive, manual</code></p>
</li>
<li>
<p>Supported splits: <code>train, validation, test</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.CityscapesDataset" title="fiftyone.zoo.datasets.base.CityscapesDataset"><code>CityscapesDataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p><img alt="cityscapes-validation" src="../../../_images/cityscapes-validation.png" /></p>
<h2 id="coco-2014">COCO-2014 <a href="#coco-2014" title="Permalink to this headline">¶</a><a class="headerlink" href="#coco-2014" title="Permanent link">&para;</a></h2>
<p>COCO is a large-scale object detection, segmentation, and captioning
dataset.</p>
<p>This version contains images, bounding boxes, and segmentations for the 2014
version of the dataset.</p>
<p>Note</p>
<p>With support from the <a href="https://cocodataset.org/#download">COCO team</a>,
FiftyOne is a recommended tool for downloading, visualizing, and evaluating
on the COCO dataset!</p>
<p>Check out <a href="../../../integrations/coco/#coco">this guide</a> for more details on using FiftyOne to
work with COCO.</p>
<p><strong>Notes</strong></p>
<ul>
<li>
<p>COCO defines 91 classes but the data only uses 80 classes</p>
</li>
<li>
<p>Some images from the train and validation sets don’t have annotations</p>
</li>
<li>
<p>The test set does not have annotations</p>
</li>
<li>
<p>COCO 2014 and 2017 use the same images, but the splits are different</p>
</li>
</ul>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>coco-2014</code></p>
</li>
<li>
<p>Dataset source: <a href="http://cocodataset.org/#home">http://cocodataset.org/#home</a></p>
</li>
<li>
<p>Dataset size: 37.57 GB</p>
</li>
<li>
<p>Tags: <code>image, detection, segmentation</code></p>
</li>
<li>
<p>Supported splits: <code>train, validation, test</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.COCO2014Dataset" title="fiftyone.zoo.datasets.base.COCO2014Dataset"><code>COCO2014Dataset</code></a></p>
</li>
</ul>
<p><strong>Full split stats</strong></p>
<ul>
<li>
<p>Train split: 82,783 images</p>
</li>
<li>
<p>Test split: 40,775 images</p>
</li>
<li>
<p>Validation split: 40,504 images</p>
</li>
</ul>
<p><strong>Partial downloads</strong></p>
<p>FiftyOne provides parameters that can be used to efficiently download specific
subsets of the COCO dataset to suit your needs. When new subsets are specified,
FiftyOne will use existing downloaded data first if possible before resorting
to downloading additional data from the web.</p>
<p>The following parameters are available to configure a partial download of
COCO-2014 by passing them to
<a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>:</p>
<ul>
<li>
<p><strong>split</strong> ( <em>None</em>) and <strong>splits</strong> ( <em>None</em>): a string or list of strings,
respectively, specifying the splits to load. Supported values are
<code>("train", "test", "validation")</code>. If neither is provided, all available
splits are loaded</p>
</li>
<li>
<p><strong>label_types</strong> ( <em>None</em>): a label type or list of label types to load.
Supported values are <code>("detections", "segmentations")</code>. By default, only
detections are loaded</p>
</li>
<li>
<p><strong>classes</strong> ( <em>None</em>): a string or list of strings specifying required
classes to load. If provided, only samples containing at least one instance
of a specified class will be loaded</p>
</li>
<li>
<p><strong>image_ids</strong> ( <em>None</em>): a list of specific image IDs to load. The IDs can
be specified either as <code>&lt;split&gt;/&lt;image-id&gt;</code> strings or <code>&lt;image-id&gt;</code>
ints of strings. Alternatively, you can provide the path to a TXT
(newline-separated), JSON, or CSV file containing the list of image IDs to
load in either of the first two formats</p>
</li>
<li>
<p><strong>include_id</strong> ( <em>False</em>): whether to include the COCO ID of each sample in
the loaded labels</p>
</li>
<li>
<p><strong>include_license</strong> ( <em>False</em>): whether to include the COCO license of each
sample in the loaded labels, if available. The supported values are:</p>
</li>
<li>
<p><code>"False"</code> (default): don’t load the license</p>
</li>
<li>
<p><code>True</code>/ <code>"name"</code>: store the string license name</p>
</li>
<li>
<p><code>"id"</code>: store the integer license ID</p>
</li>
<li>
<p><code>"url"</code>: store the license URL</p>
</li>
<li>
<p><strong>only_matching</strong> ( <em>False</em>): whether to only load labels that match the
<code>classes</code> or <code>attrs</code> requirements that you provide (True), or to load
all labels for samples that match the requirements (False)</p>
</li>
<li>
<p><strong>num_workers</strong> ( <em>None</em>): the number of processes to use when downloading
individual images. By default, <code>multiprocessing.cpu_count()</code> is used</p>
</li>
<li>
<p><strong>shuffle</strong> ( <em>False</em>): whether to randomly shuffle the order in which
samples are chosen for partial downloads</p>
</li>
<li>
<p><strong>seed</strong> ( <em>None</em>): a random seed to use when shuffling</p>
</li>
<li>
<p><strong>max_samples</strong> ( <em>None</em>): a maximum number of samples to load per split. If
<code>label_types</code> and/or <code>classes</code> are also specified, first priority will
be given to samples that contain all of the specified label types and/or
classes, followed by samples that contain at least one of the specified
labels types or classes. The actual number of samples loaded may be less
than this maximum value if the dataset does not contain sufficient samples
matching your requirements</p>
</li>
</ul>
<p>Note</p>
<p>See
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.COCO2014Dataset" title="fiftyone.zoo.datasets.base.COCO2014Dataset"><code>COCO2014Dataset</code></a> and
<a href="../api/fiftyone.utils.coco.html#fiftyone.utils.coco.COCODetectionDatasetImporter" title="fiftyone.utils.coco.COCODetectionDatasetImporter"><code>COCODetectionDatasetImporter</code></a>
for complete descriptions of the optional keyword arguments that you can
pass to <a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>.</p>
<p><strong>Example usage</strong></p>
<p><img alt="coco-2014-validation" src="../../../_images/coco-2014-validation.png" /></p>
<h2 id="coco-2017">COCO-2017 <a href="#coco-2017" title="Permalink to this headline">¶</a><a class="headerlink" href="#coco-2017" title="Permanent link">&para;</a></h2>
<p>COCO is a large-scale object detection, segmentation, and captioning
dataset.</p>
<p>This version contains images, bounding boxes, and segmentations for the 2017
version of the dataset.</p>
<p>Note</p>
<p>With support from the <a href="https://cocodataset.org/#download">COCO team</a>,
FiftyOne is a recommended tool for downloading, visualizing, and evaluating
on the COCO dataset!</p>
<p>Check out <a href="../../../integrations/coco/#coco">this guide</a> for more details on using FiftyOne to
work with COCO.</p>
<p><strong>Notes</strong></p>
<ul>
<li>
<p>COCO defines 91 classes but the data only uses 80 classes</p>
</li>
<li>
<p>Some images from the train and validation sets don’t have annotations</p>
</li>
<li>
<p>The test set does not have annotations</p>
</li>
<li>
<p>COCO 2014 and 2017 use the same images, but the splits are different</p>
</li>
</ul>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>coco-2017</code></p>
</li>
<li>
<p>Dataset source: <a href="http://cocodataset.org/#home">http://cocodataset.org/#home</a></p>
</li>
<li>
<p>Dataset size: 25.20 GB</p>
</li>
<li>
<p>Tags: <code>image, detection, segmentation</code></p>
</li>
<li>
<p>Supported splits: <code>train, validation, test</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.COCO2017Dataset" title="fiftyone.zoo.datasets.base.COCO2017Dataset"><code>COCO2017Dataset</code></a></p>
</li>
</ul>
<p><strong>Full split stats</strong></p>
<ul>
<li>
<p>Train split: 118,287 images</p>
</li>
<li>
<p>Test split: 40,670 images</p>
</li>
<li>
<p>Validation split: 5,000 images</p>
</li>
</ul>
<p><strong>Partial downloads</strong></p>
<p>FiftyOne provides parameters that can be used to efficiently download specific
subsets of the COCO dataset to suit your needs. When new subsets are specified,
FiftyOne will use existing downloaded data first if possible before resorting
to downloading additional data from the web.</p>
<p>The following parameters are available to configure a partial download of
COCO-2017 by passing them to
<a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>:</p>
<ul>
<li>
<p><strong>split</strong> ( <em>None</em>) and <strong>splits</strong> ( <em>None</em>): a string or list of strings,
respectively, specifying the splits to load. Supported values are
<code>("train", "test", "validation")</code>. If neither is provided, all available
splits are loaded</p>
</li>
<li>
<p><strong>label_types</strong> ( <em>None</em>): a label type or list of label types to load.
Supported values are <code>("detections", "segmentations")</code>. By default, only
detections are loaded</p>
</li>
<li>
<p><strong>classes</strong> ( <em>None</em>): a string or list of strings specifying required
classes to load. If provided, only samples containing at least one instance
of a specified class will be loaded</p>
</li>
<li>
<p><strong>image_ids</strong> ( <em>None</em>): a list of specific image IDs to load. The IDs can
be specified either as <code>&lt;split&gt;/&lt;image-id&gt;</code> strings or <code>&lt;image-id&gt;</code>
ints of strings. Alternatively, you can provide the path to a TXT
(newline-separated), JSON, or CSV file containing the list of image IDs to
load in either of the first two formats</p>
</li>
<li>
<p><strong>include_id</strong> ( <em>False</em>): whether to include the COCO ID of each sample in
the loaded labels</p>
</li>
<li>
<p><strong>include_license</strong> ( <em>False</em>): whether to include the COCO license of each
sample in the loaded labels, if available. The supported values are:</p>
</li>
<li>
<p><code>"False"</code> (default): don’t load the license</p>
</li>
<li>
<p><code>True</code>/ <code>"name"</code>: store the string license name</p>
</li>
<li>
<p><code>"id"</code>: store the integer license ID</p>
</li>
<li>
<p><code>"url"</code>: store the license URL</p>
</li>
<li>
<p><strong>only_matching</strong> ( <em>False</em>): whether to only load labels that match the
<code>classes</code> or <code>attrs</code> requirements that you provide (True), or to load
all labels for samples that match the requirements (False)</p>
</li>
<li>
<p><strong>num_workers</strong> ( <em>None</em>): the number of processes to use when downloading
individual images. By default, <code>multiprocessing.cpu_count()</code> is used</p>
</li>
<li>
<p><strong>shuffle</strong> ( <em>False</em>): whether to randomly shuffle the order in which
samples are chosen for partial downloads</p>
</li>
<li>
<p><strong>seed</strong> ( <em>None</em>): a random seed to use when shuffling</p>
</li>
<li>
<p><strong>max_samples</strong> ( <em>None</em>): a maximum number of samples to load per split. If
<code>label_types</code> and/or <code>classes</code> are also specified, first priority will
be given to samples that contain all of the specified label types and/or
classes, followed by samples that contain at least one of the specified
labels types or classes. The actual number of samples loaded may be less
than this maximum value if the dataset does not contain sufficient samples
matching your requirements</p>
</li>
</ul>
<p>Note</p>
<p>See
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.COCO2017Dataset" title="fiftyone.zoo.datasets.base.COCO2017Dataset"><code>COCO2017Dataset</code></a> and
<a href="../api/fiftyone.utils.coco.html#fiftyone.utils.coco.COCODetectionDatasetImporter" title="fiftyone.utils.coco.COCODetectionDatasetImporter"><code>COCODetectionDatasetImporter</code></a>
for complete descriptions of the optional keyword arguments that you can
pass to <a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>.</p>
<p><strong>Example usage</strong></p>
<p><img alt="coco-2017-validation" src="../../../_images/coco-2017-validation.png" /></p>
<h2 id="fashion-mnist">Fashion MNIST <a href="#fashion-mnist" title="Permalink to this headline">¶</a><a class="headerlink" href="#fashion-mnist" title="Permanent link">&para;</a></h2>
<p>The Fashion-MNIST database of Zalando’s fashion article images.</p>
<p>The dataset consists of 70,000 28 x 28 grayscale images in 10 classes.
There are 60,000 training images and 10,000 test images.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>fashion-mnist</code></p>
</li>
<li>
<p>Dataset source: <a href="https://github.com/zalandoresearch/fashion-mnist">https://github.com/zalandoresearch/fashion-mnist</a></p>
</li>
<li>
<p>Dataset size: 36.42 MB</p>
</li>
<li>
<p>Tags: <code>image, classification</code></p>
</li>
<li>
<p>Supported splits: <code>train, test</code></p>
</li>
<li>
<p>ZooDataset classes:</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.tf.html#fiftyone.zoo.datasets.tf.FashionMNISTDataset" title="fiftyone.zoo.datasets.tf.FashionMNISTDataset"><code>FashionMNISTDataset</code></a> (TF backend)</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.torch.html#fiftyone.zoo.datasets.torch.FashionMNISTDataset" title="fiftyone.zoo.datasets.torch.FashionMNISTDataset"><code>FashionMNISTDataset</code></a> (Torch backend)</p>
</li>
</ul>
<p>Note</p>
<p>You must have the
<a href="../api/#dataset-zoo-ml-backend">Torch or TensorFlow backend(s)</a> installed to
load this dataset.</p>
<p><strong>Example usage</strong></p>
<p><img alt="fashion-mnist-test" src="../../../_images/fashion-mnist-test.png" /></p>
<h2 id="families-in-the-wild">Families in the Wild <a href="#families-in-the-wild" title="Permalink to this headline">¶</a><a class="headerlink" href="#families-in-the-wild" title="Permanent link">&para;</a></h2>
<p>Families in the Wild is a public benchmark for recognizing families via facial
images. The dataset contains over 26,642 images of 5,037 faces collected from
978 families. A unique Family ID (FID) is assigned per family, ranging from
F0001-F1018 (i.e., some families were merged or removed since its first release
in 2016). The dataset is a continued work in progress. Any contributions are
both welcome and appreciated!</p>
<p>Faces were cropped from imagery using the five-point face detector MTCNN from
various phototypes (i.e., mostly family photos, along with several profile pics
of individuals (facial shots). The number of members per family varies from
3-to-26, with the number of faces per subject ranging from 1 to &gt;10.</p>
<p>Various levels and types of labels are associated with samples in this dataset.
Family-level labels contain a list of members, each assigned a member ID (MID)
unique to that respective family (e.g., F0011.MID2 refers to member 2 of family
11). Each member has annotations specifying gender and relationship to all
other members in that respective family.</p>
<p>The relationships in FIW are:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><span class="o">=====</span>  <span class="o">=====</span>
</span><span id="__span-2-2">  <span class="n">ID</span>    <span class="n">Type</span>
</span><span id="__span-2-3"><span class="o">=====</span>  <span class="o">=====</span>
</span><span id="__span-2-4">    <span class="mi">0</span>  <span class="ow">not</span> <span class="n">related</span> <span class="ow">or</span> <span class="bp">self</span>
</span><span id="__span-2-5">    <span class="mi">1</span>  <span class="n">child</span>
</span><span id="__span-2-6">    <span class="mi">2</span>  <span class="n">sibling</span>
</span><span id="__span-2-7">    <span class="mi">3</span>  <span class="n">grandchild</span>
</span><span id="__span-2-8">    <span class="mi">4</span>  <span class="n">parent</span>
</span><span id="__span-2-9">    <span class="mi">5</span>  <span class="n">spouse</span>
</span><span id="__span-2-10">    <span class="mi">6</span>  <span class="n">grandparent</span>
</span><span id="__span-2-11">    <span class="mi">7</span>  <span class="n">great</span> <span class="n">grandchild</span>
</span><span id="__span-2-12">    <span class="mi">8</span>  <span class="n">great</span> <span class="n">grandparent</span>
</span><span id="__span-2-13">    <span class="mi">9</span>  <span class="n">TBD</span>
</span><span id="__span-2-14"><span class="o">=====</span>  <span class="o">=====</span>
</span></code></pre></div>
<p>Within FiftyOne, each sample corresponds to a single face image and contains
primitive labels of the Family ID, Member ID, etc. The relationship labels are
stored as <a href="../../../fiftyone_concepts/using_datasets/#multilabel-classification">multi-label classifications</a>,
where each classification represents one relationship that the member has with
another member in the family. The number of relationships will differ from one
person to the next, but all faces of one person will have the same relationship
labels.</p>
<p>Additionally, the labels for the
<a href="https://competitions.codalab.org/competitions/21843">Kinship Verification task</a>
are also loaded into this dataset through FiftyOne. These labels are stored
as classifications just like relationships, but the labels of kinship differ
from those defined above. For example, rather than Parent, the label might be
<code>fd</code> representing a Father-Daughter kinship or <code>md</code> for Mother-Daughter.</p>
<p>In order to make it easier to browse the dataset in the FiftyOne App, each
sample also contains a <code>face_id</code> field containing a unique integer for each
face of a member, always starting at 0. This allows you to filter the <code>face_id</code>
field to 0 in the App to show only a single image of each person.</p>
<p>For your reference, the relationship labels are stored in disk in a matrix that
provides the relationship of each member with other members of the family as
well as names and genders. The i-th rows represent the i-th family member’s
relationship to the j-th other members.</p>
<p>For example, <code>FID0001.csv</code> contains:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><span class="n">MID</span>     <span class="mi">1</span>     <span class="mi">2</span>     <span class="mi">3</span>     <span class="n">Name</span>    <span class="n">Gender</span>
</span><span id="__span-3-2"> <span class="mi">1</span>      <span class="mi">0</span>     <span class="mi">4</span>     <span class="mi">5</span>     <span class="n">name1</span>     <span class="n">f</span>
</span><span id="__span-3-3"> <span class="mi">2</span>      <span class="mi">1</span>     <span class="mi">0</span>     <span class="mi">1</span>     <span class="n">name2</span>     <span class="n">f</span>
</span><span id="__span-3-4"> <span class="mi">3</span>      <span class="mi">5</span>     <span class="mi">4</span>     <span class="mi">0</span>     <span class="n">name3</span>     <span class="n">m</span>
</span></code></pre></div>
<p>Here we have three family members, as listed under the MID column (far-left).
Each MID reads across its row. We can see that MID1 is related to MID2 by
4 -&gt; 1 (Parent -&gt; Child), which of course can be viewed as the inverse, i.e.,
MID2 -&gt; MID1 is 1 -&gt; 4. It can also be seen that MID1 and MID3 are spouses of
one another, i.e., 5 -&gt; 5.</p>
<p>Note</p>
<p>The spouse label will likely be removed in future version of this
dataset. It serves no value to the problem of kinship.</p>
<p>For more information on the data (e.g., statistics, task evaluations,
benchmarks, and more), see the recent journal:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><span class="n">Robinson</span><span class="p">,</span> <span class="n">JP</span><span class="p">,</span> <span class="n">M</span><span class="o">.</span> <span class="n">Shao</span><span class="p">,</span> <span class="ow">and</span> <span class="n">Y</span><span class="o">.</span> <span class="n">Fu</span><span class="o">.</span> <span class="s2">&quot;Survey on the Analysis and Modeling of</span>
</span><span id="__span-4-2"><span class="n">Visual</span> <span class="n">Kinship</span><span class="p">:</span> <span class="n">A</span> <span class="n">Decade</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">Making</span><span class="o">.</span><span class="s2">&quot; IEEE Transactions on Pattern</span>
</span><span id="__span-4-3"><span class="n">Analysis</span> <span class="ow">and</span> <span class="n">Machine</span> <span class="n">Intelligence</span> <span class="p">(</span><span class="n">PAMI</span><span class="p">),</span> <span class="mf">2021.</span>
</span></code></pre></div>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>fiw</code></p>
</li>
<li>
<p>Dataset source: <a href="https://web.northeastern.edu/smilelab/fiw/">https://web.northeastern.edu/smilelab/fiw/</a></p>
</li>
<li>
<p>Dataset size: 173.00 MB</p>
</li>
<li>
<p>Tags: <code>image, kinship, verification, classification, search-and-retrieval, facial-recognition</code></p>
</li>
<li>
<p>Supported splits: <code>test, val, train</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.FIWDataset" title="fiftyone.zoo.datasets.base.FIWDataset"><code>FIWDataset</code></a></p>
</li>
</ul>
<p>Note</p>
<p>For your convenience, FiftyOne provides
<a href="../api/fiftyone.utils.fiw.html#fiftyone.utils.fiw.get_pairwise_labels" title="fiftyone.utils.fiw.get_pairwise_labels"><code>get_pairwise_labels()</code></a>
and
<a href="../api/fiftyone.utils.fiw.html#fiftyone.utils.fiw.get_identifier_filepaths_map" title="fiftyone.utils.fiw.get_identifier_filepaths_map"><code>get_identifier_filepaths_map()</code></a>
utilities for FIW.</p>
<p><strong>Example usage</strong></p>
<p><img alt="fiw" src="../../../_images/fiw.png" /></p>
<h2 id="hmbd51">HMBD51 <a href="#hmbd51" title="Permalink to this headline">¶</a><a class="headerlink" href="#hmbd51" title="Permanent link">&para;</a></h2>
<p>HMDB51 is an action recognition dataset containing a total of 6,766
clips distributed across 51 action classes.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>hmdb51</code></p>
</li>
<li>
<p>Dataset source: <a href="https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database">https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database</a></p>
</li>
<li>
<p>Dataset size: 2.16 GB</p>
</li>
<li>
<p>Tags: <code>video, action-recognition</code></p>
</li>
<li>
<p>Supported splits: <code>train, test, other</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.HMDB51Dataset" title="fiftyone.zoo.datasets.base.HMDB51Dataset"><code>HMDB51Dataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p>Note</p>
<p>In order to work with video datasets, you’ll need to have
<a href="../../../getting_started/basic/troubleshooting/#videos-do-not-load-in-the-app">ffmpeg installed</a>.</p>
<p><img alt="hmdb51-test" src="../../../_images/hmdb51-test.png" /></p>
<h2 id="imagenet-2012">ImageNet 2012 <a href="#imagenet-2012" title="Permalink to this headline">¶</a><a class="headerlink" href="#imagenet-2012" title="Permanent link">&para;</a></h2>
<p>The ImageNet 2012 dataset.</p>
<p>ImageNet, as known as ILSVRC 2012, is an image dataset organized according
to the WordNet hierarchy. Each meaningful concept in WordNet, possibly
described by multiple words or word phrases, is called a “synonym set” or
“synset”. There are more than 100,000 synsets in WordNet, majority of them
are nouns (80,000+). ImageNet provides on average 1,000 images to
illustrate each synset. Images of each concept are quality-controlled and
human-annotated. In its completion, we hope ImageNet will offer tens of
millions of cleanly sorted images for most of the concepts in the WordNet
hierarchy.</p>
<p>Note that labels were never publicly released for the test set, so only the
training and validation sets are provided.</p>
<p>Note</p>
<p>In order to load the ImageNet dataset, you must download the source data
manually. The directory should be organized in the following format:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><span class="n">source_dir</span><span class="o">/</span>
</span><span id="__span-5-2">    <span class="n">ILSVRC2012_devkit_t12</span><span class="o">.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span>    <span class="c1"># both splits</span>
</span><span id="__span-5-3">    <span class="n">ILSVRC2012_img_train</span><span class="o">.</span><span class="n">tar</span>        <span class="c1"># train split</span>
</span><span id="__span-5-4">    <span class="n">ILSVRC2012_img_val</span><span class="o">.</span><span class="n">tar</span>          <span class="c1"># validation split</span>
</span></code></pre></div>
<p>You can register at <a href="http://www.image-net.org/download-images">http://www.image-net.org/download-images</a> in order to
get links to download the data.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>imagenet-2012</code></p>
</li>
<li>
<p>Dataset source: <a href="http://image-net.org">http://image-net.org</a></p>
</li>
<li>
<p>Dataset size: 144.02 GB</p>
</li>
<li>
<p>Tags: <code>image, classification, manual</code></p>
</li>
<li>
<p>Supported splits: <code>train, validation</code></p>
</li>
<li>
<p>ZooDataset classes:</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.tf.html#fiftyone.zoo.datasets.tf.ImageNet2012Dataset" title="fiftyone.zoo.datasets.tf.ImageNet2012Dataset"><code>ImageNet2012Dataset</code></a> (TF backend)</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.torch.html#fiftyone.zoo.datasets.torch.ImageNet2012Dataset" title="fiftyone.zoo.datasets.torch.ImageNet2012Dataset"><code>ImageNet2012Dataset</code></a> (Torch backend)</p>
</li>
</ul>
<p>Note</p>
<p>You must have the
<a href="../api/#dataset-zoo-ml-backend">Torch or TensorFlow backend(s)</a> installed to
load this dataset.</p>
<p><strong>Example usage</strong></p>
<p><img alt="imagenet-2012-validation" src="../../../_images/imagenet-2012-validation.png" /></p>
<h2 id="imagenet-sample">ImageNet Sample <a href="#imagenet-sample" title="Permalink to this headline">¶</a><a class="headerlink" href="#imagenet-sample" title="Permanent link">&para;</a></h2>
<p>A small sample of images from the ImageNet 2012 dataset.</p>
<p>The dataset contains 1,000 images, one randomly chosen from each class of
the validation split of the ImageNet 2012 dataset.</p>
<p>These images are provided according to the terms below.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><span class="n">You</span> <span class="n">have</span> <span class="n">been</span> <span class="n">granted</span> <span class="n">access</span> <span class="k">for</span> <span class="n">non</span><span class="o">-</span><span class="n">commercial</span> <span class="n">research</span><span class="o">/</span><span class="n">educational</span>
</span><span id="__span-6-2"><span class="n">use</span><span class="o">.</span> <span class="n">By</span> <span class="n">accessing</span> <span class="n">the</span> <span class="n">data</span><span class="p">,</span> <span class="n">you</span> <span class="n">have</span> <span class="n">agreed</span> <span class="n">to</span> <span class="n">the</span> <span class="n">following</span> <span class="n">terms</span><span class="o">.</span>
</span><span id="__span-6-3">
</span><span id="__span-6-4"><span class="n">You</span> <span class="p">(</span><span class="n">the</span> <span class="s2">&quot;Researcher&quot;</span><span class="p">)</span> <span class="n">have</span> <span class="n">requested</span> <span class="n">permission</span> <span class="n">to</span> <span class="n">use</span> <span class="n">the</span> <span class="n">ImageNet</span>
</span><span id="__span-6-5"><span class="n">database</span> <span class="p">(</span><span class="n">the</span> <span class="s2">&quot;Database&quot;</span><span class="p">)</span> <span class="n">at</span> <span class="n">Princeton</span> <span class="n">University</span> <span class="ow">and</span> <span class="n">Stanford</span>
</span><span id="__span-6-6"><span class="n">University</span><span class="o">.</span> <span class="n">In</span> <span class="n">exchange</span> <span class="k">for</span> <span class="n">such</span> <span class="n">permission</span><span class="p">,</span> <span class="n">Researcher</span> <span class="n">hereby</span> <span class="n">agrees</span>
</span><span id="__span-6-7"><span class="n">to</span> <span class="n">the</span> <span class="n">following</span> <span class="n">terms</span> <span class="ow">and</span> <span class="n">conditions</span><span class="p">:</span>
</span><span id="__span-6-8">
</span><span id="__span-6-9"><span class="mf">1.</span>  <span class="n">Researcher</span> <span class="n">shall</span> <span class="n">use</span> <span class="n">the</span> <span class="n">Database</span> <span class="n">only</span> <span class="k">for</span> <span class="n">non</span><span class="o">-</span><span class="n">commercial</span> <span class="n">research</span>
</span><span id="__span-6-10">    <span class="ow">and</span> <span class="n">educational</span> <span class="n">purposes</span><span class="o">.</span>
</span><span id="__span-6-11"><span class="mf">2.</span>  <span class="n">Princeton</span> <span class="n">University</span> <span class="ow">and</span> <span class="n">Stanford</span> <span class="n">University</span> <span class="n">make</span> <span class="n">no</span>
</span><span id="__span-6-12">    <span class="n">representations</span> <span class="ow">or</span> <span class="n">warranties</span> <span class="n">regarding</span> <span class="n">the</span> <span class="n">Database</span><span class="p">,</span> <span class="n">including</span> <span class="n">but</span>
</span><span id="__span-6-13">    <span class="ow">not</span> <span class="n">limited</span> <span class="n">to</span> <span class="n">warranties</span> <span class="n">of</span> <span class="n">non</span><span class="o">-</span><span class="n">infringement</span> <span class="ow">or</span> <span class="n">fitness</span> <span class="k">for</span> <span class="n">a</span>
</span><span id="__span-6-14">    <span class="n">particular</span> <span class="n">purpose</span><span class="o">.</span>
</span><span id="__span-6-15"><span class="mf">3.</span>  <span class="n">Researcher</span> <span class="n">accepts</span> <span class="n">full</span> <span class="n">responsibility</span> <span class="k">for</span> <span class="n">his</span> <span class="ow">or</span> <span class="n">her</span> <span class="n">use</span> <span class="n">of</span> <span class="n">the</span>
</span><span id="__span-6-16">    <span class="n">Database</span> <span class="ow">and</span> <span class="n">shall</span> <span class="n">defend</span> <span class="ow">and</span> <span class="n">indemnify</span> <span class="n">Princeton</span> <span class="n">University</span> <span class="ow">and</span>
</span><span id="__span-6-17">    <span class="n">Stanford</span> <span class="n">University</span><span class="p">,</span> <span class="n">including</span> <span class="n">their</span> <span class="n">employees</span><span class="p">,</span> <span class="n">Trustees</span><span class="p">,</span> <span class="n">officers</span>
</span><span id="__span-6-18">    <span class="ow">and</span> <span class="n">agents</span><span class="p">,</span> <span class="n">against</span> <span class="nb">any</span> <span class="ow">and</span> <span class="nb">all</span> <span class="n">claims</span> <span class="n">arising</span> <span class="kn">from</span><span class="w"> </span><span class="nn">Researcher</span><span class="s1">&#39;s</span>
</span><span id="__span-6-19">    <span class="n">use</span> <span class="n">of</span> <span class="n">the</span> <span class="n">Database</span><span class="p">,</span> <span class="n">including</span> <span class="n">but</span> <span class="ow">not</span> <span class="n">limited</span> <span class="n">to</span> <span class="n">Researcher</span><span class="s1">&#39;s use</span>
</span><span id="__span-6-20">    <span class="n">of</span> <span class="nb">any</span> <span class="n">copies</span> <span class="n">of</span> <span class="n">copyrighted</span> <span class="n">images</span> <span class="n">that</span> <span class="n">he</span> <span class="ow">or</span> <span class="n">she</span> <span class="n">may</span> <span class="n">create</span> <span class="kn">from</span>
</span><span id="__span-6-21"><span class="w">    </span><span class="nn">the</span> <span class="n">Database</span><span class="o">.</span>
</span><span id="__span-6-22"><span class="mf">4.</span>  <span class="n">Researcher</span> <span class="n">may</span> <span class="n">provide</span> <span class="n">research</span> <span class="n">associates</span> <span class="ow">and</span> <span class="n">colleagues</span> <span class="k">with</span>
</span><span id="__span-6-23">    <span class="n">access</span> <span class="n">to</span> <span class="n">the</span> <span class="n">Database</span> <span class="n">provided</span> <span class="n">that</span> <span class="n">they</span> <span class="n">first</span> <span class="n">agree</span> <span class="n">to</span> <span class="n">be</span> <span class="n">bound</span>
</span><span id="__span-6-24">    <span class="n">by</span> <span class="n">these</span> <span class="n">terms</span> <span class="ow">and</span> <span class="n">conditions</span><span class="o">.</span>
</span><span id="__span-6-25"><span class="mf">5.</span>  <span class="n">Princeton</span> <span class="n">University</span> <span class="ow">and</span> <span class="n">Stanford</span> <span class="n">University</span> <span class="n">reserve</span> <span class="n">the</span> <span class="n">right</span> <span class="n">to</span>
</span><span id="__span-6-26">    <span class="n">terminate</span> <span class="n">Researcher</span><span class="s1">&#39;s access to the Database at any time.</span>
</span><span id="__span-6-27"><span class="mf">6.</span>  <span class="n">If</span> <span class="n">Researcher</span> <span class="ow">is</span> <span class="n">employed</span> <span class="n">by</span> <span class="n">a</span> <span class="k">for</span><span class="o">-</span><span class="n">profit</span><span class="p">,</span> <span class="n">commercial</span> <span class="n">entity</span><span class="p">,</span>
</span><span id="__span-6-28">    <span class="n">Researcher</span><span class="s1">&#39;s employer shall also be bound by these terms and</span>
</span><span id="__span-6-29">    <span class="n">conditions</span><span class="p">,</span> <span class="ow">and</span> <span class="n">Researcher</span> <span class="n">hereby</span> <span class="n">represents</span> <span class="n">that</span> <span class="n">he</span> <span class="ow">or</span> <span class="n">she</span> <span class="ow">is</span>
</span><span id="__span-6-30">    <span class="n">fully</span> <span class="n">authorized</span> <span class="n">to</span> <span class="n">enter</span> <span class="n">into</span> <span class="n">this</span> <span class="n">agreement</span> <span class="n">on</span> <span class="n">behalf</span> <span class="n">of</span> <span class="n">such</span>
</span><span id="__span-6-31">    <span class="n">employer</span><span class="o">.</span>
</span><span id="__span-6-32"><span class="mf">7.</span>  <span class="n">The</span> <span class="n">law</span> <span class="n">of</span> <span class="n">the</span> <span class="n">State</span> <span class="n">of</span> <span class="n">New</span> <span class="n">Jersey</span> <span class="n">shall</span> <span class="n">apply</span> <span class="n">to</span> <span class="nb">all</span> <span class="n">disputes</span>
</span><span id="__span-6-33">    <span class="n">under</span> <span class="n">this</span> <span class="n">agreement</span><span class="o">.</span>
</span></code></pre></div>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>imagenet-sample</code></p>
</li>
<li>
<p>Dataset source: <a href="http://image-net.org">http://image-net.org</a></p>
</li>
<li>
<p>Dataset size: 98.26 MB</p>
</li>
<li>
<p>Tags: <code>image, classification</code></p>
</li>
<li>
<p>Supported splits: <code>N/A</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.ImageNetSampleDataset" title="fiftyone.zoo.datasets.base.ImageNetSampleDataset"><code>ImageNetSampleDataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p><img alt="imagenet-sample" src="../../../_images/imagenet-sample.png" /></p>
<h2 id="kinetics-400">Kinetics 400 <a href="#kinetics-400" title="Permalink to this headline">¶</a><a class="headerlink" href="#kinetics-400" title="Permanent link">&para;</a></h2>
<p>Kinetics is a collection of large-scale, high-quality datasets of URL links of
up to 650,000 video clips that cover 400/600/700 human action classes,
depending on the dataset version. The videos include human-object interactions
such as playing instruments, as well as human-human interactions such as
shaking hands and hugging. Each action class has at least 400/600/700 video
clips. Each clip is human annotated with a single action class and lasts around
10 seconds.</p>
<p>This dataset contains videos and action classifications for the 400 class
version of the dataset.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>kinetics-400</code></p>
</li>
<li>
<p>Dataset source: <a href="https://deepmind.com/research/open-source/kinetics">https://deepmind.com/research/open-source/kinetics</a></p>
</li>
<li>
<p>Dataset size: 456 GB</p>
</li>
<li>
<p>Tags: <code>video, classification, action-recognition</code></p>
</li>
<li>
<p>Supported splits: <code>train, test, validation</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.Kinetics400Dataset" title="fiftyone.zoo.datasets.base.Kinetics400Dataset"><code>Kinetics400Dataset</code></a></p>
</li>
</ul>
<p>Original split stats:</p>
<ul>
<li>
<p>Train split: 219,782 videos</p>
</li>
<li>
<p>Test split: 35,357 videos</p>
</li>
<li>
<p>Validation split: 18,035 videos</p>
</li>
</ul>
<p>CVDF split stats:</p>
<ul>
<li>
<p>Train split: 246,534 videos</p>
</li>
<li>
<p>Test split: 39,805 videos</p>
</li>
<li>
<p>Validation split: 19,906 videos</p>
</li>
</ul>
<p>Dataset size:</p>
<ul>
<li>
<p>Train split: 370 GB</p>
</li>
<li>
<p>Test split: 56 GB</p>
</li>
<li>
<p>Validation split: 30 GB</p>
</li>
</ul>
<p><strong>Partial downloads</strong></p>
<p>Kinetics is a massive dataset, so FiftyOne provides parameters that can be used
to efficiently download specific subsets of the dataset to suit your needs.
When new subsets are specified, FiftyOne will use existing downloaded data
first if possible before resorting to downloading additional data from the web.</p>
<p>Kinetics videos were originally only accessible from YouTube. Over time, some
videos have become unavailable so the
<a href="https://github.com/cvdfoundation">CVDF</a> have hosted the Kinetics dataset on
AWS.</p>
<p>If you are partially downloading the dataset through FiftyOne, the specific
videos of interest will be downloaded from YouTube, if necessary. However,
when you load an entire split, the CVDF-provided files will be downloaded from
AWS.</p>
<p>The following parameters are available to configure a partial download of
Kinetics by passing them to
<a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>:</p>
<ul>
<li>
<p><strong>split</strong> ( <em>None</em>) and <strong>splits</strong> ( <em>None</em>): a string or list of strings,
respectively, specifying the splits to load. Supported values are
<code>("train", "test", "validation")</code>. If neither is provided, all available
splits are loaded</p>
</li>
<li>
<p><strong>classes</strong> ( <em>None</em>): a string or list of strings specifying required
classes to load. If provided, only samples containing at least one instance
of a specified class will be loaded</p>
</li>
<li>
<p><strong>num_workers</strong> ( <em>None</em>): the number of processes to use when downloading
individual videos. By default, <code>multiprocessing.cpu_count()</code> is used</p>
</li>
<li>
<p><strong>shuffle</strong> ( <em>False</em>): whether to randomly shuffle the order in which
samples are chosen for partial downloads</p>
</li>
<li>
<p><strong>seed</strong> ( <em>None</em>): a random seed to use when shuffling</p>
</li>
<li>
<p><strong>max_samples</strong> ( <em>None</em>): a maximum number of samples to load per split. If
<code>classes</code> are also specified, only up to the number of samples that
contain at least one specified class will be loaded. By default, all
matching samples are loaded</p>
</li>
</ul>
<p>Note</p>
<p>Unlike other versions, Kinteics 400 does not have zips available by class
so whenever either <code>classes</code> or <code>max_samples</code> is provided, videos will be
downloaded from YouTube.</p>
<p><strong>Example usage</strong></p>
<p>Note</p>
<p>In order to work with video datasets, you’ll need to have
<a href="../../../getting_started/basic/troubleshooting/#videos-do-not-load-in-the-app">ffmpeg installed</a>.</p>
<p><img alt="kinetics" src="../../../_images/kinetics.png" /></p>
<h2 id="kinetics-600">Kinetics 600 <a href="#kinetics-600" title="Permalink to this headline">¶</a><a class="headerlink" href="#kinetics-600" title="Permanent link">&para;</a></h2>
<p>Kinetics is a collection of large-scale, high-quality datasets of URL links of
up to 650,000 video clips that cover 400/600/700 human action classes,
depending on the dataset version. The videos include human-object interactions
such as playing instruments, as well as human-human interactions such as
shaking hands and hugging. Each action class has at least 400/600/700 video
clips. Each clip is human annotated with a single action class and lasts around
10 seconds.</p>
<p>This dataset contains videos and action classifications for the 600 class
version of the dataset.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>kinetics-600</code></p>
</li>
<li>
<p>Dataset source: <a href="https://deepmind.com/research/open-source/kinetics">https://deepmind.com/research/open-source/kinetics</a></p>
</li>
<li>
<p>Dataset size: 779 GB</p>
</li>
<li>
<p>Tags: <code>video, classification, action-recognition</code></p>
</li>
<li>
<p>Supported splits: <code>train, test, validation</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.Kinetics600Dataset" title="fiftyone.zoo.datasets.base.Kinetics600Dataset"><code>Kinetics600Dataset</code></a></p>
</li>
</ul>
<p>Original split stats:</p>
<ul>
<li>
<p>Train split: 370,582 videos</p>
</li>
<li>
<p>Test split: 56,618 videos</p>
</li>
<li>
<p>Validation split: 28,313 videos</p>
</li>
</ul>
<p>CVDF split stats:</p>
<ul>
<li>
<p>Train split: 427,549 videos</p>
</li>
<li>
<p>Test split: 72,924 videos</p>
</li>
<li>
<p>Validation split: 29,793 videos</p>
</li>
</ul>
<p>Dataset size:</p>
<ul>
<li>
<p>Train split: 648 GB</p>
</li>
<li>
<p>Test split: 88 GB</p>
</li>
<li>
<p>Validation split: 43 GB</p>
</li>
</ul>
<p><strong>Partial downloads</strong></p>
<p>Kinetics is a massive dataset, so FiftyOne provides parameters that can be used
to efficiently download specific subsets of the dataset to suit your needs.
When new subsets are specified, FiftyOne will use existing downloaded data
first if possible before resorting to downloading additional data from the web.</p>
<p>Kinetics videos were originally only accessible from YouTube. Over time, some
videos have become unavailable so the
<a href="https://github.com/cvdfoundation">CVDF</a> have hosted the Kinetics dataset on
AWS.</p>
<p>If you are partially downloading the dataset through FiftyOne, the specific
videos of interest will be downloaded from YouTube, if necessary. However,
when you load an entire split, the CVDF-provided files will be downloaded from
AWS.</p>
<p>The following parameters are available to configure a partial download of
Kinetics by passing them to
<a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>:</p>
<ul>
<li>
<p><strong>split</strong> ( <em>None</em>) and <strong>splits</strong> ( <em>None</em>): a string or list of strings,
respectively, specifying the splits to load. Supported values are
<code>("train", "test", "validation")</code>. If neither is provided, all available
splits are loaded</p>
</li>
<li>
<p><strong>classes</strong> ( <em>None</em>): a string or list of strings specifying required
classes to load. If provided, only samples containing at least one instance
of a specified class will be loaded</p>
</li>
<li>
<p><strong>num_workers</strong> ( <em>None</em>): the number of processes to use when downloading
individual videos. By default, <code>multiprocessing.cpu_count()</code> is used</p>
</li>
<li>
<p><strong>shuffle</strong> ( <em>False</em>): whether to randomly shuffle the order in which
samples are chosen for partial downloads</p>
</li>
<li>
<p><strong>seed</strong> ( <em>None</em>): a random seed to use when shuffling</p>
</li>
<li>
<p><strong>max_samples</strong> ( <em>None</em>): a maximum number of samples to load per split. If
<code>classes</code> are also specified, only up to the number of samples that
contain at least one specified class will be loaded. By default, all
matching samples are loaded</p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p>Note</p>
<p>In order to work with video datasets, you’ll need to have
<a href="../../../getting_started/basic/troubleshooting/#videos-do-not-load-in-the-app">ffmpeg installed</a>.</p>
<p><img alt="kinetics" src="../../../_images/kinetics.png" /></p>
<h2 id="kinetics-700">Kinetics 700 <a href="#kinetics-700" title="Permalink to this headline">¶</a><a class="headerlink" href="#kinetics-700" title="Permanent link">&para;</a></h2>
<p>Kinetics is a collection of large-scale, high-quality datasets of URL links of
up to 650,000 video clips that cover 400/600/700 human action classes,
depending on the dataset version. The videos include human-object interactions
such as playing instruments, as well as human-human interactions such as
shaking hands and hugging. Each action class has at least 400/600/700 video
clips. Each clip is human annotated with a single action class and lasts around
10 seconds.</p>
<p>This dataset contains videos and action classifications for the 700 class
version of the dataset.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>kinetics-700</code></p>
</li>
<li>
<p>Dataset source: <a href="https://deepmind.com/research/open-source/kinetics">https://deepmind.com/research/open-source/kinetics</a></p>
</li>
<li>
<p>Dataset size: 710 GB</p>
</li>
<li>
<p>Tags: <code>video, classification, action-recognition</code></p>
</li>
<li>
<p>Supported splits: <code>train, test, validation</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.Kinetics700Dataset" title="fiftyone.zoo.datasets.base.Kinetics700Dataset"><code>Kinetics700Dataset</code></a></p>
</li>
</ul>
<p>Split stats:</p>
<ul>
<li>
<p>Train split: 529,046 videos</p>
</li>
<li>
<p>Test split: 67,446 videos</p>
</li>
<li>
<p>Validation split: 33,925 videos</p>
</li>
</ul>
<p>Dataset size</p>
<ul>
<li>
<p>Train split: 603 GB</p>
</li>
<li>
<p>Test split: 59 GB</p>
</li>
<li>
<p>Validation split: 48 GB</p>
</li>
</ul>
<p><strong>Partial downloads</strong></p>
<p>Kinetics is a massive dataset, so FiftyOne provides parameters that can be used
to efficiently download specific subsets of the dataset to suit your needs.
When new subsets are specified, FiftyOne will use existing downloaded data
first if possible before resorting to downloading additional data from the web.</p>
<p>Kinetics videos were originally only accessible from YouTube. Over time, some
videos have become unavailable so the
<a href="https://github.com/cvdfoundation">CVDF</a> have hosted the Kinetics dataset on
AWS.</p>
<p>If you are partially downloading the dataset through FiftyOne, the specific
videos of interest will be downloaded from YouTube, if necessary. However,
when you load an entire split, the CVDF-provided files will be downloaded from
AWS.</p>
<p>The following parameters are available to configure a partial download of
Kinetics by passing them to
<a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>:</p>
<ul>
<li>
<p><strong>split</strong> ( <em>None</em>) and <strong>splits</strong> ( <em>None</em>): a string or list of strings,
respectively, specifying the splits to load. Supported values are
<code>("train", "test", "validation")</code>. If neither is provided, all available
splits are loaded</p>
</li>
<li>
<p><strong>classes</strong> ( <em>None</em>): a string or list of strings specifying required
classes to load. If provided, only samples containing at least one instance
of a specified class will be loaded</p>
</li>
<li>
<p><strong>num_workers</strong> ( <em>None</em>): the number of processes to use when downloading
individual videos. By default, <code>multiprocessing.cpu_count()</code> is used</p>
</li>
<li>
<p><strong>shuffle</strong> ( <em>False</em>): whether to randomly shuffle the order in which
samples are chosen for partial downloads</p>
</li>
<li>
<p><strong>seed</strong> ( <em>None</em>): a random seed to use when shuffling</p>
</li>
<li>
<p><strong>max_samples</strong> ( <em>None</em>): a maximum number of samples to load per split. If
<code>classes</code> are also specified, only up to the number of samples that
contain at least one specified class will be loaded. By default, all
matching samples are loaded</p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p>Note</p>
<p>In order to work with video datasets, you’ll need to have
<a href="../../../getting_started/basic/troubleshooting/#videos-do-not-load-in-the-app">ffmpeg installed</a>.</p>
<p><img alt="kinetics" src="../../../_images/kinetics.png" /></p>
<h2 id="kinetics-700-2020">Kinetics 700-2020 <a href="#kinetics-700-2020" title="Permalink to this headline">¶</a><a class="headerlink" href="#kinetics-700-2020" title="Permanent link">&para;</a></h2>
<p>Kinetics is a collection of large-scale, high-quality datasets of URL links of
up to 650,000 video clips that cover 400/600/700 human action classes,
depending on the dataset version. The videos include human-object interactions
such as playing instruments, as well as human-human interactions such as
shaking hands and hugging. Each action class has at least 400/600/700 video
clips. Each clip is human annotated with a single action class and lasts around
10 seconds.</p>
<p>This version contains videos and action classifications for the 700 class
version of the dataset that was updated with new videos in 2020. This dataset
is a superset of Kinetics 700.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>kinetics-700-2020</code></p>
</li>
<li>
<p>Dataset source: <a href="https://deepmind.com/research/open-source/kinetics">https://deepmind.com/research/open-source/kinetics</a></p>
</li>
<li>
<p>Dataset size: 710 GB</p>
</li>
<li>
<p>Tags: <code>video, classification, action-recognition</code></p>
</li>
<li>
<p>Supported splits: <code>train, test, validation</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.Kinetics7002020Dataset" title="fiftyone.zoo.datasets.base.Kinetics7002020Dataset"><code>Kinetics7002020Dataset</code></a></p>
</li>
</ul>
<p>Original split stats:</p>
<ul>
<li>
<p>Train split: 542,352 videos</p>
</li>
<li>
<p>Test split: 67,433 videos</p>
</li>
<li>
<p>Validation split: 34,125 videos</p>
</li>
</ul>
<p>CVDF split stats:</p>
<ul>
<li>
<p>Train split: 534,073 videos</p>
</li>
<li>
<p>Test split: 64,260 videos</p>
</li>
<li>
<p>Validation split: 33,914 videos</p>
</li>
</ul>
<p>Dataset size</p>
<ul>
<li>
<p>Train split: 603 GB</p>
</li>
<li>
<p>Test split: 59 GB</p>
</li>
<li>
<p>Validation split: 48 GB</p>
</li>
</ul>
<p><strong>Partial downloads</strong></p>
<p>Kinetics is a massive dataset, so FiftyOne provides parameters that can be used
to efficiently download specific subsets of the dataset to suit your needs.
When new subsets are specified, FiftyOne will use existing downloaded data
first if possible before resorting to downloading additional data from the web.</p>
<p>Kinetics videos were originally only accessible from YouTube. Over time, some
videos have become unavailable so the
<a href="https://github.com/cvdfoundation">CVDF</a> have hosted the Kinetics dataset on
AWS.</p>
<p>If you are partially downloading the dataset through FiftyOne, the specific
videos of interest will be downloaded from YouTube, if necessary. However,
when you load an entire split, the CVDF-provided files will be downloaded from
AWS.</p>
<p>The following parameters are available to configure a partial download of
Kinetics by passing them to
<a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>:</p>
<ul>
<li>
<p><strong>split</strong> ( <em>None</em>) and <strong>splits</strong> ( <em>None</em>): a string or list of strings,
respectively, specifying the splits to load. Supported values are
<code>("train", "test", "validation")</code>. If neither is provided, all available
splits are loaded</p>
</li>
<li>
<p><strong>classes</strong> ( <em>None</em>): a string or list of strings specifying required
classes to load. If provided, only samples containing at least one instance
of a specified class will be loaded</p>
</li>
<li>
<p><strong>num_workers</strong> ( <em>None</em>): the number of processes to use when downloading
individual videos. By default, <code>multiprocessing.cpu_count()</code> is used</p>
</li>
<li>
<p><strong>shuffle</strong> ( <em>False</em>): whether to randomly shuffle the order in which
samples are chosen for partial downloads</p>
</li>
<li>
<p><strong>seed</strong> ( <em>None</em>): a random seed to use when shuffling</p>
</li>
<li>
<p><strong>max_samples</strong> ( <em>None</em>): a maximum number of samples to load per split. If
<code>classes</code> are also specified, only up to the number of samples that
contain at least one specified class will be loaded. By default, all
matching samples are loaded</p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p>Note</p>
<p>In order to work with video datasets, you’ll need to have
<a href="../../../getting_started/basic/troubleshooting/#videos-do-not-load-in-the-app">ffmpeg installed</a>.</p>
<p><img alt="kinetics" src="../../../_images/kinetics.png" /></p>
<h2 id="kitti">KITTI <a href="#kitti" title="Permalink to this headline">¶</a><a class="headerlink" href="#kitti" title="Permanent link">&para;</a></h2>
<p>KITTI contains a suite of vision tasks built using an autonomous
driving platform.</p>
<p>This dataset contains the left camera images and the associated 2D object
detections.</p>
<p>The training split contains 7,481 annotated images, and the test split contains
7,518 unlabeled images.</p>
<p>A full description of the annotations can be found in the README of the
object development kit on the KITTI homepage.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>kitti</code></p>
</li>
<li>
<p>Dataset source: <a href="http://www.cvlibs.net/datasets/kitti">http://www.cvlibs.net/datasets/kitti</a></p>
</li>
<li>
<p>Dataset size: 12.57 GB</p>
</li>
<li>
<p>Tags: <code>image, detection</code></p>
</li>
<li>
<p>Supported splits: <code>train, test</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.KITTIDataset" title="fiftyone.zoo.datasets.base.KITTIDataset"><code>KITTIDataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p><img alt="kitti-train" src="../../../_images/kitti-train.png" /></p>
<h2 id="kitti-multiview">KITTI Multiview <a href="#kitti-multiview" title="Permalink to this headline">¶</a><a class="headerlink" href="#kitti-multiview" title="Permanent link">&para;</a></h2>
<p>KITTI contains a suite of vision tasks built using an autonomous
driving platform.</p>
<p>This dataset contains the following multiview data for each scene:</p>
<ul>
<li>
<p>Left camera images annotated with 2D object detections</p>
</li>
<li>
<p>Right camera images annotated with 2D object detections</p>
</li>
<li>
<p>Velodyne LIDAR point clouds annotated with 3D object detections</p>
</li>
</ul>
<p>The training split contains 7,481 annotated scenes, and the test split contains
7,518 unlabeled scenes.</p>
<p>A full description of the annotations can be found in the README of the
object development kit on the KITTI homepage.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>kitti-multiview</code></p>
</li>
<li>
<p>Dataset source: <a href="http://www.cvlibs.net/datasets/kitti">http://www.cvlibs.net/datasets/kitti</a></p>
</li>
<li>
<p>Dataset size: 53.34 GB</p>
</li>
<li>
<p>Tags: <code>image, point-cloud, detection</code></p>
</li>
<li>
<p>Supported splits: <code>train, test</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.KITTIMultiviewDataset" title="fiftyone.zoo.datasets.base.KITTIMultiviewDataset"><code>KITTIMultiviewDataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p><img alt="kitti-multiview-train" src="../../../_images/kitti-multiview-train.png" /></p>
<h2 id="labeled-faces-in-the-wild">Labeled Faces in the Wild <a href="#labeled-faces-in-the-wild" title="Permalink to this headline">¶</a><a class="headerlink" href="#labeled-faces-in-the-wild" title="Permanent link">&para;</a></h2>
<p>Labeled Faces in the Wild is a public benchmark for face verification,
also known as pair matching.</p>
<p>The dataset contains 13,233 images of 5,749 people’s faces collected from
the web. Each face has been labeled with the name of the person pictured.
1,680 of the people pictured have two or more distinct photos in the data
set. The only constraint on these faces is that they were detected by the
Viola-Jones face detector.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>lfw</code></p>
</li>
<li>
<p>Dataset source: <a href="http://vis-www.cs.umass.edu/lfw">http://vis-www.cs.umass.edu/lfw</a></p>
</li>
<li>
<p>Dataset size: 173.00 MB</p>
</li>
<li>
<p>Tags: <code>image, classification, facial-recognition</code></p>
</li>
<li>
<p>Supported splits: <code>test, train</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.LabeledFacesInTheWildDataset" title="fiftyone.zoo.datasets.base.LabeledFacesInTheWildDataset"><code>LabeledFacesInTheWildDataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p><img alt="lfw-test" src="../../../_images/lfw-test.png" /></p>
<h2 id="mnist">MNIST <a href="#mnist" title="Permalink to this headline">¶</a><a class="headerlink" href="#mnist" title="Permanent link">&para;</a></h2>
<p>The MNIST database of handwritten digits.</p>
<p>The dataset consists of 70,000 28 x 28 grayscale images in 10 classes.
There are 60,000 training images and 10,000 test images.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>mnist</code></p>
</li>
<li>
<p>Dataset source: <a href="http://yann.lecun.com/exdb/mnist">http://yann.lecun.com/exdb/mnist</a></p>
</li>
<li>
<p>Dataset size: 21.00 MB</p>
</li>
<li>
<p>Tags: <code>image, classification</code></p>
</li>
<li>
<p>Supported splits: <code>train, test</code></p>
</li>
<li>
<p>ZooDataset classes:</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.tf.html#fiftyone.zoo.datasets.tf.MNISTDataset" title="fiftyone.zoo.datasets.tf.MNISTDataset"><code>MNISTDataset</code></a> (TF backend)</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.torch.html#fiftyone.zoo.datasets.torch.MNISTDataset" title="fiftyone.zoo.datasets.torch.MNISTDataset"><code>MNISTDataset</code></a> (Torch backend)</p>
</li>
</ul>
<p>Note</p>
<p>You must have the
<a href="../api/#dataset-zoo-ml-backend">Torch or TensorFlow backend(s)</a> installed to
load this dataset.</p>
<p><strong>Example usage</strong></p>
<p><img alt="mnist-test" src="../../../_images/mnist-test.png" /></p>
<h2 id="open-images-v6">Open Images V6 <a href="#open-images-v6" title="Permalink to this headline">¶</a><a class="headerlink" href="#open-images-v6" title="Permanent link">&para;</a></h2>
<p>Open Images V6 is a dataset of ~9 million images, roughly 2 million of which
are annotated and available via this zoo dataset.</p>
<p>The dataset contains annotations for classification, detection, segmentation,
and visual relationship tasks for the 600 boxable classes.</p>
<p>Note</p>
<p>We’ve collaborated with the
<a href="https://storage.googleapis.com/openimages/web/download.html">Open Images Team at Google</a>
to make FiftyOne a recommended tool for downloading, visualizing, and
evaluating on the Open Images Dataset!</p>
<p>Check out <a href="../../../integrations/open_images/#open-images">this guide</a> for more details on using
FiftyOne to work with Open Images.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>open-images-v6</code></p>
</li>
<li>
<p>Dataset source: <a href="https://storage.googleapis.com/openimages/web/index.html">https://storage.googleapis.com/openimages/web/index.html</a></p>
</li>
<li>
<p>Dataset size: 561 GB</p>
</li>
<li>
<p>Tags: <code>image, detection, segmentation, classification</code></p>
</li>
<li>
<p>Supported splits: <code>train, test, validation</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.OpenImagesV6Dataset" title="fiftyone.zoo.datasets.base.OpenImagesV6Dataset"><code>OpenImagesV6Dataset</code></a></p>
</li>
</ul>
<p><strong>Notes</strong></p>
<ul>
<li>
<p>Not all images contain all types of labels</p>
</li>
<li>
<p>All images have been rescaled so that their largest side is at most
1024 pixels</p>
</li>
</ul>
<p><strong>Full split stats</strong></p>
<ul>
<li>
<p>Train split: 1,743,042 images (513 GB)</p>
</li>
<li>
<p>Test split: 125,436 images (36 GB)</p>
</li>
<li>
<p>Validation split: 41,620 images (12 GB)</p>
</li>
</ul>
<p><strong>Partial downloads</strong></p>
<p>Open Images is a massive dataset, so FiftyOne provides parameters that can be
used to efficiently download specific subsets of the dataset to suit your
needs. When new subsets are specified, FiftyOne will use existing downloaded
data first if possible before resorting to downloading additional data from the
web.</p>
<p>The following parameters are available to configure a partial download of Open
Images V6 by passing them to
<a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>:</p>
<ul>
<li>
<p><strong>split</strong> ( <em>None</em>) and <strong>splits</strong> ( <em>None</em>): a string or list of strings,
respectively, specifying the splits to load. Supported values are
<code>("train", "test", "validation")</code>. If neither is provided, all available
splits are loaded</p>
</li>
<li>
<p><strong>label_types</strong> ( <em>None</em>): a label type or list of label types to load.
Supported values are
<code>("detections", "classifications", "relationships", "segmentations")</code>.
By default, all labels types are loaded</p>
</li>
<li>
<p><strong>classes</strong> ( <em>None</em>): a string or list of strings specifying required
classes to load. If provided, only samples containing at least one instance
of a specified class will be loaded. You can use
<a href="../api/fiftyone.utils.openimages.html#fiftyone.utils.openimages.get_classes" title="fiftyone.utils.openimages.get_classes"><code>get_classes()</code></a> and
<a href="../api/fiftyone.utils.openimages.html#fiftyone.utils.openimages.get_segmentation_classes" title="fiftyone.utils.openimages.get_segmentation_classes"><code>get_segmentation_classes()</code></a>
to see the available classes and segmentation classes, respectively</p>
</li>
<li>
<p><strong>attrs</strong> ( <em>None</em>): a string or list of strings specifying required
relationship attributes to load. This parameter is only applicable if
<code>label_types</code> contains <code>"relationships"</code>. If provided, only samples
containing at least one instance of a specified attribute will be loaded.
You can use
<a href="../api/fiftyone.utils.openimages.html#fiftyone.utils.openimages.get_attributes" title="fiftyone.utils.openimages.get_attributes"><code>get_attributes()</code></a>
to see the available attributes</p>
</li>
<li>
<p><strong>image_ids</strong> ( <em>None</em>): a list of specific image IDs to load. The IDs can
be specified either as <code>&lt;split&gt;/&lt;image-id&gt;</code> or <code>&lt;image-id&gt;</code> strings.
Alternatively, you can provide the path to a TXT (newline-separated), JSON,
or CSV file containing the list of image IDs to load in either of the first
two formats</p>
</li>
<li>
<p><strong>include_id</strong> ( <em>True</em>): whether to include the Open Images ID of each
sample in the loaded labels</p>
</li>
<li>
<p><strong>only_matching</strong> ( <em>False</em>): whether to only load labels that match the
<code>classes</code> or <code>attrs</code> requirements that you provide (True), or to load
all labels for samples that match the requirements (False)</p>
</li>
<li>
<p><strong>num_workers</strong> ( <em>None</em>): the number of processes to use when downloading
individual images. By default, <code>multiprocessing.cpu_count()</code> is used</p>
</li>
<li>
<p><strong>shuffle</strong> ( <em>False</em>): whether to randomly shuffle the order in which
samples are chosen for partial downloads</p>
</li>
<li>
<p><strong>seed</strong> ( <em>None</em>): a random seed to use when shuffling</p>
</li>
<li>
<p><strong>max_samples</strong> ( <em>None</em>): a maximum number of samples to load per split. If
<code>label_types</code>, <code>classes</code>, and/or <code>attrs</code> are also specified, first
priority will be given to samples that contain all of the specified label
types, classes, and/or attributes, followed by samples that contain at
least one of the specified labels types or classes. The actual number of
samples loaded may be less than this maximum value if the dataset does not
contain sufficient samples matching your requirements</p>
</li>
</ul>
<p>Note</p>
<p>See
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.OpenImagesV6Dataset" title="fiftyone.zoo.datasets.base.OpenImagesV6Dataset"><code>OpenImagesV6Dataset</code></a>
and <a href="../api/fiftyone.utils.openimages.html#fiftyone.utils.openimages.OpenImagesV6DatasetImporter" title="fiftyone.utils.openimages.OpenImagesV6DatasetImporter"><code>OpenImagesV6DatasetImporter</code></a>
for complete descriptions of the optional keyword arguments that you can
pass to <a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>.</p>
<p><strong>Example usage</strong></p>
<p><img alt="open-images-v6" src="../../../_images/open-images-v6.png" /></p>
<h2 id="open-images-v7">Open Images V7 <a href="#open-images-v7" title="Permalink to this headline">¶</a><a class="headerlink" href="#open-images-v7" title="Permanent link">&para;</a></h2>
<p>Open Images V7 is a dataset of ~9 million images, roughly 2 million of which
are annotated and available via this zoo dataset.</p>
<p>The dataset contains annotations for classification, detection, segmentation,
keypoints, and visual relationship tasks for the 600 boxable classes.</p>
<p>Note</p>
<p>We’ve collaborated with the
<a href="https://storage.googleapis.com/openimages/web/download.html">Open Images Team at Google</a>
to make FiftyOne a recommended tool for downloading, visualizing, and
evaluating on the Open Images Dataset!</p>
<p>Check out <a href="../../../integrations/open_images/#open-images">this guide</a> for more details on using
FiftyOne to work with Open Images.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>open-images-v7</code></p>
</li>
<li>
<p>Dataset source: <a href="https://storage.googleapis.com/openimages/web/index.html">https://storage.googleapis.com/openimages/web/index.html</a></p>
</li>
<li>
<p>Dataset size: 561 GB</p>
</li>
<li>
<p>Tags: <code>image, detection, segmentation, classification, keypoint</code></p>
</li>
<li>
<p>Supported splits: <code>train, test, validation</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.OpenImagesV7Dataset" title="fiftyone.zoo.datasets.base.OpenImagesV7Dataset"><code>OpenImagesV7Dataset</code></a></p>
</li>
</ul>
<p><strong>Notes</strong></p>
<ul>
<li>
<p>Not all images contain all types of labels</p>
</li>
<li>
<p>All images have been rescaled so that their largest side is at most
1024 pixels</p>
</li>
</ul>
<p><strong>Full split stats</strong></p>
<ul>
<li>
<p>Train split: 1,743,042 images (513 GB)</p>
</li>
<li>
<p>Test split: 125,436 images (36 GB)</p>
</li>
<li>
<p>Validation split: 41,620 images (12 GB)</p>
</li>
</ul>
<p><strong>Partial downloads</strong></p>
<p>Open Images is a massive dataset, so FiftyOne provides parameters that can be
used to efficiently download specific subsets of the dataset to suit your
needs. When new subsets are specified, FiftyOne will use existing downloaded
data first if possible before resorting to downloading additional data from the
web.</p>
<p>The following parameters are available to configure a partial download of Open
Images V7 by passing them to
<a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>:</p>
<ul>
<li>
<p><strong>split</strong> ( <em>None</em>) and <strong>splits</strong> ( <em>None</em>): a string or list of strings,
respectively, specifying the splits to load. Supported values are
<code>("train", "test", "validation")</code>. If neither is provided, all available
splits are loaded</p>
</li>
<li>
<p><strong>label_types</strong> ( <em>None</em>): a label type or list of label types to load.
Supported values are
<code>("detections", "classifications", "relationships", "points", segmentations")</code>.
By default, all labels types are loaded</p>
</li>
<li>
<p><strong>classes</strong> ( <em>None</em>): a string or list of strings specifying required
classes to load. If provided, only samples containing at least one instance
of a specified class will be loaded. You can use
<a href="../api/fiftyone.utils.openimages.html#fiftyone.utils.openimages.get_classes" title="fiftyone.utils.openimages.get_classes"><code>get_classes()</code></a> and
<a href="../api/fiftyone.utils.openimages.html#fiftyone.utils.openimages.get_segmentation_classes" title="fiftyone.utils.openimages.get_segmentation_classes"><code>get_segmentation_classes()</code></a>
to see the available classes and segmentation classes, respectively</p>
</li>
<li>
<p><strong>attrs</strong> ( <em>None</em>): a string or list of strings specifying required
relationship attributes to load. This parameter is only applicable if
<code>label_types</code> contains <code>"relationships"</code>. If provided, only samples
containing at least one instance of a specified attribute will be loaded.
You can use
<a href="../api/fiftyone.utils.openimages.html#fiftyone.utils.openimages.get_attributes" title="fiftyone.utils.openimages.get_attributes"><code>get_attributes()</code></a>
to see the available attributes</p>
</li>
<li>
<p><strong>image_ids</strong> ( <em>None</em>): a list of specific image IDs to load. The IDs can
be specified either as <code>&lt;split&gt;/&lt;image-id&gt;</code> or <code>&lt;image-id&gt;</code> strings.
Alternatively, you can provide the path to a TXT (newline-separated), JSON,
or CSV file containing the list of image IDs to load in either of the first
two formats</p>
</li>
<li>
<p><strong>include_id</strong> ( <em>True</em>): whether to include the Open Images ID of each
sample in the loaded labels</p>
</li>
<li>
<p><strong>only_matching</strong> ( <em>False</em>): whether to only load labels that match the
<code>classes</code> or <code>attrs</code> requirements that you provide (True), or to load
all labels for samples that match the requirements (False)</p>
</li>
<li>
<p><strong>num_workers</strong> ( <em>None</em>): the number of processes to use when downloading
individual images. By default, <code>multiprocessing.cpu_count()</code> is used</p>
</li>
<li>
<p><strong>shuffle</strong> ( <em>False</em>): whether to randomly shuffle the order in which
samples are chosen for partial downloads</p>
</li>
<li>
<p><strong>seed</strong> ( <em>None</em>): a random seed to use when shuffling</p>
</li>
<li>
<p><strong>max_samples</strong> ( <em>None</em>): a maximum number of samples to load per split. If
<code>label_types</code>, <code>classes</code>, and/or <code>attrs</code> are also specified, first
priority will be given to samples that contain all of the specified label
types, classes, and/or attributes, followed by samples that contain at
least one of the specified labels types or classes. The actual number of
samples loaded may be less than this maximum value if the dataset does not
contain sufficient samples matching your requirements</p>
</li>
</ul>
<p>Note</p>
<p>See
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.OpenImagesV7Dataset" title="fiftyone.zoo.datasets.base.OpenImagesV7Dataset"><code>OpenImagesV7Dataset</code></a>
and <a href="../api/fiftyone.utils.openimages.html#fiftyone.utils.openimages.OpenImagesV7DatasetImporter" title="fiftyone.utils.openimages.OpenImagesV7DatasetImporter"><code>OpenImagesV7DatasetImporter</code></a>
for complete descriptions of the optional keyword arguments that you can
pass to <a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>.</p>
<p><strong>Example usage</strong></p>
<p><img alt="open-images-v7" src="../../../_images/open-images-v7.png" /></p>
<h2 id="places">Places <a href="#places" title="Permalink to this headline">¶</a><a class="headerlink" href="#places" title="Permanent link">&para;</a></h2>
<p>Places is a scene recognition dataset of 10 million images comprising ~400
unique scene categories.</p>
<p>The images are labeled with scene semantic categories, comprising a large
and diverse list of the types of environments encountered in the world.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>places</code></p>
</li>
<li>
<p>Dataset source: <a href="http://places2.csail.mit.edu/download-private.html">http://places2.csail.mit.edu/download-private.html</a></p>
</li>
<li>
<p>Dataset size: 29 GB</p>
</li>
<li>
<p>Tags: <code>image, classification</code></p>
</li>
<li>
<p>Supported splits: <code>train, validation, test</code></p>
</li>
<li>
<p>ZooDataset classes:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.PlacesDataset" title="fiftyone.zoo.datasets.base.PlacesDataset"><code>PlacesDataset</code></a></p>
</li>
</ul>
<p><strong>Full split stats</strong></p>
<ul>
<li>
<p>Train split: 1,803,460 images, with between 3,068 and 5,000 per category</p>
</li>
<li>
<p>Test split: 328,500 images, with 900 images per category</p>
</li>
<li>
<p>Validation split: 36,500 images, with 100 images per category</p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p><img alt="places-validation" src="../../../_images/places-validation.png" /></p>
<h2 id="quickstart">Quickstart <a href="#quickstart" title="Permalink to this headline">¶</a><a class="headerlink" href="#quickstart" title="Permanent link">&para;</a></h2>
<p>A small dataset with ground truth bounding boxes and predictions.</p>
<p>The dataset consists of 200 images from the validation split of COCO-2017,
with model predictions generated by an out-of-the-box Faster R-CNN model
from
<a href="https://pytorch.org/docs/stable/torchvision/models.html">torchvision.models</a>.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>quickstart</code></p>
</li>
<li>
<p>Dataset size: 23.40 MB</p>
</li>
<li>
<p>Tags: <code>image, quickstart</code></p>
</li>
<li>
<p>Supported splits: <code>N/A</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.QuickstartDataset" title="fiftyone.zoo.datasets.base.QuickstartDataset"><code>QuickstartDataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p><img alt="quickstart" src="../../../_images/quickstart.png" /></p>
<h2 id="quickstart-geo">Quickstart Geo <a href="#quickstart-geo" title="Permalink to this headline">¶</a><a class="headerlink" href="#quickstart-geo" title="Permanent link">&para;</a></h2>
<p>A small dataset with geolocation data.</p>
<p>The dataset consists of 500 images from the validation split of the BDD100K
dataset in the New York City area with object detections and GPS timestamps.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>quickstart-geo</code></p>
</li>
<li>
<p>Dataset size: 33.50 MB</p>
</li>
<li>
<p>Tags: <code>image, location, quickstart</code></p>
</li>
<li>
<p>Supported splits: <code>N/A</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.QuickstartGeoDataset" title="fiftyone.zoo.datasets.base.QuickstartGeoDataset"><code>QuickstartGeoDataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p><img alt="quickstart-geo" src="../../../_images/quickstart-geo.png" /></p>
<h2 id="quickstart-video">Quickstart Video <a href="#quickstart-video" title="Permalink to this headline">¶</a><a class="headerlink" href="#quickstart-video" title="Permanent link">&para;</a></h2>
<p>A small video dataset with dense annotations.</p>
<p>The dataset consists of 10 video segments with dense object detections
generated by human annotators.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>quickstart-video</code></p>
</li>
<li>
<p>Dataset size: 35.20 MB</p>
</li>
<li>
<p>Tags: <code>video, quickstart</code></p>
</li>
<li>
<p>Supported splits: <code>N/A</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.QuickstartVideoDataset" title="fiftyone.zoo.datasets.base.QuickstartVideoDataset"><code>QuickstartVideoDataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p>Note</p>
<p>In order to work with video datasets, you’ll need to have
<a href="../../../getting_started/basic/troubleshooting/#videos-do-not-load-in-the-app">ffmpeg installed</a>.</p>
<p><img alt="quickstart-video" src="../../../_images/quickstart-video.png" /></p>
<h2 id="quickstart-groups">Quickstart Groups <a href="#quickstart-groups" title="Permalink to this headline">¶</a><a class="headerlink" href="#quickstart-groups" title="Permanent link">&para;</a></h2>
<p>A small dataset with grouped image and point cloud data.</p>
<p>The dataset consists of 200 scenes from the train split of the KITTI dataset,
each containing left camera, right camera, point cloud, and 2D/3D object
annotation data.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>quickstart-groups</code></p>
</li>
<li>
<p>Dataset size: 516.3 MB</p>
</li>
<li>
<p>Tags: <code>image, point-cloud, quickstart</code></p>
</li>
<li>
<p>Supported splits: <code>N/A</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.QuickstartGroupsDataset" title="fiftyone.zoo.datasets.base.QuickstartGroupsDataset"><code>QuickstartGroupsDataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p><img alt="quickstart-groups" src="../../../_images/quickstart-groups.png" /></p>
<h2 id="quickstart-3d">Quickstart 3D <a href="#quickstart-3d" title="Permalink to this headline">¶</a><a class="headerlink" href="#quickstart-3d" title="Permanent link">&para;</a></h2>
<p>A small 3D dataset with meshes, point clouds, and oriented bounding boxes.</p>
<p>The dataset consists of 200 3D mesh samples from the test split of the
<a href="https://modelnet.cs.princeton.edu">ModelNet40</a> dataset, with point
clouds generated using a Poisson disk sampling method, and oriented
bounding boxes generated based on the convex hull.</p>
<p>Objects have been rescaled and recentered from the original dataset.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>quickstart-3d</code></p>
</li>
<li>
<p>Dataset size: 215.7 MB</p>
</li>
<li>
<p>Tags: <code>3d, point-cloud, mesh, quickstart</code></p>
</li>
<li>
<p>Supported splits: <code>N/A</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.Quickstart3DDataset" title="fiftyone.zoo.datasets.base.Quickstart3DDataset"><code>Quickstart3DDataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p><img alt="quickstart-3d" src="../../../_images/quickstart-3d.png" /></p>
<h2 id="sama-coco">Sama-COCO <a href="#sama-coco" title="Permalink to this headline">¶</a><a class="headerlink" href="#sama-coco" title="Permanent link">&para;</a></h2>
<p>Sama-COCO is a relabeling of COCO-2017 and is a large-scale object detection
and segmentation dataset. Masks in Sama-COCO are tighter and many crowd
instances have been decomposed into their components.</p>
<p>This version contains images from the COCO-2017 version of the dataset, as well
as annotations in the form of bounding boxes, and segmentation masks provided
by Sama.</p>
<p><strong>Notes</strong></p>
<ul>
<li>
<p>Sama-COCO defines 91 classes but the data only uses 80 classes (like COCO-2017)</p>
</li>
<li>
<p>Some images from the train and validation sets don’t have annotations</p>
</li>
<li>
<p>The test set does not have annotations</p>
</li>
<li>
<p>Sama-COCO has identical splits to COCO-2017</p>
</li>
</ul>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>sama-coco</code></p>
</li>
<li>
<p>Dataset source: <a href="https://www.sama.com/sama-coco-dataset/">https://www.sama.com/sama-coco-dataset/</a></p>
</li>
<li>
<p>Dataset size: 25.67 GB</p>
</li>
<li>
<p>Tags: <code>image, detection, segmentation</code></p>
</li>
<li>
<p>Supported splits: <code>train, validation, test</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.SamaCOCODataset" title="fiftyone.zoo.datasets.base.SamaCOCODataset"><code>SamaCOCODataset</code></a></p>
</li>
</ul>
<p><strong>Full split stats</strong></p>
<ul>
<li>
<p>Train split: 118,287 images</p>
</li>
<li>
<p>Test split: 40,670 images</p>
</li>
<li>
<p>Validation split: 5,000 images</p>
</li>
</ul>
<p><strong>Partial downloads</strong></p>
<p>FiftyOne provides parameters that can be used to efficiently download specific
subsets of the Sama-COCO dataset to suit your needs. When new subsets are
specified, FiftyOne will use existing downloaded data first if possible before
resorting to downloading additional data from the web.</p>
<p>The following parameters are available to configure a partial download of
Sama-COCO by passing them to
<a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>:</p>
<ul>
<li>
<p><strong>split</strong> ( <em>None</em>) and <strong>splits</strong> ( <em>None</em>): a string or list of strings,
respectively, specifying the splits to load. Supported values are
<code>("train", "test", "validation")</code>. If neither is provided, all available
splits are loaded</p>
</li>
<li>
<p><strong>label_types</strong> ( <em>None</em>): a label type or list of label types to load.
Supported values are <code>("detections", "segmentations")</code>. By default, only
detections are loaded</p>
</li>
<li>
<p><strong>classes</strong> ( <em>None</em>): a string or list of strings specifying required
classes to load. If provided, only samples containing at least one instance
of a specified class will be loaded</p>
</li>
<li>
<p><strong>image_ids</strong> ( <em>None</em>): a list of specific image IDs to load. The IDs can
be specified either as <code>&lt;split&gt;/&lt;image-id&gt;</code> strings or <code>&lt;image-id&gt;</code>
ints of strings. Alternatively, you can provide the path to a TXT
(newline-separated), JSON, or CSV file containing the list of image IDs to
load in either of the first two formats</p>
</li>
<li>
<p><strong>include_id</strong> ( <em>False</em>): whether to include the COCO ID of each sample in
the loaded labels</p>
</li>
<li>
<p><strong>include_license</strong> ( <em>False</em>): whether to include the COCO license of each
sample in the loaded labels, if available. The supported values are:</p>
</li>
<li>
<p><code>"False"</code> (default): don’t load the license</p>
</li>
<li>
<p><code>True</code>/ <code>"name"</code>: store the string license name</p>
</li>
<li>
<p><code>"id"</code>: store the integer license ID</p>
</li>
<li>
<p><code>"url"</code>: store the license URL</p>
</li>
<li>
<p><strong>only_matching</strong> ( <em>False</em>): whether to only load labels that match the
<code>classes</code> or <code>attrs</code> requirements that you provide (True), or to load
all labels for samples that match the requirements (False)</p>
</li>
<li>
<p><strong>num_workers</strong> ( <em>None</em>): the number of processes to use when downloading
individual images. By default, <code>multiprocessing.cpu_count()</code> is used</p>
</li>
<li>
<p><strong>shuffle</strong> ( <em>False</em>): whether to randomly shuffle the order in which
samples are chosen for partial downloads</p>
</li>
<li>
<p><strong>seed</strong> ( <em>None</em>): a random seed to use when shuffling</p>
</li>
<li>
<p><strong>max_samples</strong> ( <em>None</em>): a maximum number of samples to load per split. If
<code>label_types</code> and/or <code>classes</code> are also specified, first priority will
be given to samples that contain all of the specified label types and/or
classes, followed by samples that contain at least one of the specified
labels types or classes. The actual number of samples loaded may be less
than this maximum value if the dataset does not contain sufficient samples
matching your requirements</p>
</li>
</ul>
<p>Note</p>
<p>See
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.SamaCOCODataset" title="fiftyone.zoo.datasets.base.SamaCOCODataset"><code>SamaCOCODataset</code></a> and
<a href="../api/fiftyone.utils.coco.html#fiftyone.utils.coco.COCODetectionDatasetImporter" title="fiftyone.utils.coco.COCODetectionDatasetImporter"><code>COCODetectionDatasetImporter</code></a>
for complete descriptions of the optional keyword arguments that you can
pass to <a href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code>load_zoo_dataset()</code></a>.</p>
<p><strong>Example usage</strong></p>
<p><img alt="sama-coco-validation" src="../../../_images/sama-coco-validation.png" /></p>
<h2 id="ucf101">UCF101 <a href="#ucf101" title="Permalink to this headline">¶</a><a class="headerlink" href="#ucf101" title="Permanent link">&para;</a></h2>
<p>UCF101 is an action recognition data set of realistic action videos,
collected from YouTube, having 101 action categories. This data set is an
extension of UCF50 data set which has 50 action categories.</p>
<p>With 13,320 videos from 101 action categories, UCF101 gives the largest
diversity in terms of actions and with the presence of large variations in
camera motion, object appearance and pose, object scale, viewpoint,
cluttered background, illumination conditions, etc, it is the most
challenging data set to date. As most of the available action recognition
data sets are not realistic and are staged by actors, UCF101 aims to
encourage further research into action recognition by learning and
exploring new realistic action categories.</p>
<p>The videos in 101 action categories are grouped into 25 groups, where each
group can consist of 4-7 videos of an action. The videos from the same
group may share some common features, such as similar background, similar
viewpoint, etc.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>ucf101</code></p>
</li>
<li>
<p>Dataset source: <a href="https://www.crcv.ucf.edu/research/data-sets/ucf101">https://www.crcv.ucf.edu/research/data-sets/ucf101</a></p>
</li>
<li>
<p>Dataset size: 6.48 GB</p>
</li>
<li>
<p>Tags: <code>video, action-recognition</code></p>
</li>
<li>
<p>Supported splits: <code>train, test</code></p>
</li>
<li>
<p>ZooDataset class:
<a href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.UCF101Dataset" title="fiftyone.zoo.datasets.base.UCF101Dataset"><code>UCF101Dataset</code></a></p>
</li>
</ul>
<p><strong>Example usage</strong></p>
<p>Note</p>
<p>In order to work with video datasets, you’ll need to have
<a href="../../../getting_started/basic/troubleshooting/#videos-do-not-load-in-the-app">ffmpeg installed</a>.</p>
<p>Also, if you don’t already have a utility to uncompress <code>.rar</code> archives,
you may need to install one. For example, on macOS:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><span class="n">brew</span> <span class="n">install</span> <span class="n">rar</span>
</span></code></pre></div>
<p><img alt="ucf101-test" src="../../../_images/ucf101-test.png" /></p>
<h2 id="voc-2007">VOC-2007 <a href="#voc-2007" title="Permalink to this headline">¶</a><a class="headerlink" href="#voc-2007" title="Permanent link">&para;</a></h2>
<p>The dataset for the PASCAL Visual Object Classes Challenge 2007
(VOC2007) for the detection competition.</p>
<p>A total of 9,963 images are included in this dataset, where each image
contains a set of objects, out of 20 different classes, making a total of
24,640 annotated objects.</p>
<p>Note that, as per the official dataset, the test set of VOC2007 does not
contain annotations.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>voc-2007</code></p>
</li>
<li>
<p>Dataset source: <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007">http://host.robots.ox.ac.uk/pascal/VOC/voc2007</a></p>
</li>
<li>
<p>Dataset size: 868.85 MB</p>
</li>
<li>
<p>Tags: <code>image, detection</code></p>
</li>
<li>
<p>Supported splits: <code>train, validation, test</code></p>
</li>
<li>
<p>ZooDataset classes:</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.tf.html#fiftyone.zoo.datasets.tf.VOC2007Dataset" title="fiftyone.zoo.datasets.tf.VOC2007Dataset"><code>VOC2007Dataset</code></a> (TF backend)</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.torch.html#fiftyone.zoo.datasets.torch.VOC2007Dataset" title="fiftyone.zoo.datasets.torch.VOC2007Dataset"><code>VOC2007Dataset</code></a> (Torch backend)</p>
</li>
</ul>
<p>Note</p>
<p>The <code>test</code> split is only available via the
<a href="../api/#dataset-zoo-ml-backend">TensorFlow backend</a>.</p>
<p>Note</p>
<p>You must have the
<a href="../api/#dataset-zoo-ml-backend">Torch or TensorFlow backend(s)</a> installed to
load this dataset.</p>
<p><strong>Example usage</strong></p>
<p><img alt="voc-2007-validation" src="../../../_images/voc-2007-validation.png" /></p>
<h2 id="voc-2012">VOC-2012 <a href="#voc-2012" title="Permalink to this headline">¶</a><a class="headerlink" href="#voc-2012" title="Permanent link">&para;</a></h2>
<p>The dataset for the PASCAL Visual Object Classes Challenge 2012
(VOC2012) for the detection competition.</p>
<p>A total of 11540 images are included in this dataset, where each image
contains a set of objects, out of 20 different classes, making a total of
27450 annotated objects.</p>
<p>Note that, as per the official dataset, the test set of VOC2012 does not
contain annotations.</p>
<p><strong>Details</strong></p>
<ul>
<li>
<p>Dataset name: <code>voc-2012</code></p>
</li>
<li>
<p>Dataset source: <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012">http://host.robots.ox.ac.uk/pascal/VOC/voc2012</a></p>
</li>
<li>
<p>Dataset size: 3.59 GB</p>
</li>
<li>
<p>Tags: <code>image, detection</code></p>
</li>
<li>
<p>Supported splits: <code>train, validation, test</code></p>
</li>
<li>
<p>ZooDataset classes:</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.tf.html#fiftyone.zoo.datasets.tf.VOC2012Dataset" title="fiftyone.zoo.datasets.tf.VOC2012Dataset"><code>VOC2012Dataset</code></a> (TF backend)</p>
</li>
<li>
<p><a href="../api/fiftyone.zoo.datasets.torch.html#fiftyone.zoo.datasets.torch.VOC2012Dataset" title="fiftyone.zoo.datasets.torch.VOC2012Dataset"><code>VOC2012Dataset</code></a> (Torch backend)</p>
</li>
</ul>
<p>Note</p>
<p>The <code>test</code> split is only available via the
<a href="../api/#dataset-zoo-ml-backend">TensorFlow backend</a>.</p>
<p>Note</p>
<p>You must have the
<a href="../api/#dataset-zoo-ml-backend">Torch or TensorFlow backend(s)</a> installed to
load this dataset.</p>
<p><strong>Example usage</strong></p>
<p><img alt="voc-2012-validation" src="../../../_images/voc-2012-validation.png" /></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.path", "navigation.indexes", "navigation.top", {"icon": {"repo": "fontawesome/brands/github"}}, "content.action.edit"], "search": "../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.88dd0f4e.min.js"></script>
      
    
  </body>
</html>